(venv) gitesh.grover@Giteshs-MacBook-Pro ai-era-assignment15 % python train.py
/Users/gitesh.grover/Study/AI-ERA/venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020
  warnings.warn(
Model: DeepSeekTransfomerModel(
  (embedding): Embedding(49152, 768)
  (layers): ModuleList(
    (0-29): 30 x DeepSeekBlock(
      (attn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)
      (attn): MultiHeadLatentAttention(
        (kv_proj_d): Linear(in_features=768, out_features=192, bias=True)
        (q_proj_d): Linear(in_features=768, out_features=192, bias=True)
        (k_proj_u): Linear(in_features=192, out_features=384, bias=True)
        (q_proj_u): Linear(in_features=192, out_features=384, bias=True)
        (v_proj_u): Linear(in_features=192, out_features=768, bias=True)
        (rope_k): Linear(in_features=768, out_features=384, bias=True)
        (rope_q): Linear(in_features=192, out_features=384, bias=True)
        (rotatry_embedding): LlamaDeepseekRotaryEmbedding()
        (o_proj): Linear(in_features=768, out_features=768, bias=True)
      )
      (ffn_norm): RMSNorm((768,), eps=None, elementwise_affine=True)
      (ffn): DeepSeekFFN(
        (shared_experts): ModuleList(
          (0): DeepSeekExpert(
            (gate): Linear(in_features=768, out_features=1536, bias=False)
            (up): Linear(in_features=768, out_features=1536, bias=False)
            (down): Linear(in_features=1536, out_features=768, bias=False)
            (act_fn): SiLU()
          )
        )
        (routed_experts): ModuleList(
          (0-6): 7 x DeepSeekExpert(
            (gate): Linear(in_features=768, out_features=1536, bias=False)
            (up): Linear(in_features=768, out_features=1536, bias=False)
            (down): Linear(in_features=1536, out_features=768, bias=False)
            (act_fn): SiLU()
          )
        )
        (routing): Linear(in_features=768, out_features=7, bias=True)
      )
    )
  )
  (norm): RMSNorm((768,), eps=None, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=49152, bias=True)
)
=========================================================================================================================================================================================
Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #                   Mult-Adds                 Param %
=========================================================================================================================================================================================
DeepSeekTransfomerModel                                      [8, 64]                   [8, 64, 49152]            --                        --                         -3.88%
├─Embedding: 1-1                                             [8, 64]                   [8, 64, 768]              37,748,736                301,989,888                 3.88%
├─ModuleList: 1-2                                            --                        --                        --                        --                             --
│    └─DeepSeekBlock: 2-1                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-1                                     [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-2                    [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-3                                     [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-4                                 [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-2                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-5                                     [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-6                    [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-7                                     [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-8                                 [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-3                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-9                                     [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-10                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-11                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-12                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-4                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-13                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-14                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-15                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-16                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-5                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-17                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-18                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-19                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-20                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-6                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-21                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-22                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-23                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-24                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-7                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-25                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-26                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-27                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-28                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-8                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-29                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-30                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-31                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-32                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-9                                    [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-33                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-34                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-35                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-36                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-10                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-37                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-38                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-39                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-40                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-11                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-41                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-42                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-43                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-44                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-12                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-45                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-46                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-47                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-48                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-13                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-49                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-50                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-51                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-52                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-14                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-53                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-54                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-55                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-56                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-15                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-57                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-58                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-59                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-60                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-16                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-61                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-62                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-63                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-64                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-17                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-65                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-66                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-67                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-68                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-18                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-69                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-70                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-71                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-72                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-19                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-73                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-74                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-75                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-76                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-20                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-77                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-78                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-79                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-80                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-21                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-81                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-82                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-83                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-84                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-22                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-85                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-86                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-87                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-88                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-23                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-89                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-90                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-91                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-92                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-24                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-93                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-94                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-95                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-96                                [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-25                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-97                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-98                   [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-99                                    [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-100                               [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-26                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-101                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-102                  [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-103                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-104                               [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-27                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-105                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-106                  [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-107                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-108                               [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-28                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-109                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-110                  [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-111                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-112                               [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-29                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-113                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-114                  [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-115                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-116                               [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
│    └─DeepSeekBlock: 2-30                                   [8, 64, 768]              [8, 64, 768]              --                        --                             --
│    │    └─RMSNorm: 3-117                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─MultiHeadLatentAttention: 3-118                  [8, 64, 768]              [8, 64, 768]              1,551,744                 12,413,952                  0.16%
│    │    └─RMSNorm: 3-119                                   [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
│    │    └─DeepSeekFFN: 3-120                               [8, 64, 768]              [8, 64, 768]              28,316,942                3,652,233,272               2.91%
├─RMSNorm: 1-3                                               [8, 64, 768]              [8, 64, 768]              768                       6,144                       0.00%
├─Linear: 1-4                                                [8, 64, 768]              [8, 64, 49152]            37,797,888                302,383,104                 3.89%
=========================================================================================================================================================================================
Total params: 971,654,052
Trainable params: 971,654,052
Non-trainable params: 0
Total mult-adds (G): 110.54
=========================================================================================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 2237.47
Params size (MB): 3886.62
Estimated Total Size (MB): 6124.09
=========================================================================================================================================================================================
Device: mps
Resolving data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:00<00:00, 303.46it/s]
Resolving data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 104/104 [00:00<00:00, 457720.48it/s]
gradient_accumalate_steps: 8
inputs: torch.Size([8, 64]), targets: torch.Size([8, 64])
input.device: mps:0, targets.device: mps:0
Updating MLP bias
Epoch: 0, Step: 0, Batch(micro): 0, Batch (considering grad accum): 0,  Loss: 11.4515, Time: 6.05s, Token/s: 84.64
Saved checkpoint at step 0
What is Gravity? '', � coated ################ positively agendasblemsmusgenericamazon sty nih EVBuilder Mandaringoalorce-----------------------------------------------clidistance FR�instonTexdishwallet dies![ouncil insol
Epoch: 0, Step: 1, Batch(micro): 1, Batch (considering grad accum): 0,  Loss: 11.4738, Time: 4.48s, Token/s: 114.25
Epoch: 0, Step: 2, Batch(micro): 2, Batch (considering grad accum): 0,  Loss: 11.4767, Time: 3.92s, Token/s: 130.73
Epoch: 0, Step: 3, Batch(micro): 3, Batch (considering grad accum): 0,  Loss: 11.4856, Time: 3.91s, Token/s: 131.07
Epoch: 0, Step: 4, Batch(micro): 4, Batch (considering grad accum): 0,  Loss: 11.4411, Time: 3.73s, Token/s: 137.23
Epoch: 0, Step: 5, Batch(micro): 5, Batch (considering grad accum): 0,  Loss: 11.4629, Time: 3.19s, Token/s: 160.55
Epoch: 0, Step: 6, Batch(micro): 6, Batch (considering grad accum): 0,  Loss: 11.4540, Time: 3.33s, Token/s: 153.77
Epoch: 0, Step: 7, Batch(micro): 7, Batch (considering grad accum): 0,  Loss: 11.4693, Time: 6.70s, Token/s: 76.40
Epoch: 0, Step: 8, Batch(micro): 8, Batch (considering grad accum): 1,  Loss: 11.4924, Time: 4.72s, Token/s: 108.54
Epoch: 0, Step: 9, Batch(micro): 9, Batch (considering grad accum): 1,  Loss: 11.4508, Time: 4.08s, Token/s: 125.44
Epoch: 0, Step: 10, Batch(micro): 10, Batch (considering grad accum): 1,  Loss: 11.3990, Time: 3.67s, Token/s: 139.45
Epoch: 0, Step: 11, Batch(micro): 11, Batch (considering grad accum): 1,  Loss: 11.3143, Time: 3.08s, Token/s: 166.00
Epoch: 0, Step: 12, Batch(micro): 12, Batch (considering grad accum): 1,  Loss: 11.4249, Time: 3.49s, Token/s: 146.90
Epoch: 0, Step: 13, Batch(micro): 13, Batch (considering grad accum): 1,  Loss: 11.4611, Time: 3.07s, Token/s: 166.91
Epoch: 0, Step: 14, Batch(micro): 14, Batch (considering grad accum): 1,  Loss: 11.5453, Time: 3.05s, Token/s: 167.84
Epoch: 0, Step: 15, Batch(micro): 15, Batch (considering grad accum): 1,  Loss: 11.4460, Time: 6.96s, Token/s: 73.61
Epoch: 0, Step: 16, Batch(micro): 16, Batch (considering grad accum): 2,  Loss: 11.4628, Time: 4.51s, Token/s: 113.53
Epoch: 0, Step: 17, Batch(micro): 17, Batch (considering grad accum): 2,  Loss: 11.4933, Time: 3.33s, Token/s: 153.95
Epoch: 0, Step: 18, Batch(micro): 18, Batch (considering grad accum): 2,  Loss: 11.4367, Time: 3.06s, Token/s: 167.24
Epoch: 0, Step: 19, Batch(micro): 19, Batch (considering grad accum): 2,  Loss: 11.3645, Time: 3.15s, Token/s: 162.77
Epoch: 0, Step: 20, Batch(micro): 20, Batch (considering grad accum): 2,  Loss: 11.4317, Time: 3.31s, Token/s: 154.76
Epoch: 0, Step: 21, Batch(micro): 21, Batch (considering grad accum): 2,  Loss: 11.4534, Time: 3.52s, Token/s: 145.35
Epoch: 0, Step: 22, Batch(micro): 22, Batch (considering grad accum): 2,  Loss: 11.4604, Time: 3.15s, Token/s: 162.60
Epoch: 0, Step: 23, Batch(micro): 23, Batch (considering grad accum): 2,  Loss: 11.4431, Time: 7.25s, Token/s: 70.64
Epoch: 0, Step: 24, Batch(micro): 24, Batch (considering grad accum): 3,  Loss: 11.4816, Time: 3.87s, Token/s: 132.16
Epoch: 0, Step: 25, Batch(micro): 25, Batch (considering grad accum): 3,  Loss: 11.4341, Time: 3.33s, Token/s: 153.54
Epoch: 0, Step: 26, Batch(micro): 26, Batch (considering grad accum): 3,  Loss: 11.5664, Time: 3.00s, Token/s: 170.43
Epoch: 0, Step: 27, Batch(micro): 27, Batch (considering grad accum): 3,  Loss: 11.4677, Time: 3.00s, Token/s: 170.70
Epoch: 0, Step: 28, Batch(micro): 28, Batch (considering grad accum): 3,  Loss: 11.5160, Time: 3.33s, Token/s: 153.67
Epoch: 0, Step: 29, Batch(micro): 29, Batch (considering grad accum): 3,  Loss: 11.3777, Time: 3.15s, Token/s: 162.55
Epoch: 0, Step: 30, Batch(micro): 30, Batch (considering grad accum): 3,  Loss: 11.4418, Time: 3.12s, Token/s: 164.26
Epoch: 0, Step: 31, Batch(micro): 31, Batch (considering grad accum): 3,  Loss: 11.4906, Time: 4.36s, Token/s: 117.49
Epoch: 0, Step: 32, Batch(micro): 32, Batch (considering grad accum): 4,  Loss: 11.5399, Time: 4.54s, Token/s: 112.81
Epoch: 0, Step: 33, Batch(micro): 33, Batch (considering grad accum): 4,  Loss: 11.3988, Time: 3.47s, Token/s: 147.35
Epoch: 0, Step: 34, Batch(micro): 34, Batch (considering grad accum): 4,  Loss: 11.5392, Time: 3.70s, Token/s: 138.48
Epoch: 0, Step: 35, Batch(micro): 35, Batch (considering grad accum): 4,  Loss: 11.4472, Time: 3.70s, Token/s: 138.44
Epoch: 0, Step: 36, Batch(micro): 36, Batch (considering grad accum): 4,  Loss: 11.4518, Time: 3.10s, Token/s: 165.07
Epoch: 0, Step: 37, Batch(micro): 37, Batch (considering grad accum): 4,  Loss: 11.5225, Time: 3.08s, Token/s: 166.21
Epoch: 0, Step: 38, Batch(micro): 38, Batch (considering grad accum): 4,  Loss: 11.4920, Time: 3.02s, Token/s: 169.35
Epoch: 0, Step: 39, Batch(micro): 39, Batch (considering grad accum): 4,  Loss: 11.4836, Time: 31.18s, Token/s: 16.42
Epoch: 0, Step: 40, Batch(micro): 40, Batch (considering grad accum): 5,  Loss: 11.3990, Time: 6.22s, Token/s: 82.34
Epoch: 0, Step: 41, Batch(micro): 41, Batch (considering grad accum): 5,  Loss: 11.4357, Time: 3.74s, Token/s: 136.94
Epoch: 0, Step: 42, Batch(micro): 42, Batch (considering grad accum): 5,  Loss: 11.4343, Time: 3.79s, Token/s: 135.23
Epoch: 0, Step: 43, Batch(micro): 43, Batch (considering grad accum): 5,  Loss: 11.4571, Time: 3.75s, Token/s: 136.49
Epoch: 0, Step: 44, Batch(micro): 44, Batch (considering grad accum): 5,  Loss: 11.4993, Time: 3.08s, Token/s: 166.25
Epoch: 0, Step: 45, Batch(micro): 45, Batch (considering grad accum): 5,  Loss: 11.3776, Time: 3.71s, Token/s: 138.17
Epoch: 0, Step: 46, Batch(micro): 46, Batch (considering grad accum): 5,  Loss: 11.4062, Time: 3.19s, Token/s: 160.41
Epoch: 0, Step: 47, Batch(micro): 47, Batch (considering grad accum): 5,  Loss: 11.4385, Time: 25.63s, Token/s: 19.97
Epoch: 0, Step: 48, Batch(micro): 48, Batch (considering grad accum): 6,  Loss: 11.5510, Time: 7.59s, Token/s: 67.48
Epoch: 0, Step: 49, Batch(micro): 49, Batch (considering grad accum): 6,  Loss: 11.4290, Time: 4.22s, Token/s: 121.47
Epoch: 0, Step: 50, Batch(micro): 50, Batch (considering grad accum): 6,  Loss: 11.4365, Time: 3.72s, Token/s: 137.63
Epoch: 0, Step: 51, Batch(micro): 51, Batch (considering grad accum): 6,  Loss: 11.4255, Time: 3.59s, Token/s: 142.43
Epoch: 0, Step: 52, Batch(micro): 52, Batch (considering grad accum): 6,  Loss: 11.4592, Time: 3.72s, Token/s: 137.57
Epoch: 0, Step: 53, Batch(micro): 53, Batch (considering grad accum): 6,  Loss: 11.4897, Time: 3.56s, Token/s: 143.77
Epoch: 0, Step: 54, Batch(micro): 54, Batch (considering grad accum): 6,  Loss: 11.4135, Time: 3.02s, Token/s: 169.55
Epoch: 0, Step: 55, Batch(micro): 55, Batch (considering grad accum): 6,  Loss: 11.4620, Time: 26.56s, Token/s: 19.27
Epoch: 0, Step: 56, Batch(micro): 56, Batch (considering grad accum): 7,  Loss: 11.4863, Time: 5.20s, Token/s: 98.51
Epoch: 0, Step: 57, Batch(micro): 57, Batch (considering grad accum): 7,  Loss: 11.4286, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 58, Batch(micro): 58, Batch (considering grad accum): 7,  Loss: 11.3963, Time: 3.40s, Token/s: 150.59
Epoch: 0, Step: 59, Batch(micro): 59, Batch (considering grad accum): 7,  Loss: 11.4377, Time: 3.48s, Token/s: 147.29
Epoch: 0, Step: 60, Batch(micro): 60, Batch (considering grad accum): 7,  Loss: 11.5037, Time: 3.42s, Token/s: 149.65
Epoch: 0, Step: 61, Batch(micro): 61, Batch (considering grad accum): 7,  Loss: 11.3989, Time: 3.10s, Token/s: 165.08
Epoch: 0, Step: 62, Batch(micro): 62, Batch (considering grad accum): 7,  Loss: 11.4858, Time: 3.53s, Token/s: 144.99
Epoch: 0, Step: 63, Batch(micro): 63, Batch (considering grad accum): 7,  Loss: 11.4575, Time: 23.58s, Token/s: 21.71
Epoch: 0, Step: 64, Batch(micro): 64, Batch (considering grad accum): 8,  Loss: 11.4446, Time: 5.28s, Token/s: 96.98
Epoch: 0, Step: 65, Batch(micro): 65, Batch (considering grad accum): 8,  Loss: 11.4983, Time: 3.92s, Token/s: 130.71
Epoch: 0, Step: 66, Batch(micro): 66, Batch (considering grad accum): 8,  Loss: 11.4910, Time: 3.29s, Token/s: 155.53
Epoch: 0, Step: 67, Batch(micro): 67, Batch (considering grad accum): 8,  Loss: 11.4023, Time: 3.29s, Token/s: 155.77
Epoch: 0, Step: 68, Batch(micro): 68, Batch (considering grad accum): 8,  Loss: 11.5259, Time: 3.31s, Token/s: 154.77
Epoch: 0, Step: 69, Batch(micro): 69, Batch (considering grad accum): 8,  Loss: 11.5235, Time: 3.14s, Token/s: 163.25
Epoch: 0, Step: 70, Batch(micro): 70, Batch (considering grad accum): 8,  Loss: 11.4381, Time: 3.12s, Token/s: 163.95
Epoch: 0, Step: 71, Batch(micro): 71, Batch (considering grad accum): 8,  Loss: 11.4658, Time: 27.25s, Token/s: 18.79
Epoch: 0, Step: 72, Batch(micro): 72, Batch (considering grad accum): 9,  Loss: 11.4753, Time: 5.70s, Token/s: 89.90
Epoch: 0, Step: 73, Batch(micro): 73, Batch (considering grad accum): 9,  Loss: 11.4293, Time: 3.79s, Token/s: 135.26
Epoch: 0, Step: 74, Batch(micro): 74, Batch (considering grad accum): 9,  Loss: 11.3872, Time: 3.11s, Token/s: 164.79
Epoch: 0, Step: 75, Batch(micro): 75, Batch (considering grad accum): 9,  Loss: 11.4007, Time: 3.67s, Token/s: 139.62
Epoch: 0, Step: 76, Batch(micro): 76, Batch (considering grad accum): 9,  Loss: 11.4132, Time: 3.15s, Token/s: 162.31
Epoch: 0, Step: 77, Batch(micro): 77, Batch (considering grad accum): 9,  Loss: 11.3394, Time: 3.20s, Token/s: 159.96
Epoch: 0, Step: 78, Batch(micro): 78, Batch (considering grad accum): 9,  Loss: 11.3373, Time: 3.30s, Token/s: 154.99
Epoch: 0, Step: 79, Batch(micro): 79, Batch (considering grad accum): 9,  Loss: 11.4650, Time: 28.05s, Token/s: 18.25
Epoch: 0, Step: 80, Batch(micro): 80, Batch (considering grad accum): 10,  Loss: 11.4719, Time: 6.13s, Token/s: 83.56
Epoch: 0, Step: 81, Batch(micro): 81, Batch (considering grad accum): 10,  Loss: 11.4181, Time: 3.72s, Token/s: 137.75
Epoch: 0, Step: 82, Batch(micro): 82, Batch (considering grad accum): 10,  Loss: 11.3982, Time: 3.16s, Token/s: 162.21
Epoch: 0, Step: 83, Batch(micro): 83, Batch (considering grad accum): 10,  Loss: 11.3651, Time: 3.16s, Token/s: 162.20
Epoch: 0, Step: 84, Batch(micro): 84, Batch (considering grad accum): 10,  Loss: 11.5200, Time: 3.08s, Token/s: 166.03
Epoch: 0, Step: 85, Batch(micro): 85, Batch (considering grad accum): 10,  Loss: 11.4209, Time: 3.21s, Token/s: 159.33
Epoch: 0, Step: 86, Batch(micro): 86, Batch (considering grad accum): 10,  Loss: 11.4798, Time: 3.17s, Token/s: 161.53
Epoch: 0, Step: 87, Batch(micro): 87, Batch (considering grad accum): 10,  Loss: 11.5122, Time: 25.57s, Token/s: 20.03
Epoch: 0, Step: 88, Batch(micro): 88, Batch (considering grad accum): 11,  Loss: 11.3824, Time: 5.71s, Token/s: 89.63
Epoch: 0, Step: 89, Batch(micro): 89, Batch (considering grad accum): 11,  Loss: 11.4029, Time: 3.63s, Token/s: 141.19
Epoch: 0, Step: 90, Batch(micro): 90, Batch (considering grad accum): 11,  Loss: 11.4948, Time: 3.18s, Token/s: 161.08
Epoch: 0, Step: 91, Batch(micro): 91, Batch (considering grad accum): 11,  Loss: 11.3790, Time: 3.22s, Token/s: 159.05
Epoch: 0, Step: 92, Batch(micro): 92, Batch (considering grad accum): 11,  Loss: 11.4472, Time: 3.14s, Token/s: 162.87
Epoch: 0, Step: 93, Batch(micro): 93, Batch (considering grad accum): 11,  Loss: 11.3892, Time: 3.20s, Token/s: 159.92
Epoch: 0, Step: 94, Batch(micro): 94, Batch (considering grad accum): 11,  Loss: 11.5121, Time: 3.27s, Token/s: 156.67
Epoch: 0, Step: 95, Batch(micro): 95, Batch (considering grad accum): 11,  Loss: 11.3937, Time: 25.95s, Token/s: 19.73
Epoch: 0, Step: 96, Batch(micro): 96, Batch (considering grad accum): 12,  Loss: 11.3287, Time: 5.70s, Token/s: 89.80
Epoch: 0, Step: 97, Batch(micro): 97, Batch (considering grad accum): 12,  Loss: 11.3540, Time: 3.73s, Token/s: 137.26
Epoch: 0, Step: 98, Batch(micro): 98, Batch (considering grad accum): 12,  Loss: 11.4064, Time: 3.04s, Token/s: 168.17
Epoch: 0, Step: 99, Batch(micro): 99, Batch (considering grad accum): 12,  Loss: 11.4343, Time: 3.02s, Token/s: 169.43
Updating MLP bias
Epoch: 0, Step: 100, Batch(micro): 100, Batch (considering grad accum): 12,  Loss: 11.4119, Time: 3.32s, Token/s: 154.20
Epoch: 0, Step: 101, Batch(micro): 101, Batch (considering grad accum): 12,  Loss: 11.4857, Time: 3.39s, Token/s: 150.98
Epoch: 0, Step: 102, Batch(micro): 102, Batch (considering grad accum): 12,  Loss: 11.4834, Time: 3.52s, Token/s: 145.52
Epoch: 0, Step: 103, Batch(micro): 103, Batch (considering grad accum): 12,  Loss: 11.5085, Time: 26.17s, Token/s: 19.56
Epoch: 0, Step: 104, Batch(micro): 104, Batch (considering grad accum): 13,  Loss: 11.5260, Time: 5.88s, Token/s: 87.00
Epoch: 0, Step: 105, Batch(micro): 105, Batch (considering grad accum): 13,  Loss: 11.4084, Time: 3.35s, Token/s: 152.74
Epoch: 0, Step: 106, Batch(micro): 106, Batch (considering grad accum): 13,  Loss: 11.3584, Time: 3.27s, Token/s: 156.49
Epoch: 0, Step: 107, Batch(micro): 107, Batch (considering grad accum): 13,  Loss: 11.4271, Time: 3.45s, Token/s: 148.57
Epoch: 0, Step: 108, Batch(micro): 108, Batch (considering grad accum): 13,  Loss: 11.4757, Time: 3.44s, Token/s: 148.71
Epoch: 0, Step: 109, Batch(micro): 109, Batch (considering grad accum): 13,  Loss: 11.4396, Time: 3.18s, Token/s: 161.00
Epoch: 0, Step: 110, Batch(micro): 110, Batch (considering grad accum): 13,  Loss: 11.4501, Time: 3.15s, Token/s: 162.42
Epoch: 0, Step: 111, Batch(micro): 111, Batch (considering grad accum): 13,  Loss: 11.4717, Time: 26.05s, Token/s: 19.65
Epoch: 0, Step: 112, Batch(micro): 112, Batch (considering grad accum): 14,  Loss: 11.4816, Time: 6.64s, Token/s: 77.09
Epoch: 0, Step: 113, Batch(micro): 113, Batch (considering grad accum): 14,  Loss: 11.5100, Time: 6.33s, Token/s: 80.87
Epoch: 0, Step: 114, Batch(micro): 114, Batch (considering grad accum): 14,  Loss: 11.4444, Time: 3.23s, Token/s: 158.68
Epoch: 0, Step: 115, Batch(micro): 115, Batch (considering grad accum): 14,  Loss: 11.4730, Time: 3.16s, Token/s: 162.01
Epoch: 0, Step: 116, Batch(micro): 116, Batch (considering grad accum): 14,  Loss: 11.4382, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 117, Batch(micro): 117, Batch (considering grad accum): 14,  Loss: 11.4629, Time: 3.18s, Token/s: 160.79
Epoch: 0, Step: 118, Batch(micro): 118, Batch (considering grad accum): 14,  Loss: 11.4087, Time: 3.23s, Token/s: 158.55
Epoch: 0, Step: 119, Batch(micro): 119, Batch (considering grad accum): 14,  Loss: 11.3802, Time: 26.29s, Token/s: 19.47
Epoch: 0, Step: 120, Batch(micro): 120, Batch (considering grad accum): 15,  Loss: 11.4184, Time: 6.39s, Token/s: 80.08
Epoch: 0, Step: 121, Batch(micro): 121, Batch (considering grad accum): 15,  Loss: 11.4124, Time: 4.00s, Token/s: 127.99
Epoch: 0, Step: 122, Batch(micro): 122, Batch (considering grad accum): 15,  Loss: 11.4788, Time: 3.87s, Token/s: 132.17
Epoch: 0, Step: 123, Batch(micro): 123, Batch (considering grad accum): 15,  Loss: 11.4223, Time: 3.17s, Token/s: 161.44
Epoch: 0, Step: 124, Batch(micro): 124, Batch (considering grad accum): 15,  Loss: 11.4553, Time: 3.20s, Token/s: 160.10
Epoch: 0, Step: 125, Batch(micro): 125, Batch (considering grad accum): 15,  Loss: 11.4584, Time: 3.15s, Token/s: 162.73
Epoch: 0, Step: 126, Batch(micro): 126, Batch (considering grad accum): 15,  Loss: 11.4483, Time: 4.21s, Token/s: 121.56
Epoch: 0, Step: 127, Batch(micro): 127, Batch (considering grad accum): 15,  Loss: 11.3570, Time: 25.17s, Token/s: 20.34
Epoch: 0, Step: 128, Batch(micro): 128, Batch (considering grad accum): 16,  Loss: 11.4364, Time: 5.52s, Token/s: 92.74
Epoch: 0, Step: 129, Batch(micro): 129, Batch (considering grad accum): 16,  Loss: 11.4363, Time: 3.94s, Token/s: 129.84
Epoch: 0, Step: 130, Batch(micro): 130, Batch (considering grad accum): 16,  Loss: 11.4503, Time: 3.30s, Token/s: 155.09
Epoch: 0, Step: 131, Batch(micro): 131, Batch (considering grad accum): 16,  Loss: 11.4396, Time: 3.26s, Token/s: 157.22
Epoch: 0, Step: 132, Batch(micro): 132, Batch (considering grad accum): 16,  Loss: 11.4525, Time: 3.24s, Token/s: 158.19
Epoch: 0, Step: 133, Batch(micro): 133, Batch (considering grad accum): 16,  Loss: 11.4003, Time: 3.17s, Token/s: 161.64
Epoch: 0, Step: 134, Batch(micro): 134, Batch (considering grad accum): 16,  Loss: 11.4157, Time: 3.30s, Token/s: 155.09
Epoch: 0, Step: 135, Batch(micro): 135, Batch (considering grad accum): 16,  Loss: 11.4735, Time: 26.54s, Token/s: 19.29
Epoch: 0, Step: 136, Batch(micro): 136, Batch (considering grad accum): 17,  Loss: 11.3460, Time: 5.83s, Token/s: 87.76
Epoch: 0, Step: 137, Batch(micro): 137, Batch (considering grad accum): 17,  Loss: 11.4098, Time: 3.64s, Token/s: 140.55
Epoch: 0, Step: 138, Batch(micro): 138, Batch (considering grad accum): 17,  Loss: 11.4311, Time: 2.98s, Token/s: 171.70
Epoch: 0, Step: 139, Batch(micro): 139, Batch (considering grad accum): 17,  Loss: 11.4128, Time: 3.28s, Token/s: 156.02
Epoch: 0, Step: 140, Batch(micro): 140, Batch (considering grad accum): 17,  Loss: 11.4661, Time: 3.39s, Token/s: 151.22
Epoch: 0, Step: 141, Batch(micro): 141, Batch (considering grad accum): 17,  Loss: 11.4233, Time: 2.98s, Token/s: 171.62
Epoch: 0, Step: 142, Batch(micro): 142, Batch (considering grad accum): 17,  Loss: 11.3334, Time: 3.24s, Token/s: 158.07
Epoch: 0, Step: 143, Batch(micro): 143, Batch (considering grad accum): 17,  Loss: 11.3688, Time: 23.06s, Token/s: 22.20
Epoch: 0, Step: 144, Batch(micro): 144, Batch (considering grad accum): 18,  Loss: 11.3453, Time: 5.94s, Token/s: 86.21
Epoch: 0, Step: 145, Batch(micro): 145, Batch (considering grad accum): 18,  Loss: 11.4078, Time: 3.59s, Token/s: 142.60
Epoch: 0, Step: 146, Batch(micro): 146, Batch (considering grad accum): 18,  Loss: 11.3966, Time: 2.96s, Token/s: 173.10
Epoch: 0, Step: 147, Batch(micro): 147, Batch (considering grad accum): 18,  Loss: 11.4292, Time: 3.06s, Token/s: 167.31
Epoch: 0, Step: 148, Batch(micro): 148, Batch (considering grad accum): 18,  Loss: 11.4778, Time: 2.96s, Token/s: 173.17
Epoch: 0, Step: 149, Batch(micro): 149, Batch (considering grad accum): 18,  Loss: 11.3968, Time: 3.67s, Token/s: 139.37
Epoch: 0, Step: 150, Batch(micro): 150, Batch (considering grad accum): 18,  Loss: 11.5172, Time: 3.68s, Token/s: 138.99
Epoch: 0, Step: 151, Batch(micro): 151, Batch (considering grad accum): 18,  Loss: 11.3835, Time: 19.68s, Token/s: 26.02
Epoch: 0, Step: 152, Batch(micro): 152, Batch (considering grad accum): 19,  Loss: 11.3412, Time: 5.16s, Token/s: 99.27
Epoch: 0, Step: 153, Batch(micro): 153, Batch (considering grad accum): 19,  Loss: 11.3531, Time: 3.63s, Token/s: 141.08
Epoch: 0, Step: 154, Batch(micro): 154, Batch (considering grad accum): 19,  Loss: 11.3306, Time: 3.11s, Token/s: 164.49
Epoch: 0, Step: 155, Batch(micro): 155, Batch (considering grad accum): 19,  Loss: 11.5308, Time: 2.90s, Token/s: 176.56
Epoch: 0, Step: 156, Batch(micro): 156, Batch (considering grad accum): 19,  Loss: 11.4066, Time: 2.86s, Token/s: 178.90
Epoch: 0, Step: 157, Batch(micro): 157, Batch (considering grad accum): 19,  Loss: 11.2511, Time: 3.10s, Token/s: 165.38
Epoch: 0, Step: 158, Batch(micro): 158, Batch (considering grad accum): 19,  Loss: 11.4418, Time: 3.13s, Token/s: 163.70
Epoch: 0, Step: 159, Batch(micro): 159, Batch (considering grad accum): 19,  Loss: 11.3906, Time: 17.36s, Token/s: 29.50
Epoch: 0, Step: 160, Batch(micro): 160, Batch (considering grad accum): 20,  Loss: 11.3645, Time: 5.90s, Token/s: 86.79
Epoch: 0, Step: 161, Batch(micro): 161, Batch (considering grad accum): 20,  Loss: 11.4952, Time: 3.76s, Token/s: 136.04
Epoch: 0, Step: 162, Batch(micro): 162, Batch (considering grad accum): 20,  Loss: 11.3861, Time: 3.22s, Token/s: 158.90
Epoch: 0, Step: 163, Batch(micro): 163, Batch (considering grad accum): 20,  Loss: 11.3125, Time: 2.98s, Token/s: 172.08
Epoch: 0, Step: 164, Batch(micro): 164, Batch (considering grad accum): 20,  Loss: 11.3833, Time: 2.93s, Token/s: 174.72
Epoch: 0, Step: 165, Batch(micro): 165, Batch (considering grad accum): 20,  Loss: 11.3321, Time: 2.85s, Token/s: 179.50
Epoch: 0, Step: 166, Batch(micro): 166, Batch (considering grad accum): 20,  Loss: 11.3927, Time: 2.91s, Token/s: 175.83
Epoch: 0, Step: 167, Batch(micro): 167, Batch (considering grad accum): 20,  Loss: 11.3820, Time: 16.12s, Token/s: 31.75
Epoch: 0, Step: 168, Batch(micro): 168, Batch (considering grad accum): 21,  Loss: 11.3895, Time: 4.29s, Token/s: 119.26
Epoch: 0, Step: 169, Batch(micro): 169, Batch (considering grad accum): 21,  Loss: 11.4015, Time: 3.01s, Token/s: 170.36
Epoch: 0, Step: 170, Batch(micro): 170, Batch (considering grad accum): 21,  Loss: 11.3798, Time: 3.83s, Token/s: 133.71
Epoch: 0, Step: 171, Batch(micro): 171, Batch (considering grad accum): 21,  Loss: 11.3500, Time: 3.42s, Token/s: 149.52
Epoch: 0, Step: 172, Batch(micro): 172, Batch (considering grad accum): 21,  Loss: 11.4154, Time: 3.32s, Token/s: 154.39
Epoch: 0, Step: 173, Batch(micro): 173, Batch (considering grad accum): 21,  Loss: 11.3779, Time: 2.98s, Token/s: 171.66
Epoch: 0, Step: 174, Batch(micro): 174, Batch (considering grad accum): 21,  Loss: 11.5032, Time: 2.96s, Token/s: 172.92
Epoch: 0, Step: 175, Batch(micro): 175, Batch (considering grad accum): 21,  Loss: 11.2686, Time: 5.19s, Token/s: 98.71
Epoch: 0, Step: 176, Batch(micro): 176, Batch (considering grad accum): 22,  Loss: 11.3455, Time: 3.73s, Token/s: 137.09
Epoch: 0, Step: 177, Batch(micro): 177, Batch (considering grad accum): 22,  Loss: 11.3950, Time: 3.32s, Token/s: 154.33
Epoch: 0, Step: 178, Batch(micro): 178, Batch (considering grad accum): 22,  Loss: 11.3665, Time: 2.99s, Token/s: 171.47
Epoch: 0, Step: 179, Batch(micro): 179, Batch (considering grad accum): 22,  Loss: 11.4556, Time: 2.93s, Token/s: 174.71
Epoch: 0, Step: 180, Batch(micro): 180, Batch (considering grad accum): 22,  Loss: 11.3734, Time: 2.94s, Token/s: 174.12
Epoch: 0, Step: 181, Batch(micro): 181, Batch (considering grad accum): 22,  Loss: 11.3413, Time: 2.96s, Token/s: 172.79
Epoch: 0, Step: 182, Batch(micro): 182, Batch (considering grad accum): 22,  Loss: 11.3878, Time: 3.34s, Token/s: 153.22
Epoch: 0, Step: 183, Batch(micro): 183, Batch (considering grad accum): 22,  Loss: 11.4495, Time: 4.33s, Token/s: 118.35
Epoch: 0, Step: 184, Batch(micro): 184, Batch (considering grad accum): 23,  Loss: 11.3389, Time: 2.95s, Token/s: 173.66
Epoch: 0, Step: 185, Batch(micro): 185, Batch (considering grad accum): 23,  Loss: 11.4452, Time: 3.49s, Token/s: 146.83
Epoch: 0, Step: 186, Batch(micro): 186, Batch (considering grad accum): 23,  Loss: 11.4231, Time: 3.49s, Token/s: 146.61
Epoch: 0, Step: 187, Batch(micro): 187, Batch (considering grad accum): 23,  Loss: 11.3743, Time: 3.32s, Token/s: 154.19
Epoch: 0, Step: 188, Batch(micro): 188, Batch (considering grad accum): 23,  Loss: 11.3711, Time: 2.99s, Token/s: 171.13
Epoch: 0, Step: 189, Batch(micro): 189, Batch (considering grad accum): 23,  Loss: 11.2871, Time: 3.51s, Token/s: 146.03
Epoch: 0, Step: 190, Batch(micro): 190, Batch (considering grad accum): 23,  Loss: 11.2913, Time: 3.46s, Token/s: 148.17
Epoch: 0, Step: 191, Batch(micro): 191, Batch (considering grad accum): 23,  Loss: 11.2787, Time: 4.91s, Token/s: 104.23
Epoch: 0, Step: 192, Batch(micro): 192, Batch (considering grad accum): 24,  Loss: 11.3943, Time: 4.01s, Token/s: 127.79
Epoch: 0, Step: 193, Batch(micro): 193, Batch (considering grad accum): 24,  Loss: 11.3705, Time: 3.79s, Token/s: 135.02
Epoch: 0, Step: 194, Batch(micro): 194, Batch (considering grad accum): 24,  Loss: 11.4197, Time: 3.15s, Token/s: 162.45
Epoch: 0, Step: 195, Batch(micro): 195, Batch (considering grad accum): 24,  Loss: 11.4052, Time: 2.85s, Token/s: 179.79
Epoch: 0, Step: 196, Batch(micro): 196, Batch (considering grad accum): 24,  Loss: 11.2808, Time: 2.92s, Token/s: 175.08
Epoch: 0, Step: 197, Batch(micro): 197, Batch (considering grad accum): 24,  Loss: 11.4057, Time: 2.88s, Token/s: 177.62
Epoch: 0, Step: 198, Batch(micro): 198, Batch (considering grad accum): 24,  Loss: 11.2714, Time: 2.87s, Token/s: 178.16
Epoch: 0, Step: 199, Batch(micro): 199, Batch (considering grad accum): 24,  Loss: 11.4414, Time: 4.97s, Token/s: 103.00
Updating MLP bias
Epoch: 0, Step: 200, Batch(micro): 200, Batch (considering grad accum): 25,  Loss: 11.4300, Time: 3.10s, Token/s: 164.95
Epoch: 0, Step: 201, Batch(micro): 201, Batch (considering grad accum): 25,  Loss: 11.4250, Time: 3.18s, Token/s: 161.18
Epoch: 0, Step: 202, Batch(micro): 202, Batch (considering grad accum): 25,  Loss: 11.4405, Time: 2.85s, Token/s: 179.93
Epoch: 0, Step: 203, Batch(micro): 203, Batch (considering grad accum): 25,  Loss: 11.3967, Time: 3.00s, Token/s: 170.38
Epoch: 0, Step: 204, Batch(micro): 204, Batch (considering grad accum): 25,  Loss: 11.4097, Time: 2.82s, Token/s: 181.49
Epoch: 0, Step: 205, Batch(micro): 205, Batch (considering grad accum): 25,  Loss: 11.3763, Time: 2.82s, Token/s: 181.37
Epoch: 0, Step: 206, Batch(micro): 206, Batch (considering grad accum): 25,  Loss: 11.4272, Time: 3.02s, Token/s: 169.28
Epoch: 0, Step: 207, Batch(micro): 207, Batch (considering grad accum): 25,  Loss: 11.3147, Time: 4.42s, Token/s: 115.90
Epoch: 0, Step: 208, Batch(micro): 208, Batch (considering grad accum): 26,  Loss: 11.2995, Time: 4.60s, Token/s: 111.35
Epoch: 0, Step: 209, Batch(micro): 209, Batch (considering grad accum): 26,  Loss: 11.3815, Time: 3.81s, Token/s: 134.22
Epoch: 0, Step: 210, Batch(micro): 210, Batch (considering grad accum): 26,  Loss: 11.3860, Time: 2.79s, Token/s: 183.41
Epoch: 0, Step: 211, Batch(micro): 211, Batch (considering grad accum): 26,  Loss: 11.3440, Time: 2.86s, Token/s: 178.72
Epoch: 0, Step: 212, Batch(micro): 212, Batch (considering grad accum): 26,  Loss: 11.3740, Time: 3.10s, Token/s: 165.08
Epoch: 0, Step: 213, Batch(micro): 213, Batch (considering grad accum): 26,  Loss: 11.3869, Time: 2.86s, Token/s: 178.82
Epoch: 0, Step: 214, Batch(micro): 214, Batch (considering grad accum): 26,  Loss: 11.4139, Time: 2.79s, Token/s: 183.81
Epoch: 0, Step: 215, Batch(micro): 215, Batch (considering grad accum): 26,  Loss: 11.4005, Time: 21.06s, Token/s: 24.31
Epoch: 0, Step: 216, Batch(micro): 216, Batch (considering grad accum): 27,  Loss: 11.3533, Time: 4.83s, Token/s: 106.06
Epoch: 0, Step: 217, Batch(micro): 217, Batch (considering grad accum): 27,  Loss: 11.3571, Time: 3.50s, Token/s: 146.49
Epoch: 0, Step: 218, Batch(micro): 218, Batch (considering grad accum): 27,  Loss: 11.3898, Time: 3.17s, Token/s: 161.53
Epoch: 0, Step: 219, Batch(micro): 219, Batch (considering grad accum): 27,  Loss: 11.2818, Time: 3.28s, Token/s: 156.31
Epoch: 0, Step: 220, Batch(micro): 220, Batch (considering grad accum): 27,  Loss: 11.3826, Time: 3.13s, Token/s: 163.80
Epoch: 0, Step: 221, Batch(micro): 221, Batch (considering grad accum): 27,  Loss: 11.3107, Time: 3.72s, Token/s: 137.69
Epoch: 0, Step: 222, Batch(micro): 222, Batch (considering grad accum): 27,  Loss: 11.3795, Time: 3.02s, Token/s: 169.50
Epoch: 0, Step: 223, Batch(micro): 223, Batch (considering grad accum): 27,  Loss: 11.3394, Time: 18.59s, Token/s: 27.54
Epoch: 0, Step: 224, Batch(micro): 224, Batch (considering grad accum): 28,  Loss: 11.3805, Time: 5.38s, Token/s: 95.24
Epoch: 0, Step: 225, Batch(micro): 225, Batch (considering grad accum): 28,  Loss: 11.3632, Time: 3.18s, Token/s: 160.92
Epoch: 0, Step: 226, Batch(micro): 226, Batch (considering grad accum): 28,  Loss: 11.3137, Time: 3.17s, Token/s: 161.64
Epoch: 0, Step: 227, Batch(micro): 227, Batch (considering grad accum): 28,  Loss: 11.3282, Time: 3.14s, Token/s: 163.14
Epoch: 0, Step: 228, Batch(micro): 228, Batch (considering grad accum): 28,  Loss: 11.3106, Time: 3.53s, Token/s: 145.02
Epoch: 0, Step: 229, Batch(micro): 229, Batch (considering grad accum): 28,  Loss: 11.3497, Time: 3.10s, Token/s: 165.18
Epoch: 0, Step: 230, Batch(micro): 230, Batch (considering grad accum): 28,  Loss: 11.2783, Time: 3.11s, Token/s: 164.37
Epoch: 0, Step: 231, Batch(micro): 231, Batch (considering grad accum): 28,  Loss: 11.2197, Time: 16.80s, Token/s: 30.47
Epoch: 0, Step: 232, Batch(micro): 232, Batch (considering grad accum): 29,  Loss: 11.2589, Time: 5.31s, Token/s: 96.44
Epoch: 0, Step: 233, Batch(micro): 233, Batch (considering grad accum): 29,  Loss: 11.3659, Time: 3.17s, Token/s: 161.43
Epoch: 0, Step: 234, Batch(micro): 234, Batch (considering grad accum): 29,  Loss: 11.3533, Time: 3.07s, Token/s: 166.72
Epoch: 0, Step: 235, Batch(micro): 235, Batch (considering grad accum): 29,  Loss: 11.3595, Time: 3.08s, Token/s: 166.50
Epoch: 0, Step: 236, Batch(micro): 236, Batch (considering grad accum): 29,  Loss: 11.2859, Time: 3.17s, Token/s: 161.39
Epoch: 0, Step: 237, Batch(micro): 237, Batch (considering grad accum): 29,  Loss: 11.4506, Time: 3.51s, Token/s: 145.70
Epoch: 0, Step: 238, Batch(micro): 238, Batch (considering grad accum): 29,  Loss: 11.3145, Time: 3.09s, Token/s: 165.65
Epoch: 0, Step: 239, Batch(micro): 239, Batch (considering grad accum): 29,  Loss: 11.3212, Time: 19.44s, Token/s: 26.34
Epoch: 0, Step: 240, Batch(micro): 240, Batch (considering grad accum): 30,  Loss: 11.2932, Time: 4.99s, Token/s: 102.69
Epoch: 0, Step: 241, Batch(micro): 241, Batch (considering grad accum): 30,  Loss: 11.4659, Time: 3.85s, Token/s: 132.89
Epoch: 0, Step: 242, Batch(micro): 242, Batch (considering grad accum): 30,  Loss: 11.4083, Time: 3.13s, Token/s: 163.69
Epoch: 0, Step: 243, Batch(micro): 243, Batch (considering grad accum): 30,  Loss: 11.3866, Time: 3.10s, Token/s: 164.94
Epoch: 0, Step: 244, Batch(micro): 244, Batch (considering grad accum): 30,  Loss: 11.4321, Time: 3.08s, Token/s: 166.09
Epoch: 0, Step: 245, Batch(micro): 245, Batch (considering grad accum): 30,  Loss: 11.3705, Time: 3.46s, Token/s: 148.07
Epoch: 0, Step: 246, Batch(micro): 246, Batch (considering grad accum): 30,  Loss: 11.2506, Time: 3.47s, Token/s: 147.34
Epoch: 0, Step: 247, Batch(micro): 247, Batch (considering grad accum): 30,  Loss: 11.2612, Time: 19.08s, Token/s: 26.84
Epoch: 0, Step: 248, Batch(micro): 248, Batch (considering grad accum): 31,  Loss: 11.3631, Time: 5.14s, Token/s: 99.61
Epoch: 0, Step: 249, Batch(micro): 249, Batch (considering grad accum): 31,  Loss: 11.3319, Time: 3.47s, Token/s: 147.60
Epoch: 0, Step: 250, Batch(micro): 250, Batch (considering grad accum): 31,  Loss: 11.2370, Time: 3.13s, Token/s: 163.76
Epoch: 0, Step: 251, Batch(micro): 251, Batch (considering grad accum): 31,  Loss: 11.3908, Time: 3.10s, Token/s: 165.10
Epoch: 0, Step: 252, Batch(micro): 252, Batch (considering grad accum): 31,  Loss: 11.3745, Time: 3.06s, Token/s: 167.42
Epoch: 0, Step: 253, Batch(micro): 253, Batch (considering grad accum): 31,  Loss: 11.2507, Time: 3.10s, Token/s: 164.97
Epoch: 0, Step: 254, Batch(micro): 254, Batch (considering grad accum): 31,  Loss: 11.3284, Time: 3.10s, Token/s: 164.99
Epoch: 0, Step: 255, Batch(micro): 255, Batch (considering grad accum): 31,  Loss: 11.3660, Time: 5.96s, Token/s: 85.90
Epoch: 0, Step: 256, Batch(micro): 256, Batch (considering grad accum): 32,  Loss: 11.3763, Time: 3.71s, Token/s: 137.91
Epoch: 0, Step: 257, Batch(micro): 257, Batch (considering grad accum): 32,  Loss: 11.3474, Time: 3.36s, Token/s: 152.18
Epoch: 0, Step: 258, Batch(micro): 258, Batch (considering grad accum): 32,  Loss: 11.3059, Time: 3.06s, Token/s: 167.50
Epoch: 0, Step: 259, Batch(micro): 259, Batch (considering grad accum): 32,  Loss: 11.3063, Time: 3.03s, Token/s: 169.21
Epoch: 0, Step: 260, Batch(micro): 260, Batch (considering grad accum): 32,  Loss: 11.3280, Time: 3.17s, Token/s: 161.27
Epoch: 0, Step: 261, Batch(micro): 261, Batch (considering grad accum): 32,  Loss: 11.2843, Time: 3.00s, Token/s: 170.73
Epoch: 0, Step: 262, Batch(micro): 262, Batch (considering grad accum): 32,  Loss: 11.3640, Time: 3.03s, Token/s: 168.73
Epoch: 0, Step: 263, Batch(micro): 263, Batch (considering grad accum): 32,  Loss: 11.2652, Time: 4.93s, Token/s: 103.91
Epoch: 0, Step: 264, Batch(micro): 264, Batch (considering grad accum): 33,  Loss: 11.2490, Time: 3.27s, Token/s: 156.81
Epoch: 0, Step: 265, Batch(micro): 265, Batch (considering grad accum): 33,  Loss: 11.1837, Time: 3.29s, Token/s: 155.40
Epoch: 0, Step: 266, Batch(micro): 266, Batch (considering grad accum): 33,  Loss: 11.2680, Time: 3.05s, Token/s: 168.00
Epoch: 0, Step: 267, Batch(micro): 267, Batch (considering grad accum): 33,  Loss: 11.3610, Time: 3.17s, Token/s: 161.45
Epoch: 0, Step: 268, Batch(micro): 268, Batch (considering grad accum): 33,  Loss: 11.3324, Time: 3.16s, Token/s: 162.16
Epoch: 0, Step: 269, Batch(micro): 269, Batch (considering grad accum): 33,  Loss: 11.2244, Time: 3.10s, Token/s: 164.90
Epoch: 0, Step: 270, Batch(micro): 270, Batch (considering grad accum): 33,  Loss: 11.2789, Time: 3.01s, Token/s: 170.21
Epoch: 0, Step: 271, Batch(micro): 271, Batch (considering grad accum): 33,  Loss: 11.1889, Time: 5.94s, Token/s: 86.24
Epoch: 0, Step: 272, Batch(micro): 272, Batch (considering grad accum): 34,  Loss: 11.1602, Time: 3.65s, Token/s: 140.23
Epoch: 0, Step: 273, Batch(micro): 273, Batch (considering grad accum): 34,  Loss: 11.2488, Time: 3.44s, Token/s: 148.85
Epoch: 0, Step: 274, Batch(micro): 274, Batch (considering grad accum): 34,  Loss: 11.2901, Time: 3.08s, Token/s: 166.48
Epoch: 0, Step: 275, Batch(micro): 275, Batch (considering grad accum): 34,  Loss: 11.2210, Time: 3.06s, Token/s: 167.25
Epoch: 0, Step: 276, Batch(micro): 276, Batch (considering grad accum): 34,  Loss: 11.2606, Time: 3.29s, Token/s: 155.47
Epoch: 0, Step: 277, Batch(micro): 277, Batch (considering grad accum): 34,  Loss: 11.2905, Time: 3.20s, Token/s: 159.93
Epoch: 0, Step: 278, Batch(micro): 278, Batch (considering grad accum): 34,  Loss: 11.2228, Time: 3.55s, Token/s: 144.41
Epoch: 0, Step: 279, Batch(micro): 279, Batch (considering grad accum): 34,  Loss: 11.2442, Time: 6.53s, Token/s: 78.36
Epoch: 0, Step: 280, Batch(micro): 280, Batch (considering grad accum): 35,  Loss: 11.3742, Time: 3.44s, Token/s: 148.78
Epoch: 0, Step: 281, Batch(micro): 281, Batch (considering grad accum): 35,  Loss: 11.2674, Time: 3.91s, Token/s: 130.83
Epoch: 0, Step: 282, Batch(micro): 282, Batch (considering grad accum): 35,  Loss: 11.2476, Time: 3.20s, Token/s: 160.05
Epoch: 0, Step: 283, Batch(micro): 283, Batch (considering grad accum): 35,  Loss: 11.1006, Time: 3.10s, Token/s: 165.23
Epoch: 0, Step: 284, Batch(micro): 284, Batch (considering grad accum): 35,  Loss: 11.1761, Time: 3.07s, Token/s: 166.72
Epoch: 0, Step: 285, Batch(micro): 285, Batch (considering grad accum): 35,  Loss: 11.3311, Time: 3.12s, Token/s: 163.91
Epoch: 0, Step: 286, Batch(micro): 286, Batch (considering grad accum): 35,  Loss: 11.2748, Time: 3.23s, Token/s: 158.67
Epoch: 0, Step: 287, Batch(micro): 287, Batch (considering grad accum): 35,  Loss: 11.2889, Time: 6.06s, Token/s: 84.43
Epoch: 0, Step: 288, Batch(micro): 288, Batch (considering grad accum): 36,  Loss: 11.2942, Time: 4.60s, Token/s: 111.24
Epoch: 0, Step: 289, Batch(micro): 289, Batch (considering grad accum): 36,  Loss: 11.2821, Time: 3.46s, Token/s: 147.85
Epoch: 0, Step: 290, Batch(micro): 290, Batch (considering grad accum): 36,  Loss: 11.2521, Time: 3.03s, Token/s: 169.20
Epoch: 0, Step: 291, Batch(micro): 291, Batch (considering grad accum): 36,  Loss: 11.2439, Time: 3.04s, Token/s: 168.40
Epoch: 0, Step: 292, Batch(micro): 292, Batch (considering grad accum): 36,  Loss: 11.3388, Time: 3.09s, Token/s: 165.83
Epoch: 0, Step: 293, Batch(micro): 293, Batch (considering grad accum): 36,  Loss: 11.3193, Time: 3.11s, Token/s: 164.80
Epoch: 0, Step: 294, Batch(micro): 294, Batch (considering grad accum): 36,  Loss: 11.2378, Time: 3.06s, Token/s: 167.58
Epoch: 0, Step: 295, Batch(micro): 295, Batch (considering grad accum): 36,  Loss: 11.1007, Time: 5.06s, Token/s: 101.24
Epoch: 0, Step: 296, Batch(micro): 296, Batch (considering grad accum): 37,  Loss: 11.0966, Time: 3.54s, Token/s: 144.49
Epoch: 0, Step: 297, Batch(micro): 297, Batch (considering grad accum): 37,  Loss: 11.3446, Time: 3.97s, Token/s: 129.08
Epoch: 0, Step: 298, Batch(micro): 298, Batch (considering grad accum): 37,  Loss: 11.2787, Time: 3.07s, Token/s: 167.01
Epoch: 0, Step: 299, Batch(micro): 299, Batch (considering grad accum): 37,  Loss: 11.3045, Time: 3.04s, Token/s: 168.66
Updating MLP bias
Epoch: 0, Step: 300, Batch(micro): 300, Batch (considering grad accum): 37,  Loss: 11.2337, Time: 3.04s, Token/s: 168.58
Epoch: 0, Step: 301, Batch(micro): 301, Batch (considering grad accum): 37,  Loss: 11.2464, Time: 3.02s, Token/s: 169.68
Epoch: 0, Step: 302, Batch(micro): 302, Batch (considering grad accum): 37,  Loss: 11.2134, Time: 3.78s, Token/s: 135.28
Epoch: 0, Step: 303, Batch(micro): 303, Batch (considering grad accum): 37,  Loss: 11.2163, Time: 6.93s, Token/s: 73.84
Epoch: 0, Step: 304, Batch(micro): 304, Batch (considering grad accum): 38,  Loss: 11.2582, Time: 3.94s, Token/s: 129.81
Epoch: 0, Step: 305, Batch(micro): 305, Batch (considering grad accum): 38,  Loss: 11.2525, Time: 3.35s, Token/s: 152.82
Epoch: 0, Step: 306, Batch(micro): 306, Batch (considering grad accum): 38,  Loss: 11.2792, Time: 2.99s, Token/s: 171.10
Epoch: 0, Step: 307, Batch(micro): 307, Batch (considering grad accum): 38,  Loss: 11.1669, Time: 3.10s, Token/s: 164.91
Epoch: 0, Step: 308, Batch(micro): 308, Batch (considering grad accum): 38,  Loss: 11.2093, Time: 3.13s, Token/s: 163.55
Epoch: 0, Step: 309, Batch(micro): 309, Batch (considering grad accum): 38,  Loss: 11.2864, Time: 3.01s, Token/s: 170.16
Epoch: 0, Step: 310, Batch(micro): 310, Batch (considering grad accum): 38,  Loss: 11.1630, Time: 3.01s, Token/s: 169.92
Epoch: 0, Step: 311, Batch(micro): 311, Batch (considering grad accum): 38,  Loss: 11.0969, Time: 5.12s, Token/s: 99.99
Epoch: 0, Step: 312, Batch(micro): 312, Batch (considering grad accum): 39,  Loss: 11.2696, Time: 3.84s, Token/s: 133.29
Epoch: 0, Step: 313, Batch(micro): 313, Batch (considering grad accum): 39,  Loss: 11.2471, Time: 3.44s, Token/s: 148.78
Epoch: 0, Step: 314, Batch(micro): 314, Batch (considering grad accum): 39,  Loss: 11.1904, Time: 3.27s, Token/s: 156.44
Epoch: 0, Step: 315, Batch(micro): 315, Batch (considering grad accum): 39,  Loss: 11.1402, Time: 3.10s, Token/s: 165.35
Epoch: 0, Step: 316, Batch(micro): 316, Batch (considering grad accum): 39,  Loss: 11.2552, Time: 3.12s, Token/s: 163.86
Epoch: 0, Step: 317, Batch(micro): 317, Batch (considering grad accum): 39,  Loss: 11.1652, Time: 3.09s, Token/s: 165.94
Epoch: 0, Step: 318, Batch(micro): 318, Batch (considering grad accum): 39,  Loss: 11.2394, Time: 3.28s, Token/s: 156.21
Epoch: 0, Step: 319, Batch(micro): 319, Batch (considering grad accum): 39,  Loss: 11.3288, Time: 15.63s, Token/s: 32.75
Epoch: 0, Step: 320, Batch(micro): 320, Batch (considering grad accum): 40,  Loss: 11.1907, Time: 4.64s, Token/s: 110.27
Epoch: 0, Step: 321, Batch(micro): 321, Batch (considering grad accum): 40,  Loss: 11.0495, Time: 3.02s, Token/s: 169.41
Epoch: 0, Step: 322, Batch(micro): 322, Batch (considering grad accum): 40,  Loss: 11.1621, Time: 3.09s, Token/s: 165.93
Epoch: 0, Step: 323, Batch(micro): 323, Batch (considering grad accum): 40,  Loss: 11.1234, Time: 2.85s, Token/s: 179.36
Epoch: 0, Step: 324, Batch(micro): 324, Batch (considering grad accum): 40,  Loss: 11.0033, Time: 2.73s, Token/s: 187.85
Epoch: 0, Step: 325, Batch(micro): 325, Batch (considering grad accum): 40,  Loss: 11.1894, Time: 2.94s, Token/s: 174.09
Epoch: 0, Step: 326, Batch(micro): 326, Batch (considering grad accum): 40,  Loss: 11.2306, Time: 2.98s, Token/s: 171.76
Epoch: 0, Step: 327, Batch(micro): 327, Batch (considering grad accum): 40,  Loss: 11.1778, Time: 18.02s, Token/s: 28.41
Epoch: 0, Step: 328, Batch(micro): 328, Batch (considering grad accum): 41,  Loss: 11.2863, Time: 4.57s, Token/s: 112.10
Epoch: 0, Step: 329, Batch(micro): 329, Batch (considering grad accum): 41,  Loss: 11.1705, Time: 3.24s, Token/s: 158.18
Epoch: 0, Step: 330, Batch(micro): 330, Batch (considering grad accum): 41,  Loss: 11.2111, Time: 2.84s, Token/s: 180.27
Epoch: 0, Step: 331, Batch(micro): 331, Batch (considering grad accum): 41,  Loss: 11.0806, Time: 2.89s, Token/s: 177.33
Epoch: 0, Step: 332, Batch(micro): 332, Batch (considering grad accum): 41,  Loss: 11.2697, Time: 2.61s, Token/s: 196.32
Epoch: 0, Step: 333, Batch(micro): 333, Batch (considering grad accum): 41,  Loss: 11.2840, Time: 2.88s, Token/s: 178.00
Epoch: 0, Step: 334, Batch(micro): 334, Batch (considering grad accum): 41,  Loss: 11.3223, Time: 2.76s, Token/s: 185.25
Epoch: 0, Step: 335, Batch(micro): 335, Batch (considering grad accum): 41,  Loss: 11.2068, Time: 5.14s, Token/s: 99.53
Epoch: 0, Step: 336, Batch(micro): 336, Batch (considering grad accum): 42,  Loss: 11.1248, Time: 3.27s, Token/s: 156.38
Epoch: 0, Step: 337, Batch(micro): 337, Batch (considering grad accum): 42,  Loss: 11.1233, Time: 2.95s, Token/s: 173.32
Epoch: 0, Step: 338, Batch(micro): 338, Batch (considering grad accum): 42,  Loss: 11.1560, Time: 2.74s, Token/s: 187.16
Epoch: 0, Step: 339, Batch(micro): 339, Batch (considering grad accum): 42,  Loss: 11.0429, Time: 2.83s, Token/s: 180.97
Epoch: 0, Step: 340, Batch(micro): 340, Batch (considering grad accum): 42,  Loss: 11.2784, Time: 2.83s, Token/s: 180.63
Epoch: 0, Step: 341, Batch(micro): 341, Batch (considering grad accum): 42,  Loss: 11.1162, Time: 2.77s, Token/s: 184.51
Epoch: 0, Step: 342, Batch(micro): 342, Batch (considering grad accum): 42,  Loss: 11.1216, Time: 2.67s, Token/s: 191.68
Epoch: 0, Step: 343, Batch(micro): 343, Batch (considering grad accum): 42,  Loss: 11.1896, Time: 3.91s, Token/s: 131.08
Epoch: 0, Step: 344, Batch(micro): 344, Batch (considering grad accum): 43,  Loss: 11.2315, Time: 2.74s, Token/s: 186.94
Epoch: 0, Step: 345, Batch(micro): 345, Batch (considering grad accum): 43,  Loss: 11.3141, Time: 2.71s, Token/s: 188.73
Epoch: 0, Step: 346, Batch(micro): 346, Batch (considering grad accum): 43,  Loss: 11.0646, Time: 2.75s, Token/s: 186.17
Epoch: 0, Step: 347, Batch(micro): 347, Batch (considering grad accum): 43,  Loss: 11.2176, Time: 2.80s, Token/s: 183.12
Epoch: 0, Step: 348, Batch(micro): 348, Batch (considering grad accum): 43,  Loss: 11.1238, Time: 2.69s, Token/s: 190.65
Epoch: 0, Step: 349, Batch(micro): 349, Batch (considering grad accum): 43,  Loss: 11.1755, Time: 3.13s, Token/s: 163.32
Epoch: 0, Step: 350, Batch(micro): 350, Batch (considering grad accum): 43,  Loss: 11.0875, Time: 2.88s, Token/s: 177.99
Epoch: 0, Step: 351, Batch(micro): 351, Batch (considering grad accum): 43,  Loss: 11.0302, Time: 4.02s, Token/s: 127.43
Epoch: 0, Step: 352, Batch(micro): 352, Batch (considering grad accum): 44,  Loss: 11.0971, Time: 3.06s, Token/s: 167.32
Epoch: 0, Step: 353, Batch(micro): 353, Batch (considering grad accum): 44,  Loss: 11.0100, Time: 3.25s, Token/s: 157.68
Epoch: 0, Step: 354, Batch(micro): 354, Batch (considering grad accum): 44,  Loss: 11.1950, Time: 2.86s, Token/s: 179.15
Epoch: 0, Step: 355, Batch(micro): 355, Batch (considering grad accum): 44,  Loss: 11.2165, Time: 3.28s, Token/s: 155.92
Epoch: 0, Step: 356, Batch(micro): 356, Batch (considering grad accum): 44,  Loss: 11.1719, Time: 2.80s, Token/s: 182.55
Epoch: 0, Step: 357, Batch(micro): 357, Batch (considering grad accum): 44,  Loss: 11.1546, Time: 2.98s, Token/s: 171.91
Epoch: 0, Step: 358, Batch(micro): 358, Batch (considering grad accum): 44,  Loss: 11.1142, Time: 3.12s, Token/s: 164.20
Epoch: 0, Step: 359, Batch(micro): 359, Batch (considering grad accum): 44,  Loss: 11.1103, Time: 5.88s, Token/s: 87.11
Epoch: 0, Step: 360, Batch(micro): 360, Batch (considering grad accum): 45,  Loss: 11.0576, Time: 3.97s, Token/s: 129.11
Epoch: 0, Step: 361, Batch(micro): 361, Batch (considering grad accum): 45,  Loss: 11.1244, Time: 3.41s, Token/s: 150.06
Epoch: 0, Step: 362, Batch(micro): 362, Batch (considering grad accum): 45,  Loss: 11.3005, Time: 3.06s, Token/s: 167.05
Epoch: 0, Step: 363, Batch(micro): 363, Batch (considering grad accum): 45,  Loss: 11.1023, Time: 3.01s, Token/s: 170.19
Epoch: 0, Step: 364, Batch(micro): 364, Batch (considering grad accum): 45,  Loss: 11.1966, Time: 3.08s, Token/s: 166.36
Epoch: 0, Step: 365, Batch(micro): 365, Batch (considering grad accum): 45,  Loss: 11.0691, Time: 3.09s, Token/s: 165.57
Epoch: 0, Step: 366, Batch(micro): 366, Batch (considering grad accum): 45,  Loss: 11.2074, Time: 3.08s, Token/s: 166.47
Epoch: 0, Step: 367, Batch(micro): 367, Batch (considering grad accum): 45,  Loss: 11.2217, Time: 5.50s, Token/s: 93.06
Epoch: 0, Step: 368, Batch(micro): 368, Batch (considering grad accum): 46,  Loss: 11.0473, Time: 3.68s, Token/s: 139.21
Epoch: 0, Step: 369, Batch(micro): 369, Batch (considering grad accum): 46,  Loss: 10.9805, Time: 3.34s, Token/s: 153.22
Epoch: 0, Step: 370, Batch(micro): 370, Batch (considering grad accum): 46,  Loss: 11.0041, Time: 3.06s, Token/s: 167.20
Epoch: 0, Step: 371, Batch(micro): 371, Batch (considering grad accum): 46,  Loss: 11.1374, Time: 3.09s, Token/s: 165.60
Epoch: 0, Step: 372, Batch(micro): 372, Batch (considering grad accum): 46,  Loss: 11.2133, Time: 3.08s, Token/s: 166.33
Epoch: 0, Step: 373, Batch(micro): 373, Batch (considering grad accum): 46,  Loss: 11.1411, Time: 3.01s, Token/s: 169.89
Epoch: 0, Step: 374, Batch(micro): 374, Batch (considering grad accum): 46,  Loss: 11.1187, Time: 3.36s, Token/s: 152.49
Epoch: 0, Step: 375, Batch(micro): 375, Batch (considering grad accum): 46,  Loss: 11.0530, Time: 4.33s, Token/s: 118.18
Epoch: 0, Step: 376, Batch(micro): 376, Batch (considering grad accum): 47,  Loss: 10.9355, Time: 3.40s, Token/s: 150.72
Epoch: 0, Step: 377, Batch(micro): 377, Batch (considering grad accum): 47,  Loss: 11.0310, Time: 3.46s, Token/s: 147.86
Epoch: 0, Step: 378, Batch(micro): 378, Batch (considering grad accum): 47,  Loss: 11.0178, Time: 3.29s, Token/s: 155.39
Epoch: 0, Step: 379, Batch(micro): 379, Batch (considering grad accum): 47,  Loss: 11.0482, Time: 3.21s, Token/s: 159.68
Epoch: 0, Step: 380, Batch(micro): 380, Batch (considering grad accum): 47,  Loss: 11.1484, Time: 3.08s, Token/s: 166.07
Epoch: 0, Step: 381, Batch(micro): 381, Batch (considering grad accum): 47,  Loss: 11.1418, Time: 3.10s, Token/s: 164.98
Epoch: 0, Step: 382, Batch(micro): 382, Batch (considering grad accum): 47,  Loss: 11.0309, Time: 3.26s, Token/s: 157.22
Epoch: 0, Step: 383, Batch(micro): 383, Batch (considering grad accum): 47,  Loss: 11.1384, Time: 4.99s, Token/s: 102.69
Epoch: 0, Step: 384, Batch(micro): 384, Batch (considering grad accum): 48,  Loss: 10.9450, Time: 4.29s, Token/s: 119.47
Epoch: 0, Step: 385, Batch(micro): 385, Batch (considering grad accum): 48,  Loss: 11.0118, Time: 3.18s, Token/s: 161.18
Epoch: 0, Step: 386, Batch(micro): 386, Batch (considering grad accum): 48,  Loss: 11.0443, Time: 3.09s, Token/s: 165.94
Epoch: 0, Step: 387, Batch(micro): 387, Batch (considering grad accum): 48,  Loss: 11.0191, Time: 3.26s, Token/s: 156.96
Epoch: 0, Step: 388, Batch(micro): 388, Batch (considering grad accum): 48,  Loss: 11.0888, Time: 2.94s, Token/s: 173.89
Epoch: 0, Step: 389, Batch(micro): 389, Batch (considering grad accum): 48,  Loss: 10.9451, Time: 2.86s, Token/s: 179.03
Epoch: 0, Step: 390, Batch(micro): 390, Batch (considering grad accum): 48,  Loss: 11.0983, Time: 2.84s, Token/s: 180.49
Epoch: 0, Step: 391, Batch(micro): 391, Batch (considering grad accum): 48,  Loss: 11.3241, Time: 5.47s, Token/s: 93.67
Epoch: 0, Step: 392, Batch(micro): 392, Batch (considering grad accum): 49,  Loss: 11.0422, Time: 3.44s, Token/s: 148.96
Epoch: 0, Step: 393, Batch(micro): 393, Batch (considering grad accum): 49,  Loss: 10.9571, Time: 3.15s, Token/s: 162.58
Epoch: 0, Step: 394, Batch(micro): 394, Batch (considering grad accum): 49,  Loss: 10.9537, Time: 2.83s, Token/s: 180.69
Epoch: 0, Step: 395, Batch(micro): 395, Batch (considering grad accum): 49,  Loss: 11.1204, Time: 2.78s, Token/s: 183.97
Epoch: 0, Step: 396, Batch(micro): 396, Batch (considering grad accum): 49,  Loss: 11.0878, Time: 2.86s, Token/s: 179.21
Epoch: 0, Step: 397, Batch(micro): 397, Batch (considering grad accum): 49,  Loss: 11.0771, Time: 2.94s, Token/s: 173.86
Epoch: 0, Step: 398, Batch(micro): 398, Batch (considering grad accum): 49,  Loss: 10.9892, Time: 2.90s, Token/s: 176.51
Epoch: 0, Step: 399, Batch(micro): 399, Batch (considering grad accum): 49,  Loss: 10.9983, Time: 4.27s, Token/s: 119.99
Updating MLP bias
Epoch: 0, Step: 400, Batch(micro): 400, Batch (considering grad accum): 50,  Loss: 10.9761, Time: 3.39s, Token/s: 151.23
Epoch: 0, Step: 401, Batch(micro): 401, Batch (considering grad accum): 50,  Loss: 10.9664, Time: 3.04s, Token/s: 168.59
Epoch: 0, Step: 402, Batch(micro): 402, Batch (considering grad accum): 50,  Loss: 11.1486, Time: 3.10s, Token/s: 164.95
Epoch: 0, Step: 403, Batch(micro): 403, Batch (considering grad accum): 50,  Loss: 11.1473, Time: 3.55s, Token/s: 144.43
Epoch: 0, Step: 404, Batch(micro): 404, Batch (considering grad accum): 50,  Loss: 10.9693, Time: 2.95s, Token/s: 173.52
Epoch: 0, Step: 405, Batch(micro): 405, Batch (considering grad accum): 50,  Loss: 11.0435, Time: 2.90s, Token/s: 176.64
Epoch: 0, Step: 406, Batch(micro): 406, Batch (considering grad accum): 50,  Loss: 11.1118, Time: 2.90s, Token/s: 176.80
Epoch: 0, Step: 407, Batch(micro): 407, Batch (considering grad accum): 50,  Loss: 11.1020, Time: 11.79s, Token/s: 43.41
Epoch: 0, Step: 408, Batch(micro): 408, Batch (considering grad accum): 51,  Loss: 10.9844, Time: 4.83s, Token/s: 106.08
Epoch: 0, Step: 409, Batch(micro): 409, Batch (considering grad accum): 51,  Loss: 10.9146, Time: 3.35s, Token/s: 152.75
Epoch: 0, Step: 410, Batch(micro): 410, Batch (considering grad accum): 51,  Loss: 11.1386, Time: 2.91s, Token/s: 175.64
Epoch: 0, Step: 411, Batch(micro): 411, Batch (considering grad accum): 51,  Loss: 11.0902, Time: 3.01s, Token/s: 170.34
Epoch: 0, Step: 412, Batch(micro): 412, Batch (considering grad accum): 51,  Loss: 11.0983, Time: 2.99s, Token/s: 171.19
Epoch: 0, Step: 413, Batch(micro): 413, Batch (considering grad accum): 51,  Loss: 11.0103, Time: 2.98s, Token/s: 171.66
Epoch: 0, Step: 414, Batch(micro): 414, Batch (considering grad accum): 51,  Loss: 11.0060, Time: 2.89s, Token/s: 176.87
Epoch: 0, Step: 415, Batch(micro): 415, Batch (considering grad accum): 51,  Loss: 11.0482, Time: 13.49s, Token/s: 37.95
Epoch: 0, Step: 416, Batch(micro): 416, Batch (considering grad accum): 52,  Loss: 10.8310, Time: 4.47s, Token/s: 114.62
Epoch: 0, Step: 417, Batch(micro): 417, Batch (considering grad accum): 52,  Loss: 10.8199, Time: 3.18s, Token/s: 160.78
Epoch: 0, Step: 418, Batch(micro): 418, Batch (considering grad accum): 52,  Loss: 10.8533, Time: 3.01s, Token/s: 170.08
Epoch: 0, Step: 419, Batch(micro): 419, Batch (considering grad accum): 52,  Loss: 10.9692, Time: 3.00s, Token/s: 170.89
Epoch: 0, Step: 420, Batch(micro): 420, Batch (considering grad accum): 52,  Loss: 11.1719, Time: 3.26s, Token/s: 157.27
Epoch: 0, Step: 421, Batch(micro): 421, Batch (considering grad accum): 52,  Loss: 11.1454, Time: 2.93s, Token/s: 174.55
Epoch: 0, Step: 422, Batch(micro): 422, Batch (considering grad accum): 52,  Loss: 11.0610, Time: 2.95s, Token/s: 173.46
Epoch: 0, Step: 423, Batch(micro): 423, Batch (considering grad accum): 52,  Loss: 11.1335, Time: 4.69s, Token/s: 109.13
Epoch: 0, Step: 424, Batch(micro): 424, Batch (considering grad accum): 53,  Loss: 10.9632, Time: 3.47s, Token/s: 147.76
Epoch: 0, Step: 425, Batch(micro): 425, Batch (considering grad accum): 53,  Loss: 10.9693, Time: 3.17s, Token/s: 161.39
Epoch: 0, Step: 426, Batch(micro): 426, Batch (considering grad accum): 53,  Loss: 10.8316, Time: 3.28s, Token/s: 156.14
Epoch: 0, Step: 427, Batch(micro): 427, Batch (considering grad accum): 53,  Loss: 11.0311, Time: 3.11s, Token/s: 164.59
Epoch: 0, Step: 428, Batch(micro): 428, Batch (considering grad accum): 53,  Loss: 10.9755, Time: 2.98s, Token/s: 172.05
Epoch: 0, Step: 429, Batch(micro): 429, Batch (considering grad accum): 53,  Loss: 10.9071, Time: 2.97s, Token/s: 172.21
Epoch: 0, Step: 430, Batch(micro): 430, Batch (considering grad accum): 53,  Loss: 10.9091, Time: 2.99s, Token/s: 171.46
Epoch: 0, Step: 431, Batch(micro): 431, Batch (considering grad accum): 53,  Loss: 11.2092, Time: 4.12s, Token/s: 124.36
Epoch: 0, Step: 432, Batch(micro): 432, Batch (considering grad accum): 54,  Loss: 11.1200, Time: 3.03s, Token/s: 168.93
Epoch: 0, Step: 433, Batch(micro): 433, Batch (considering grad accum): 54,  Loss: 10.8515, Time: 3.40s, Token/s: 150.42
Epoch: 0, Step: 434, Batch(micro): 434, Batch (considering grad accum): 54,  Loss: 10.6944, Time: 3.16s, Token/s: 162.27
Epoch: 0, Step: 435, Batch(micro): 435, Batch (considering grad accum): 54,  Loss: 10.8910, Time: 2.93s, Token/s: 174.60
Epoch: 0, Step: 436, Batch(micro): 436, Batch (considering grad accum): 54,  Loss: 10.9858, Time: 3.14s, Token/s: 163.25
Epoch: 0, Step: 437, Batch(micro): 437, Batch (considering grad accum): 54,  Loss: 10.9904, Time: 2.94s, Token/s: 173.89
Epoch: 0, Step: 438, Batch(micro): 438, Batch (considering grad accum): 54,  Loss: 10.8337, Time: 3.00s, Token/s: 170.71
Epoch: 0, Step: 439, Batch(micro): 439, Batch (considering grad accum): 54,  Loss: 10.8455, Time: 3.94s, Token/s: 130.06
Epoch: 0, Step: 440, Batch(micro): 440, Batch (considering grad accum): 55,  Loss: 10.8848, Time: 3.29s, Token/s: 155.52
Epoch: 0, Step: 441, Batch(micro): 441, Batch (considering grad accum): 55,  Loss: 11.0859, Time: 3.12s, Token/s: 163.92
Epoch: 0, Step: 442, Batch(micro): 442, Batch (considering grad accum): 55,  Loss: 10.9357, Time: 3.18s, Token/s: 161.24
Epoch: 0, Step: 443, Batch(micro): 443, Batch (considering grad accum): 55,  Loss: 10.9057, Time: 3.15s, Token/s: 162.75
Epoch: 0, Step: 444, Batch(micro): 444, Batch (considering grad accum): 55,  Loss: 10.9391, Time: 3.05s, Token/s: 167.69
Epoch: 0, Step: 445, Batch(micro): 445, Batch (considering grad accum): 55,  Loss: 10.9782, Time: 2.97s, Token/s: 172.40
Epoch: 0, Step: 446, Batch(micro): 446, Batch (considering grad accum): 55,  Loss: 10.9878, Time: 3.00s, Token/s: 170.45
Epoch: 0, Step: 447, Batch(micro): 447, Batch (considering grad accum): 55,  Loss: 10.8277, Time: 3.94s, Token/s: 129.92
Epoch: 0, Step: 448, Batch(micro): 448, Batch (considering grad accum): 56,  Loss: 10.8438, Time: 3.67s, Token/s: 139.55
Epoch: 0, Step: 449, Batch(micro): 449, Batch (considering grad accum): 56,  Loss: 10.8856, Time: 3.35s, Token/s: 152.69
Epoch: 0, Step: 450, Batch(micro): 450, Batch (considering grad accum): 56,  Loss: 10.8766, Time: 3.32s, Token/s: 154.12
Epoch: 0, Step: 451, Batch(micro): 451, Batch (considering grad accum): 56,  Loss: 10.9484, Time: 3.06s, Token/s: 167.46
Epoch: 0, Step: 452, Batch(micro): 452, Batch (considering grad accum): 56,  Loss: 10.9812, Time: 2.99s, Token/s: 171.01
Epoch: 0, Step: 453, Batch(micro): 453, Batch (considering grad accum): 56,  Loss: 11.0073, Time: 3.06s, Token/s: 167.56
Epoch: 0, Step: 454, Batch(micro): 454, Batch (considering grad accum): 56,  Loss: 10.8920, Time: 2.93s, Token/s: 174.78
Epoch: 0, Step: 455, Batch(micro): 455, Batch (considering grad accum): 56,  Loss: 10.8443, Time: 5.54s, Token/s: 92.36
Epoch: 0, Step: 456, Batch(micro): 456, Batch (considering grad accum): 57,  Loss: 10.9508, Time: 3.42s, Token/s: 149.58
Epoch: 0, Step: 457, Batch(micro): 457, Batch (considering grad accum): 57,  Loss: 10.8790, Time: 3.38s, Token/s: 151.39
Epoch: 0, Step: 458, Batch(micro): 458, Batch (considering grad accum): 57,  Loss: 10.8824, Time: 3.08s, Token/s: 166.00
Epoch: 0, Step: 459, Batch(micro): 459, Batch (considering grad accum): 57,  Loss: 10.9244, Time: 2.96s, Token/s: 172.90
Epoch: 0, Step: 460, Batch(micro): 460, Batch (considering grad accum): 57,  Loss: 10.8280, Time: 3.01s, Token/s: 169.97
Epoch: 0, Step: 461, Batch(micro): 461, Batch (considering grad accum): 57,  Loss: 10.9314, Time: 3.01s, Token/s: 170.21
Epoch: 0, Step: 462, Batch(micro): 462, Batch (considering grad accum): 57,  Loss: 10.8666, Time: 3.02s, Token/s: 169.68
Epoch: 0, Step: 463, Batch(micro): 463, Batch (considering grad accum): 57,  Loss: 10.8442, Time: 6.43s, Token/s: 79.64
Epoch: 0, Step: 464, Batch(micro): 464, Batch (considering grad accum): 58,  Loss: 10.9624, Time: 3.77s, Token/s: 135.99
Epoch: 0, Step: 465, Batch(micro): 465, Batch (considering grad accum): 58,  Loss: 10.8767, Time: 3.92s, Token/s: 130.65
Epoch: 0, Step: 466, Batch(micro): 466, Batch (considering grad accum): 58,  Loss: 10.9835, Time: 3.04s, Token/s: 168.61
Epoch: 0, Step: 467, Batch(micro): 467, Batch (considering grad accum): 58,  Loss: 10.7940, Time: 3.10s, Token/s: 165.39
Epoch: 0, Step: 468, Batch(micro): 468, Batch (considering grad accum): 58,  Loss: 10.8358, Time: 2.94s, Token/s: 174.28
Epoch: 0, Step: 469, Batch(micro): 469, Batch (considering grad accum): 58,  Loss: 10.9371, Time: 2.88s, Token/s: 177.97
Epoch: 0, Step: 470, Batch(micro): 470, Batch (considering grad accum): 58,  Loss: 10.8713, Time: 2.91s, Token/s: 175.91
Epoch: 0, Step: 471, Batch(micro): 471, Batch (considering grad accum): 58,  Loss: 10.9579, Time: 4.04s, Token/s: 126.79
Epoch: 0, Step: 472, Batch(micro): 472, Batch (considering grad accum): 59,  Loss: 10.5445, Time: 3.68s, Token/s: 139.23
Epoch: 0, Step: 473, Batch(micro): 473, Batch (considering grad accum): 59,  Loss: 10.8793, Time: 3.28s, Token/s: 155.92
Epoch: 0, Step: 474, Batch(micro): 474, Batch (considering grad accum): 59,  Loss: 10.8655, Time: 3.06s, Token/s: 167.39
Epoch: 0, Step: 475, Batch(micro): 475, Batch (considering grad accum): 59,  Loss: 10.8254, Time: 2.97s, Token/s: 172.56
Epoch: 0, Step: 476, Batch(micro): 476, Batch (considering grad accum): 59,  Loss: 10.8961, Time: 3.00s, Token/s: 170.56
Epoch: 0, Step: 477, Batch(micro): 477, Batch (considering grad accum): 59,  Loss: 10.8475, Time: 3.04s, Token/s: 168.53
Epoch: 0, Step: 478, Batch(micro): 478, Batch (considering grad accum): 59,  Loss: 10.8848, Time: 2.99s, Token/s: 171.05
Epoch: 0, Step: 479, Batch(micro): 479, Batch (considering grad accum): 59,  Loss: 10.6656, Time: 4.35s, Token/s: 117.78
Epoch: 0, Step: 480, Batch(micro): 480, Batch (considering grad accum): 60,  Loss: 10.8097, Time: 3.58s, Token/s: 143.10
Epoch: 0, Step: 481, Batch(micro): 481, Batch (considering grad accum): 60,  Loss: 10.8748, Time: 3.57s, Token/s: 143.34
Epoch: 0, Step: 482, Batch(micro): 482, Batch (considering grad accum): 60,  Loss: 10.9307, Time: 3.04s, Token/s: 168.41
Epoch: 0, Step: 483, Batch(micro): 483, Batch (considering grad accum): 60,  Loss: 10.8021, Time: 2.98s, Token/s: 171.53
Epoch: 0, Step: 484, Batch(micro): 484, Batch (considering grad accum): 60,  Loss: 10.9359, Time: 3.10s, Token/s: 165.20
Epoch: 0, Step: 485, Batch(micro): 485, Batch (considering grad accum): 60,  Loss: 10.8209, Time: 3.11s, Token/s: 164.60
Epoch: 0, Step: 486, Batch(micro): 486, Batch (considering grad accum): 60,  Loss: 10.8874, Time: 2.95s, Token/s: 173.60
Epoch: 0, Step: 487, Batch(micro): 487, Batch (considering grad accum): 60,  Loss: 10.8577, Time: 5.35s, Token/s: 95.72
Epoch: 0, Step: 488, Batch(micro): 488, Batch (considering grad accum): 61,  Loss: 10.6988, Time: 3.57s, Token/s: 143.32
Epoch: 0, Step: 489, Batch(micro): 489, Batch (considering grad accum): 61,  Loss: 10.8732, Time: 3.37s, Token/s: 151.79
Epoch: 0, Step: 490, Batch(micro): 490, Batch (considering grad accum): 61,  Loss: 10.8708, Time: 3.37s, Token/s: 151.96
Epoch: 0, Step: 491, Batch(micro): 491, Batch (considering grad accum): 61,  Loss: 10.7877, Time: 3.31s, Token/s: 154.68
Epoch: 0, Step: 492, Batch(micro): 492, Batch (considering grad accum): 61,  Loss: 11.1284, Time: 3.22s, Token/s: 159.08
Epoch: 0, Step: 493, Batch(micro): 493, Batch (considering grad accum): 61,  Loss: 10.8511, Time: 3.19s, Token/s: 160.71
Epoch: 0, Step: 494, Batch(micro): 494, Batch (considering grad accum): 61,  Loss: 10.8408, Time: 3.02s, Token/s: 169.34
Epoch: 0, Step: 495, Batch(micro): 495, Batch (considering grad accum): 61,  Loss: 10.6609, Time: 4.90s, Token/s: 104.40
Epoch: 0, Step: 496, Batch(micro): 496, Batch (considering grad accum): 62,  Loss: 10.5204, Time: 3.93s, Token/s: 130.23
Epoch: 0, Step: 497, Batch(micro): 497, Batch (considering grad accum): 62,  Loss: 10.8114, Time: 3.36s, Token/s: 152.30
Epoch: 0, Step: 498, Batch(micro): 498, Batch (considering grad accum): 62,  Loss: 10.6672, Time: 3.06s, Token/s: 167.13
Epoch: 0, Step: 499, Batch(micro): 499, Batch (considering grad accum): 62,  Loss: 10.7995, Time: 2.97s, Token/s: 172.21
Updating MLP bias
Epoch: 0, Step: 500, Batch(micro): 500, Batch (considering grad accum): 62,  Loss: 10.6088, Time: 3.00s, Token/s: 170.86
Epoch: 0, Step: 501, Batch(micro): 501, Batch (considering grad accum): 62,  Loss: 10.6087, Time: 3.64s, Token/s: 140.55
Epoch: 0, Step: 502, Batch(micro): 502, Batch (considering grad accum): 62,  Loss: 10.6778, Time: 3.28s, Token/s: 156.15
Epoch: 0, Step: 503, Batch(micro): 503, Batch (considering grad accum): 62,  Loss: 10.6459, Time: 5.01s, Token/s: 102.18
Epoch: 0, Step: 504, Batch(micro): 504, Batch (considering grad accum): 63,  Loss: 10.6128, Time: 3.88s, Token/s: 131.82
Epoch: 0, Step: 505, Batch(micro): 505, Batch (considering grad accum): 63,  Loss: 10.6240, Time: 3.19s, Token/s: 160.55
Epoch: 0, Step: 506, Batch(micro): 506, Batch (considering grad accum): 63,  Loss: 10.7391, Time: 2.88s, Token/s: 177.63
Epoch: 0, Step: 507, Batch(micro): 507, Batch (considering grad accum): 63,  Loss: 10.7169, Time: 2.99s, Token/s: 171.46
Epoch: 0, Step: 508, Batch(micro): 508, Batch (considering grad accum): 63,  Loss: 10.7069, Time: 2.91s, Token/s: 176.04
Epoch: 0, Step: 509, Batch(micro): 509, Batch (considering grad accum): 63,  Loss: 10.6465, Time: 3.13s, Token/s: 163.32
Epoch: 0, Step: 510, Batch(micro): 510, Batch (considering grad accum): 63,  Loss: 10.6209, Time: 2.93s, Token/s: 174.62
Epoch: 0, Step: 511, Batch(micro): 511, Batch (considering grad accum): 63,  Loss: 10.9670, Time: 3.90s, Token/s: 131.36
Epoch: 0, Step: 512, Batch(micro): 512, Batch (considering grad accum): 64,  Loss: 10.9745, Time: 2.98s, Token/s: 171.77
Epoch: 0, Step: 513, Batch(micro): 513, Batch (considering grad accum): 64,  Loss: 10.8293, Time: 3.21s, Token/s: 159.37
Epoch: 0, Step: 514, Batch(micro): 514, Batch (considering grad accum): 64,  Loss: 10.6034, Time: 3.09s, Token/s: 165.88
Epoch: 0, Step: 515, Batch(micro): 515, Batch (considering grad accum): 64,  Loss: 10.5425, Time: 3.07s, Token/s: 167.04
Epoch: 0, Step: 516, Batch(micro): 516, Batch (considering grad accum): 64,  Loss: 10.8255, Time: 2.96s, Token/s: 173.15
Epoch: 0, Step: 517, Batch(micro): 517, Batch (considering grad accum): 64,  Loss: 10.6995, Time: 2.95s, Token/s: 173.68
Epoch: 0, Step: 518, Batch(micro): 518, Batch (considering grad accum): 64,  Loss: 10.5076, Time: 2.97s, Token/s: 172.36
Epoch: 0, Step: 519, Batch(micro): 519, Batch (considering grad accum): 64,  Loss: 10.5571, Time: 4.20s, Token/s: 121.87
Epoch: 0, Step: 520, Batch(micro): 520, Batch (considering grad accum): 65,  Loss: 10.6065, Time: 3.60s, Token/s: 142.25
Epoch: 0, Step: 521, Batch(micro): 521, Batch (considering grad accum): 65,  Loss: 10.5661, Time: 3.08s, Token/s: 166.38
Epoch: 0, Step: 522, Batch(micro): 522, Batch (considering grad accum): 65,  Loss: 10.6252, Time: 3.24s, Token/s: 157.88
Epoch: 0, Step: 523, Batch(micro): 523, Batch (considering grad accum): 65,  Loss: 10.7136, Time: 3.03s, Token/s: 169.19
Epoch: 0, Step: 524, Batch(micro): 524, Batch (considering grad accum): 65,  Loss: 10.6442, Time: 3.11s, Token/s: 164.77
Epoch: 0, Step: 525, Batch(micro): 525, Batch (considering grad accum): 65,  Loss: 10.7102, Time: 2.93s, Token/s: 175.01
Epoch: 0, Step: 526, Batch(micro): 526, Batch (considering grad accum): 65,  Loss: 10.9613, Time: 2.98s, Token/s: 171.91
Epoch: 0, Step: 527, Batch(micro): 527, Batch (considering grad accum): 65,  Loss: 10.6599, Time: 4.21s, Token/s: 121.62
Epoch: 0, Step: 528, Batch(micro): 528, Batch (considering grad accum): 66,  Loss: 10.7189, Time: 3.91s, Token/s: 130.83
Epoch: 0, Step: 529, Batch(micro): 529, Batch (considering grad accum): 66,  Loss: 10.6776, Time: 3.20s, Token/s: 160.02
Epoch: 0, Step: 530, Batch(micro): 530, Batch (considering grad accum): 66,  Loss: 10.6882, Time: 2.97s, Token/s: 172.58
Epoch: 0, Step: 531, Batch(micro): 531, Batch (considering grad accum): 66,  Loss: 10.5697, Time: 2.93s, Token/s: 174.79
Epoch: 0, Step: 532, Batch(micro): 532, Batch (considering grad accum): 66,  Loss: 10.6808, Time: 2.91s, Token/s: 176.05
Epoch: 0, Step: 533, Batch(micro): 533, Batch (considering grad accum): 66,  Loss: 10.5340, Time: 2.94s, Token/s: 174.35
Epoch: 0, Step: 534, Batch(micro): 534, Batch (considering grad accum): 66,  Loss: 10.6732, Time: 3.00s, Token/s: 170.78
Epoch: 0, Step: 535, Batch(micro): 535, Batch (considering grad accum): 66,  Loss: 10.4445, Time: 4.74s, Token/s: 108.09
Epoch: 0, Step: 536, Batch(micro): 536, Batch (considering grad accum): 67,  Loss: 10.3704, Time: 3.27s, Token/s: 156.43
Epoch: 0, Step: 537, Batch(micro): 537, Batch (considering grad accum): 67,  Loss: 10.2616, Time: 4.21s, Token/s: 121.69
Epoch: 0, Step: 538, Batch(micro): 538, Batch (considering grad accum): 67,  Loss: 10.5969, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 539, Batch(micro): 539, Batch (considering grad accum): 67,  Loss: 10.7308, Time: 3.39s, Token/s: 150.90
Epoch: 0, Step: 540, Batch(micro): 540, Batch (considering grad accum): 67,  Loss: 10.7864, Time: 3.42s, Token/s: 149.65
Epoch: 0, Step: 541, Batch(micro): 541, Batch (considering grad accum): 67,  Loss: 10.6253, Time: 3.13s, Token/s: 163.56
Epoch: 0, Step: 542, Batch(micro): 542, Batch (considering grad accum): 67,  Loss: 10.4390, Time: 3.03s, Token/s: 168.80
Epoch: 0, Step: 543, Batch(micro): 543, Batch (considering grad accum): 67,  Loss: 10.4829, Time: 5.33s, Token/s: 95.97
Epoch: 0, Step: 544, Batch(micro): 544, Batch (considering grad accum): 68,  Loss: 10.5174, Time: 3.78s, Token/s: 135.43
Epoch: 0, Step: 545, Batch(micro): 545, Batch (considering grad accum): 68,  Loss: 10.5788, Time: 3.36s, Token/s: 152.36
Epoch: 0, Step: 546, Batch(micro): 546, Batch (considering grad accum): 68,  Loss: 10.3174, Time: 3.02s, Token/s: 169.30
Epoch: 0, Step: 547, Batch(micro): 547, Batch (considering grad accum): 68,  Loss: 10.4691, Time: 3.02s, Token/s: 169.31
Epoch: 0, Step: 548, Batch(micro): 548, Batch (considering grad accum): 68,  Loss: 10.7822, Time: 3.15s, Token/s: 162.56
Epoch: 0, Step: 549, Batch(micro): 549, Batch (considering grad accum): 68,  Loss: 10.4097, Time: 3.28s, Token/s: 155.90
Epoch: 0, Step: 550, Batch(micro): 550, Batch (considering grad accum): 68,  Loss: 10.5532, Time: 3.39s, Token/s: 151.11
Epoch: 0, Step: 551, Batch(micro): 551, Batch (considering grad accum): 68,  Loss: 10.6891, Time: 12.35s, Token/s: 41.45
Epoch: 0, Step: 552, Batch(micro): 552, Batch (considering grad accum): 69,  Loss: 10.3380, Time: 5.09s, Token/s: 100.59
Epoch: 0, Step: 553, Batch(micro): 553, Batch (considering grad accum): 69,  Loss: 10.4244, Time: 3.87s, Token/s: 132.19
Epoch: 0, Step: 554, Batch(micro): 554, Batch (considering grad accum): 69,  Loss: 10.2836, Time: 3.08s, Token/s: 166.35
Epoch: 0, Step: 555, Batch(micro): 555, Batch (considering grad accum): 69,  Loss: 10.4542, Time: 3.00s, Token/s: 170.86
Epoch: 0, Step: 556, Batch(micro): 556, Batch (considering grad accum): 69,  Loss: 10.6850, Time: 3.01s, Token/s: 170.03
Epoch: 0, Step: 557, Batch(micro): 557, Batch (considering grad accum): 69,  Loss: 10.2791, Time: 3.43s, Token/s: 149.48
Epoch: 0, Step: 558, Batch(micro): 558, Batch (considering grad accum): 69,  Loss: 10.4795, Time: 3.40s, Token/s: 150.46
Epoch: 0, Step: 559, Batch(micro): 559, Batch (considering grad accum): 69,  Loss: 10.1875, Time: 18.85s, Token/s: 27.16
Epoch: 0, Step: 560, Batch(micro): 560, Batch (considering grad accum): 70,  Loss: 10.4551, Time: 4.83s, Token/s: 106.08
Epoch: 0, Step: 561, Batch(micro): 561, Batch (considering grad accum): 70,  Loss: 10.2052, Time: 3.75s, Token/s: 136.40
Epoch: 0, Step: 562, Batch(micro): 562, Batch (considering grad accum): 70,  Loss: 10.3141, Time: 3.09s, Token/s: 165.64
Epoch: 0, Step: 563, Batch(micro): 563, Batch (considering grad accum): 70,  Loss: 10.3825, Time: 3.31s, Token/s: 154.91
Epoch: 0, Step: 564, Batch(micro): 564, Batch (considering grad accum): 70,  Loss: 10.3227, Time: 3.06s, Token/s: 167.16
Epoch: 0, Step: 565, Batch(micro): 565, Batch (considering grad accum): 70,  Loss: 10.0709, Time: 2.93s, Token/s: 174.74
Epoch: 0, Step: 566, Batch(micro): 566, Batch (considering grad accum): 70,  Loss: 10.4787, Time: 3.41s, Token/s: 150.15
Epoch: 0, Step: 567, Batch(micro): 567, Batch (considering grad accum): 70,  Loss: 10.4152, Time: 17.40s, Token/s: 29.43
Epoch: 0, Step: 568, Batch(micro): 568, Batch (considering grad accum): 71,  Loss: 9.9726, Time: 4.26s, Token/s: 120.29
Epoch: 0, Step: 569, Batch(micro): 569, Batch (considering grad accum): 71,  Loss: 10.1726, Time: 3.44s, Token/s: 149.05
Epoch: 0, Step: 570, Batch(micro): 570, Batch (considering grad accum): 71,  Loss: 10.0165, Time: 3.44s, Token/s: 148.75
Epoch: 0, Step: 571, Batch(micro): 571, Batch (considering grad accum): 71,  Loss: 9.9416, Time: 3.39s, Token/s: 150.85
Epoch: 0, Step: 572, Batch(micro): 572, Batch (considering grad accum): 71,  Loss: 10.3798, Time: 3.70s, Token/s: 138.37
Epoch: 0, Step: 573, Batch(micro): 573, Batch (considering grad accum): 71,  Loss: 10.2921, Time: 3.28s, Token/s: 156.33
Epoch: 0, Step: 574, Batch(micro): 574, Batch (considering grad accum): 71,  Loss: 10.1968, Time: 3.22s, Token/s: 158.83
Epoch: 0, Step: 575, Batch(micro): 575, Batch (considering grad accum): 71,  Loss: 10.3087, Time: 15.49s, Token/s: 33.05
Epoch: 0, Step: 576, Batch(micro): 576, Batch (considering grad accum): 72,  Loss: 9.9852, Time: 4.47s, Token/s: 114.61
Epoch: 0, Step: 577, Batch(micro): 577, Batch (considering grad accum): 72,  Loss: 10.4368, Time: 3.28s, Token/s: 156.22
Epoch: 0, Step: 578, Batch(micro): 578, Batch (considering grad accum): 72,  Loss: 10.1116, Time: 3.02s, Token/s: 169.64
Epoch: 0, Step: 579, Batch(micro): 579, Batch (considering grad accum): 72,  Loss: 10.2458, Time: 2.96s, Token/s: 172.75
Epoch: 0, Step: 580, Batch(micro): 580, Batch (considering grad accum): 72,  Loss: 9.9813, Time: 2.92s, Token/s: 175.11
Epoch: 0, Step: 581, Batch(micro): 581, Batch (considering grad accum): 72,  Loss: 10.1923, Time: 2.98s, Token/s: 172.01
Epoch: 0, Step: 582, Batch(micro): 582, Batch (considering grad accum): 72,  Loss: 10.2495, Time: 2.99s, Token/s: 171.39
Epoch: 0, Step: 583, Batch(micro): 583, Batch (considering grad accum): 72,  Loss: 10.1089, Time: 6.13s, Token/s: 83.56
Epoch: 0, Step: 584, Batch(micro): 584, Batch (considering grad accum): 73,  Loss: 10.0725, Time: 3.52s, Token/s: 145.61
Epoch: 0, Step: 585, Batch(micro): 585, Batch (considering grad accum): 73,  Loss: 9.9795, Time: 3.38s, Token/s: 151.61
Epoch: 0, Step: 586, Batch(micro): 586, Batch (considering grad accum): 73,  Loss: 10.1944, Time: 3.39s, Token/s: 150.98
Epoch: 0, Step: 587, Batch(micro): 587, Batch (considering grad accum): 73,  Loss: 9.9991, Time: 3.23s, Token/s: 158.60
Epoch: 0, Step: 588, Batch(micro): 588, Batch (considering grad accum): 73,  Loss: 9.9868, Time: 3.27s, Token/s: 156.56
Epoch: 0, Step: 589, Batch(micro): 589, Batch (considering grad accum): 73,  Loss: 10.3349, Time: 3.00s, Token/s: 170.73
Epoch: 0, Step: 590, Batch(micro): 590, Batch (considering grad accum): 73,  Loss: 9.9650, Time: 3.25s, Token/s: 157.49
Epoch: 0, Step: 591, Batch(micro): 591, Batch (considering grad accum): 73,  Loss: 10.1208, Time: 4.58s, Token/s: 111.80
Epoch: 0, Step: 592, Batch(micro): 592, Batch (considering grad accum): 74,  Loss: 10.2097, Time: 3.73s, Token/s: 137.20
Epoch: 0, Step: 593, Batch(micro): 593, Batch (considering grad accum): 74,  Loss: 9.8724, Time: 3.39s, Token/s: 151.21
Epoch: 0, Step: 594, Batch(micro): 594, Batch (considering grad accum): 74,  Loss: 10.2033, Time: 3.31s, Token/s: 154.91
Epoch: 0, Step: 595, Batch(micro): 595, Batch (considering grad accum): 74,  Loss: 9.9069, Time: 3.05s, Token/s: 168.10
Epoch: 0, Step: 596, Batch(micro): 596, Batch (considering grad accum): 74,  Loss: 10.0738, Time: 2.94s, Token/s: 174.08
Epoch: 0, Step: 597, Batch(micro): 597, Batch (considering grad accum): 74,  Loss: 10.1259, Time: 3.40s, Token/s: 150.80
Epoch: 0, Step: 598, Batch(micro): 598, Batch (considering grad accum): 74,  Loss: 10.2547, Time: 3.00s, Token/s: 170.53
Epoch: 0, Step: 599, Batch(micro): 599, Batch (considering grad accum): 74,  Loss: 10.3026, Time: 4.41s, Token/s: 116.18
Updating MLP bias
Epoch: 0, Step: 600, Batch(micro): 600, Batch (considering grad accum): 75,  Loss: 9.9750, Time: 3.32s, Token/s: 154.13
Epoch: 0, Step: 601, Batch(micro): 601, Batch (considering grad accum): 75,  Loss: 10.0119, Time: 3.22s, Token/s: 159.11
Epoch: 0, Step: 602, Batch(micro): 602, Batch (considering grad accum): 75,  Loss: 9.9573, Time: 3.54s, Token/s: 144.74
Epoch: 0, Step: 603, Batch(micro): 603, Batch (considering grad accum): 75,  Loss: 10.1444, Time: 3.39s, Token/s: 151.05
Epoch: 0, Step: 604, Batch(micro): 604, Batch (considering grad accum): 75,  Loss: 10.1408, Time: 3.40s, Token/s: 150.61
Epoch: 0, Step: 605, Batch(micro): 605, Batch (considering grad accum): 75,  Loss: 10.1016, Time: 3.28s, Token/s: 156.25
Epoch: 0, Step: 606, Batch(micro): 606, Batch (considering grad accum): 75,  Loss: 9.8521, Time: 3.10s, Token/s: 165.19
Epoch: 0, Step: 607, Batch(micro): 607, Batch (considering grad accum): 75,  Loss: 9.9399, Time: 7.09s, Token/s: 72.19
Epoch: 0, Step: 608, Batch(micro): 608, Batch (considering grad accum): 76,  Loss: 9.9004, Time: 3.89s, Token/s: 131.74
Epoch: 0, Step: 609, Batch(micro): 609, Batch (considering grad accum): 76,  Loss: 10.1735, Time: 3.37s, Token/s: 151.94
Epoch: 0, Step: 610, Batch(micro): 610, Batch (considering grad accum): 76,  Loss: 9.7701, Time: 3.51s, Token/s: 146.00
Epoch: 0, Step: 611, Batch(micro): 611, Batch (considering grad accum): 76,  Loss: 9.7707, Time: 3.48s, Token/s: 147.19
Epoch: 0, Step: 612, Batch(micro): 612, Batch (considering grad accum): 76,  Loss: 9.6548, Time: 3.52s, Token/s: 145.42
Epoch: 0, Step: 613, Batch(micro): 613, Batch (considering grad accum): 76,  Loss: 10.0770, Time: 3.23s, Token/s: 158.57
Epoch: 0, Step: 614, Batch(micro): 614, Batch (considering grad accum): 76,  Loss: 10.0060, Time: 3.06s, Token/s: 167.44
Epoch: 0, Step: 615, Batch(micro): 615, Batch (considering grad accum): 76,  Loss: 9.6920, Time: 5.32s, Token/s: 96.18
Epoch: 0, Step: 616, Batch(micro): 616, Batch (considering grad accum): 77,  Loss: 9.8585, Time: 3.75s, Token/s: 136.68
Epoch: 0, Step: 617, Batch(micro): 617, Batch (considering grad accum): 77,  Loss: 10.0273, Time: 3.59s, Token/s: 142.66
Epoch: 0, Step: 618, Batch(micro): 618, Batch (considering grad accum): 77,  Loss: 9.7268, Time: 3.41s, Token/s: 150.28
Epoch: 0, Step: 619, Batch(micro): 619, Batch (considering grad accum): 77,  Loss: 10.1129, Time: 3.22s, Token/s: 159.12
Epoch: 0, Step: 620, Batch(micro): 620, Batch (considering grad accum): 77,  Loss: 9.9045, Time: 3.34s, Token/s: 153.42
Epoch: 0, Step: 621, Batch(micro): 621, Batch (considering grad accum): 77,  Loss: 9.9222, Time: 3.44s, Token/s: 148.63
Epoch: 0, Step: 622, Batch(micro): 622, Batch (considering grad accum): 77,  Loss: 10.2386, Time: 3.15s, Token/s: 162.53
Epoch: 0, Step: 623, Batch(micro): 623, Batch (considering grad accum): 77,  Loss: 10.3427, Time: 5.95s, Token/s: 86.01
Epoch: 0, Step: 624, Batch(micro): 624, Batch (considering grad accum): 78,  Loss: 10.2264, Time: 3.87s, Token/s: 132.30
Epoch: 0, Step: 625, Batch(micro): 625, Batch (considering grad accum): 78,  Loss: 10.0356, Time: 3.50s, Token/s: 146.18
Epoch: 0, Step: 626, Batch(micro): 626, Batch (considering grad accum): 78,  Loss: 9.6785, Time: 3.13s, Token/s: 163.53
Epoch: 0, Step: 627, Batch(micro): 627, Batch (considering grad accum): 78,  Loss: 9.8315, Time: 2.96s, Token/s: 173.19
Epoch: 0, Step: 628, Batch(micro): 628, Batch (considering grad accum): 78,  Loss: 9.9677, Time: 3.00s, Token/s: 170.57
Epoch: 0, Step: 629, Batch(micro): 629, Batch (considering grad accum): 78,  Loss: 10.0708, Time: 3.13s, Token/s: 163.47
Epoch: 0, Step: 630, Batch(micro): 630, Batch (considering grad accum): 78,  Loss: 9.8099, Time: 3.15s, Token/s: 162.36
Epoch: 0, Step: 631, Batch(micro): 631, Batch (considering grad accum): 78,  Loss: 9.6107, Time: 5.81s, Token/s: 88.12
Epoch: 0, Step: 632, Batch(micro): 632, Batch (considering grad accum): 79,  Loss: 9.7195, Time: 4.11s, Token/s: 124.72
Epoch: 0, Step: 633, Batch(micro): 633, Batch (considering grad accum): 79,  Loss: 9.8122, Time: 3.88s, Token/s: 131.92
Epoch: 0, Step: 634, Batch(micro): 634, Batch (considering grad accum): 79,  Loss: 9.5324, Time: 3.44s, Token/s: 148.94
Epoch: 0, Step: 635, Batch(micro): 635, Batch (considering grad accum): 79,  Loss: 9.7564, Time: 3.11s, Token/s: 164.49
Epoch: 0, Step: 636, Batch(micro): 636, Batch (considering grad accum): 79,  Loss: 9.9008, Time: 3.04s, Token/s: 168.65
Epoch: 0, Step: 637, Batch(micro): 637, Batch (considering grad accum): 79,  Loss: 10.2123, Time: 2.95s, Token/s: 173.60
Epoch: 0, Step: 638, Batch(micro): 638, Batch (considering grad accum): 79,  Loss: 10.1257, Time: 3.09s, Token/s: 165.45
Epoch: 0, Step: 639, Batch(micro): 639, Batch (considering grad accum): 79,  Loss: 9.6899, Time: 18.68s, Token/s: 27.41
Epoch: 0, Step: 640, Batch(micro): 640, Batch (considering grad accum): 80,  Loss: 9.8819, Time: 5.88s, Token/s: 87.14
Epoch: 0, Step: 641, Batch(micro): 641, Batch (considering grad accum): 80,  Loss: 10.0984, Time: 3.76s, Token/s: 136.34
Epoch: 0, Step: 642, Batch(micro): 642, Batch (considering grad accum): 80,  Loss: 10.1942, Time: 3.36s, Token/s: 152.39
Epoch: 0, Step: 643, Batch(micro): 643, Batch (considering grad accum): 80,  Loss: 9.5173, Time: 3.26s, Token/s: 156.95
Epoch: 0, Step: 644, Batch(micro): 644, Batch (considering grad accum): 80,  Loss: 9.5548, Time: 3.42s, Token/s: 149.81
Epoch: 0, Step: 645, Batch(micro): 645, Batch (considering grad accum): 80,  Loss: 9.6846, Time: 3.36s, Token/s: 152.51
Epoch: 0, Step: 646, Batch(micro): 646, Batch (considering grad accum): 80,  Loss: 9.7820, Time: 3.23s, Token/s: 158.52
Epoch: 0, Step: 647, Batch(micro): 647, Batch (considering grad accum): 80,  Loss: 9.8028, Time: 26.74s, Token/s: 19.15
Epoch: 0, Step: 648, Batch(micro): 648, Batch (considering grad accum): 81,  Loss: 9.6659, Time: 5.18s, Token/s: 98.93
Epoch: 0, Step: 649, Batch(micro): 649, Batch (considering grad accum): 81,  Loss: 9.8118, Time: 3.29s, Token/s: 155.62
Epoch: 0, Step: 650, Batch(micro): 650, Batch (considering grad accum): 81,  Loss: 9.6338, Time: 3.46s, Token/s: 148.15
Epoch: 0, Step: 651, Batch(micro): 651, Batch (considering grad accum): 81,  Loss: 9.2389, Time: 3.06s, Token/s: 167.54
Epoch: 0, Step: 652, Batch(micro): 652, Batch (considering grad accum): 81,  Loss: 9.7614, Time: 3.37s, Token/s: 152.02
Epoch: 0, Step: 653, Batch(micro): 653, Batch (considering grad accum): 81,  Loss: 10.0359, Time: 3.41s, Token/s: 150.11
Epoch: 0, Step: 654, Batch(micro): 654, Batch (considering grad accum): 81,  Loss: 9.4016, Time: 3.62s, Token/s: 141.60
Epoch: 0, Step: 655, Batch(micro): 655, Batch (considering grad accum): 81,  Loss: 9.7454, Time: 26.06s, Token/s: 19.64
Epoch: 0, Step: 656, Batch(micro): 656, Batch (considering grad accum): 82,  Loss: 9.8849, Time: 5.58s, Token/s: 91.74
Epoch: 0, Step: 657, Batch(micro): 657, Batch (considering grad accum): 82,  Loss: 10.2664, Time: 3.18s, Token/s: 160.85
Epoch: 0, Step: 658, Batch(micro): 658, Batch (considering grad accum): 82,  Loss: 9.6566, Time: 3.31s, Token/s: 154.88
Epoch: 0, Step: 659, Batch(micro): 659, Batch (considering grad accum): 82,  Loss: 9.5553, Time: 2.99s, Token/s: 171.51
Epoch: 0, Step: 660, Batch(micro): 660, Batch (considering grad accum): 82,  Loss: 9.9695, Time: 3.27s, Token/s: 156.35
Epoch: 0, Step: 661, Batch(micro): 661, Batch (considering grad accum): 82,  Loss: 9.4308, Time: 4.38s, Token/s: 116.82
Epoch: 0, Step: 662, Batch(micro): 662, Batch (considering grad accum): 82,  Loss: 9.7335, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 663, Batch(micro): 663, Batch (considering grad accum): 82,  Loss: 9.7612, Time: 26.32s, Token/s: 19.45
Epoch: 0, Step: 664, Batch(micro): 664, Batch (considering grad accum): 83,  Loss: 9.5034, Time: 5.93s, Token/s: 86.36
Epoch: 0, Step: 665, Batch(micro): 665, Batch (considering grad accum): 83,  Loss: 9.6758, Time: 3.45s, Token/s: 148.50
Epoch: 0, Step: 666, Batch(micro): 666, Batch (considering grad accum): 83,  Loss: 9.4348, Time: 3.53s, Token/s: 145.01
Epoch: 0, Step: 667, Batch(micro): 667, Batch (considering grad accum): 83,  Loss: 9.8770, Time: 3.03s, Token/s: 168.87
Epoch: 0, Step: 668, Batch(micro): 668, Batch (considering grad accum): 83,  Loss: 9.7369, Time: 3.07s, Token/s: 166.52
Epoch: 0, Step: 669, Batch(micro): 669, Batch (considering grad accum): 83,  Loss: 9.3805, Time: 3.16s, Token/s: 162.27
Epoch: 0, Step: 670, Batch(micro): 670, Batch (considering grad accum): 83,  Loss: 9.4454, Time: 3.29s, Token/s: 155.54
Epoch: 0, Step: 671, Batch(micro): 671, Batch (considering grad accum): 83,  Loss: 9.5597, Time: 26.20s, Token/s: 19.54
Epoch: 0, Step: 672, Batch(micro): 672, Batch (considering grad accum): 84,  Loss: 9.6475, Time: 5.43s, Token/s: 94.37
Epoch: 0, Step: 673, Batch(micro): 673, Batch (considering grad accum): 84,  Loss: 9.4517, Time: 3.83s, Token/s: 133.76
Epoch: 0, Step: 674, Batch(micro): 674, Batch (considering grad accum): 84,  Loss: 9.8751, Time: 3.47s, Token/s: 147.41
Epoch: 0, Step: 675, Batch(micro): 675, Batch (considering grad accum): 84,  Loss: 9.9164, Time: 3.75s, Token/s: 136.66
Epoch: 0, Step: 676, Batch(micro): 676, Batch (considering grad accum): 84,  Loss: 9.3133, Time: 3.51s, Token/s: 145.80
Epoch: 0, Step: 677, Batch(micro): 677, Batch (considering grad accum): 84,  Loss: 9.5311, Time: 3.25s, Token/s: 157.75
Epoch: 0, Step: 678, Batch(micro): 678, Batch (considering grad accum): 84,  Loss: 9.8359, Time: 3.02s, Token/s: 169.50
Epoch: 0, Step: 679, Batch(micro): 679, Batch (considering grad accum): 84,  Loss: 9.7449, Time: 25.16s, Token/s: 20.35
Epoch: 0, Step: 680, Batch(micro): 680, Batch (considering grad accum): 85,  Loss: 9.6775, Time: 5.80s, Token/s: 88.29
Epoch: 0, Step: 681, Batch(micro): 681, Batch (considering grad accum): 85,  Loss: 9.5500, Time: 3.06s, Token/s: 167.43
Epoch: 0, Step: 682, Batch(micro): 682, Batch (considering grad accum): 85,  Loss: 9.3254, Time: 3.70s, Token/s: 138.20
Epoch: 0, Step: 683, Batch(micro): 683, Batch (considering grad accum): 85,  Loss: 9.2245, Time: 3.53s, Token/s: 145.21
Epoch: 0, Step: 684, Batch(micro): 684, Batch (considering grad accum): 85,  Loss: 9.4692, Time: 3.28s, Token/s: 156.07
Epoch: 0, Step: 685, Batch(micro): 685, Batch (considering grad accum): 85,  Loss: 9.8311, Time: 3.22s, Token/s: 158.80
Epoch: 0, Step: 686, Batch(micro): 686, Batch (considering grad accum): 85,  Loss: 9.8151, Time: 3.35s, Token/s: 152.87
Epoch: 0, Step: 687, Batch(micro): 687, Batch (considering grad accum): 85,  Loss: 9.1785, Time: 25.71s, Token/s: 19.92
Epoch: 0, Step: 688, Batch(micro): 688, Batch (considering grad accum): 86,  Loss: 9.5097, Time: 5.61s, Token/s: 91.29
Epoch: 0, Step: 689, Batch(micro): 689, Batch (considering grad accum): 86,  Loss: 9.6083, Time: 3.26s, Token/s: 156.95
Epoch: 0, Step: 690, Batch(micro): 690, Batch (considering grad accum): 86,  Loss: 9.5577, Time: 3.38s, Token/s: 151.68
Epoch: 0, Step: 691, Batch(micro): 691, Batch (considering grad accum): 86,  Loss: 9.2746, Time: 3.10s, Token/s: 164.97
Epoch: 0, Step: 692, Batch(micro): 692, Batch (considering grad accum): 86,  Loss: 9.5353, Time: 3.36s, Token/s: 152.52
Epoch: 0, Step: 693, Batch(micro): 693, Batch (considering grad accum): 86,  Loss: 9.4455, Time: 3.17s, Token/s: 161.57
Epoch: 0, Step: 694, Batch(micro): 694, Batch (considering grad accum): 86,  Loss: 9.7160, Time: 3.21s, Token/s: 159.48
Epoch: 0, Step: 695, Batch(micro): 695, Batch (considering grad accum): 86,  Loss: 9.5058, Time: 27.02s, Token/s: 18.95
Epoch: 0, Step: 696, Batch(micro): 696, Batch (considering grad accum): 87,  Loss: 9.3921, Time: 5.61s, Token/s: 91.24
Epoch: 0, Step: 697, Batch(micro): 697, Batch (considering grad accum): 87,  Loss: 9.4553, Time: 3.13s, Token/s: 163.82
Epoch: 0, Step: 698, Batch(micro): 698, Batch (considering grad accum): 87,  Loss: 9.6843, Time: 3.36s, Token/s: 152.28
Epoch: 0, Step: 699, Batch(micro): 699, Batch (considering grad accum): 87,  Loss: 9.4788, Time: 3.19s, Token/s: 160.49
Updating MLP bias
Epoch: 0, Step: 700, Batch(micro): 700, Batch (considering grad accum): 87,  Loss: 9.7694, Time: 3.62s, Token/s: 141.36
Epoch: 0, Step: 701, Batch(micro): 701, Batch (considering grad accum): 87,  Loss: 9.2769, Time: 3.82s, Token/s: 134.13
Epoch: 0, Step: 702, Batch(micro): 702, Batch (considering grad accum): 87,  Loss: 9.3290, Time: 3.46s, Token/s: 148.01
Epoch: 0, Step: 703, Batch(micro): 703, Batch (considering grad accum): 87,  Loss: 9.5964, Time: 29.07s, Token/s: 17.61
Epoch: 0, Step: 704, Batch(micro): 704, Batch (considering grad accum): 88,  Loss: 9.3438, Time: 5.44s, Token/s: 94.06
Epoch: 0, Step: 705, Batch(micro): 705, Batch (considering grad accum): 88,  Loss: 8.9350, Time: 3.14s, Token/s: 163.24
Epoch: 0, Step: 706, Batch(micro): 706, Batch (considering grad accum): 88,  Loss: 9.6957, Time: 3.69s, Token/s: 138.72
Epoch: 0, Step: 707, Batch(micro): 707, Batch (considering grad accum): 88,  Loss: 9.2853, Time: 3.30s, Token/s: 155.10
Epoch: 0, Step: 708, Batch(micro): 708, Batch (considering grad accum): 88,  Loss: 9.4885, Time: 3.33s, Token/s: 153.55
Epoch: 0, Step: 709, Batch(micro): 709, Batch (considering grad accum): 88,  Loss: 9.8606, Time: 3.50s, Token/s: 146.19
Epoch: 0, Step: 710, Batch(micro): 710, Batch (considering grad accum): 88,  Loss: 9.6371, Time: 3.70s, Token/s: 138.38
Epoch: 0, Step: 711, Batch(micro): 711, Batch (considering grad accum): 88,  Loss: 9.6104, Time: 26.53s, Token/s: 19.30
Epoch: 0, Step: 712, Batch(micro): 712, Batch (considering grad accum): 89,  Loss: 9.6871, Time: 6.29s, Token/s: 81.39
Epoch: 0, Step: 713, Batch(micro): 713, Batch (considering grad accum): 89,  Loss: 9.8511, Time: 3.21s, Token/s: 159.58
Epoch: 0, Step: 714, Batch(micro): 714, Batch (considering grad accum): 89,  Loss: 9.2852, Time: 3.67s, Token/s: 139.66
Epoch: 0, Step: 715, Batch(micro): 715, Batch (considering grad accum): 89,  Loss: 9.4879, Time: 3.49s, Token/s: 146.92
Epoch: 0, Step: 716, Batch(micro): 716, Batch (considering grad accum): 89,  Loss: 9.2207, Time: 3.48s, Token/s: 147.20
Epoch: 0, Step: 717, Batch(micro): 717, Batch (considering grad accum): 89,  Loss: 9.2255, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 718, Batch(micro): 718, Batch (considering grad accum): 89,  Loss: 9.7653, Time: 3.26s, Token/s: 156.97
Epoch: 0, Step: 719, Batch(micro): 719, Batch (considering grad accum): 89,  Loss: 9.1566, Time: 27.86s, Token/s: 18.38
Epoch: 0, Step: 720, Batch(micro): 720, Batch (considering grad accum): 90,  Loss: 9.6965, Time: 5.27s, Token/s: 97.17
Epoch: 0, Step: 721, Batch(micro): 721, Batch (considering grad accum): 90,  Loss: 9.3711, Time: 3.15s, Token/s: 162.41
Epoch: 0, Step: 722, Batch(micro): 722, Batch (considering grad accum): 90,  Loss: 9.2401, Time: 3.54s, Token/s: 144.72
Epoch: 0, Step: 723, Batch(micro): 723, Batch (considering grad accum): 90,  Loss: 9.1446, Time: 3.51s, Token/s: 145.68
Epoch: 0, Step: 724, Batch(micro): 724, Batch (considering grad accum): 90,  Loss: 9.2243, Time: 3.63s, Token/s: 141.18
Epoch: 0, Step: 725, Batch(micro): 725, Batch (considering grad accum): 90,  Loss: 9.5388, Time: 3.46s, Token/s: 148.08
Epoch: 0, Step: 726, Batch(micro): 726, Batch (considering grad accum): 90,  Loss: 9.4951, Time: 3.24s, Token/s: 158.17
Epoch: 0, Step: 727, Batch(micro): 727, Batch (considering grad accum): 90,  Loss: 9.1655, Time: 24.50s, Token/s: 20.90
Epoch: 0, Step: 728, Batch(micro): 728, Batch (considering grad accum): 91,  Loss: 9.4065, Time: 5.08s, Token/s: 100.83
Epoch: 0, Step: 729, Batch(micro): 729, Batch (considering grad accum): 91,  Loss: 9.6633, Time: 4.01s, Token/s: 127.55
Epoch: 0, Step: 730, Batch(micro): 730, Batch (considering grad accum): 91,  Loss: 8.9512, Time: 3.80s, Token/s: 134.91
Epoch: 0, Step: 731, Batch(micro): 731, Batch (considering grad accum): 91,  Loss: 9.4340, Time: 3.58s, Token/s: 143.00
Epoch: 0, Step: 732, Batch(micro): 732, Batch (considering grad accum): 91,  Loss: 9.3255, Time: 3.80s, Token/s: 134.57
Epoch: 0, Step: 733, Batch(micro): 733, Batch (considering grad accum): 91,  Loss: 9.4850, Time: 3.69s, Token/s: 138.81
Epoch: 0, Step: 734, Batch(micro): 734, Batch (considering grad accum): 91,  Loss: 9.3327, Time: 3.32s, Token/s: 154.41
Epoch: 0, Step: 735, Batch(micro): 735, Batch (considering grad accum): 91,  Loss: 9.1438, Time: 27.51s, Token/s: 18.61
Epoch: 0, Step: 736, Batch(micro): 736, Batch (considering grad accum): 92,  Loss: 9.1086, Time: 5.26s, Token/s: 97.39
Epoch: 0, Step: 737, Batch(micro): 737, Batch (considering grad accum): 92,  Loss: 9.3061, Time: 3.22s, Token/s: 158.96
Epoch: 0, Step: 738, Batch(micro): 738, Batch (considering grad accum): 92,  Loss: 9.6332, Time: 3.50s, Token/s: 146.30
Epoch: 0, Step: 739, Batch(micro): 739, Batch (considering grad accum): 92,  Loss: 9.4113, Time: 3.44s, Token/s: 148.89
Epoch: 0, Step: 740, Batch(micro): 740, Batch (considering grad accum): 92,  Loss: 9.0253, Time: 3.27s, Token/s: 156.37
Epoch: 0, Step: 741, Batch(micro): 741, Batch (considering grad accum): 92,  Loss: 9.3416, Time: 3.16s, Token/s: 162.22
Epoch: 0, Step: 742, Batch(micro): 742, Batch (considering grad accum): 92,  Loss: 9.3751, Time: 3.12s, Token/s: 164.32
Epoch: 0, Step: 743, Batch(micro): 743, Batch (considering grad accum): 92,  Loss: 9.2291, Time: 26.44s, Token/s: 19.37
Epoch: 0, Step: 744, Batch(micro): 744, Batch (considering grad accum): 93,  Loss: 9.5917, Time: 5.59s, Token/s: 91.58
Epoch: 0, Step: 745, Batch(micro): 745, Batch (considering grad accum): 93,  Loss: 9.2677, Time: 3.93s, Token/s: 130.13
Epoch: 0, Step: 746, Batch(micro): 746, Batch (considering grad accum): 93,  Loss: 9.2937, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 747, Batch(micro): 747, Batch (considering grad accum): 93,  Loss: 9.5153, Time: 3.92s, Token/s: 130.49
Epoch: 0, Step: 748, Batch(micro): 748, Batch (considering grad accum): 93,  Loss: 9.2124, Time: 3.34s, Token/s: 153.19
Epoch: 0, Step: 749, Batch(micro): 749, Batch (considering grad accum): 93,  Loss: 9.2853, Time: 3.39s, Token/s: 151.06
Epoch: 0, Step: 750, Batch(micro): 750, Batch (considering grad accum): 93,  Loss: 9.3698, Time: 3.70s, Token/s: 138.45
Epoch: 0, Step: 751, Batch(micro): 751, Batch (considering grad accum): 93,  Loss: 9.2628, Time: 29.71s, Token/s: 17.24
Epoch: 0, Step: 752, Batch(micro): 752, Batch (considering grad accum): 94,  Loss: 9.3204, Time: 5.71s, Token/s: 89.73
Epoch: 0, Step: 753, Batch(micro): 753, Batch (considering grad accum): 94,  Loss: 9.6375, Time: 4.07s, Token/s: 125.77
Epoch: 0, Step: 754, Batch(micro): 754, Batch (considering grad accum): 94,  Loss: 9.4503, Time: 3.46s, Token/s: 147.86
Epoch: 0, Step: 755, Batch(micro): 755, Batch (considering grad accum): 94,  Loss: 9.3902, Time: 3.07s, Token/s: 166.66
Epoch: 0, Step: 756, Batch(micro): 756, Batch (considering grad accum): 94,  Loss: 9.3217, Time: 3.11s, Token/s: 164.87
Epoch: 0, Step: 757, Batch(micro): 757, Batch (considering grad accum): 94,  Loss: 9.3249, Time: 3.29s, Token/s: 155.74
Epoch: 0, Step: 758, Batch(micro): 758, Batch (considering grad accum): 94,  Loss: 9.4119, Time: 3.53s, Token/s: 145.15
Epoch: 0, Step: 759, Batch(micro): 759, Batch (considering grad accum): 94,  Loss: 9.4683, Time: 21.09s, Token/s: 24.28
Epoch: 0, Step: 760, Batch(micro): 760, Batch (considering grad accum): 95,  Loss: 9.1146, Time: 6.71s, Token/s: 76.30
Epoch: 0, Step: 761, Batch(micro): 761, Batch (considering grad accum): 95,  Loss: 9.2710, Time: 3.46s, Token/s: 148.05
Epoch: 0, Step: 762, Batch(micro): 762, Batch (considering grad accum): 95,  Loss: 9.1061, Time: 3.22s, Token/s: 159.12
Epoch: 0, Step: 763, Batch(micro): 763, Batch (considering grad accum): 95,  Loss: 9.2706, Time: 3.28s, Token/s: 156.31
Epoch: 0, Step: 764, Batch(micro): 764, Batch (considering grad accum): 95,  Loss: 9.3053, Time: 3.06s, Token/s: 167.53
Epoch: 0, Step: 765, Batch(micro): 765, Batch (considering grad accum): 95,  Loss: 9.5503, Time: 3.19s, Token/s: 160.40
Epoch: 0, Step: 766, Batch(micro): 766, Batch (considering grad accum): 95,  Loss: 9.4379, Time: 3.02s, Token/s: 169.42
Epoch: 0, Step: 767, Batch(micro): 767, Batch (considering grad accum): 95,  Loss: 9.1597, Time: 21.11s, Token/s: 24.25
Epoch: 0, Step: 768, Batch(micro): 768, Batch (considering grad accum): 96,  Loss: 8.8358, Time: 6.69s, Token/s: 76.57
Epoch: 0, Step: 769, Batch(micro): 769, Batch (considering grad accum): 96,  Loss: 8.7369, Time: 4.53s, Token/s: 113.11
Epoch: 0, Step: 770, Batch(micro): 770, Batch (considering grad accum): 96,  Loss: 8.8868, Time: 3.58s, Token/s: 142.89
Epoch: 0, Step: 771, Batch(micro): 771, Batch (considering grad accum): 96,  Loss: 9.1812, Time: 3.65s, Token/s: 140.35
Epoch: 0, Step: 772, Batch(micro): 772, Batch (considering grad accum): 96,  Loss: 9.1914, Time: 3.58s, Token/s: 143.16
Epoch: 0, Step: 773, Batch(micro): 773, Batch (considering grad accum): 96,  Loss: 9.3472, Time: 3.57s, Token/s: 143.52
Epoch: 0, Step: 774, Batch(micro): 774, Batch (considering grad accum): 96,  Loss: 9.0705, Time: 3.72s, Token/s: 137.70
Epoch: 0, Step: 775, Batch(micro): 775, Batch (considering grad accum): 96,  Loss: 9.4078, Time: 23.19s, Token/s: 22.08
Epoch: 0, Step: 776, Batch(micro): 776, Batch (considering grad accum): 97,  Loss: 9.2979, Time: 6.83s, Token/s: 74.98
Epoch: 0, Step: 777, Batch(micro): 777, Batch (considering grad accum): 97,  Loss: 9.2110, Time: 4.56s, Token/s: 112.38
Epoch: 0, Step: 778, Batch(micro): 778, Batch (considering grad accum): 97,  Loss: 9.2675, Time: 3.68s, Token/s: 139.29
Epoch: 0, Step: 779, Batch(micro): 779, Batch (considering grad accum): 97,  Loss: 8.9764, Time: 3.27s, Token/s: 156.45
Epoch: 0, Step: 780, Batch(micro): 780, Batch (considering grad accum): 97,  Loss: 9.0782, Time: 3.49s, Token/s: 146.89
Epoch: 0, Step: 781, Batch(micro): 781, Batch (considering grad accum): 97,  Loss: 9.4071, Time: 3.58s, Token/s: 143.07
Epoch: 0, Step: 782, Batch(micro): 782, Batch (considering grad accum): 97,  Loss: 9.4219, Time: 3.51s, Token/s: 145.97
Epoch: 0, Step: 783, Batch(micro): 783, Batch (considering grad accum): 97,  Loss: 9.4269, Time: 22.33s, Token/s: 22.93
Epoch: 0, Step: 784, Batch(micro): 784, Batch (considering grad accum): 98,  Loss: 9.8294, Time: 5.91s, Token/s: 86.65
Epoch: 0, Step: 785, Batch(micro): 785, Batch (considering grad accum): 98,  Loss: 9.1900, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 786, Batch(micro): 786, Batch (considering grad accum): 98,  Loss: 9.1714, Time: 4.15s, Token/s: 123.45
Epoch: 0, Step: 787, Batch(micro): 787, Batch (considering grad accum): 98,  Loss: 9.0051, Time: 3.83s, Token/s: 133.82
Epoch: 0, Step: 788, Batch(micro): 788, Batch (considering grad accum): 98,  Loss: 9.4550, Time: 4.21s, Token/s: 121.73
Epoch: 0, Step: 789, Batch(micro): 789, Batch (considering grad accum): 98,  Loss: 9.3562, Time: 3.60s, Token/s: 142.28
Epoch: 0, Step: 790, Batch(micro): 790, Batch (considering grad accum): 98,  Loss: 9.1653, Time: 5.47s, Token/s: 93.54
Epoch: 0, Step: 791, Batch(micro): 791, Batch (considering grad accum): 98,  Loss: 9.0645, Time: 23.23s, Token/s: 22.04
Epoch: 0, Step: 792, Batch(micro): 792, Batch (considering grad accum): 99,  Loss: 9.0389, Time: 6.27s, Token/s: 81.63
Epoch: 0, Step: 793, Batch(micro): 793, Batch (considering grad accum): 99,  Loss: 8.7321, Time: 4.02s, Token/s: 127.28
Epoch: 0, Step: 794, Batch(micro): 794, Batch (considering grad accum): 99,  Loss: 9.0404, Time: 3.67s, Token/s: 139.47
Epoch: 0, Step: 795, Batch(micro): 795, Batch (considering grad accum): 99,  Loss: 9.2860, Time: 3.39s, Token/s: 151.06
Epoch: 0, Step: 796, Batch(micro): 796, Batch (considering grad accum): 99,  Loss: 8.7576, Time: 3.25s, Token/s: 157.59
Epoch: 0, Step: 797, Batch(micro): 797, Batch (considering grad accum): 99,  Loss: 8.9472, Time: 3.32s, Token/s: 154.28
Epoch: 0, Step: 798, Batch(micro): 798, Batch (considering grad accum): 99,  Loss: 8.9363, Time: 3.86s, Token/s: 132.48
Epoch: 0, Step: 799, Batch(micro): 799, Batch (considering grad accum): 99,  Loss: 8.8505, Time: 26.29s, Token/s: 19.47
Updating MLP bias
Epoch: 0, Step: 800, Batch(micro): 800, Batch (considering grad accum): 100,  Loss: 9.0954, Time: 6.20s, Token/s: 82.57
Epoch: 0, Step: 801, Batch(micro): 801, Batch (considering grad accum): 100,  Loss: 9.1555, Time: 3.45s, Token/s: 148.39
Epoch: 0, Step: 802, Batch(micro): 802, Batch (considering grad accum): 100,  Loss: 8.9047, Time: 3.43s, Token/s: 149.32
Epoch: 0, Step: 803, Batch(micro): 803, Batch (considering grad accum): 100,  Loss: 8.8010, Time: 3.52s, Token/s: 145.28
Epoch: 0, Step: 804, Batch(micro): 804, Batch (considering grad accum): 100,  Loss: 9.0922, Time: 3.49s, Token/s: 146.56
Epoch: 0, Step: 805, Batch(micro): 805, Batch (considering grad accum): 100,  Loss: 9.2101, Time: 3.33s, Token/s: 153.60
Epoch: 0, Step: 806, Batch(micro): 806, Batch (considering grad accum): 100,  Loss: 9.2989, Time: 3.31s, Token/s: 154.84
Epoch: 0, Step: 807, Batch(micro): 807, Batch (considering grad accum): 100,  Loss: 8.8598, Time: 19.54s, Token/s: 26.20
Epoch: 0, Step: 808, Batch(micro): 808, Batch (considering grad accum): 101,  Loss: 8.9267, Time: 8.23s, Token/s: 62.18
Epoch: 0, Step: 809, Batch(micro): 809, Batch (considering grad accum): 101,  Loss: 9.1824, Time: 4.17s, Token/s: 122.69
Epoch: 0, Step: 810, Batch(micro): 810, Batch (considering grad accum): 101,  Loss: 8.9752, Time: 3.58s, Token/s: 143.03
Epoch: 0, Step: 811, Batch(micro): 811, Batch (considering grad accum): 101,  Loss: 9.3595, Time: 3.19s, Token/s: 160.59
Epoch: 0, Step: 812, Batch(micro): 812, Batch (considering grad accum): 101,  Loss: 9.0818, Time: 3.21s, Token/s: 159.45
Epoch: 0, Step: 813, Batch(micro): 813, Batch (considering grad accum): 101,  Loss: 9.2720, Time: 3.42s, Token/s: 149.70
Epoch: 0, Step: 814, Batch(micro): 814, Batch (considering grad accum): 101,  Loss: 9.0907, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 815, Batch(micro): 815, Batch (considering grad accum): 101,  Loss: 9.2137, Time: 18.83s, Token/s: 27.19
Epoch: 0, Step: 816, Batch(micro): 816, Batch (considering grad accum): 102,  Loss: 9.0861, Time: 7.27s, Token/s: 70.45
Epoch: 0, Step: 817, Batch(micro): 817, Batch (considering grad accum): 102,  Loss: 8.7820, Time: 4.25s, Token/s: 120.42
Epoch: 0, Step: 818, Batch(micro): 818, Batch (considering grad accum): 102,  Loss: 8.7483, Time: 3.32s, Token/s: 154.21
Epoch: 0, Step: 819, Batch(micro): 819, Batch (considering grad accum): 102,  Loss: 9.0586, Time: 3.58s, Token/s: 142.99
Epoch: 0, Step: 820, Batch(micro): 820, Batch (considering grad accum): 102,  Loss: 9.4362, Time: 3.75s, Token/s: 136.71
Epoch: 0, Step: 821, Batch(micro): 821, Batch (considering grad accum): 102,  Loss: 9.2785, Time: 3.31s, Token/s: 154.88
Epoch: 0, Step: 822, Batch(micro): 822, Batch (considering grad accum): 102,  Loss: 9.1147, Time: 3.53s, Token/s: 145.08
Epoch: 0, Step: 823, Batch(micro): 823, Batch (considering grad accum): 102,  Loss: 9.3125, Time: 17.45s, Token/s: 29.35
Epoch: 0, Step: 824, Batch(micro): 824, Batch (considering grad accum): 103,  Loss: 9.3181, Time: 5.78s, Token/s: 88.63
Epoch: 0, Step: 825, Batch(micro): 825, Batch (considering grad accum): 103,  Loss: 8.8601, Time: 3.89s, Token/s: 131.55
Epoch: 0, Step: 826, Batch(micro): 826, Batch (considering grad accum): 103,  Loss: 9.2363, Time: 3.52s, Token/s: 145.30
Epoch: 0, Step: 827, Batch(micro): 827, Batch (considering grad accum): 103,  Loss: 8.9199, Time: 3.17s, Token/s: 161.61
Epoch: 0, Step: 828, Batch(micro): 828, Batch (considering grad accum): 103,  Loss: 8.8099, Time: 3.15s, Token/s: 162.32
Epoch: 0, Step: 829, Batch(micro): 829, Batch (considering grad accum): 103,  Loss: 8.8673, Time: 3.18s, Token/s: 161.07
Epoch: 0, Step: 830, Batch(micro): 830, Batch (considering grad accum): 103,  Loss: 8.8465, Time: 3.07s, Token/s: 166.89
Epoch: 0, Step: 831, Batch(micro): 831, Batch (considering grad accum): 103,  Loss: 8.9924, Time: 17.16s, Token/s: 29.83
Epoch: 0, Step: 832, Batch(micro): 832, Batch (considering grad accum): 104,  Loss: 9.4769, Time: 6.73s, Token/s: 76.02
Epoch: 0, Step: 833, Batch(micro): 833, Batch (considering grad accum): 104,  Loss: 9.2584, Time: 3.92s, Token/s: 130.59
Epoch: 0, Step: 834, Batch(micro): 834, Batch (considering grad accum): 104,  Loss: 9.1144, Time: 3.71s, Token/s: 138.01
Epoch: 0, Step: 835, Batch(micro): 835, Batch (considering grad accum): 104,  Loss: 8.9483, Time: 3.52s, Token/s: 145.52
Epoch: 0, Step: 836, Batch(micro): 836, Batch (considering grad accum): 104,  Loss: 9.0853, Time: 3.51s, Token/s: 145.88
Epoch: 0, Step: 837, Batch(micro): 837, Batch (considering grad accum): 104,  Loss: 8.8306, Time: 3.71s, Token/s: 138.08
Epoch: 0, Step: 838, Batch(micro): 838, Batch (considering grad accum): 104,  Loss: 8.9649, Time: 3.45s, Token/s: 148.45
Epoch: 0, Step: 839, Batch(micro): 839, Batch (considering grad accum): 104,  Loss: 8.9237, Time: 18.37s, Token/s: 27.87
Epoch: 0, Step: 840, Batch(micro): 840, Batch (considering grad accum): 105,  Loss: 8.8494, Time: 7.44s, Token/s: 68.82
Epoch: 0, Step: 841, Batch(micro): 841, Batch (considering grad accum): 105,  Loss: 9.0388, Time: 4.14s, Token/s: 123.54
Epoch: 0, Step: 842, Batch(micro): 842, Batch (considering grad accum): 105,  Loss: 8.9725, Time: 3.70s, Token/s: 138.51
Epoch: 0, Step: 843, Batch(micro): 843, Batch (considering grad accum): 105,  Loss: 9.0535, Time: 3.31s, Token/s: 154.78
Epoch: 0, Step: 844, Batch(micro): 844, Batch (considering grad accum): 105,  Loss: 8.7936, Time: 2.91s, Token/s: 176.13
Epoch: 0, Step: 845, Batch(micro): 845, Batch (considering grad accum): 105,  Loss: 8.9189, Time: 3.38s, Token/s: 151.64
Epoch: 0, Step: 846, Batch(micro): 846, Batch (considering grad accum): 105,  Loss: 8.9867, Time: 3.45s, Token/s: 148.25
Epoch: 0, Step: 847, Batch(micro): 847, Batch (considering grad accum): 105,  Loss: 9.0423, Time: 21.30s, Token/s: 24.04
Epoch: 0, Step: 848, Batch(micro): 848, Batch (considering grad accum): 106,  Loss: 9.0575, Time: 7.19s, Token/s: 71.26
Epoch: 0, Step: 849, Batch(micro): 849, Batch (considering grad accum): 106,  Loss: 9.1809, Time: 3.82s, Token/s: 133.97
Epoch: 0, Step: 850, Batch(micro): 850, Batch (considering grad accum): 106,  Loss: 9.0327, Time: 3.66s, Token/s: 139.80
Epoch: 0, Step: 851, Batch(micro): 851, Batch (considering grad accum): 106,  Loss: 9.0196, Time: 3.23s, Token/s: 158.32
Epoch: 0, Step: 852, Batch(micro): 852, Batch (considering grad accum): 106,  Loss: 9.2777, Time: 3.12s, Token/s: 164.21
Epoch: 0, Step: 853, Batch(micro): 853, Batch (considering grad accum): 106,  Loss: 8.8386, Time: 2.79s, Token/s: 183.63
Epoch: 0, Step: 854, Batch(micro): 854, Batch (considering grad accum): 106,  Loss: 9.1069, Time: 3.12s, Token/s: 164.28
Epoch: 0, Step: 855, Batch(micro): 855, Batch (considering grad accum): 106,  Loss: 9.3074, Time: 18.91s, Token/s: 27.08
Epoch: 0, Step: 856, Batch(micro): 856, Batch (considering grad accum): 107,  Loss: 8.9733, Time: 8.93s, Token/s: 57.33
Epoch: 0, Step: 857, Batch(micro): 857, Batch (considering grad accum): 107,  Loss: 9.0353, Time: 3.81s, Token/s: 134.40
Epoch: 0, Step: 858, Batch(micro): 858, Batch (considering grad accum): 107,  Loss: 9.3952, Time: 3.19s, Token/s: 160.37
Epoch: 0, Step: 859, Batch(micro): 859, Batch (considering grad accum): 107,  Loss: 9.3519, Time: 3.83s, Token/s: 133.72
Epoch: 0, Step: 860, Batch(micro): 860, Batch (considering grad accum): 107,  Loss: 9.0611, Time: 2.94s, Token/s: 174.13
Epoch: 0, Step: 861, Batch(micro): 861, Batch (considering grad accum): 107,  Loss: 8.9646, Time: 2.90s, Token/s: 176.57
Epoch: 0, Step: 862, Batch(micro): 862, Batch (considering grad accum): 107,  Loss: 9.1369, Time: 3.28s, Token/s: 155.86
Epoch: 0, Step: 863, Batch(micro): 863, Batch (considering grad accum): 107,  Loss: 9.0453, Time: 20.67s, Token/s: 24.77
Epoch: 0, Step: 864, Batch(micro): 864, Batch (considering grad accum): 108,  Loss: 9.0946, Time: 8.42s, Token/s: 60.78
Epoch: 0, Step: 865, Batch(micro): 865, Batch (considering grad accum): 108,  Loss: 8.8508, Time: 3.76s, Token/s: 136.00
Epoch: 0, Step: 866, Batch(micro): 866, Batch (considering grad accum): 108,  Loss: 9.0697, Time: 3.27s, Token/s: 156.66
Epoch: 0, Step: 867, Batch(micro): 867, Batch (considering grad accum): 108,  Loss: 9.3641, Time: 3.26s, Token/s: 157.04
Epoch: 0, Step: 868, Batch(micro): 868, Batch (considering grad accum): 108,  Loss: 8.6762, Time: 3.48s, Token/s: 147.15
Epoch: 0, Step: 869, Batch(micro): 869, Batch (considering grad accum): 108,  Loss: 8.6567, Time: 3.43s, Token/s: 149.36
Epoch: 0, Step: 870, Batch(micro): 870, Batch (considering grad accum): 108,  Loss: 8.6778, Time: 2.92s, Token/s: 175.12
Epoch: 0, Step: 871, Batch(micro): 871, Batch (considering grad accum): 108,  Loss: 8.8246, Time: 18.20s, Token/s: 28.13
Epoch: 0, Step: 872, Batch(micro): 872, Batch (considering grad accum): 109,  Loss: 9.0656, Time: 6.24s, Token/s: 82.08
Epoch: 0, Step: 873, Batch(micro): 873, Batch (considering grad accum): 109,  Loss: 9.2162, Time: 3.03s, Token/s: 169.16
Epoch: 0, Step: 874, Batch(micro): 874, Batch (considering grad accum): 109,  Loss: 8.7481, Time: 3.19s, Token/s: 160.38
Epoch: 0, Step: 875, Batch(micro): 875, Batch (considering grad accum): 109,  Loss: 9.0394, Time: 3.03s, Token/s: 168.72
Epoch: 0, Step: 876, Batch(micro): 876, Batch (considering grad accum): 109,  Loss: 9.0189, Time: 3.69s, Token/s: 138.69
Epoch: 0, Step: 877, Batch(micro): 877, Batch (considering grad accum): 109,  Loss: 8.9014, Time: 3.21s, Token/s: 159.47
Epoch: 0, Step: 878, Batch(micro): 878, Batch (considering grad accum): 109,  Loss: 8.9227, Time: 3.02s, Token/s: 169.80
Epoch: 0, Step: 879, Batch(micro): 879, Batch (considering grad accum): 109,  Loss: 8.7103, Time: 17.97s, Token/s: 28.49
Epoch: 0, Step: 880, Batch(micro): 880, Batch (considering grad accum): 110,  Loss: 9.0917, Time: 6.72s, Token/s: 76.14
Epoch: 0, Step: 881, Batch(micro): 881, Batch (considering grad accum): 110,  Loss: 8.9468, Time: 3.31s, Token/s: 154.91
Epoch: 0, Step: 882, Batch(micro): 882, Batch (considering grad accum): 110,  Loss: 8.5224, Time: 3.20s, Token/s: 160.13
Epoch: 0, Step: 883, Batch(micro): 883, Batch (considering grad accum): 110,  Loss: 8.6983, Time: 3.26s, Token/s: 156.82
Epoch: 0, Step: 884, Batch(micro): 884, Batch (considering grad accum): 110,  Loss: 8.8511, Time: 3.31s, Token/s: 154.80
Epoch: 0, Step: 885, Batch(micro): 885, Batch (considering grad accum): 110,  Loss: 8.7490, Time: 3.34s, Token/s: 153.40
Epoch: 0, Step: 886, Batch(micro): 886, Batch (considering grad accum): 110,  Loss: 9.4834, Time: 3.31s, Token/s: 154.49
Epoch: 0, Step: 887, Batch(micro): 887, Batch (considering grad accum): 110,  Loss: 9.2327, Time: 21.12s, Token/s: 24.24
Epoch: 0, Step: 888, Batch(micro): 888, Batch (considering grad accum): 111,  Loss: 9.0711, Time: 10.83s, Token/s: 47.28
Epoch: 0, Step: 889, Batch(micro): 889, Batch (considering grad accum): 111,  Loss: 8.7297, Time: 4.73s, Token/s: 108.15
Epoch: 0, Step: 890, Batch(micro): 890, Batch (considering grad accum): 111,  Loss: 9.0000, Time: 3.81s, Token/s: 134.44
Epoch: 0, Step: 891, Batch(micro): 891, Batch (considering grad accum): 111,  Loss: 8.8173, Time: 3.39s, Token/s: 150.98
Epoch: 0, Step: 892, Batch(micro): 892, Batch (considering grad accum): 111,  Loss: 9.0326, Time: 3.31s, Token/s: 154.90
Epoch: 0, Step: 893, Batch(micro): 893, Batch (considering grad accum): 111,  Loss: 9.0875, Time: 3.00s, Token/s: 170.74
Epoch: 0, Step: 894, Batch(micro): 894, Batch (considering grad accum): 111,  Loss: 9.1284, Time: 3.53s, Token/s: 145.09
Epoch: 0, Step: 895, Batch(micro): 895, Batch (considering grad accum): 111,  Loss: 8.8545, Time: 22.59s, Token/s: 22.66
Epoch: 0, Step: 896, Batch(micro): 896, Batch (considering grad accum): 112,  Loss: 8.8108, Time: 7.72s, Token/s: 66.33
Epoch: 0, Step: 897, Batch(micro): 897, Batch (considering grad accum): 112,  Loss: 8.8929, Time: 3.87s, Token/s: 132.45
Epoch: 0, Step: 898, Batch(micro): 898, Batch (considering grad accum): 112,  Loss: 8.7308, Time: 3.82s, Token/s: 133.88
Epoch: 0, Step: 899, Batch(micro): 899, Batch (considering grad accum): 112,  Loss: 9.0481, Time: 3.78s, Token/s: 135.49
Updating MLP bias
Epoch: 0, Step: 900, Batch(micro): 900, Batch (considering grad accum): 112,  Loss: 8.9798, Time: 3.67s, Token/s: 139.36
Epoch: 0, Step: 901, Batch(micro): 901, Batch (considering grad accum): 112,  Loss: 9.1198, Time: 3.42s, Token/s: 149.55
Epoch: 0, Step: 902, Batch(micro): 902, Batch (considering grad accum): 112,  Loss: 9.4850, Time: 3.33s, Token/s: 153.68
Epoch: 0, Step: 903, Batch(micro): 903, Batch (considering grad accum): 112,  Loss: 9.3846, Time: 29.94s, Token/s: 17.10
Epoch: 0, Step: 904, Batch(micro): 904, Batch (considering grad accum): 113,  Loss: 9.2899, Time: 8.20s, Token/s: 62.45
Epoch: 0, Step: 905, Batch(micro): 905, Batch (considering grad accum): 113,  Loss: 8.8936, Time: 4.30s, Token/s: 119.18
Epoch: 0, Step: 906, Batch(micro): 906, Batch (considering grad accum): 113,  Loss: 9.2536, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 907, Batch(micro): 907, Batch (considering grad accum): 113,  Loss: 8.9774, Time: 4.07s, Token/s: 125.68
Epoch: 0, Step: 908, Batch(micro): 908, Batch (considering grad accum): 113,  Loss: 8.7961, Time: 2.95s, Token/s: 173.54
Epoch: 0, Step: 909, Batch(micro): 909, Batch (considering grad accum): 113,  Loss: 8.7016, Time: 3.20s, Token/s: 159.96
Epoch: 0, Step: 910, Batch(micro): 910, Batch (considering grad accum): 113,  Loss: 8.4040, Time: 3.12s, Token/s: 164.11
Epoch: 0, Step: 911, Batch(micro): 911, Batch (considering grad accum): 113,  Loss: 8.8607, Time: 23.43s, Token/s: 21.85
Epoch: 0, Step: 912, Batch(micro): 912, Batch (considering grad accum): 114,  Loss: 8.8533, Time: 7.60s, Token/s: 67.35
Epoch: 0, Step: 913, Batch(micro): 913, Batch (considering grad accum): 114,  Loss: 8.8292, Time: 4.24s, Token/s: 120.79
Epoch: 0, Step: 914, Batch(micro): 914, Batch (considering grad accum): 114,  Loss: 8.8309, Time: 3.50s, Token/s: 146.43
Epoch: 0, Step: 915, Batch(micro): 915, Batch (considering grad accum): 114,  Loss: 8.7622, Time: 3.66s, Token/s: 139.72
Epoch: 0, Step: 916, Batch(micro): 916, Batch (considering grad accum): 114,  Loss: 9.0693, Time: 3.46s, Token/s: 148.05
Epoch: 0, Step: 917, Batch(micro): 917, Batch (considering grad accum): 114,  Loss: 8.8140, Time: 3.72s, Token/s: 137.69
Epoch: 0, Step: 918, Batch(micro): 918, Batch (considering grad accum): 114,  Loss: 8.5195, Time: 3.36s, Token/s: 152.44
Epoch: 0, Step: 919, Batch(micro): 919, Batch (considering grad accum): 114,  Loss: 8.7430, Time: 24.76s, Token/s: 20.68
Epoch: 0, Step: 920, Batch(micro): 920, Batch (considering grad accum): 115,  Loss: 8.6767, Time: 10.60s, Token/s: 48.31
Epoch: 0, Step: 921, Batch(micro): 921, Batch (considering grad accum): 115,  Loss: 8.9105, Time: 3.76s, Token/s: 136.18
Epoch: 0, Step: 922, Batch(micro): 922, Batch (considering grad accum): 115,  Loss: 9.0054, Time: 4.02s, Token/s: 127.28
Epoch: 0, Step: 923, Batch(micro): 923, Batch (considering grad accum): 115,  Loss: 8.9874, Time: 3.50s, Token/s: 146.08
Epoch: 0, Step: 924, Batch(micro): 924, Batch (considering grad accum): 115,  Loss: 8.8714, Time: 3.52s, Token/s: 145.27
Epoch: 0, Step: 925, Batch(micro): 925, Batch (considering grad accum): 115,  Loss: 9.0099, Time: 3.68s, Token/s: 139.11
Epoch: 0, Step: 926, Batch(micro): 926, Batch (considering grad accum): 115,  Loss: 9.0804, Time: 3.22s, Token/s: 159.21
Epoch: 0, Step: 927, Batch(micro): 927, Batch (considering grad accum): 115,  Loss: 9.0339, Time: 21.98s, Token/s: 23.29
Epoch: 0, Step: 928, Batch(micro): 928, Batch (considering grad accum): 116,  Loss: 8.6594, Time: 9.19s, Token/s: 55.71
Epoch: 0, Step: 929, Batch(micro): 929, Batch (considering grad accum): 116,  Loss: 9.0181, Time: 6.43s, Token/s: 79.61
Epoch: 0, Step: 930, Batch(micro): 930, Batch (considering grad accum): 116,  Loss: 8.9156, Time: 3.09s, Token/s: 165.83
Epoch: 0, Step: 931, Batch(micro): 931, Batch (considering grad accum): 116,  Loss: 9.1204, Time: 2.95s, Token/s: 173.37
Epoch: 0, Step: 932, Batch(micro): 932, Batch (considering grad accum): 116,  Loss: 8.9913, Time: 3.06s, Token/s: 167.45
Epoch: 0, Step: 933, Batch(micro): 933, Batch (considering grad accum): 116,  Loss: 8.9112, Time: 3.00s, Token/s: 170.78
Epoch: 0, Step: 934, Batch(micro): 934, Batch (considering grad accum): 116,  Loss: 9.1339, Time: 2.99s, Token/s: 171.00
Epoch: 0, Step: 935, Batch(micro): 935, Batch (considering grad accum): 116,  Loss: 8.9934, Time: 25.74s, Token/s: 19.89
Epoch: 0, Step: 936, Batch(micro): 936, Batch (considering grad accum): 117,  Loss: 9.1719, Time: 9.04s, Token/s: 56.66
Epoch: 0, Step: 937, Batch(micro): 937, Batch (considering grad accum): 117,  Loss: 9.2696, Time: 4.23s, Token/s: 121.15
Epoch: 0, Step: 938, Batch(micro): 938, Batch (considering grad accum): 117,  Loss: 8.7476, Time: 3.36s, Token/s: 152.41
Epoch: 0, Step: 939, Batch(micro): 939, Batch (considering grad accum): 117,  Loss: 9.1880, Time: 3.03s, Token/s: 169.18
Epoch: 0, Step: 940, Batch(micro): 940, Batch (considering grad accum): 117,  Loss: 9.1997, Time: 3.10s, Token/s: 165.20
Epoch: 0, Step: 941, Batch(micro): 941, Batch (considering grad accum): 117,  Loss: 8.4566, Time: 3.41s, Token/s: 150.24
Epoch: 0, Step: 942, Batch(micro): 942, Batch (considering grad accum): 117,  Loss: 8.4261, Time: 3.38s, Token/s: 151.37
Epoch: 0, Step: 943, Batch(micro): 943, Batch (considering grad accum): 117,  Loss: 8.7812, Time: 26.45s, Token/s: 19.35
Epoch: 0, Step: 944, Batch(micro): 944, Batch (considering grad accum): 118,  Loss: 8.9724, Time: 9.53s, Token/s: 53.70
Epoch: 0, Step: 945, Batch(micro): 945, Batch (considering grad accum): 118,  Loss: 9.1724, Time: 3.85s, Token/s: 133.05
Epoch: 0, Step: 946, Batch(micro): 946, Batch (considering grad accum): 118,  Loss: 9.0890, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 947, Batch(micro): 947, Batch (considering grad accum): 118,  Loss: 9.0412, Time: 3.11s, Token/s: 164.41
Epoch: 0, Step: 948, Batch(micro): 948, Batch (considering grad accum): 118,  Loss: 8.8894, Time: 3.11s, Token/s: 164.58
Epoch: 0, Step: 949, Batch(micro): 949, Batch (considering grad accum): 118,  Loss: 8.5829, Time: 3.57s, Token/s: 143.56
Epoch: 0, Step: 950, Batch(micro): 950, Batch (considering grad accum): 118,  Loss: 8.8341, Time: 3.52s, Token/s: 145.50
Epoch: 0, Step: 951, Batch(micro): 951, Batch (considering grad accum): 118,  Loss: 8.7749, Time: 23.83s, Token/s: 21.49
Epoch: 0, Step: 952, Batch(micro): 952, Batch (considering grad accum): 119,  Loss: 8.6333, Time: 8.67s, Token/s: 59.08
Epoch: 0, Step: 953, Batch(micro): 953, Batch (considering grad accum): 119,  Loss: 8.7663, Time: 3.90s, Token/s: 131.21
Epoch: 0, Step: 954, Batch(micro): 954, Batch (considering grad accum): 119,  Loss: 8.9505, Time: 3.88s, Token/s: 131.92
Epoch: 0, Step: 955, Batch(micro): 955, Batch (considering grad accum): 119,  Loss: 9.0050, Time: 3.18s, Token/s: 161.19
Epoch: 0, Step: 956, Batch(micro): 956, Batch (considering grad accum): 119,  Loss: 8.6105, Time: 3.17s, Token/s: 161.56
Epoch: 0, Step: 957, Batch(micro): 957, Batch (considering grad accum): 119,  Loss: 8.8534, Time: 3.39s, Token/s: 151.11
Epoch: 0, Step: 958, Batch(micro): 958, Batch (considering grad accum): 119,  Loss: 9.0699, Time: 3.38s, Token/s: 151.37
Epoch: 0, Step: 959, Batch(micro): 959, Batch (considering grad accum): 119,  Loss: 9.1362, Time: 24.65s, Token/s: 20.77
Epoch: 0, Step: 960, Batch(micro): 960, Batch (considering grad accum): 120,  Loss: 8.6235, Time: 9.01s, Token/s: 56.85
Epoch: 0, Step: 961, Batch(micro): 961, Batch (considering grad accum): 120,  Loss: 8.7998, Time: 3.79s, Token/s: 135.12
Epoch: 0, Step: 962, Batch(micro): 962, Batch (considering grad accum): 120,  Loss: 8.8962, Time: 3.48s, Token/s: 147.03
Epoch: 0, Step: 963, Batch(micro): 963, Batch (considering grad accum): 120,  Loss: 8.9515, Time: 3.28s, Token/s: 155.86
Epoch: 0, Step: 964, Batch(micro): 964, Batch (considering grad accum): 120,  Loss: 8.9773, Time: 3.36s, Token/s: 152.32
Epoch: 0, Step: 965, Batch(micro): 965, Batch (considering grad accum): 120,  Loss: 8.6906, Time: 3.35s, Token/s: 152.80
Epoch: 0, Step: 966, Batch(micro): 966, Batch (considering grad accum): 120,  Loss: 8.8103, Time: 3.15s, Token/s: 162.65
Epoch: 0, Step: 967, Batch(micro): 967, Batch (considering grad accum): 120,  Loss: 8.9599, Time: 23.23s, Token/s: 22.04
Epoch: 0, Step: 968, Batch(micro): 968, Batch (considering grad accum): 121,  Loss: 9.6373, Time: 8.45s, Token/s: 60.62
Epoch: 0, Step: 969, Batch(micro): 969, Batch (considering grad accum): 121,  Loss: 9.5359, Time: 3.69s, Token/s: 138.58
Epoch: 0, Step: 970, Batch(micro): 970, Batch (considering grad accum): 121,  Loss: 8.8016, Time: 3.90s, Token/s: 131.23
Epoch: 0, Step: 971, Batch(micro): 971, Batch (considering grad accum): 121,  Loss: 8.8102, Time: 3.16s, Token/s: 161.80
Epoch: 0, Step: 972, Batch(micro): 972, Batch (considering grad accum): 121,  Loss: 8.9263, Time: 3.20s, Token/s: 159.75
Epoch: 0, Step: 973, Batch(micro): 973, Batch (considering grad accum): 121,  Loss: 8.5759, Time: 3.84s, Token/s: 133.44
Epoch: 0, Step: 974, Batch(micro): 974, Batch (considering grad accum): 121,  Loss: 8.8146, Time: 3.62s, Token/s: 141.33
Epoch: 0, Step: 975, Batch(micro): 975, Batch (considering grad accum): 121,  Loss: 8.9328, Time: 26.00s, Token/s: 19.69
Epoch: 0, Step: 976, Batch(micro): 976, Batch (considering grad accum): 122,  Loss: 8.9211, Time: 10.71s, Token/s: 47.80
Epoch: 0, Step: 977, Batch(micro): 977, Batch (considering grad accum): 122,  Loss: 9.1624, Time: 3.71s, Token/s: 137.85
Epoch: 0, Step: 978, Batch(micro): 978, Batch (considering grad accum): 122,  Loss: 8.7893, Time: 3.37s, Token/s: 151.92
Epoch: 0, Step: 979, Batch(micro): 979, Batch (considering grad accum): 122,  Loss: 8.8461, Time: 3.49s, Token/s: 146.69
Epoch: 0, Step: 980, Batch(micro): 980, Batch (considering grad accum): 122,  Loss: 8.9452, Time: 3.38s, Token/s: 151.60
Epoch: 0, Step: 981, Batch(micro): 981, Batch (considering grad accum): 122,  Loss: 9.0955, Time: 3.38s, Token/s: 151.42
Epoch: 0, Step: 982, Batch(micro): 982, Batch (considering grad accum): 122,  Loss: 8.5757, Time: 3.33s, Token/s: 153.58
Epoch: 0, Step: 983, Batch(micro): 983, Batch (considering grad accum): 122,  Loss: 8.4445, Time: 22.67s, Token/s: 22.59
Epoch: 0, Step: 984, Batch(micro): 984, Batch (considering grad accum): 123,  Loss: 8.9496, Time: 12.00s, Token/s: 42.68
Epoch: 0, Step: 985, Batch(micro): 985, Batch (considering grad accum): 123,  Loss: 8.9874, Time: 3.53s, Token/s: 144.88
Epoch: 0, Step: 986, Batch(micro): 986, Batch (considering grad accum): 123,  Loss: 8.7010, Time: 3.19s, Token/s: 160.52
Epoch: 0, Step: 987, Batch(micro): 987, Batch (considering grad accum): 123,  Loss: 8.4479, Time: 3.09s, Token/s: 165.53
Epoch: 0, Step: 988, Batch(micro): 988, Batch (considering grad accum): 123,  Loss: 9.0961, Time: 3.13s, Token/s: 163.63
Epoch: 0, Step: 989, Batch(micro): 989, Batch (considering grad accum): 123,  Loss: 8.9904, Time: 3.12s, Token/s: 164.29
Epoch: 0, Step: 990, Batch(micro): 990, Batch (considering grad accum): 123,  Loss: 8.6473, Time: 3.29s, Token/s: 155.42
Epoch: 0, Step: 991, Batch(micro): 991, Batch (considering grad accum): 123,  Loss: 9.0184, Time: 23.05s, Token/s: 22.22
Epoch: 0, Step: 992, Batch(micro): 992, Batch (considering grad accum): 124,  Loss: 8.8064, Time: 8.44s, Token/s: 60.69
Epoch: 0, Step: 993, Batch(micro): 993, Batch (considering grad accum): 124,  Loss: 8.6397, Time: 4.02s, Token/s: 127.50
Epoch: 0, Step: 994, Batch(micro): 994, Batch (considering grad accum): 124,  Loss: 8.8596, Time: 3.60s, Token/s: 142.15
Epoch: 0, Step: 995, Batch(micro): 995, Batch (considering grad accum): 124,  Loss: 8.8767, Time: 3.28s, Token/s: 156.03
Epoch: 0, Step: 996, Batch(micro): 996, Batch (considering grad accum): 124,  Loss: 8.5253, Time: 3.19s, Token/s: 160.40
Epoch: 0, Step: 997, Batch(micro): 997, Batch (considering grad accum): 124,  Loss: 8.6706, Time: 3.17s, Token/s: 161.34
Epoch: 0, Step: 998, Batch(micro): 998, Batch (considering grad accum): 124,  Loss: 8.8188, Time: 3.11s, Token/s: 164.60
Epoch: 0, Step: 999, Batch(micro): 999, Batch (considering grad accum): 124,  Loss: 8.8353, Time: 23.18s, Token/s: 22.09
Updating MLP bias
Epoch: 0, Step: 1000, Batch(micro): 1000, Batch (considering grad accum): 125,  Loss: 8.4972, Time: 6.95s, Token/s: 73.67
Saved checkpoint at step 1000
What is Gravity?, the, have to, hydrocarbon, you in and and


















Epoch: 0, Step: 1001, Batch(micro): 1001, Batch (considering grad accum): 125,  Loss: 8.5710, Time: 13.42s, Token/s: 38.15
Epoch: 0, Step: 1002, Batch(micro): 1002, Batch (considering grad accum): 125,  Loss: 8.7321, Time: 4.00s, Token/s: 127.89
Epoch: 0, Step: 1003, Batch(micro): 1003, Batch (considering grad accum): 125,  Loss: 8.9582, Time: 3.37s, Token/s: 152.00
Epoch: 0, Step: 1004, Batch(micro): 1004, Batch (considering grad accum): 125,  Loss: 8.7386, Time: 4.40s, Token/s: 116.25
Epoch: 0, Step: 1005, Batch(micro): 1005, Batch (considering grad accum): 125,  Loss: 9.2100, Time: 3.86s, Token/s: 132.71
Epoch: 0, Step: 1006, Batch(micro): 1006, Batch (considering grad accum): 125,  Loss: 8.5199, Time: 3.82s, Token/s: 134.21
Epoch: 0, Step: 1007, Batch(micro): 1007, Batch (considering grad accum): 125,  Loss: 8.8103, Time: 20.17s, Token/s: 25.39
Epoch: 0, Step: 1008, Batch(micro): 1008, Batch (considering grad accum): 126,  Loss: 8.5340, Time: 7.44s, Token/s: 68.81
Epoch: 0, Step: 1009, Batch(micro): 1009, Batch (considering grad accum): 126,  Loss: 8.5553, Time: 3.87s, Token/s: 132.37
Epoch: 0, Step: 1010, Batch(micro): 1010, Batch (considering grad accum): 126,  Loss: 8.6888, Time: 4.03s, Token/s: 127.17
Epoch: 0, Step: 1011, Batch(micro): 1011, Batch (considering grad accum): 126,  Loss: 8.7900, Time: 4.83s, Token/s: 105.98
Epoch: 0, Step: 1012, Batch(micro): 1012, Batch (considering grad accum): 126,  Loss: 8.8392, Time: 3.19s, Token/s: 160.33
Epoch: 0, Step: 1013, Batch(micro): 1013, Batch (considering grad accum): 126,  Loss: 8.6015, Time: 3.36s, Token/s: 152.26
Epoch: 0, Step: 1014, Batch(micro): 1014, Batch (considering grad accum): 126,  Loss: 8.3056, Time: 3.33s, Token/s: 153.87
Epoch: 0, Step: 1015, Batch(micro): 1015, Batch (considering grad accum): 126,  Loss: 8.1878, Time: 20.10s, Token/s: 25.47
Epoch: 0, Step: 1016, Batch(micro): 1016, Batch (considering grad accum): 127,  Loss: 8.4133, Time: 8.53s, Token/s: 60.00
Epoch: 0, Step: 1017, Batch(micro): 1017, Batch (considering grad accum): 127,  Loss: 8.9527, Time: 4.25s, Token/s: 120.50
Epoch: 0, Step: 1018, Batch(micro): 1018, Batch (considering grad accum): 127,  Loss: 8.9097, Time: 3.93s, Token/s: 130.12
Epoch: 0, Step: 1019, Batch(micro): 1019, Batch (considering grad accum): 127,  Loss: 8.7838, Time: 3.52s, Token/s: 145.27
Epoch: 0, Step: 1020, Batch(micro): 1020, Batch (considering grad accum): 127,  Loss: 9.0326, Time: 3.50s, Token/s: 146.25
Epoch: 0, Step: 1021, Batch(micro): 1021, Batch (considering grad accum): 127,  Loss: 8.6875, Time: 3.84s, Token/s: 133.43
Epoch: 0, Step: 1022, Batch(micro): 1022, Batch (considering grad accum): 127,  Loss: 8.9042, Time: 4.01s, Token/s: 127.55
Epoch: 0, Step: 1023, Batch(micro): 1023, Batch (considering grad accum): 127,  Loss: 8.6688, Time: 18.56s, Token/s: 27.59
Epoch: 0, Step: 1024, Batch(micro): 1024, Batch (considering grad accum): 128,  Loss: 8.3013, Time: 6.99s, Token/s: 73.21
Epoch: 0, Step: 1025, Batch(micro): 1025, Batch (considering grad accum): 128,  Loss: 8.6263, Time: 4.42s, Token/s: 115.73
Epoch: 0, Step: 1026, Batch(micro): 1026, Batch (considering grad accum): 128,  Loss: 8.8999, Time: 3.39s, Token/s: 150.85
Epoch: 0, Step: 1027, Batch(micro): 1027, Batch (considering grad accum): 128,  Loss: 8.8136, Time: 3.52s, Token/s: 145.64
Epoch: 0, Step: 1028, Batch(micro): 1028, Batch (considering grad accum): 128,  Loss: 8.9277, Time: 3.55s, Token/s: 144.42
Epoch: 0, Step: 1029, Batch(micro): 1029, Batch (considering grad accum): 128,  Loss: 8.8313, Time: 3.69s, Token/s: 138.69
Epoch: 0, Step: 1030, Batch(micro): 1030, Batch (considering grad accum): 128,  Loss: 8.4010, Time: 3.60s, Token/s: 142.27
Epoch: 0, Step: 1031, Batch(micro): 1031, Batch (considering grad accum): 128,  Loss: 8.4358, Time: 18.64s, Token/s: 27.47
Epoch: 0, Step: 1032, Batch(micro): 1032, Batch (considering grad accum): 129,  Loss: 8.6199, Time: 6.63s, Token/s: 77.20
Epoch: 0, Step: 1033, Batch(micro): 1033, Batch (considering grad accum): 129,  Loss: 8.7382, Time: 4.00s, Token/s: 128.14
Epoch: 0, Step: 1034, Batch(micro): 1034, Batch (considering grad accum): 129,  Loss: 8.9003, Time: 4.02s, Token/s: 127.48
Epoch: 0, Step: 1035, Batch(micro): 1035, Batch (considering grad accum): 129,  Loss: 8.8153, Time: 4.32s, Token/s: 118.60
Epoch: 0, Step: 1036, Batch(micro): 1036, Batch (considering grad accum): 129,  Loss: 8.7200, Time: 3.56s, Token/s: 143.93
Epoch: 0, Step: 1037, Batch(micro): 1037, Batch (considering grad accum): 129,  Loss: 8.6768, Time: 3.59s, Token/s: 142.58
Epoch: 0, Step: 1038, Batch(micro): 1038, Batch (considering grad accum): 129,  Loss: 8.5084, Time: 4.54s, Token/s: 112.68
Epoch: 0, Step: 1039, Batch(micro): 1039, Batch (considering grad accum): 129,  Loss: 8.6930, Time: 18.41s, Token/s: 27.81
Epoch: 0, Step: 1040, Batch(micro): 1040, Batch (considering grad accum): 130,  Loss: 8.5663, Time: 7.78s, Token/s: 65.80
Epoch: 0, Step: 1041, Batch(micro): 1041, Batch (considering grad accum): 130,  Loss: 8.6565, Time: 4.41s, Token/s: 116.04
Epoch: 0, Step: 1042, Batch(micro): 1042, Batch (considering grad accum): 130,  Loss: 8.7770, Time: 3.78s, Token/s: 135.49
Epoch: 0, Step: 1043, Batch(micro): 1043, Batch (considering grad accum): 130,  Loss: 8.6276, Time: 3.42s, Token/s: 149.74
Epoch: 0, Step: 1044, Batch(micro): 1044, Batch (considering grad accum): 130,  Loss: 8.6557, Time: 3.59s, Token/s: 142.56
Epoch: 0, Step: 1045, Batch(micro): 1045, Batch (considering grad accum): 130,  Loss: 8.6983, Time: 3.74s, Token/s: 136.82
Epoch: 0, Step: 1046, Batch(micro): 1046, Batch (considering grad accum): 130,  Loss: 8.9106, Time: 3.65s, Token/s: 140.13
Epoch: 0, Step: 1047, Batch(micro): 1047, Batch (considering grad accum): 130,  Loss: 8.5359, Time: 18.76s, Token/s: 27.30
Epoch: 0, Step: 1048, Batch(micro): 1048, Batch (considering grad accum): 131,  Loss: 8.5028, Time: 7.47s, Token/s: 68.51
Epoch: 0, Step: 1049, Batch(micro): 1049, Batch (considering grad accum): 131,  Loss: 8.6727, Time: 4.00s, Token/s: 128.13
Epoch: 0, Step: 1050, Batch(micro): 1050, Batch (considering grad accum): 131,  Loss: 8.6772, Time: 3.85s, Token/s: 133.14
Epoch: 0, Step: 1051, Batch(micro): 1051, Batch (considering grad accum): 131,  Loss: 8.6235, Time: 4.27s, Token/s: 119.77
Epoch: 0, Step: 1052, Batch(micro): 1052, Batch (considering grad accum): 131,  Loss: 8.6287, Time: 3.99s, Token/s: 128.46
Epoch: 0, Step: 1053, Batch(micro): 1053, Batch (considering grad accum): 131,  Loss: 8.6994, Time: 3.62s, Token/s: 141.41
Epoch: 0, Step: 1054, Batch(micro): 1054, Batch (considering grad accum): 131,  Loss: 8.6631, Time: 3.84s, Token/s: 133.37
Epoch: 0, Step: 1055, Batch(micro): 1055, Batch (considering grad accum): 131,  Loss: 8.6070, Time: 19.21s, Token/s: 26.65
Epoch: 0, Step: 1056, Batch(micro): 1056, Batch (considering grad accum): 132,  Loss: 9.1030, Time: 7.15s, Token/s: 71.65
Epoch: 0, Step: 1057, Batch(micro): 1057, Batch (considering grad accum): 132,  Loss: 8.6281, Time: 4.48s, Token/s: 114.19
Epoch: 0, Step: 1058, Batch(micro): 1058, Batch (considering grad accum): 132,  Loss: 8.6899, Time: 3.82s, Token/s: 134.16
Epoch: 0, Step: 1059, Batch(micro): 1059, Batch (considering grad accum): 132,  Loss: 8.7214, Time: 3.31s, Token/s: 154.47
Epoch: 0, Step: 1060, Batch(micro): 1060, Batch (considering grad accum): 132,  Loss: 8.5938, Time: 3.68s, Token/s: 139.28
Epoch: 0, Step: 1061, Batch(micro): 1061, Batch (considering grad accum): 132,  Loss: 8.4282, Time: 3.76s, Token/s: 136.01
Epoch: 0, Step: 1062, Batch(micro): 1062, Batch (considering grad accum): 132,  Loss: 8.6956, Time: 3.63s, Token/s: 141.11
Epoch: 0, Step: 1063, Batch(micro): 1063, Batch (considering grad accum): 132,  Loss: 8.9942, Time: 19.26s, Token/s: 26.58
Epoch: 0, Step: 1064, Batch(micro): 1064, Batch (considering grad accum): 133,  Loss: 8.4192, Time: 7.09s, Token/s: 72.20
Epoch: 0, Step: 1065, Batch(micro): 1065, Batch (considering grad accum): 133,  Loss: 8.8182, Time: 4.60s, Token/s: 111.24
Epoch: 0, Step: 1066, Batch(micro): 1066, Batch (considering grad accum): 133,  Loss: 8.5949, Time: 3.43s, Token/s: 149.06
Epoch: 0, Step: 1067, Batch(micro): 1067, Batch (considering grad accum): 133,  Loss: 8.4113, Time: 3.71s, Token/s: 138.15
Epoch: 0, Step: 1068, Batch(micro): 1068, Batch (considering grad accum): 133,  Loss: 8.7378, Time: 3.88s, Token/s: 131.98
Epoch: 0, Step: 1069, Batch(micro): 1069, Batch (considering grad accum): 133,  Loss: 8.7488, Time: 3.76s, Token/s: 136.12
Epoch: 0, Step: 1070, Batch(micro): 1070, Batch (considering grad accum): 133,  Loss: 8.5735, Time: 3.66s, Token/s: 139.95
Epoch: 0, Step: 1071, Batch(micro): 1071, Batch (considering grad accum): 133,  Loss: 8.7691, Time: 18.91s, Token/s: 27.08
Epoch: 0, Step: 1072, Batch(micro): 1072, Batch (considering grad accum): 134,  Loss: 8.5275, Time: 7.72s, Token/s: 66.34
Epoch: 0, Step: 1073, Batch(micro): 1073, Batch (considering grad accum): 134,  Loss: 8.8036, Time: 4.31s, Token/s: 118.80
Epoch: 0, Step: 1074, Batch(micro): 1074, Batch (considering grad accum): 134,  Loss: 8.6649, Time: 3.89s, Token/s: 131.76
Epoch: 0, Step: 1075, Batch(micro): 1075, Batch (considering grad accum): 134,  Loss: 8.6626, Time: 3.58s, Token/s: 142.95
Epoch: 0, Step: 1076, Batch(micro): 1076, Batch (considering grad accum): 134,  Loss: 8.7002, Time: 3.23s, Token/s: 158.28
Epoch: 0, Step: 1077, Batch(micro): 1077, Batch (considering grad accum): 134,  Loss: 8.6779, Time: 3.41s, Token/s: 150.03
Epoch: 0, Step: 1078, Batch(micro): 1078, Batch (considering grad accum): 134,  Loss: 8.8985, Time: 3.57s, Token/s: 143.40
Epoch: 0, Step: 1079, Batch(micro): 1079, Batch (considering grad accum): 134,  Loss: 8.9721, Time: 18.63s, Token/s: 27.48
Epoch: 0, Step: 1080, Batch(micro): 1080, Batch (considering grad accum): 135,  Loss: 9.4007, Time: 6.61s, Token/s: 77.47
Epoch: 0, Step: 1081, Batch(micro): 1081, Batch (considering grad accum): 135,  Loss: 9.2749, Time: 4.10s, Token/s: 124.83
Epoch: 0, Step: 1082, Batch(micro): 1082, Batch (considering grad accum): 135,  Loss: 8.5279, Time: 3.92s, Token/s: 130.65
Epoch: 0, Step: 1083, Batch(micro): 1083, Batch (considering grad accum): 135,  Loss: 8.6666, Time: 4.05s, Token/s: 126.38
Epoch: 0, Step: 1084, Batch(micro): 1084, Batch (considering grad accum): 135,  Loss: 8.8187, Time: 3.49s, Token/s: 146.70
Epoch: 0, Step: 1085, Batch(micro): 1085, Batch (considering grad accum): 135,  Loss: 8.6355, Time: 3.59s, Token/s: 142.74
Epoch: 0, Step: 1086, Batch(micro): 1086, Batch (considering grad accum): 135,  Loss: 8.8482, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 1087, Batch(micro): 1087, Batch (considering grad accum): 135,  Loss: 8.7908, Time: 19.03s, Token/s: 26.91
Epoch: 0, Step: 1088, Batch(micro): 1088, Batch (considering grad accum): 136,  Loss: 8.7404, Time: 6.88s, Token/s: 74.47
Epoch: 0, Step: 1089, Batch(micro): 1089, Batch (considering grad accum): 136,  Loss: 8.7008, Time: 4.62s, Token/s: 110.91
Epoch: 0, Step: 1090, Batch(micro): 1090, Batch (considering grad accum): 136,  Loss: 8.6772, Time: 3.77s, Token/s: 135.82
Epoch: 0, Step: 1091, Batch(micro): 1091, Batch (considering grad accum): 136,  Loss: 8.5668, Time: 3.47s, Token/s: 147.63
Epoch: 0, Step: 1092, Batch(micro): 1092, Batch (considering grad accum): 136,  Loss: 8.8752, Time: 3.80s, Token/s: 134.60
Epoch: 0, Step: 1093, Batch(micro): 1093, Batch (considering grad accum): 136,  Loss: 8.8214, Time: 3.63s, Token/s: 141.21
Epoch: 0, Step: 1094, Batch(micro): 1094, Batch (considering grad accum): 136,  Loss: 8.7004, Time: 3.65s, Token/s: 140.13
Epoch: 0, Step: 1095, Batch(micro): 1095, Batch (considering grad accum): 136,  Loss: 8.5418, Time: 23.41s, Token/s: 21.87
Epoch: 0, Step: 1096, Batch(micro): 1096, Batch (considering grad accum): 137,  Loss: 8.7344, Time: 9.79s, Token/s: 52.31
Epoch: 0, Step: 1097, Batch(micro): 1097, Batch (considering grad accum): 137,  Loss: 8.6328, Time: 4.52s, Token/s: 113.18
Epoch: 0, Step: 1098, Batch(micro): 1098, Batch (considering grad accum): 137,  Loss: 8.1602, Time: 3.50s, Token/s: 146.41
Epoch: 0, Step: 1099, Batch(micro): 1099, Batch (considering grad accum): 137,  Loss: 8.2061, Time: 3.87s, Token/s: 132.39
Updating MLP bias
Epoch: 0, Step: 1100, Batch(micro): 1100, Batch (considering grad accum): 137,  Loss: 8.6033, Time: 3.72s, Token/s: 137.60
Epoch: 0, Step: 1101, Batch(micro): 1101, Batch (considering grad accum): 137,  Loss: 8.5599, Time: 3.62s, Token/s: 141.53
Epoch: 0, Step: 1102, Batch(micro): 1102, Batch (considering grad accum): 137,  Loss: 8.7525, Time: 3.73s, Token/s: 137.17
Epoch: 0, Step: 1103, Batch(micro): 1103, Batch (considering grad accum): 137,  Loss: 8.2186, Time: 25.59s, Token/s: 20.01
Epoch: 0, Step: 1104, Batch(micro): 1104, Batch (considering grad accum): 138,  Loss: 8.3876, Time: 8.67s, Token/s: 59.09
Epoch: 0, Step: 1105, Batch(micro): 1105, Batch (considering grad accum): 138,  Loss: 8.4225, Time: 4.39s, Token/s: 116.67
Epoch: 0, Step: 1106, Batch(micro): 1106, Batch (considering grad accum): 138,  Loss: 8.7807, Time: 3.67s, Token/s: 139.46
Epoch: 0, Step: 1107, Batch(micro): 1107, Batch (considering grad accum): 138,  Loss: 8.8521, Time: 3.63s, Token/s: 141.12
Epoch: 0, Step: 1108, Batch(micro): 1108, Batch (considering grad accum): 138,  Loss: 8.9569, Time: 3.94s, Token/s: 129.81
Epoch: 0, Step: 1109, Batch(micro): 1109, Batch (considering grad accum): 138,  Loss: 8.6764, Time: 3.92s, Token/s: 130.71
Epoch: 0, Step: 1110, Batch(micro): 1110, Batch (considering grad accum): 138,  Loss: 8.7908, Time: 3.81s, Token/s: 134.32
Epoch: 0, Step: 1111, Batch(micro): 1111, Batch (considering grad accum): 138,  Loss: 8.5760, Time: 24.47s, Token/s: 20.92
Epoch: 0, Step: 1112, Batch(micro): 1112, Batch (considering grad accum): 139,  Loss: 8.7823, Time: 6.66s, Token/s: 76.93
Epoch: 0, Step: 1113, Batch(micro): 1113, Batch (considering grad accum): 139,  Loss: 8.6854, Time: 4.43s, Token/s: 115.52
Epoch: 0, Step: 1114, Batch(micro): 1114, Batch (considering grad accum): 139,  Loss: 8.4588, Time: 4.03s, Token/s: 127.13
Epoch: 0, Step: 1115, Batch(micro): 1115, Batch (considering grad accum): 139,  Loss: 8.3648, Time: 3.49s, Token/s: 146.83
Epoch: 0, Step: 1116, Batch(micro): 1116, Batch (considering grad accum): 139,  Loss: 8.4855, Time: 3.83s, Token/s: 133.54
Epoch: 0, Step: 1117, Batch(micro): 1117, Batch (considering grad accum): 139,  Loss: 8.5705, Time: 3.86s, Token/s: 132.75
Epoch: 0, Step: 1118, Batch(micro): 1118, Batch (considering grad accum): 139,  Loss: 9.3809, Time: 3.71s, Token/s: 137.96
Epoch: 0, Step: 1119, Batch(micro): 1119, Batch (considering grad accum): 139,  Loss: 8.9256, Time: 22.54s, Token/s: 22.72
Epoch: 0, Step: 1120, Batch(micro): 1120, Batch (considering grad accum): 140,  Loss: 8.3026, Time: 7.58s, Token/s: 67.53
Epoch: 0, Step: 1121, Batch(micro): 1121, Batch (considering grad accum): 140,  Loss: 8.5395, Time: 4.61s, Token/s: 111.06
Epoch: 0, Step: 1122, Batch(micro): 1122, Batch (considering grad accum): 140,  Loss: 8.5654, Time: 4.15s, Token/s: 123.44
Epoch: 0, Step: 1123, Batch(micro): 1123, Batch (considering grad accum): 140,  Loss: 8.9715, Time: 3.53s, Token/s: 145.22
Epoch: 0, Step: 1124, Batch(micro): 1124, Batch (considering grad accum): 140,  Loss: 8.9436, Time: 4.07s, Token/s: 125.93
Epoch: 0, Step: 1125, Batch(micro): 1125, Batch (considering grad accum): 140,  Loss: 8.5299, Time: 3.98s, Token/s: 128.62
Epoch: 0, Step: 1126, Batch(micro): 1126, Batch (considering grad accum): 140,  Loss: 8.6703, Time: 3.42s, Token/s: 149.76
Epoch: 0, Step: 1127, Batch(micro): 1127, Batch (considering grad accum): 140,  Loss: 8.7322, Time: 23.05s, Token/s: 22.21
Epoch: 0, Step: 1128, Batch(micro): 1128, Batch (considering grad accum): 141,  Loss: 8.3960, Time: 7.65s, Token/s: 66.90
Epoch: 0, Step: 1129, Batch(micro): 1129, Batch (considering grad accum): 141,  Loss: 8.8087, Time: 4.41s, Token/s: 116.04
Epoch: 0, Step: 1130, Batch(micro): 1130, Batch (considering grad accum): 141,  Loss: 8.5769, Time: 3.17s, Token/s: 161.43
Epoch: 0, Step: 1131, Batch(micro): 1131, Batch (considering grad accum): 141,  Loss: 8.4268, Time: 3.94s, Token/s: 129.81
Epoch: 0, Step: 1132, Batch(micro): 1132, Batch (considering grad accum): 141,  Loss: 8.3701, Time: 3.42s, Token/s: 149.65
Epoch: 0, Step: 1133, Batch(micro): 1133, Batch (considering grad accum): 141,  Loss: 8.3818, Time: 3.40s, Token/s: 150.42
Epoch: 0, Step: 1134, Batch(micro): 1134, Batch (considering grad accum): 141,  Loss: 8.4742, Time: 3.58s, Token/s: 142.86
Epoch: 0, Step: 1135, Batch(micro): 1135, Batch (considering grad accum): 141,  Loss: 8.4308, Time: 23.32s, Token/s: 21.96
Epoch: 0, Step: 1136, Batch(micro): 1136, Batch (considering grad accum): 142,  Loss: 8.6173, Time: 8.65s, Token/s: 59.21
Epoch: 0, Step: 1137, Batch(micro): 1137, Batch (considering grad accum): 142,  Loss: 8.3763, Time: 4.00s, Token/s: 127.99
Epoch: 0, Step: 1138, Batch(micro): 1138, Batch (considering grad accum): 142,  Loss: 8.2692, Time: 3.64s, Token/s: 140.75
Epoch: 0, Step: 1139, Batch(micro): 1139, Batch (considering grad accum): 142,  Loss: 8.4958, Time: 3.86s, Token/s: 132.64
Epoch: 0, Step: 1140, Batch(micro): 1140, Batch (considering grad accum): 142,  Loss: 8.6985, Time: 4.01s, Token/s: 127.58
Epoch: 0, Step: 1141, Batch(micro): 1141, Batch (considering grad accum): 142,  Loss: 8.8344, Time: 3.55s, Token/s: 144.14
Epoch: 0, Step: 1142, Batch(micro): 1142, Batch (considering grad accum): 142,  Loss: 8.6663, Time: 3.63s, Token/s: 141.12
Epoch: 0, Step: 1143, Batch(micro): 1143, Batch (considering grad accum): 142,  Loss: 8.6276, Time: 24.31s, Token/s: 21.06
Epoch: 0, Step: 1144, Batch(micro): 1144, Batch (considering grad accum): 143,  Loss: 8.9042, Time: 8.27s, Token/s: 61.89
Epoch: 0, Step: 1145, Batch(micro): 1145, Batch (considering grad accum): 143,  Loss: 8.6658, Time: 4.54s, Token/s: 112.69
Epoch: 0, Step: 1146, Batch(micro): 1146, Batch (considering grad accum): 143,  Loss: 8.5194, Time: 4.35s, Token/s: 117.82
Epoch: 0, Step: 1147, Batch(micro): 1147, Batch (considering grad accum): 143,  Loss: 8.5912, Time: 3.42s, Token/s: 149.91
Epoch: 0, Step: 1148, Batch(micro): 1148, Batch (considering grad accum): 143,  Loss: 8.6662, Time: 4.10s, Token/s: 124.92
Epoch: 0, Step: 1149, Batch(micro): 1149, Batch (considering grad accum): 143,  Loss: 8.4905, Time: 3.43s, Token/s: 149.12
Epoch: 0, Step: 1150, Batch(micro): 1150, Batch (considering grad accum): 143,  Loss: 8.6961, Time: 3.47s, Token/s: 147.59
Epoch: 0, Step: 1151, Batch(micro): 1151, Batch (considering grad accum): 143,  Loss: 8.3923, Time: 27.25s, Token/s: 18.79
Epoch: 0, Step: 1152, Batch(micro): 1152, Batch (considering grad accum): 144,  Loss: 8.3356, Time: 8.55s, Token/s: 59.92
Epoch: 0, Step: 1153, Batch(micro): 1153, Batch (considering grad accum): 144,  Loss: 8.6607, Time: 4.34s, Token/s: 118.08
Epoch: 0, Step: 1154, Batch(micro): 1154, Batch (considering grad accum): 144,  Loss: 8.3348, Time: 3.79s, Token/s: 135.26
Epoch: 0, Step: 1155, Batch(micro): 1155, Batch (considering grad accum): 144,  Loss: 8.2241, Time: 3.96s, Token/s: 129.21
Epoch: 0, Step: 1156, Batch(micro): 1156, Batch (considering grad accum): 144,  Loss: 8.3048, Time: 3.64s, Token/s: 140.74
Epoch: 0, Step: 1157, Batch(micro): 1157, Batch (considering grad accum): 144,  Loss: 8.3540, Time: 3.58s, Token/s: 142.97
Epoch: 0, Step: 1158, Batch(micro): 1158, Batch (considering grad accum): 144,  Loss: 8.7690, Time: 3.89s, Token/s: 131.57
Epoch: 0, Step: 1159, Batch(micro): 1159, Batch (considering grad accum): 144,  Loss: 8.3504, Time: 30.08s, Token/s: 17.02
Epoch: 0, Step: 1160, Batch(micro): 1160, Batch (considering grad accum): 145,  Loss: 8.4041, Time: 9.02s, Token/s: 56.75
Epoch: 0, Step: 1161, Batch(micro): 1161, Batch (considering grad accum): 145,  Loss: 8.2494, Time: 4.25s, Token/s: 120.44
Epoch: 0, Step: 1162, Batch(micro): 1162, Batch (considering grad accum): 145,  Loss: 8.4419, Time: 3.87s, Token/s: 132.20
Epoch: 0, Step: 1163, Batch(micro): 1163, Batch (considering grad accum): 145,  Loss: 8.6308, Time: 3.63s, Token/s: 141.22
Epoch: 0, Step: 1164, Batch(micro): 1164, Batch (considering grad accum): 145,  Loss: 8.6716, Time: 3.59s, Token/s: 142.82
Epoch: 0, Step: 1165, Batch(micro): 1165, Batch (considering grad accum): 145,  Loss: 8.4705, Time: 3.55s, Token/s: 144.29
Epoch: 0, Step: 1166, Batch(micro): 1166, Batch (considering grad accum): 145,  Loss: 8.5894, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 1167, Batch(micro): 1167, Batch (considering grad accum): 145,  Loss: 8.3957, Time: 23.53s, Token/s: 21.76
Epoch: 0, Step: 1168, Batch(micro): 1168, Batch (considering grad accum): 146,  Loss: 8.7043, Time: 7.08s, Token/s: 72.33
Epoch: 0, Step: 1169, Batch(micro): 1169, Batch (considering grad accum): 146,  Loss: 8.3217, Time: 4.09s, Token/s: 125.15
Epoch: 0, Step: 1170, Batch(micro): 1170, Batch (considering grad accum): 146,  Loss: 8.4541, Time: 3.66s, Token/s: 139.99
Epoch: 0, Step: 1171, Batch(micro): 1171, Batch (considering grad accum): 146,  Loss: 8.3579, Time: 3.53s, Token/s: 145.14
Epoch: 0, Step: 1172, Batch(micro): 1172, Batch (considering grad accum): 146,  Loss: 8.4837, Time: 3.51s, Token/s: 145.96
Epoch: 0, Step: 1173, Batch(micro): 1173, Batch (considering grad accum): 146,  Loss: 8.5898, Time: 3.74s, Token/s: 136.77
Epoch: 0, Step: 1174, Batch(micro): 1174, Batch (considering grad accum): 146,  Loss: 8.5584, Time: 3.89s, Token/s: 131.65
Epoch: 0, Step: 1175, Batch(micro): 1175, Batch (considering grad accum): 146,  Loss: 8.1271, Time: 22.38s, Token/s: 22.88
Epoch: 0, Step: 1176, Batch(micro): 1176, Batch (considering grad accum): 147,  Loss: 8.6569, Time: 6.20s, Token/s: 82.54
Epoch: 0, Step: 1177, Batch(micro): 1177, Batch (considering grad accum): 147,  Loss: 8.6150, Time: 4.01s, Token/s: 127.73
Epoch: 0, Step: 1178, Batch(micro): 1178, Batch (considering grad accum): 147,  Loss: 8.0061, Time: 4.21s, Token/s: 121.70
Epoch: 0, Step: 1179, Batch(micro): 1179, Batch (considering grad accum): 147,  Loss: 8.4243, Time: 4.16s, Token/s: 122.94
Epoch: 0, Step: 1180, Batch(micro): 1180, Batch (considering grad accum): 147,  Loss: 8.2601, Time: 3.87s, Token/s: 132.35
Epoch: 0, Step: 1181, Batch(micro): 1181, Batch (considering grad accum): 147,  Loss: 8.6857, Time: 3.74s, Token/s: 136.91
Epoch: 0, Step: 1182, Batch(micro): 1182, Batch (considering grad accum): 147,  Loss: 8.5632, Time: 3.64s, Token/s: 140.75
Epoch: 0, Step: 1183, Batch(micro): 1183, Batch (considering grad accum): 147,  Loss: 8.4391, Time: 21.58s, Token/s: 23.72
Epoch: 0, Step: 1184, Batch(micro): 1184, Batch (considering grad accum): 148,  Loss: 8.4675, Time: 8.20s, Token/s: 62.42
Epoch: 0, Step: 1185, Batch(micro): 1185, Batch (considering grad accum): 148,  Loss: 8.5252, Time: 3.34s, Token/s: 153.15
Epoch: 0, Step: 1186, Batch(micro): 1186, Batch (considering grad accum): 148,  Loss: 9.0246, Time: 3.28s, Token/s: 156.29
Epoch: 0, Step: 1187, Batch(micro): 1187, Batch (considering grad accum): 148,  Loss: 8.3888, Time: 3.30s, Token/s: 155.13
Epoch: 0, Step: 1188, Batch(micro): 1188, Batch (considering grad accum): 148,  Loss: 8.5647, Time: 3.63s, Token/s: 141.08
Epoch: 0, Step: 1189, Batch(micro): 1189, Batch (considering grad accum): 148,  Loss: 8.6953, Time: 3.58s, Token/s: 142.99
Epoch: 0, Step: 1190, Batch(micro): 1190, Batch (considering grad accum): 148,  Loss: 8.4200, Time: 3.98s, Token/s: 128.72
Epoch: 0, Step: 1191, Batch(micro): 1191, Batch (considering grad accum): 148,  Loss: 8.4019, Time: 21.25s, Token/s: 24.09
Epoch: 0, Step: 1192, Batch(micro): 1192, Batch (considering grad accum): 149,  Loss: 8.3510, Time: 7.95s, Token/s: 64.42
Epoch: 0, Step: 1193, Batch(micro): 1193, Batch (considering grad accum): 149,  Loss: 8.3977, Time: 3.82s, Token/s: 133.90
Epoch: 0, Step: 1194, Batch(micro): 1194, Batch (considering grad accum): 149,  Loss: 8.1791, Time: 4.64s, Token/s: 110.25
Epoch: 0, Step: 1195, Batch(micro): 1195, Batch (considering grad accum): 149,  Loss: 8.0013, Time: 3.66s, Token/s: 139.70
Epoch: 0, Step: 1196, Batch(micro): 1196, Batch (considering grad accum): 149,  Loss: 8.4117, Time: 3.78s, Token/s: 135.33
Epoch: 0, Step: 1197, Batch(micro): 1197, Batch (considering grad accum): 149,  Loss: 8.1735, Time: 3.61s, Token/s: 141.63
Epoch: 0, Step: 1198, Batch(micro): 1198, Batch (considering grad accum): 149,  Loss: 8.4470, Time: 4.08s, Token/s: 125.34
Epoch: 0, Step: 1199, Batch(micro): 1199, Batch (considering grad accum): 149,  Loss: 8.4230, Time: 23.84s, Token/s: 21.47
Updating MLP bias
Epoch: 0, Step: 1200, Batch(micro): 1200, Batch (considering grad accum): 150,  Loss: 8.3961, Time: 7.63s, Token/s: 67.10
Epoch: 0, Step: 1201, Batch(micro): 1201, Batch (considering grad accum): 150,  Loss: 8.5415, Time: 3.87s, Token/s: 132.15
Epoch: 0, Step: 1202, Batch(micro): 1202, Batch (considering grad accum): 150,  Loss: 8.6330, Time: 3.51s, Token/s: 145.86
Epoch: 0, Step: 1203, Batch(micro): 1203, Batch (considering grad accum): 150,  Loss: 8.9120, Time: 3.66s, Token/s: 139.82
Epoch: 0, Step: 1204, Batch(micro): 1204, Batch (considering grad accum): 150,  Loss: 8.3322, Time: 3.70s, Token/s: 138.21
Epoch: 0, Step: 1205, Batch(micro): 1205, Batch (considering grad accum): 150,  Loss: 8.4323, Time: 3.75s, Token/s: 136.66
Epoch: 0, Step: 1206, Batch(micro): 1206, Batch (considering grad accum): 150,  Loss: 8.8047, Time: 3.79s, Token/s: 135.05
Epoch: 0, Step: 1207, Batch(micro): 1207, Batch (considering grad accum): 150,  Loss: 8.4223, Time: 20.79s, Token/s: 24.63
Epoch: 0, Step: 1208, Batch(micro): 1208, Batch (considering grad accum): 151,  Loss: 8.3671, Time: 8.82s, Token/s: 58.03
Epoch: 0, Step: 1209, Batch(micro): 1209, Batch (considering grad accum): 151,  Loss: 8.0362, Time: 4.37s, Token/s: 117.11
Epoch: 0, Step: 1210, Batch(micro): 1210, Batch (considering grad accum): 151,  Loss: 8.2694, Time: 4.14s, Token/s: 123.61
Epoch: 0, Step: 1211, Batch(micro): 1211, Batch (considering grad accum): 151,  Loss: 8.8295, Time: 3.71s, Token/s: 137.97
Epoch: 0, Step: 1212, Batch(micro): 1212, Batch (considering grad accum): 151,  Loss: 8.1690, Time: 3.50s, Token/s: 146.32
Epoch: 0, Step: 1213, Batch(micro): 1213, Batch (considering grad accum): 151,  Loss: 8.2974, Time: 3.54s, Token/s: 144.68
Epoch: 0, Step: 1214, Batch(micro): 1214, Batch (considering grad accum): 151,  Loss: 8.4528, Time: 3.67s, Token/s: 139.66
Epoch: 0, Step: 1215, Batch(micro): 1215, Batch (considering grad accum): 151,  Loss: 8.6578, Time: 22.34s, Token/s: 22.92
Epoch: 0, Step: 1216, Batch(micro): 1216, Batch (considering grad accum): 152,  Loss: 8.5654, Time: 8.05s, Token/s: 63.59
Epoch: 0, Step: 1217, Batch(micro): 1217, Batch (considering grad accum): 152,  Loss: 8.2636, Time: 3.98s, Token/s: 128.78
Epoch: 0, Step: 1218, Batch(micro): 1218, Batch (considering grad accum): 152,  Loss: 8.1717, Time: 3.71s, Token/s: 138.11
Epoch: 0, Step: 1219, Batch(micro): 1219, Batch (considering grad accum): 152,  Loss: 8.3745, Time: 3.70s, Token/s: 138.34
Epoch: 0, Step: 1220, Batch(micro): 1220, Batch (considering grad accum): 152,  Loss: 8.6958, Time: 3.71s, Token/s: 137.95
Epoch: 0, Step: 1221, Batch(micro): 1221, Batch (considering grad accum): 152,  Loss: 8.2821, Time: 3.80s, Token/s: 134.68
Epoch: 0, Step: 1222, Batch(micro): 1222, Batch (considering grad accum): 152,  Loss: 8.3464, Time: 3.71s, Token/s: 137.98
Epoch: 0, Step: 1223, Batch(micro): 1223, Batch (considering grad accum): 152,  Loss: 8.2690, Time: 25.48s, Token/s: 20.09
Epoch: 0, Step: 1224, Batch(micro): 1224, Batch (considering grad accum): 153,  Loss: 8.4948, Time: 8.14s, Token/s: 62.89
Epoch: 0, Step: 1225, Batch(micro): 1225, Batch (considering grad accum): 153,  Loss: 8.8075, Time: 4.38s, Token/s: 116.93
Epoch: 0, Step: 1226, Batch(micro): 1226, Batch (considering grad accum): 153,  Loss: 8.4594, Time: 3.72s, Token/s: 137.58
Epoch: 0, Step: 1227, Batch(micro): 1227, Batch (considering grad accum): 153,  Loss: 8.0156, Time: 3.52s, Token/s: 145.48
Epoch: 0, Step: 1228, Batch(micro): 1228, Batch (considering grad accum): 153,  Loss: 8.1744, Time: 3.65s, Token/s: 140.17
Epoch: 0, Step: 1229, Batch(micro): 1229, Batch (considering grad accum): 153,  Loss: 8.3404, Time: 3.57s, Token/s: 143.56
Epoch: 0, Step: 1230, Batch(micro): 1230, Batch (considering grad accum): 153,  Loss: 8.1026, Time: 3.68s, Token/s: 139.03
Epoch: 0, Step: 1231, Batch(micro): 1231, Batch (considering grad accum): 153,  Loss: 8.4753, Time: 24.54s, Token/s: 20.86
Epoch: 0, Step: 1232, Batch(micro): 1232, Batch (considering grad accum): 154,  Loss: 8.5794, Time: 8.07s, Token/s: 63.42
Epoch: 0, Step: 1233, Batch(micro): 1233, Batch (considering grad accum): 154,  Loss: 8.4510, Time: 4.53s, Token/s: 112.99
Epoch: 0, Step: 1234, Batch(micro): 1234, Batch (considering grad accum): 154,  Loss: 8.0667, Time: 3.73s, Token/s: 137.12
Epoch: 0, Step: 1235, Batch(micro): 1235, Batch (considering grad accum): 154,  Loss: 8.4707, Time: 3.81s, Token/s: 134.41
Epoch: 0, Step: 1236, Batch(micro): 1236, Batch (considering grad accum): 154,  Loss: 8.5729, Time: 3.73s, Token/s: 137.32
Epoch: 0, Step: 1237, Batch(micro): 1237, Batch (considering grad accum): 154,  Loss: 8.7340, Time: 3.50s, Token/s: 146.49
Epoch: 0, Step: 1238, Batch(micro): 1238, Batch (considering grad accum): 154,  Loss: 8.3102, Time: 3.47s, Token/s: 147.43
Epoch: 0, Step: 1239, Batch(micro): 1239, Batch (considering grad accum): 154,  Loss: 8.2804, Time: 23.55s, Token/s: 21.74
Epoch: 0, Step: 1240, Batch(micro): 1240, Batch (considering grad accum): 155,  Loss: 8.4448, Time: 7.70s, Token/s: 66.46
Epoch: 0, Step: 1241, Batch(micro): 1241, Batch (considering grad accum): 155,  Loss: 9.0579, Time: 4.06s, Token/s: 126.17
Epoch: 0, Step: 1242, Batch(micro): 1242, Batch (considering grad accum): 155,  Loss: 8.7122, Time: 4.54s, Token/s: 112.77
Epoch: 0, Step: 1243, Batch(micro): 1243, Batch (considering grad accum): 155,  Loss: 8.8081, Time: 3.63s, Token/s: 140.86
Epoch: 0, Step: 1244, Batch(micro): 1244, Batch (considering grad accum): 155,  Loss: 8.1822, Time: 3.48s, Token/s: 147.14
Epoch: 0, Step: 1245, Batch(micro): 1245, Batch (considering grad accum): 155,  Loss: 8.0725, Time: 3.62s, Token/s: 141.48
Epoch: 0, Step: 1246, Batch(micro): 1246, Batch (considering grad accum): 155,  Loss: 8.0510, Time: 3.93s, Token/s: 130.33
Epoch: 0, Step: 1247, Batch(micro): 1247, Batch (considering grad accum): 155,  Loss: 8.2524, Time: 22.74s, Token/s: 22.51
Epoch: 0, Step: 1248, Batch(micro): 1248, Batch (considering grad accum): 156,  Loss: 8.3320, Time: 9.04s, Token/s: 56.64
Epoch: 0, Step: 1249, Batch(micro): 1249, Batch (considering grad accum): 156,  Loss: 8.5740, Time: 4.62s, Token/s: 110.73
Epoch: 0, Step: 1250, Batch(micro): 1250, Batch (considering grad accum): 156,  Loss: 8.1597, Time: 4.20s, Token/s: 121.87
Epoch: 0, Step: 1251, Batch(micro): 1251, Batch (considering grad accum): 156,  Loss: 8.3102, Time: 4.11s, Token/s: 124.63
Epoch: 0, Step: 1252, Batch(micro): 1252, Batch (considering grad accum): 156,  Loss: 8.4453, Time: 3.91s, Token/s: 131.05
Epoch: 0, Step: 1253, Batch(micro): 1253, Batch (considering grad accum): 156,  Loss: 8.5213, Time: 3.77s, Token/s: 135.74
Epoch: 0, Step: 1254, Batch(micro): 1254, Batch (considering grad accum): 156,  Loss: 8.4131, Time: 3.81s, Token/s: 134.31
Epoch: 0, Step: 1255, Batch(micro): 1255, Batch (considering grad accum): 156,  Loss: 7.9459, Time: 28.95s, Token/s: 17.68
Epoch: 0, Step: 1256, Batch(micro): 1256, Batch (considering grad accum): 157,  Loss: 8.2365, Time: 7.75s, Token/s: 66.07
Epoch: 0, Step: 1257, Batch(micro): 1257, Batch (considering grad accum): 157,  Loss: 8.4220, Time: 4.02s, Token/s: 127.40
Epoch: 0, Step: 1258, Batch(micro): 1258, Batch (considering grad accum): 157,  Loss: 8.0694, Time: 4.63s, Token/s: 110.69
Epoch: 0, Step: 1259, Batch(micro): 1259, Batch (considering grad accum): 157,  Loss: 8.4995, Time: 3.74s, Token/s: 137.08
Epoch: 0, Step: 1260, Batch(micro): 1260, Batch (considering grad accum): 157,  Loss: 8.2636, Time: 4.07s, Token/s: 125.74
Epoch: 0, Step: 1261, Batch(micro): 1261, Batch (considering grad accum): 157,  Loss: 8.2656, Time: 3.48s, Token/s: 146.95
Epoch: 0, Step: 1262, Batch(micro): 1262, Batch (considering grad accum): 157,  Loss: 8.0662, Time: 3.65s, Token/s: 140.29
Epoch: 0, Step: 1263, Batch(micro): 1263, Batch (considering grad accum): 157,  Loss: 8.5936, Time: 25.66s, Token/s: 19.95
Epoch: 0, Step: 1264, Batch(micro): 1264, Batch (considering grad accum): 158,  Loss: 8.4827, Time: 8.95s, Token/s: 57.23
Epoch: 0, Step: 1265, Batch(micro): 1265, Batch (considering grad accum): 158,  Loss: 8.1844, Time: 4.16s, Token/s: 122.98
Epoch: 0, Step: 1266, Batch(micro): 1266, Batch (considering grad accum): 158,  Loss: 8.5663, Time: 4.01s, Token/s: 127.80
Epoch: 0, Step: 1267, Batch(micro): 1267, Batch (considering grad accum): 158,  Loss: 8.6939, Time: 3.90s, Token/s: 131.33
Epoch: 0, Step: 1268, Batch(micro): 1268, Batch (considering grad accum): 158,  Loss: 8.1550, Time: 3.66s, Token/s: 139.98
Epoch: 0, Step: 1269, Batch(micro): 1269, Batch (considering grad accum): 158,  Loss: 8.3302, Time: 3.24s, Token/s: 158.04
Epoch: 0, Step: 1270, Batch(micro): 1270, Batch (considering grad accum): 158,  Loss: 8.3179, Time: 3.46s, Token/s: 148.05
Epoch: 0, Step: 1271, Batch(micro): 1271, Batch (considering grad accum): 158,  Loss: 8.5537, Time: 24.06s, Token/s: 21.28
Epoch: 0, Step: 1272, Batch(micro): 1272, Batch (considering grad accum): 159,  Loss: 8.3135, Time: 7.41s, Token/s: 69.10
Epoch: 0, Step: 1273, Batch(micro): 1273, Batch (considering grad accum): 159,  Loss: 8.2445, Time: 4.15s, Token/s: 123.46
Epoch: 0, Step: 1274, Batch(micro): 1274, Batch (considering grad accum): 159,  Loss: 8.5509, Time: 4.13s, Token/s: 123.87
Epoch: 0, Step: 1275, Batch(micro): 1275, Batch (considering grad accum): 159,  Loss: 8.2908, Time: 4.07s, Token/s: 125.89
Epoch: 0, Step: 1276, Batch(micro): 1276, Batch (considering grad accum): 159,  Loss: 8.0620, Time: 3.60s, Token/s: 142.27
Epoch: 0, Step: 1277, Batch(micro): 1277, Batch (considering grad accum): 159,  Loss: 8.1496, Time: 3.70s, Token/s: 138.32
Epoch: 0, Step: 1278, Batch(micro): 1278, Batch (considering grad accum): 159,  Loss: 8.6347, Time: 3.87s, Token/s: 132.27
Epoch: 0, Step: 1279, Batch(micro): 1279, Batch (considering grad accum): 159,  Loss: 8.3647, Time: 21.47s, Token/s: 23.85
Epoch: 0, Step: 1280, Batch(micro): 1280, Batch (considering grad accum): 160,  Loss: 8.4537, Time: 7.96s, Token/s: 64.33
Epoch: 0, Step: 1281, Batch(micro): 1281, Batch (considering grad accum): 160,  Loss: 8.2593, Time: 4.25s, Token/s: 120.47
Epoch: 0, Step: 1282, Batch(micro): 1282, Batch (considering grad accum): 160,  Loss: 8.2307, Time: 3.44s, Token/s: 148.70
Epoch: 0, Step: 1283, Batch(micro): 1283, Batch (considering grad accum): 160,  Loss: 7.9264, Time: 3.82s, Token/s: 133.87
Epoch: 0, Step: 1284, Batch(micro): 1284, Batch (considering grad accum): 160,  Loss: 8.1499, Time: 4.14s, Token/s: 123.71
Epoch: 0, Step: 1285, Batch(micro): 1285, Batch (considering grad accum): 160,  Loss: 8.4043, Time: 3.96s, Token/s: 129.42
Epoch: 0, Step: 1286, Batch(micro): 1286, Batch (considering grad accum): 160,  Loss: 8.2274, Time: 3.73s, Token/s: 137.38
Epoch: 0, Step: 1287, Batch(micro): 1287, Batch (considering grad accum): 160,  Loss: 8.4040, Time: 19.32s, Token/s: 26.51
Epoch: 0, Step: 1288, Batch(micro): 1288, Batch (considering grad accum): 161,  Loss: 8.6022, Time: 6.17s, Token/s: 83.04
Epoch: 0, Step: 1289, Batch(micro): 1289, Batch (considering grad accum): 161,  Loss: 8.2341, Time: 4.30s, Token/s: 118.96
Epoch: 0, Step: 1290, Batch(micro): 1290, Batch (considering grad accum): 161,  Loss: 8.0536, Time: 3.50s, Token/s: 146.28
Epoch: 0, Step: 1291, Batch(micro): 1291, Batch (considering grad accum): 161,  Loss: 8.2431, Time: 3.99s, Token/s: 128.26
Epoch: 0, Step: 1292, Batch(micro): 1292, Batch (considering grad accum): 161,  Loss: 7.8944, Time: 3.33s, Token/s: 153.93
Epoch: 0, Step: 1293, Batch(micro): 1293, Batch (considering grad accum): 161,  Loss: 8.7606, Time: 4.05s, Token/s: 126.53
Epoch: 0, Step: 1294, Batch(micro): 1294, Batch (considering grad accum): 161,  Loss: 8.7609, Time: 3.56s, Token/s: 143.86
Epoch: 0, Step: 1295, Batch(micro): 1295, Batch (considering grad accum): 161,  Loss: 8.5702, Time: 20.07s, Token/s: 25.51
Epoch: 0, Step: 1296, Batch(micro): 1296, Batch (considering grad accum): 162,  Loss: 8.3096, Time: 6.63s, Token/s: 77.21
Epoch: 0, Step: 1297, Batch(micro): 1297, Batch (considering grad accum): 162,  Loss: 8.3165, Time: 3.89s, Token/s: 131.69
Epoch: 0, Step: 1298, Batch(micro): 1298, Batch (considering grad accum): 162,  Loss: 8.0487, Time: 3.61s, Token/s: 141.85
Epoch: 0, Step: 1299, Batch(micro): 1299, Batch (considering grad accum): 162,  Loss: 8.1129, Time: 3.61s, Token/s: 141.78
Updating MLP bias
Epoch: 0, Step: 1300, Batch(micro): 1300, Batch (considering grad accum): 162,  Loss: 8.0091, Time: 3.82s, Token/s: 134.06
Epoch: 0, Step: 1301, Batch(micro): 1301, Batch (considering grad accum): 162,  Loss: 7.8079, Time: 4.02s, Token/s: 127.46
Epoch: 0, Step: 1302, Batch(micro): 1302, Batch (considering grad accum): 162,  Loss: 8.1718, Time: 3.37s, Token/s: 151.87
Epoch: 0, Step: 1303, Batch(micro): 1303, Batch (considering grad accum): 162,  Loss: 7.9925, Time: 20.46s, Token/s: 25.03
Epoch: 0, Step: 1304, Batch(micro): 1304, Batch (considering grad accum): 163,  Loss: 8.4797, Time: 7.41s, Token/s: 69.12
Epoch: 0, Step: 1305, Batch(micro): 1305, Batch (considering grad accum): 163,  Loss: 8.2468, Time: 3.76s, Token/s: 136.26
Epoch: 0, Step: 1306, Batch(micro): 1306, Batch (considering grad accum): 163,  Loss: 8.3804, Time: 3.60s, Token/s: 142.04
Epoch: 0, Step: 1307, Batch(micro): 1307, Batch (considering grad accum): 163,  Loss: 8.4029, Time: 3.89s, Token/s: 131.74
Epoch: 0, Step: 1308, Batch(micro): 1308, Batch (considering grad accum): 163,  Loss: 8.2550, Time: 3.51s, Token/s: 145.93
Epoch: 0, Step: 1309, Batch(micro): 1309, Batch (considering grad accum): 163,  Loss: 8.3539, Time: 3.65s, Token/s: 140.28
Epoch: 0, Step: 1310, Batch(micro): 1310, Batch (considering grad accum): 163,  Loss: 8.1758, Time: 3.44s, Token/s: 148.83
Epoch: 0, Step: 1311, Batch(micro): 1311, Batch (considering grad accum): 163,  Loss: 8.5849, Time: 23.48s, Token/s: 21.80
Epoch: 0, Step: 1312, Batch(micro): 1312, Batch (considering grad accum): 164,  Loss: 8.0737, Time: 8.18s, Token/s: 62.59
Epoch: 0, Step: 1313, Batch(micro): 1313, Batch (considering grad accum): 164,  Loss: 8.1189, Time: 4.38s, Token/s: 116.83
Epoch: 0, Step: 1314, Batch(micro): 1314, Batch (considering grad accum): 164,  Loss: 8.1655, Time: 3.91s, Token/s: 130.84
Epoch: 0, Step: 1315, Batch(micro): 1315, Batch (considering grad accum): 164,  Loss: 8.4000, Time: 3.57s, Token/s: 143.39
Epoch: 0, Step: 1316, Batch(micro): 1316, Batch (considering grad accum): 164,  Loss: 8.4197, Time: 3.91s, Token/s: 131.06
Epoch: 0, Step: 1317, Batch(micro): 1317, Batch (considering grad accum): 164,  Loss: 8.6500, Time: 3.61s, Token/s: 141.88
Epoch: 0, Step: 1318, Batch(micro): 1318, Batch (considering grad accum): 164,  Loss: 8.6232, Time: 3.92s, Token/s: 130.67
Epoch: 0, Step: 1319, Batch(micro): 1319, Batch (considering grad accum): 164,  Loss: 7.7316, Time: 19.52s, Token/s: 26.23
Epoch: 0, Step: 1320, Batch(micro): 1320, Batch (considering grad accum): 165,  Loss: 8.4440, Time: 7.97s, Token/s: 64.26
Epoch: 0, Step: 1321, Batch(micro): 1321, Batch (considering grad accum): 165,  Loss: 8.1141, Time: 3.99s, Token/s: 128.42
Epoch: 0, Step: 1322, Batch(micro): 1322, Batch (considering grad accum): 165,  Loss: 8.3309, Time: 3.70s, Token/s: 138.55
Epoch: 0, Step: 1323, Batch(micro): 1323, Batch (considering grad accum): 165,  Loss: 8.2615, Time: 3.72s, Token/s: 137.69
Epoch: 0, Step: 1324, Batch(micro): 1324, Batch (considering grad accum): 165,  Loss: 8.4969, Time: 4.38s, Token/s: 116.99
Epoch: 0, Step: 1325, Batch(micro): 1325, Batch (considering grad accum): 165,  Loss: 8.5212, Time: 3.53s, Token/s: 144.94
Epoch: 0, Step: 1326, Batch(micro): 1326, Batch (considering grad accum): 165,  Loss: 8.3017, Time: 3.82s, Token/s: 134.10
Epoch: 0, Step: 1327, Batch(micro): 1327, Batch (considering grad accum): 165,  Loss: 8.4029, Time: 23.48s, Token/s: 21.80
Epoch: 0, Step: 1328, Batch(micro): 1328, Batch (considering grad accum): 166,  Loss: 8.1061, Time: 8.89s, Token/s: 57.58
Epoch: 0, Step: 1329, Batch(micro): 1329, Batch (considering grad accum): 166,  Loss: 8.0175, Time: 4.01s, Token/s: 127.61
Epoch: 0, Step: 1330, Batch(micro): 1330, Batch (considering grad accum): 166,  Loss: 8.0263, Time: 3.40s, Token/s: 150.41
Epoch: 0, Step: 1331, Batch(micro): 1331, Batch (considering grad accum): 166,  Loss: 8.8905, Time: 3.53s, Token/s: 145.22
Epoch: 0, Step: 1332, Batch(micro): 1332, Batch (considering grad accum): 166,  Loss: 8.2060, Time: 3.46s, Token/s: 147.92
Epoch: 0, Step: 1333, Batch(micro): 1333, Batch (considering grad accum): 166,  Loss: 8.0406, Time: 3.55s, Token/s: 144.14
Epoch: 0, Step: 1334, Batch(micro): 1334, Batch (considering grad accum): 166,  Loss: 7.6727, Time: 3.64s, Token/s: 140.50
Epoch: 0, Step: 1335, Batch(micro): 1335, Batch (considering grad accum): 166,  Loss: 7.9114, Time: 25.05s, Token/s: 20.44
Epoch: 0, Step: 1336, Batch(micro): 1336, Batch (considering grad accum): 167,  Loss: 8.1191, Time: 10.33s, Token/s: 49.57
Epoch: 0, Step: 1337, Batch(micro): 1337, Batch (considering grad accum): 167,  Loss: 8.1216, Time: 4.00s, Token/s: 127.99
Epoch: 0, Step: 1338, Batch(micro): 1338, Batch (considering grad accum): 167,  Loss: 8.2085, Time: 3.74s, Token/s: 136.82
Epoch: 0, Step: 1339, Batch(micro): 1339, Batch (considering grad accum): 167,  Loss: 8.4122, Time: 3.72s, Token/s: 137.65
Epoch: 0, Step: 1340, Batch(micro): 1340, Batch (considering grad accum): 167,  Loss: 8.6262, Time: 3.52s, Token/s: 145.62
Epoch: 0, Step: 1341, Batch(micro): 1341, Batch (considering grad accum): 167,  Loss: 8.0601, Time: 3.81s, Token/s: 134.28
Epoch: 0, Step: 1342, Batch(micro): 1342, Batch (considering grad accum): 167,  Loss: 8.0110, Time: 3.65s, Token/s: 140.15
Epoch: 0, Step: 1343, Batch(micro): 1343, Batch (considering grad accum): 167,  Loss: 8.0526, Time: 23.33s, Token/s: 21.95
Epoch: 0, Step: 1344, Batch(micro): 1344, Batch (considering grad accum): 168,  Loss: 8.2021, Time: 8.58s, Token/s: 59.65
Epoch: 0, Step: 1345, Batch(micro): 1345, Batch (considering grad accum): 168,  Loss: 8.1326, Time: 4.13s, Token/s: 124.08
Epoch: 0, Step: 1346, Batch(micro): 1346, Batch (considering grad accum): 168,  Loss: 8.1173, Time: 3.92s, Token/s: 130.78
Epoch: 0, Step: 1347, Batch(micro): 1347, Batch (considering grad accum): 168,  Loss: 8.2015, Time: 4.12s, Token/s: 124.37
Epoch: 0, Step: 1348, Batch(micro): 1348, Batch (considering grad accum): 168,  Loss: 8.2146, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 1349, Batch(micro): 1349, Batch (considering grad accum): 168,  Loss: 8.3269, Time: 3.85s, Token/s: 132.91
Epoch: 0, Step: 1350, Batch(micro): 1350, Batch (considering grad accum): 168,  Loss: 8.4273, Time: 4.05s, Token/s: 126.35
Epoch: 0, Step: 1351, Batch(micro): 1351, Batch (considering grad accum): 168,  Loss: 8.2561, Time: 24.61s, Token/s: 20.81
Epoch: 0, Step: 1352, Batch(micro): 1352, Batch (considering grad accum): 169,  Loss: 8.2216, Time: 7.29s, Token/s: 70.20
Epoch: 0, Step: 1353, Batch(micro): 1353, Batch (considering grad accum): 169,  Loss: 8.2587, Time: 4.31s, Token/s: 118.74
Epoch: 0, Step: 1354, Batch(micro): 1354, Batch (considering grad accum): 169,  Loss: 8.2172, Time: 3.94s, Token/s: 129.89
Epoch: 0, Step: 1355, Batch(micro): 1355, Batch (considering grad accum): 169,  Loss: 8.5453, Time: 3.94s, Token/s: 130.07
Epoch: 0, Step: 1356, Batch(micro): 1356, Batch (considering grad accum): 169,  Loss: 7.8510, Time: 3.63s, Token/s: 141.20
Epoch: 0, Step: 1357, Batch(micro): 1357, Batch (considering grad accum): 169,  Loss: 8.0775, Time: 3.62s, Token/s: 141.41
Epoch: 0, Step: 1358, Batch(micro): 1358, Batch (considering grad accum): 169,  Loss: 8.1950, Time: 3.46s, Token/s: 147.79
Epoch: 0, Step: 1359, Batch(micro): 1359, Batch (considering grad accum): 169,  Loss: 8.3852, Time: 24.32s, Token/s: 21.05
Epoch: 0, Step: 1360, Batch(micro): 1360, Batch (considering grad accum): 170,  Loss: 8.4847, Time: 6.91s, Token/s: 74.06
Epoch: 0, Step: 1361, Batch(micro): 1361, Batch (considering grad accum): 170,  Loss: 8.2398, Time: 4.77s, Token/s: 107.30
Epoch: 0, Step: 1362, Batch(micro): 1362, Batch (considering grad accum): 170,  Loss: 8.4504, Time: 4.43s, Token/s: 115.64
Epoch: 0, Step: 1363, Batch(micro): 1363, Batch (considering grad accum): 170,  Loss: 8.7201, Time: 3.77s, Token/s: 135.89
Epoch: 0, Step: 1364, Batch(micro): 1364, Batch (considering grad accum): 170,  Loss: 8.5175, Time: 3.74s, Token/s: 136.95
Epoch: 0, Step: 1365, Batch(micro): 1365, Batch (considering grad accum): 170,  Loss: 7.9531, Time: 3.88s, Token/s: 131.96
Epoch: 0, Step: 1366, Batch(micro): 1366, Batch (considering grad accum): 170,  Loss: 8.1236, Time: 3.63s, Token/s: 141.00
Epoch: 0, Step: 1367, Batch(micro): 1367, Batch (considering grad accum): 170,  Loss: 8.2862, Time: 24.79s, Token/s: 20.65
Epoch: 0, Step: 1368, Batch(micro): 1368, Batch (considering grad accum): 171,  Loss: 8.1413, Time: 7.96s, Token/s: 64.32
Epoch: 0, Step: 1369, Batch(micro): 1369, Batch (considering grad accum): 171,  Loss: 7.5298, Time: 4.61s, Token/s: 111.17
Epoch: 0, Step: 1370, Batch(micro): 1370, Batch (considering grad accum): 171,  Loss: 7.7163, Time: 4.37s, Token/s: 117.14
Epoch: 0, Step: 1371, Batch(micro): 1371, Batch (considering grad accum): 171,  Loss: 8.0562, Time: 4.18s, Token/s: 122.63
Epoch: 0, Step: 1372, Batch(micro): 1372, Batch (considering grad accum): 171,  Loss: 8.2429, Time: 3.50s, Token/s: 146.24
Epoch: 0, Step: 1373, Batch(micro): 1373, Batch (considering grad accum): 171,  Loss: 8.1962, Time: 3.82s, Token/s: 133.92
Epoch: 0, Step: 1374, Batch(micro): 1374, Batch (considering grad accum): 171,  Loss: 8.3613, Time: 3.82s, Token/s: 134.15
Epoch: 0, Step: 1375, Batch(micro): 1375, Batch (considering grad accum): 171,  Loss: 7.9255, Time: 25.46s, Token/s: 20.11
Epoch: 0, Step: 1376, Batch(micro): 1376, Batch (considering grad accum): 172,  Loss: 7.7784, Time: 7.89s, Token/s: 64.93
Epoch: 0, Step: 1377, Batch(micro): 1377, Batch (considering grad accum): 172,  Loss: 8.1637, Time: 4.68s, Token/s: 109.35
Epoch: 0, Step: 1378, Batch(micro): 1378, Batch (considering grad accum): 172,  Loss: 8.3009, Time: 3.62s, Token/s: 141.43
Epoch: 0, Step: 1379, Batch(micro): 1379, Batch (considering grad accum): 172,  Loss: 8.3123, Time: 3.72s, Token/s: 137.80
Epoch: 0, Step: 1380, Batch(micro): 1380, Batch (considering grad accum): 172,  Loss: 8.0754, Time: 3.60s, Token/s: 142.09
Epoch: 0, Step: 1381, Batch(micro): 1381, Batch (considering grad accum): 172,  Loss: 7.9717, Time: 4.78s, Token/s: 107.20
Epoch: 0, Step: 1382, Batch(micro): 1382, Batch (considering grad accum): 172,  Loss: 8.2688, Time: 3.85s, Token/s: 132.84
Epoch: 0, Step: 1383, Batch(micro): 1383, Batch (considering grad accum): 172,  Loss: 8.2103, Time: 25.18s, Token/s: 20.34
Epoch: 0, Step: 1384, Batch(micro): 1384, Batch (considering grad accum): 173,  Loss: 8.2377, Time: 6.90s, Token/s: 74.23
Epoch: 0, Step: 1385, Batch(micro): 1385, Batch (considering grad accum): 173,  Loss: 8.8322, Time: 4.48s, Token/s: 114.34
Epoch: 0, Step: 1386, Batch(micro): 1386, Batch (considering grad accum): 173,  Loss: 8.0870, Time: 3.98s, Token/s: 128.52
Epoch: 0, Step: 1387, Batch(micro): 1387, Batch (considering grad accum): 173,  Loss: 7.5442, Time: 4.00s, Token/s: 128.13
Epoch: 0, Step: 1388, Batch(micro): 1388, Batch (considering grad accum): 173,  Loss: 8.2463, Time: 3.90s, Token/s: 131.33
Epoch: 0, Step: 1389, Batch(micro): 1389, Batch (considering grad accum): 173,  Loss: 8.1455, Time: 3.85s, Token/s: 132.84
Epoch: 0, Step: 1390, Batch(micro): 1390, Batch (considering grad accum): 173,  Loss: 8.3588, Time: 3.73s, Token/s: 137.19
Epoch: 0, Step: 1391, Batch(micro): 1391, Batch (considering grad accum): 173,  Loss: 8.5875, Time: 27.34s, Token/s: 18.72
Epoch: 0, Step: 1392, Batch(micro): 1392, Batch (considering grad accum): 174,  Loss: 8.2482, Time: 7.50s, Token/s: 68.25
Epoch: 0, Step: 1393, Batch(micro): 1393, Batch (considering grad accum): 174,  Loss: 8.0937, Time: 4.48s, Token/s: 114.20
Epoch: 0, Step: 1394, Batch(micro): 1394, Batch (considering grad accum): 174,  Loss: 8.6149, Time: 4.33s, Token/s: 118.19
Epoch: 0, Step: 1395, Batch(micro): 1395, Batch (considering grad accum): 174,  Loss: 8.1213, Time: 3.91s, Token/s: 131.09
Epoch: 0, Step: 1396, Batch(micro): 1396, Batch (considering grad accum): 174,  Loss: 8.1602, Time: 3.64s, Token/s: 140.69
Epoch: 0, Step: 1397, Batch(micro): 1397, Batch (considering grad accum): 174,  Loss: 9.0301, Time: 3.81s, Token/s: 134.34
Epoch: 0, Step: 1398, Batch(micro): 1398, Batch (considering grad accum): 174,  Loss: 8.6292, Time: 3.63s, Token/s: 140.94
Epoch: 0, Step: 1399, Batch(micro): 1399, Batch (considering grad accum): 174,  Loss: 7.9818, Time: 26.10s, Token/s: 19.62
Updating MLP bias
Epoch: 0, Step: 1400, Batch(micro): 1400, Batch (considering grad accum): 175,  Loss: 7.6981, Time: 8.68s, Token/s: 58.99
Epoch: 0, Step: 1401, Batch(micro): 1401, Batch (considering grad accum): 175,  Loss: 8.3414, Time: 4.55s, Token/s: 112.55
Epoch: 0, Step: 1402, Batch(micro): 1402, Batch (considering grad accum): 175,  Loss: 8.0008, Time: 4.19s, Token/s: 122.31
Epoch: 0, Step: 1403, Batch(micro): 1403, Batch (considering grad accum): 175,  Loss: 7.8497, Time: 3.59s, Token/s: 142.48
Epoch: 0, Step: 1404, Batch(micro): 1404, Batch (considering grad accum): 175,  Loss: 8.5895, Time: 3.70s, Token/s: 138.45
Epoch: 0, Step: 1405, Batch(micro): 1405, Batch (considering grad accum): 175,  Loss: 8.1185, Time: 3.23s, Token/s: 158.68
Epoch: 0, Step: 1406, Batch(micro): 1406, Batch (considering grad accum): 175,  Loss: 8.6429, Time: 3.26s, Token/s: 157.21
Epoch: 0, Step: 1407, Batch(micro): 1407, Batch (considering grad accum): 175,  Loss: 8.1612, Time: 25.14s, Token/s: 20.36
Epoch: 0, Step: 1408, Batch(micro): 1408, Batch (considering grad accum): 176,  Loss: 8.1015, Time: 6.95s, Token/s: 73.67
Epoch: 0, Step: 1409, Batch(micro): 1409, Batch (considering grad accum): 176,  Loss: 8.1820, Time: 4.59s, Token/s: 111.52
Epoch: 0, Step: 1410, Batch(micro): 1410, Batch (considering grad accum): 176,  Loss: 8.0017, Time: 4.17s, Token/s: 122.77
Epoch: 0, Step: 1411, Batch(micro): 1411, Batch (considering grad accum): 176,  Loss: 7.8171, Time: 3.36s, Token/s: 152.29
Epoch: 0, Step: 1412, Batch(micro): 1412, Batch (considering grad accum): 176,  Loss: 8.2057, Time: 3.33s, Token/s: 153.74
Epoch: 0, Step: 1413, Batch(micro): 1413, Batch (considering grad accum): 176,  Loss: 8.2837, Time: 3.53s, Token/s: 144.88
Epoch: 0, Step: 1414, Batch(micro): 1414, Batch (considering grad accum): 176,  Loss: 8.1932, Time: 3.17s, Token/s: 161.35
Epoch: 0, Step: 1415, Batch(micro): 1415, Batch (considering grad accum): 176,  Loss: 8.1769, Time: 28.53s, Token/s: 17.95
Epoch: 0, Step: 1416, Batch(micro): 1416, Batch (considering grad accum): 177,  Loss: 8.2333, Time: 8.13s, Token/s: 62.99
Epoch: 0, Step: 1417, Batch(micro): 1417, Batch (considering grad accum): 177,  Loss: 7.7789, Time: 4.75s, Token/s: 107.87
Epoch: 0, Step: 1418, Batch(micro): 1418, Batch (considering grad accum): 177,  Loss: 8.1709, Time: 3.69s, Token/s: 138.79
Epoch: 0, Step: 1419, Batch(micro): 1419, Batch (considering grad accum): 177,  Loss: 8.2171, Time: 3.29s, Token/s: 155.65
Epoch: 0, Step: 1420, Batch(micro): 1420, Batch (considering grad accum): 177,  Loss: 8.1909, Time: 3.45s, Token/s: 148.50
Epoch: 0, Step: 1421, Batch(micro): 1421, Batch (considering grad accum): 177,  Loss: 8.5151, Time: 3.45s, Token/s: 148.36
Epoch: 0, Step: 1422, Batch(micro): 1422, Batch (considering grad accum): 177,  Loss: 8.2362, Time: 3.66s, Token/s: 139.75
Epoch: 0, Step: 1423, Batch(micro): 1423, Batch (considering grad accum): 177,  Loss: 8.1377, Time: 24.71s, Token/s: 20.72
Epoch: 0, Step: 1424, Batch(micro): 1424, Batch (considering grad accum): 178,  Loss: 7.7339, Time: 6.90s, Token/s: 74.25
Epoch: 0, Step: 1425, Batch(micro): 1425, Batch (considering grad accum): 178,  Loss: 7.8635, Time: 4.16s, Token/s: 122.96
Epoch: 0, Step: 1426, Batch(micro): 1426, Batch (considering grad accum): 178,  Loss: 8.0080, Time: 4.22s, Token/s: 121.20
Epoch: 0, Step: 1427, Batch(micro): 1427, Batch (considering grad accum): 178,  Loss: 8.1145, Time: 3.91s, Token/s: 130.91
Epoch: 0, Step: 1428, Batch(micro): 1428, Batch (considering grad accum): 178,  Loss: 7.7395, Time: 3.79s, Token/s: 135.24
Epoch: 0, Step: 1429, Batch(micro): 1429, Batch (considering grad accum): 178,  Loss: 7.8804, Time: 3.63s, Token/s: 140.89
Epoch: 0, Step: 1430, Batch(micro): 1430, Batch (considering grad accum): 178,  Loss: 7.9065, Time: 3.70s, Token/s: 138.34
Epoch: 0, Step: 1431, Batch(micro): 1431, Batch (considering grad accum): 178,  Loss: 8.3222, Time: 25.60s, Token/s: 20.00
Epoch: 0, Step: 1432, Batch(micro): 1432, Batch (considering grad accum): 179,  Loss: 8.1638, Time: 9.19s, Token/s: 55.71
Epoch: 0, Step: 1433, Batch(micro): 1433, Batch (considering grad accum): 179,  Loss: 8.0589, Time: 4.60s, Token/s: 111.41
Epoch: 0, Step: 1434, Batch(micro): 1434, Batch (considering grad accum): 179,  Loss: 7.7332, Time: 4.56s, Token/s: 112.26
Epoch: 0, Step: 1435, Batch(micro): 1435, Batch (considering grad accum): 179,  Loss: 8.1720, Time: 3.71s, Token/s: 138.10
Epoch: 0, Step: 1436, Batch(micro): 1436, Batch (considering grad accum): 179,  Loss: 8.2574, Time: 3.55s, Token/s: 144.24
Epoch: 0, Step: 1437, Batch(micro): 1437, Batch (considering grad accum): 179,  Loss: 8.2875, Time: 3.82s, Token/s: 133.91
Epoch: 0, Step: 1438, Batch(micro): 1438, Batch (considering grad accum): 179,  Loss: 8.0502, Time: 3.78s, Token/s: 135.29
Epoch: 0, Step: 1439, Batch(micro): 1439, Batch (considering grad accum): 179,  Loss: 7.9334, Time: 23.43s, Token/s: 21.86
Epoch: 0, Step: 1440, Batch(micro): 1440, Batch (considering grad accum): 180,  Loss: 8.1226, Time: 8.15s, Token/s: 62.80
Epoch: 0, Step: 1441, Batch(micro): 1441, Batch (considering grad accum): 180,  Loss: 8.0080, Time: 4.08s, Token/s: 125.36
Epoch: 0, Step: 1442, Batch(micro): 1442, Batch (considering grad accum): 180,  Loss: 8.1024, Time: 3.96s, Token/s: 129.18
Epoch: 0, Step: 1443, Batch(micro): 1443, Batch (considering grad accum): 180,  Loss: 8.2284, Time: 3.84s, Token/s: 133.42
Epoch: 0, Step: 1444, Batch(micro): 1444, Batch (considering grad accum): 180,  Loss: 8.0353, Time: 3.68s, Token/s: 138.99
Epoch: 0, Step: 1445, Batch(micro): 1445, Batch (considering grad accum): 180,  Loss: 8.3794, Time: 4.17s, Token/s: 122.80
Epoch: 0, Step: 1446, Batch(micro): 1446, Batch (considering grad accum): 180,  Loss: 8.3119, Time: 3.71s, Token/s: 138.12
Epoch: 0, Step: 1447, Batch(micro): 1447, Batch (considering grad accum): 180,  Loss: 8.0554, Time: 23.65s, Token/s: 21.65
Epoch: 0, Step: 1448, Batch(micro): 1448, Batch (considering grad accum): 181,  Loss: 7.8900, Time: 6.81s, Token/s: 75.16
Epoch: 0, Step: 1449, Batch(micro): 1449, Batch (considering grad accum): 181,  Loss: 8.1693, Time: 4.28s, Token/s: 119.54
Epoch: 0, Step: 1450, Batch(micro): 1450, Batch (considering grad accum): 181,  Loss: 8.0152, Time: 3.88s, Token/s: 132.05
Epoch: 0, Step: 1451, Batch(micro): 1451, Batch (considering grad accum): 181,  Loss: 7.9563, Time: 4.62s, Token/s: 110.89
Epoch: 0, Step: 1452, Batch(micro): 1452, Batch (considering grad accum): 181,  Loss: 8.0749, Time: 3.43s, Token/s: 149.27
Epoch: 0, Step: 1453, Batch(micro): 1453, Batch (considering grad accum): 181,  Loss: 7.9964, Time: 3.84s, Token/s: 133.40
Epoch: 0, Step: 1454, Batch(micro): 1454, Batch (considering grad accum): 181,  Loss: 8.2588, Time: 3.81s, Token/s: 134.38
Epoch: 0, Step: 1455, Batch(micro): 1455, Batch (considering grad accum): 181,  Loss: 8.2260, Time: 22.52s, Token/s: 22.73
Epoch: 0, Step: 1456, Batch(micro): 1456, Batch (considering grad accum): 182,  Loss: 8.2440, Time: 6.80s, Token/s: 75.29
Epoch: 0, Step: 1457, Batch(micro): 1457, Batch (considering grad accum): 182,  Loss: 8.1585, Time: 3.95s, Token/s: 129.47
Epoch: 0, Step: 1458, Batch(micro): 1458, Batch (considering grad accum): 182,  Loss: 8.0298, Time: 3.43s, Token/s: 149.07
Epoch: 0, Step: 1459, Batch(micro): 1459, Batch (considering grad accum): 182,  Loss: 7.5114, Time: 3.29s, Token/s: 155.48
Epoch: 0, Step: 1460, Batch(micro): 1460, Batch (considering grad accum): 182,  Loss: 7.6661, Time: 3.32s, Token/s: 154.27
Epoch: 0, Step: 1461, Batch(micro): 1461, Batch (considering grad accum): 182,  Loss: 8.6395, Time: 3.84s, Token/s: 133.19
Epoch: 0, Step: 1462, Batch(micro): 1462, Batch (considering grad accum): 182,  Loss: 7.8054, Time: 3.65s, Token/s: 140.14
Epoch: 0, Step: 1463, Batch(micro): 1463, Batch (considering grad accum): 182,  Loss: 7.6715, Time: 24.02s, Token/s: 21.32
Epoch: 0, Step: 1464, Batch(micro): 1464, Batch (considering grad accum): 183,  Loss: 7.9211, Time: 8.06s, Token/s: 63.50
Epoch: 0, Step: 1465, Batch(micro): 1465, Batch (considering grad accum): 183,  Loss: 8.1424, Time: 3.98s, Token/s: 128.57
Epoch: 0, Step: 1466, Batch(micro): 1466, Batch (considering grad accum): 183,  Loss: 8.0394, Time: 3.85s, Token/s: 132.95
Epoch: 0, Step: 1467, Batch(micro): 1467, Batch (considering grad accum): 183,  Loss: 7.8932, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 1468, Batch(micro): 1468, Batch (considering grad accum): 183,  Loss: 8.1633, Time: 3.86s, Token/s: 132.49
Epoch: 0, Step: 1469, Batch(micro): 1469, Batch (considering grad accum): 183,  Loss: 7.8113, Time: 3.95s, Token/s: 129.64
Epoch: 0, Step: 1470, Batch(micro): 1470, Batch (considering grad accum): 183,  Loss: 7.7291, Time: 3.55s, Token/s: 144.21
Epoch: 0, Step: 1471, Batch(micro): 1471, Batch (considering grad accum): 183,  Loss: 8.1904, Time: 24.44s, Token/s: 20.95
Epoch: 0, Step: 1472, Batch(micro): 1472, Batch (considering grad accum): 184,  Loss: 8.1763, Time: 7.47s, Token/s: 68.51
Epoch: 0, Step: 1473, Batch(micro): 1473, Batch (considering grad accum): 184,  Loss: 8.4799, Time: 4.13s, Token/s: 124.12
Epoch: 0, Step: 1474, Batch(micro): 1474, Batch (considering grad accum): 184,  Loss: 8.2916, Time: 3.83s, Token/s: 133.85
Epoch: 0, Step: 1475, Batch(micro): 1475, Batch (considering grad accum): 184,  Loss: 8.3392, Time: 3.54s, Token/s: 144.73
Epoch: 0, Step: 1476, Batch(micro): 1476, Batch (considering grad accum): 184,  Loss: 8.1173, Time: 3.61s, Token/s: 141.77
Epoch: 0, Step: 1477, Batch(micro): 1477, Batch (considering grad accum): 184,  Loss: 8.0490, Time: 4.33s, Token/s: 118.35
Epoch: 0, Step: 1478, Batch(micro): 1478, Batch (considering grad accum): 184,  Loss: 7.7781, Time: 4.11s, Token/s: 124.65
Epoch: 0, Step: 1479, Batch(micro): 1479, Batch (considering grad accum): 184,  Loss: 8.0226, Time: 24.25s, Token/s: 21.12
Epoch: 0, Step: 1480, Batch(micro): 1480, Batch (considering grad accum): 185,  Loss: 8.1963, Time: 7.60s, Token/s: 67.36
Epoch: 0, Step: 1481, Batch(micro): 1481, Batch (considering grad accum): 185,  Loss: 7.8761, Time: 4.43s, Token/s: 115.59
Epoch: 0, Step: 1482, Batch(micro): 1482, Batch (considering grad accum): 185,  Loss: 7.6832, Time: 3.70s, Token/s: 138.37
Epoch: 0, Step: 1483, Batch(micro): 1483, Batch (considering grad accum): 185,  Loss: 7.6841, Time: 3.35s, Token/s: 152.73
Epoch: 0, Step: 1484, Batch(micro): 1484, Batch (considering grad accum): 185,  Loss: 8.0984, Time: 3.49s, Token/s: 146.89
Epoch: 0, Step: 1485, Batch(micro): 1485, Batch (considering grad accum): 185,  Loss: 7.7023, Time: 3.57s, Token/s: 143.56
Epoch: 0, Step: 1486, Batch(micro): 1486, Batch (considering grad accum): 185,  Loss: 8.1464, Time: 3.74s, Token/s: 136.86
Epoch: 0, Step: 1487, Batch(micro): 1487, Batch (considering grad accum): 185,  Loss: 8.5387, Time: 24.09s, Token/s: 21.25
Epoch: 0, Step: 1488, Batch(micro): 1488, Batch (considering grad accum): 186,  Loss: 7.5040, Time: 7.24s, Token/s: 70.68
Epoch: 0, Step: 1489, Batch(micro): 1489, Batch (considering grad accum): 186,  Loss: 7.5222, Time: 3.99s, Token/s: 128.29
Epoch: 0, Step: 1490, Batch(micro): 1490, Batch (considering grad accum): 186,  Loss: 7.7747, Time: 4.02s, Token/s: 127.40
Epoch: 0, Step: 1491, Batch(micro): 1491, Batch (considering grad accum): 186,  Loss: 8.2408, Time: 4.03s, Token/s: 126.96
Epoch: 0, Step: 1492, Batch(micro): 1492, Batch (considering grad accum): 186,  Loss: 7.9816, Time: 3.85s, Token/s: 132.95
Epoch: 0, Step: 1493, Batch(micro): 1493, Batch (considering grad accum): 186,  Loss: 8.0164, Time: 3.86s, Token/s: 132.81
Epoch: 0, Step: 1494, Batch(micro): 1494, Batch (considering grad accum): 186,  Loss: 8.3382, Time: 3.66s, Token/s: 139.88
Epoch: 0, Step: 1495, Batch(micro): 1495, Batch (considering grad accum): 186,  Loss: 7.8212, Time: 23.35s, Token/s: 21.93
Epoch: 0, Step: 1496, Batch(micro): 1496, Batch (considering grad accum): 187,  Loss: 8.2796, Time: 8.00s, Token/s: 64.00
Epoch: 0, Step: 1497, Batch(micro): 1497, Batch (considering grad accum): 187,  Loss: 8.8523, Time: 4.36s, Token/s: 117.56
Epoch: 0, Step: 1498, Batch(micro): 1498, Batch (considering grad accum): 187,  Loss: 8.2601, Time: 4.67s, Token/s: 109.58
Epoch: 0, Step: 1499, Batch(micro): 1499, Batch (considering grad accum): 187,  Loss: 8.3429, Time: 3.74s, Token/s: 136.84
Updating MLP bias
Epoch: 0, Step: 1500, Batch(micro): 1500, Batch (considering grad accum): 187,  Loss: 7.6622, Time: 3.31s, Token/s: 154.69
Epoch: 0, Step: 1501, Batch(micro): 1501, Batch (considering grad accum): 187,  Loss: 8.0308, Time: 3.37s, Token/s: 151.97
Epoch: 0, Step: 1502, Batch(micro): 1502, Batch (considering grad accum): 187,  Loss: 8.0012, Time: 3.54s, Token/s: 144.59
Epoch: 0, Step: 1503, Batch(micro): 1503, Batch (considering grad accum): 187,  Loss: 7.7175, Time: 27.00s, Token/s: 18.96
Epoch: 0, Step: 1504, Batch(micro): 1504, Batch (considering grad accum): 188,  Loss: 7.5438, Time: 9.52s, Token/s: 53.77
Epoch: 0, Step: 1505, Batch(micro): 1505, Batch (considering grad accum): 188,  Loss: 7.3676, Time: 4.66s, Token/s: 109.97
Epoch: 0, Step: 1506, Batch(micro): 1506, Batch (considering grad accum): 188,  Loss: 7.8292, Time: 3.38s, Token/s: 151.44
Epoch: 0, Step: 1507, Batch(micro): 1507, Batch (considering grad accum): 188,  Loss: 8.2194, Time: 3.55s, Token/s: 144.16
Epoch: 0, Step: 1508, Batch(micro): 1508, Batch (considering grad accum): 188,  Loss: 8.2116, Time: 3.55s, Token/s: 144.38
Epoch: 0, Step: 1509, Batch(micro): 1509, Batch (considering grad accum): 188,  Loss: 8.3497, Time: 3.46s, Token/s: 147.80
Epoch: 0, Step: 1510, Batch(micro): 1510, Batch (considering grad accum): 188,  Loss: 7.4991, Time: 3.47s, Token/s: 147.59
Epoch: 0, Step: 1511, Batch(micro): 1511, Batch (considering grad accum): 188,  Loss: 7.4929, Time: 21.26s, Token/s: 24.08
Epoch: 0, Step: 1512, Batch(micro): 1512, Batch (considering grad accum): 189,  Loss: 7.5665, Time: 7.71s, Token/s: 66.44
Epoch: 0, Step: 1513, Batch(micro): 1513, Batch (considering grad accum): 189,  Loss: 8.0479, Time: 3.80s, Token/s: 134.61
Epoch: 0, Step: 1514, Batch(micro): 1514, Batch (considering grad accum): 189,  Loss: 8.0351, Time: 3.71s, Token/s: 138.16
Epoch: 0, Step: 1515, Batch(micro): 1515, Batch (considering grad accum): 189,  Loss: 8.0940, Time: 3.80s, Token/s: 134.91
Epoch: 0, Step: 1516, Batch(micro): 1516, Batch (considering grad accum): 189,  Loss: 8.3452, Time: 3.69s, Token/s: 138.85
Epoch: 0, Step: 1517, Batch(micro): 1517, Batch (considering grad accum): 189,  Loss: 7.8746, Time: 4.07s, Token/s: 125.76
Epoch: 0, Step: 1518, Batch(micro): 1518, Batch (considering grad accum): 189,  Loss: 7.9147, Time: 3.47s, Token/s: 147.59
Epoch: 0, Step: 1519, Batch(micro): 1519, Batch (considering grad accum): 189,  Loss: 7.8243, Time: 21.44s, Token/s: 23.88
Epoch: 0, Step: 1520, Batch(micro): 1520, Batch (considering grad accum): 190,  Loss: 8.5685, Time: 7.11s, Token/s: 72.00
Epoch: 0, Step: 1521, Batch(micro): 1521, Batch (considering grad accum): 190,  Loss: 7.7457, Time: 4.29s, Token/s: 119.44
Epoch: 0, Step: 1522, Batch(micro): 1522, Batch (considering grad accum): 190,  Loss: 8.0402, Time: 4.07s, Token/s: 125.90
Epoch: 0, Step: 1523, Batch(micro): 1523, Batch (considering grad accum): 190,  Loss: 8.3726, Time: 3.83s, Token/s: 133.53
Epoch: 0, Step: 1524, Batch(micro): 1524, Batch (considering grad accum): 190,  Loss: 8.4183, Time: 3.71s, Token/s: 138.13
Epoch: 0, Step: 1525, Batch(micro): 1525, Batch (considering grad accum): 190,  Loss: 8.7218, Time: 3.74s, Token/s: 137.08
Epoch: 0, Step: 1526, Batch(micro): 1526, Batch (considering grad accum): 190,  Loss: 7.7956, Time: 3.72s, Token/s: 137.53
Epoch: 0, Step: 1527, Batch(micro): 1527, Batch (considering grad accum): 190,  Loss: 7.9605, Time: 20.16s, Token/s: 25.40
Epoch: 0, Step: 1528, Batch(micro): 1528, Batch (considering grad accum): 191,  Loss: 8.3146, Time: 6.82s, Token/s: 75.11
Epoch: 0, Step: 1529, Batch(micro): 1529, Batch (considering grad accum): 191,  Loss: 8.1901, Time: 4.45s, Token/s: 115.16
Epoch: 0, Step: 1530, Batch(micro): 1530, Batch (considering grad accum): 191,  Loss: 8.0175, Time: 3.73s, Token/s: 137.45
Epoch: 0, Step: 1531, Batch(micro): 1531, Batch (considering grad accum): 191,  Loss: 8.0635, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 1532, Batch(micro): 1532, Batch (considering grad accum): 191,  Loss: 7.7530, Time: 3.84s, Token/s: 133.22
Epoch: 0, Step: 1533, Batch(micro): 1533, Batch (considering grad accum): 191,  Loss: 8.2497, Time: 3.81s, Token/s: 134.32
Epoch: 0, Step: 1534, Batch(micro): 1534, Batch (considering grad accum): 191,  Loss: 8.0188, Time: 3.52s, Token/s: 145.45
Epoch: 0, Step: 1535, Batch(micro): 1535, Batch (considering grad accum): 191,  Loss: 7.9535, Time: 19.40s, Token/s: 26.39
Epoch: 0, Step: 1536, Batch(micro): 1536, Batch (considering grad accum): 192,  Loss: 7.8742, Time: 5.57s, Token/s: 91.98
Epoch: 0, Step: 1537, Batch(micro): 1537, Batch (considering grad accum): 192,  Loss: 8.0579, Time: 4.48s, Token/s: 114.41
Epoch: 0, Step: 1538, Batch(micro): 1538, Batch (considering grad accum): 192,  Loss: 7.7694, Time: 3.70s, Token/s: 138.22
Epoch: 0, Step: 1539, Batch(micro): 1539, Batch (considering grad accum): 192,  Loss: 7.9282, Time: 3.69s, Token/s: 138.64
Epoch: 0, Step: 1540, Batch(micro): 1540, Batch (considering grad accum): 192,  Loss: 8.1845, Time: 3.64s, Token/s: 140.59
Epoch: 0, Step: 1541, Batch(micro): 1541, Batch (considering grad accum): 192,  Loss: 7.7357, Time: 3.87s, Token/s: 132.39
Epoch: 0, Step: 1542, Batch(micro): 1542, Batch (considering grad accum): 192,  Loss: 7.9303, Time: 3.79s, Token/s: 134.98
Epoch: 0, Step: 1543, Batch(micro): 1543, Batch (considering grad accum): 192,  Loss: 8.0295, Time: 19.80s, Token/s: 25.85
Epoch: 0, Step: 1544, Batch(micro): 1544, Batch (considering grad accum): 193,  Loss: 7.5431, Time: 6.96s, Token/s: 73.59
Epoch: 0, Step: 1545, Batch(micro): 1545, Batch (considering grad accum): 193,  Loss: 7.6518, Time: 3.94s, Token/s: 130.00
Epoch: 0, Step: 1546, Batch(micro): 1546, Batch (considering grad accum): 193,  Loss: 7.7284, Time: 3.77s, Token/s: 135.76
Epoch: 0, Step: 1547, Batch(micro): 1547, Batch (considering grad accum): 193,  Loss: 7.5376, Time: 3.77s, Token/s: 135.86
Epoch: 0, Step: 1548, Batch(micro): 1548, Batch (considering grad accum): 193,  Loss: 7.4641, Time: 3.77s, Token/s: 135.92
Epoch: 0, Step: 1549, Batch(micro): 1549, Batch (considering grad accum): 193,  Loss: 7.8255, Time: 3.79s, Token/s: 135.26
Epoch: 0, Step: 1550, Batch(micro): 1550, Batch (considering grad accum): 193,  Loss: 8.5307, Time: 3.97s, Token/s: 128.85
Epoch: 0, Step: 1551, Batch(micro): 1551, Batch (considering grad accum): 193,  Loss: 7.7880, Time: 18.98s, Token/s: 26.97
Epoch: 0, Step: 1552, Batch(micro): 1552, Batch (considering grad accum): 194,  Loss: 7.5646, Time: 6.54s, Token/s: 78.27
Epoch: 0, Step: 1553, Batch(micro): 1553, Batch (considering grad accum): 194,  Loss: 8.1693, Time: 4.33s, Token/s: 118.32
Epoch: 0, Step: 1554, Batch(micro): 1554, Batch (considering grad accum): 194,  Loss: 8.1397, Time: 3.95s, Token/s: 129.78
Epoch: 0, Step: 1555, Batch(micro): 1555, Batch (considering grad accum): 194,  Loss: 8.0523, Time: 3.68s, Token/s: 139.24
Epoch: 0, Step: 1556, Batch(micro): 1556, Batch (considering grad accum): 194,  Loss: 7.3774, Time: 3.51s, Token/s: 146.06
Epoch: 0, Step: 1557, Batch(micro): 1557, Batch (considering grad accum): 194,  Loss: 8.1227, Time: 3.90s, Token/s: 131.39
Epoch: 0, Step: 1558, Batch(micro): 1558, Batch (considering grad accum): 194,  Loss: 7.7992, Time: 3.71s, Token/s: 137.92
Epoch: 0, Step: 1559, Batch(micro): 1559, Batch (considering grad accum): 194,  Loss: 7.8311, Time: 20.60s, Token/s: 24.85
Epoch: 0, Step: 1560, Batch(micro): 1560, Batch (considering grad accum): 195,  Loss: 8.0853, Time: 10.09s, Token/s: 50.75
Epoch: 0, Step: 1561, Batch(micro): 1561, Batch (considering grad accum): 195,  Loss: 8.0451, Time: 4.37s, Token/s: 117.09
Epoch: 0, Step: 1562, Batch(micro): 1562, Batch (considering grad accum): 195,  Loss: 7.7957, Time: 4.13s, Token/s: 123.85
Epoch: 0, Step: 1563, Batch(micro): 1563, Batch (considering grad accum): 195,  Loss: 7.8192, Time: 3.73s, Token/s: 137.40
Epoch: 0, Step: 1564, Batch(micro): 1564, Batch (considering grad accum): 195,  Loss: 7.9167, Time: 3.70s, Token/s: 138.42
Epoch: 0, Step: 1565, Batch(micro): 1565, Batch (considering grad accum): 195,  Loss: 7.6628, Time: 3.80s, Token/s: 134.60
Epoch: 0, Step: 1566, Batch(micro): 1566, Batch (considering grad accum): 195,  Loss: 8.1873, Time: 3.93s, Token/s: 130.12
Epoch: 0, Step: 1567, Batch(micro): 1567, Batch (considering grad accum): 195,  Loss: 7.4387, Time: 24.65s, Token/s: 20.77
Epoch: 0, Step: 1568, Batch(micro): 1568, Batch (considering grad accum): 196,  Loss: 7.7294, Time: 8.96s, Token/s: 57.12
Epoch: 0, Step: 1569, Batch(micro): 1569, Batch (considering grad accum): 196,  Loss: 7.6844, Time: 4.08s, Token/s: 125.58
Epoch: 0, Step: 1570, Batch(micro): 1570, Batch (considering grad accum): 196,  Loss: 7.7100, Time: 3.84s, Token/s: 133.19
Epoch: 0, Step: 1571, Batch(micro): 1571, Batch (considering grad accum): 196,  Loss: 8.5142, Time: 4.03s, Token/s: 127.19
Epoch: 0, Step: 1572, Batch(micro): 1572, Batch (considering grad accum): 196,  Loss: 7.9590, Time: 3.85s, Token/s: 132.94
Epoch: 0, Step: 1573, Batch(micro): 1573, Batch (considering grad accum): 196,  Loss: 7.7984, Time: 3.84s, Token/s: 133.44
Epoch: 0, Step: 1574, Batch(micro): 1574, Batch (considering grad accum): 196,  Loss: 7.9910, Time: 3.67s, Token/s: 139.58
Epoch: 0, Step: 1575, Batch(micro): 1575, Batch (considering grad accum): 196,  Loss: 7.6169, Time: 21.90s, Token/s: 23.38
Epoch: 0, Step: 1576, Batch(micro): 1576, Batch (considering grad accum): 197,  Loss: 8.0720, Time: 6.54s, Token/s: 78.27
Epoch: 0, Step: 1577, Batch(micro): 1577, Batch (considering grad accum): 197,  Loss: 8.0465, Time: 3.80s, Token/s: 134.62
Epoch: 0, Step: 1578, Batch(micro): 1578, Batch (considering grad accum): 197,  Loss: 8.2705, Time: 3.93s, Token/s: 130.25
Epoch: 0, Step: 1579, Batch(micro): 1579, Batch (considering grad accum): 197,  Loss: 8.0746, Time: 3.85s, Token/s: 133.15
Epoch: 0, Step: 1580, Batch(micro): 1580, Batch (considering grad accum): 197,  Loss: 7.9166, Time: 3.91s, Token/s: 131.06
Epoch: 0, Step: 1581, Batch(micro): 1581, Batch (considering grad accum): 197,  Loss: 7.7443, Time: 3.91s, Token/s: 131.11
Epoch: 0, Step: 1582, Batch(micro): 1582, Batch (considering grad accum): 197,  Loss: 7.5050, Time: 4.06s, Token/s: 126.11
Epoch: 0, Step: 1583, Batch(micro): 1583, Batch (considering grad accum): 197,  Loss: 8.1550, Time: 24.08s, Token/s: 21.26
Epoch: 0, Step: 1584, Batch(micro): 1584, Batch (considering grad accum): 198,  Loss: 7.8761, Time: 7.68s, Token/s: 66.69
Epoch: 0, Step: 1585, Batch(micro): 1585, Batch (considering grad accum): 198,  Loss: 7.9381, Time: 4.42s, Token/s: 115.72
Epoch: 0, Step: 1586, Batch(micro): 1586, Batch (considering grad accum): 198,  Loss: 7.2317, Time: 3.75s, Token/s: 136.41
Epoch: 0, Step: 1587, Batch(micro): 1587, Batch (considering grad accum): 198,  Loss: 7.6675, Time: 3.90s, Token/s: 131.41
Epoch: 0, Step: 1588, Batch(micro): 1588, Batch (considering grad accum): 198,  Loss: 8.2139, Time: 3.95s, Token/s: 129.69
Epoch: 0, Step: 1589, Batch(micro): 1589, Batch (considering grad accum): 198,  Loss: 7.7745, Time: 3.73s, Token/s: 137.43
Epoch: 0, Step: 1590, Batch(micro): 1590, Batch (considering grad accum): 198,  Loss: 7.7568, Time: 3.35s, Token/s: 152.63
Epoch: 0, Step: 1591, Batch(micro): 1591, Batch (considering grad accum): 198,  Loss: 7.6615, Time: 24.93s, Token/s: 20.54
Epoch: 0, Step: 1592, Batch(micro): 1592, Batch (considering grad accum): 199,  Loss: 7.6354, Time: 7.31s, Token/s: 70.03
Epoch: 0, Step: 1593, Batch(micro): 1593, Batch (considering grad accum): 199,  Loss: 8.0201, Time: 3.88s, Token/s: 132.11
Epoch: 0, Step: 1594, Batch(micro): 1594, Batch (considering grad accum): 199,  Loss: 8.2345, Time: 3.66s, Token/s: 139.87
Epoch: 0, Step: 1595, Batch(micro): 1595, Batch (considering grad accum): 199,  Loss: 7.9291, Time: 3.69s, Token/s: 138.73
Epoch: 0, Step: 1596, Batch(micro): 1596, Batch (considering grad accum): 199,  Loss: 8.7070, Time: 3.65s, Token/s: 140.34
Epoch: 0, Step: 1597, Batch(micro): 1597, Batch (considering grad accum): 199,  Loss: 7.5410, Time: 3.72s, Token/s: 137.76
Epoch: 0, Step: 1598, Batch(micro): 1598, Batch (considering grad accum): 199,  Loss: 7.6215, Time: 3.81s, Token/s: 134.56
Epoch: 0, Step: 1599, Batch(micro): 1599, Batch (considering grad accum): 199,  Loss: 7.5395, Time: 23.00s, Token/s: 22.26
Updating MLP bias
Epoch: 0, Step: 1600, Batch(micro): 1600, Batch (considering grad accum): 200,  Loss: 8.1779, Time: 6.59s, Token/s: 77.67
Epoch: 0, Step: 1601, Batch(micro): 1601, Batch (considering grad accum): 200,  Loss: 7.8527, Time: 4.15s, Token/s: 123.43
Epoch: 0, Step: 1602, Batch(micro): 1602, Batch (considering grad accum): 200,  Loss: 7.1133, Time: 4.13s, Token/s: 123.93
Epoch: 0, Step: 1603, Batch(micro): 1603, Batch (considering grad accum): 200,  Loss: 7.2465, Time: 3.65s, Token/s: 140.46
Epoch: 0, Step: 1604, Batch(micro): 1604, Batch (considering grad accum): 200,  Loss: 7.7605, Time: 4.06s, Token/s: 126.04
Epoch: 0, Step: 1605, Batch(micro): 1605, Batch (considering grad accum): 200,  Loss: 7.4980, Time: 4.37s, Token/s: 117.11
Epoch: 0, Step: 1606, Batch(micro): 1606, Batch (considering grad accum): 200,  Loss: 7.7748, Time: 3.97s, Token/s: 129.02
Epoch: 0, Step: 1607, Batch(micro): 1607, Batch (considering grad accum): 200,  Loss: 7.6308, Time: 24.22s, Token/s: 21.14
Epoch: 0, Step: 1608, Batch(micro): 1608, Batch (considering grad accum): 201,  Loss: 7.5055, Time: 9.60s, Token/s: 53.32
Epoch: 0, Step: 1609, Batch(micro): 1609, Batch (considering grad accum): 201,  Loss: 7.5590, Time: 4.33s, Token/s: 118.13
Epoch: 0, Step: 1610, Batch(micro): 1610, Batch (considering grad accum): 201,  Loss: 7.4679, Time: 3.84s, Token/s: 133.44
Epoch: 0, Step: 1611, Batch(micro): 1611, Batch (considering grad accum): 201,  Loss: 7.6336, Time: 3.56s, Token/s: 143.94
Epoch: 0, Step: 1612, Batch(micro): 1612, Batch (considering grad accum): 201,  Loss: 7.7407, Time: 3.77s, Token/s: 135.66
Epoch: 0, Step: 1613, Batch(micro): 1613, Batch (considering grad accum): 201,  Loss: 7.1407, Time: 3.76s, Token/s: 136.14
Epoch: 0, Step: 1614, Batch(micro): 1614, Batch (considering grad accum): 201,  Loss: 7.1676, Time: 3.56s, Token/s: 143.79
Epoch: 0, Step: 1615, Batch(micro): 1615, Batch (considering grad accum): 201,  Loss: 7.3340, Time: 24.89s, Token/s: 20.57
Epoch: 0, Step: 1616, Batch(micro): 1616, Batch (considering grad accum): 202,  Loss: 7.4893, Time: 7.63s, Token/s: 67.14
Epoch: 0, Step: 1617, Batch(micro): 1617, Batch (considering grad accum): 202,  Loss: 7.8450, Time: 4.07s, Token/s: 125.88
Epoch: 0, Step: 1618, Batch(micro): 1618, Batch (considering grad accum): 202,  Loss: 7.6531, Time: 4.23s, Token/s: 120.96
Epoch: 0, Step: 1619, Batch(micro): 1619, Batch (considering grad accum): 202,  Loss: 7.3886, Time: 3.42s, Token/s: 149.92
Epoch: 0, Step: 1620, Batch(micro): 1620, Batch (considering grad accum): 202,  Loss: 7.6312, Time: 3.77s, Token/s: 135.79
Epoch: 0, Step: 1621, Batch(micro): 1621, Batch (considering grad accum): 202,  Loss: 8.2170, Time: 3.80s, Token/s: 134.81
Epoch: 0, Step: 1622, Batch(micro): 1622, Batch (considering grad accum): 202,  Loss: 7.9338, Time: 3.68s, Token/s: 139.30
Epoch: 0, Step: 1623, Batch(micro): 1623, Batch (considering grad accum): 202,  Loss: 7.3422, Time: 23.79s, Token/s: 21.52
Epoch: 0, Step: 1624, Batch(micro): 1624, Batch (considering grad accum): 203,  Loss: 7.5950, Time: 7.75s, Token/s: 66.04
Epoch: 0, Step: 1625, Batch(micro): 1625, Batch (considering grad accum): 203,  Loss: 7.8848, Time: 4.18s, Token/s: 122.40
Epoch: 0, Step: 1626, Batch(micro): 1626, Batch (considering grad accum): 203,  Loss: 7.6087, Time: 3.86s, Token/s: 132.79
Epoch: 0, Step: 1627, Batch(micro): 1627, Batch (considering grad accum): 203,  Loss: 7.6080, Time: 3.96s, Token/s: 129.35
Epoch: 0, Step: 1628, Batch(micro): 1628, Batch (considering grad accum): 203,  Loss: 8.0342, Time: 3.74s, Token/s: 136.75
Epoch: 0, Step: 1629, Batch(micro): 1629, Batch (considering grad accum): 203,  Loss: 8.0396, Time: 3.98s, Token/s: 128.77
Epoch: 0, Step: 1630, Batch(micro): 1630, Batch (considering grad accum): 203,  Loss: 7.7196, Time: 4.66s, Token/s: 109.77
Epoch: 0, Step: 1631, Batch(micro): 1631, Batch (considering grad accum): 203,  Loss: 7.6916, Time: 22.54s, Token/s: 22.72
Epoch: 0, Step: 1632, Batch(micro): 1632, Batch (considering grad accum): 204,  Loss: 8.0320, Time: 8.21s, Token/s: 62.37
Epoch: 0, Step: 1633, Batch(micro): 1633, Batch (considering grad accum): 204,  Loss: 7.7561, Time: 4.25s, Token/s: 120.40
Epoch: 0, Step: 1634, Batch(micro): 1634, Batch (considering grad accum): 204,  Loss: 7.9711, Time: 3.52s, Token/s: 145.66
Epoch: 0, Step: 1635, Batch(micro): 1635, Batch (considering grad accum): 204,  Loss: 7.5515, Time: 3.91s, Token/s: 131.02
Epoch: 0, Step: 1636, Batch(micro): 1636, Batch (considering grad accum): 204,  Loss: 7.6225, Time: 3.63s, Token/s: 141.06
Epoch: 0, Step: 1637, Batch(micro): 1637, Batch (considering grad accum): 204,  Loss: 7.7625, Time: 3.44s, Token/s: 148.94
Epoch: 0, Step: 1638, Batch(micro): 1638, Batch (considering grad accum): 204,  Loss: 7.3057, Time: 3.90s, Token/s: 131.24
Epoch: 0, Step: 1639, Batch(micro): 1639, Batch (considering grad accum): 204,  Loss: 7.4640, Time: 24.67s, Token/s: 20.76
Epoch: 0, Step: 1640, Batch(micro): 1640, Batch (considering grad accum): 205,  Loss: 7.6235, Time: 7.45s, Token/s: 68.72
Epoch: 0, Step: 1641, Batch(micro): 1641, Batch (considering grad accum): 205,  Loss: 8.4443, Time: 4.06s, Token/s: 126.15
Epoch: 0, Step: 1642, Batch(micro): 1642, Batch (considering grad accum): 205,  Loss: 8.1040, Time: 3.86s, Token/s: 132.75
Epoch: 0, Step: 1643, Batch(micro): 1643, Batch (considering grad accum): 205,  Loss: 7.7015, Time: 3.63s, Token/s: 141.04
Epoch: 0, Step: 1644, Batch(micro): 1644, Batch (considering grad accum): 205,  Loss: 7.5390, Time: 3.85s, Token/s: 132.97
Epoch: 0, Step: 1645, Batch(micro): 1645, Batch (considering grad accum): 205,  Loss: 7.9184, Time: 3.93s, Token/s: 130.36
Epoch: 0, Step: 1646, Batch(micro): 1646, Batch (considering grad accum): 205,  Loss: 7.5836, Time: 3.69s, Token/s: 138.73
Epoch: 0, Step: 1647, Batch(micro): 1647, Batch (considering grad accum): 205,  Loss: 8.0389, Time: 25.14s, Token/s: 20.37
Epoch: 0, Step: 1648, Batch(micro): 1648, Batch (considering grad accum): 206,  Loss: 7.8865, Time: 8.89s, Token/s: 57.62
Epoch: 0, Step: 1649, Batch(micro): 1649, Batch (considering grad accum): 206,  Loss: 7.5983, Time: 4.18s, Token/s: 122.42
Epoch: 0, Step: 1650, Batch(micro): 1650, Batch (considering grad accum): 206,  Loss: 8.1123, Time: 3.58s, Token/s: 143.19
Epoch: 0, Step: 1651, Batch(micro): 1651, Batch (considering grad accum): 206,  Loss: 7.9337, Time: 3.83s, Token/s: 133.55
Epoch: 0, Step: 1652, Batch(micro): 1652, Batch (considering grad accum): 206,  Loss: 7.9087, Time: 3.91s, Token/s: 131.02
Epoch: 0, Step: 1653, Batch(micro): 1653, Batch (considering grad accum): 206,  Loss: 7.8451, Time: 3.84s, Token/s: 133.21
Epoch: 0, Step: 1654, Batch(micro): 1654, Batch (considering grad accum): 206,  Loss: 7.9632, Time: 3.59s, Token/s: 142.70
Epoch: 0, Step: 1655, Batch(micro): 1655, Batch (considering grad accum): 206,  Loss: 8.3629, Time: 22.61s, Token/s: 22.65
Epoch: 0, Step: 1656, Batch(micro): 1656, Batch (considering grad accum): 207,  Loss: 8.0581, Time: 8.08s, Token/s: 63.38
Epoch: 0, Step: 1657, Batch(micro): 1657, Batch (considering grad accum): 207,  Loss: 8.0228, Time: 4.17s, Token/s: 122.80
Epoch: 0, Step: 1658, Batch(micro): 1658, Batch (considering grad accum): 207,  Loss: 7.5859, Time: 4.33s, Token/s: 118.26
Epoch: 0, Step: 1659, Batch(micro): 1659, Batch (considering grad accum): 207,  Loss: 8.1069, Time: 4.02s, Token/s: 127.30
Epoch: 0, Step: 1660, Batch(micro): 1660, Batch (considering grad accum): 207,  Loss: 7.7502, Time: 4.56s, Token/s: 112.40
Epoch: 0, Step: 1661, Batch(micro): 1661, Batch (considering grad accum): 207,  Loss: 7.5383, Time: 3.54s, Token/s: 144.56
Epoch: 0, Step: 1662, Batch(micro): 1662, Batch (considering grad accum): 207,  Loss: 7.4934, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 1663, Batch(micro): 1663, Batch (considering grad accum): 207,  Loss: 7.9616, Time: 24.27s, Token/s: 21.10
Epoch: 0, Step: 1664, Batch(micro): 1664, Batch (considering grad accum): 208,  Loss: 8.0656, Time: 7.66s, Token/s: 66.81
Epoch: 0, Step: 1665, Batch(micro): 1665, Batch (considering grad accum): 208,  Loss: 7.5534, Time: 4.63s, Token/s: 110.67
Epoch: 0, Step: 1666, Batch(micro): 1666, Batch (considering grad accum): 208,  Loss: 7.5287, Time: 3.73s, Token/s: 137.16
Epoch: 0, Step: 1667, Batch(micro): 1667, Batch (considering grad accum): 208,  Loss: 7.5430, Time: 4.06s, Token/s: 126.22
Epoch: 0, Step: 1668, Batch(micro): 1668, Batch (considering grad accum): 208,  Loss: 7.2165, Time: 4.10s, Token/s: 124.83
Epoch: 0, Step: 1669, Batch(micro): 1669, Batch (considering grad accum): 208,  Loss: 7.2746, Time: 3.65s, Token/s: 140.17
Epoch: 0, Step: 1670, Batch(micro): 1670, Batch (considering grad accum): 208,  Loss: 7.5113, Time: 3.95s, Token/s: 129.64
Epoch: 0, Step: 1671, Batch(micro): 1671, Batch (considering grad accum): 208,  Loss: 7.8578, Time: 23.14s, Token/s: 22.12
Epoch: 0, Step: 1672, Batch(micro): 1672, Batch (considering grad accum): 209,  Loss: 7.7911, Time: 8.56s, Token/s: 59.80
Epoch: 0, Step: 1673, Batch(micro): 1673, Batch (considering grad accum): 209,  Loss: 7.5856, Time: 4.44s, Token/s: 115.27
Epoch: 0, Step: 1674, Batch(micro): 1674, Batch (considering grad accum): 209,  Loss: 7.9265, Time: 3.87s, Token/s: 132.22
Epoch: 0, Step: 1675, Batch(micro): 1675, Batch (considering grad accum): 209,  Loss: 7.9970, Time: 3.94s, Token/s: 130.05
Epoch: 0, Step: 1676, Batch(micro): 1676, Batch (considering grad accum): 209,  Loss: 7.6653, Time: 3.62s, Token/s: 141.58
Epoch: 0, Step: 1677, Batch(micro): 1677, Batch (considering grad accum): 209,  Loss: 7.6304, Time: 3.41s, Token/s: 150.19
Epoch: 0, Step: 1678, Batch(micro): 1678, Batch (considering grad accum): 209,  Loss: 7.8816, Time: 3.72s, Token/s: 137.72
Epoch: 0, Step: 1679, Batch(micro): 1679, Batch (considering grad accum): 209,  Loss: 7.8868, Time: 24.25s, Token/s: 21.12
Epoch: 0, Step: 1680, Batch(micro): 1680, Batch (considering grad accum): 210,  Loss: 7.5694, Time: 7.63s, Token/s: 67.14
Epoch: 0, Step: 1681, Batch(micro): 1681, Batch (considering grad accum): 210,  Loss: 7.8961, Time: 4.10s, Token/s: 124.83
Epoch: 0, Step: 1682, Batch(micro): 1682, Batch (considering grad accum): 210,  Loss: 7.6761, Time: 3.82s, Token/s: 133.89
Epoch: 0, Step: 1683, Batch(micro): 1683, Batch (considering grad accum): 210,  Loss: 8.3119, Time: 3.75s, Token/s: 136.51
Epoch: 0, Step: 1684, Batch(micro): 1684, Batch (considering grad accum): 210,  Loss: 7.4697, Time: 3.56s, Token/s: 143.69
Epoch: 0, Step: 1685, Batch(micro): 1685, Batch (considering grad accum): 210,  Loss: 7.5618, Time: 3.92s, Token/s: 130.47
Epoch: 0, Step: 1686, Batch(micro): 1686, Batch (considering grad accum): 210,  Loss: 6.9369, Time: 3.70s, Token/s: 138.42
Epoch: 0, Step: 1687, Batch(micro): 1687, Batch (considering grad accum): 210,  Loss: 7.4434, Time: 26.08s, Token/s: 19.63
Epoch: 0, Step: 1688, Batch(micro): 1688, Batch (considering grad accum): 211,  Loss: 7.4383, Time: 8.13s, Token/s: 62.99
Epoch: 0, Step: 1689, Batch(micro): 1689, Batch (considering grad accum): 211,  Loss: 7.4304, Time: 4.19s, Token/s: 122.14
Epoch: 0, Step: 1690, Batch(micro): 1690, Batch (considering grad accum): 211,  Loss: 7.7097, Time: 3.93s, Token/s: 130.19
Epoch: 0, Step: 1691, Batch(micro): 1691, Batch (considering grad accum): 211,  Loss: 8.1555, Time: 3.87s, Token/s: 132.33
Epoch: 0, Step: 1692, Batch(micro): 1692, Batch (considering grad accum): 211,  Loss: 7.8686, Time: 3.69s, Token/s: 138.64
Epoch: 0, Step: 1693, Batch(micro): 1693, Batch (considering grad accum): 211,  Loss: 7.3174, Time: 3.65s, Token/s: 140.22
Epoch: 0, Step: 1694, Batch(micro): 1694, Batch (considering grad accum): 211,  Loss: 7.0900, Time: 3.63s, Token/s: 141.17
Epoch: 0, Step: 1695, Batch(micro): 1695, Batch (considering grad accum): 211,  Loss: 7.3529, Time: 24.13s, Token/s: 21.22
Epoch: 0, Step: 1696, Batch(micro): 1696, Batch (considering grad accum): 212,  Loss: 7.6363, Time: 7.89s, Token/s: 64.93
Epoch: 0, Step: 1697, Batch(micro): 1697, Batch (considering grad accum): 212,  Loss: 7.3527, Time: 4.24s, Token/s: 120.72
Epoch: 0, Step: 1698, Batch(micro): 1698, Batch (considering grad accum): 212,  Loss: 7.4322, Time: 3.81s, Token/s: 134.42
Epoch: 0, Step: 1699, Batch(micro): 1699, Batch (considering grad accum): 212,  Loss: 7.0163, Time: 3.60s, Token/s: 142.17
Updating MLP bias
Epoch: 0, Step: 1700, Batch(micro): 1700, Batch (considering grad accum): 212,  Loss: 7.7566, Time: 3.66s, Token/s: 140.03
Epoch: 0, Step: 1701, Batch(micro): 1701, Batch (considering grad accum): 212,  Loss: 7.6962, Time: 3.87s, Token/s: 132.44
Epoch: 0, Step: 1702, Batch(micro): 1702, Batch (considering grad accum): 212,  Loss: 7.5744, Time: 3.64s, Token/s: 140.79
Epoch: 0, Step: 1703, Batch(micro): 1703, Batch (considering grad accum): 212,  Loss: 7.7379, Time: 23.31s, Token/s: 21.96
Epoch: 0, Step: 1704, Batch(micro): 1704, Batch (considering grad accum): 213,  Loss: 7.4264, Time: 8.39s, Token/s: 61.03
Epoch: 0, Step: 1705, Batch(micro): 1705, Batch (considering grad accum): 213,  Loss: 7.4373, Time: 4.59s, Token/s: 111.59
Epoch: 0, Step: 1706, Batch(micro): 1706, Batch (considering grad accum): 213,  Loss: 7.6508, Time: 3.73s, Token/s: 137.35
Epoch: 0, Step: 1707, Batch(micro): 1707, Batch (considering grad accum): 213,  Loss: 7.7810, Time: 3.76s, Token/s: 136.04
Epoch: 0, Step: 1708, Batch(micro): 1708, Batch (considering grad accum): 213,  Loss: 9.0348, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 1709, Batch(micro): 1709, Batch (considering grad accum): 213,  Loss: 7.8787, Time: 3.88s, Token/s: 132.09
Epoch: 0, Step: 1710, Batch(micro): 1710, Batch (considering grad accum): 213,  Loss: 7.7122, Time: 3.80s, Token/s: 134.81
Epoch: 0, Step: 1711, Batch(micro): 1711, Batch (considering grad accum): 213,  Loss: 7.6424, Time: 21.64s, Token/s: 23.66
Epoch: 0, Step: 1712, Batch(micro): 1712, Batch (considering grad accum): 214,  Loss: 7.5301, Time: 7.56s, Token/s: 67.72
Epoch: 0, Step: 1713, Batch(micro): 1713, Batch (considering grad accum): 214,  Loss: 7.4845, Time: 3.93s, Token/s: 130.35
Epoch: 0, Step: 1714, Batch(micro): 1714, Batch (considering grad accum): 214,  Loss: 7.8086, Time: 3.64s, Token/s: 140.67
Epoch: 0, Step: 1715, Batch(micro): 1715, Batch (considering grad accum): 214,  Loss: 7.3867, Time: 4.32s, Token/s: 118.39
Epoch: 0, Step: 1716, Batch(micro): 1716, Batch (considering grad accum): 214,  Loss: 7.3403, Time: 3.54s, Token/s: 144.44
Epoch: 0, Step: 1717, Batch(micro): 1717, Batch (considering grad accum): 214,  Loss: 7.4609, Time: 4.12s, Token/s: 124.30
Epoch: 0, Step: 1718, Batch(micro): 1718, Batch (considering grad accum): 214,  Loss: 7.5644, Time: 3.44s, Token/s: 148.78
Epoch: 0, Step: 1719, Batch(micro): 1719, Batch (considering grad accum): 214,  Loss: 7.8433, Time: 18.90s, Token/s: 27.09
Epoch: 0, Step: 1720, Batch(micro): 1720, Batch (considering grad accum): 215,  Loss: 7.6161, Time: 6.70s, Token/s: 76.43
Epoch: 0, Step: 1721, Batch(micro): 1721, Batch (considering grad accum): 215,  Loss: 7.6820, Time: 4.05s, Token/s: 126.40
Epoch: 0, Step: 1722, Batch(micro): 1722, Batch (considering grad accum): 215,  Loss: 7.6119, Time: 3.70s, Token/s: 138.23
Epoch: 0, Step: 1723, Batch(micro): 1723, Batch (considering grad accum): 215,  Loss: 7.7426, Time: 3.94s, Token/s: 129.89
Epoch: 0, Step: 1724, Batch(micro): 1724, Batch (considering grad accum): 215,  Loss: 7.6766, Time: 3.79s, Token/s: 135.23
Epoch: 0, Step: 1725, Batch(micro): 1725, Batch (considering grad accum): 215,  Loss: 7.6493, Time: 4.03s, Token/s: 127.04
Epoch: 0, Step: 1726, Batch(micro): 1726, Batch (considering grad accum): 215,  Loss: 7.5391, Time: 3.68s, Token/s: 139.10
Epoch: 0, Step: 1727, Batch(micro): 1727, Batch (considering grad accum): 215,  Loss: 7.9972, Time: 19.40s, Token/s: 26.39
Epoch: 0, Step: 1728, Batch(micro): 1728, Batch (considering grad accum): 216,  Loss: 7.3171, Time: 7.16s, Token/s: 71.54
Epoch: 0, Step: 1729, Batch(micro): 1729, Batch (considering grad accum): 216,  Loss: 7.6939, Time: 4.48s, Token/s: 114.35
Epoch: 0, Step: 1730, Batch(micro): 1730, Batch (considering grad accum): 216,  Loss: 7.2072, Time: 3.48s, Token/s: 146.94
Epoch: 0, Step: 1731, Batch(micro): 1731, Batch (considering grad accum): 216,  Loss: 7.5269, Time: 3.72s, Token/s: 137.56
Epoch: 0, Step: 1732, Batch(micro): 1732, Batch (considering grad accum): 216,  Loss: 7.2131, Time: 3.74s, Token/s: 136.91
Epoch: 0, Step: 1733, Batch(micro): 1733, Batch (considering grad accum): 216,  Loss: 7.3567, Time: 3.51s, Token/s: 145.74
Epoch: 0, Step: 1734, Batch(micro): 1734, Batch (considering grad accum): 216,  Loss: 7.1753, Time: 3.72s, Token/s: 137.67
Epoch: 0, Step: 1735, Batch(micro): 1735, Batch (considering grad accum): 216,  Loss: 7.2947, Time: 19.89s, Token/s: 25.75
Epoch: 0, Step: 1736, Batch(micro): 1736, Batch (considering grad accum): 217,  Loss: 7.6021, Time: 7.51s, Token/s: 68.20
Epoch: 0, Step: 1737, Batch(micro): 1737, Batch (considering grad accum): 217,  Loss: 7.8363, Time: 4.32s, Token/s: 118.44
Epoch: 0, Step: 1738, Batch(micro): 1738, Batch (considering grad accum): 217,  Loss: 8.4876, Time: 3.92s, Token/s: 130.73
Epoch: 0, Step: 1739, Batch(micro): 1739, Batch (considering grad accum): 217,  Loss: 8.2962, Time: 3.79s, Token/s: 135.06
Epoch: 0, Step: 1740, Batch(micro): 1740, Batch (considering grad accum): 217,  Loss: 7.3388, Time: 4.07s, Token/s: 125.87
Epoch: 0, Step: 1741, Batch(micro): 1741, Batch (considering grad accum): 217,  Loss: 7.8108, Time: 4.03s, Token/s: 127.08
Epoch: 0, Step: 1742, Batch(micro): 1742, Batch (considering grad accum): 217,  Loss: 7.5287, Time: 3.76s, Token/s: 136.21
Epoch: 0, Step: 1743, Batch(micro): 1743, Batch (considering grad accum): 217,  Loss: 8.4812, Time: 19.17s, Token/s: 26.71
Epoch: 0, Step: 1744, Batch(micro): 1744, Batch (considering grad accum): 218,  Loss: 7.7835, Time: 6.73s, Token/s: 76.03
Epoch: 0, Step: 1745, Batch(micro): 1745, Batch (considering grad accum): 218,  Loss: 7.7981, Time: 3.92s, Token/s: 130.60
Epoch: 0, Step: 1746, Batch(micro): 1746, Batch (considering grad accum): 218,  Loss: 7.9557, Time: 3.21s, Token/s: 159.69
Epoch: 0, Step: 1747, Batch(micro): 1747, Batch (considering grad accum): 218,  Loss: 7.9357, Time: 3.36s, Token/s: 152.26
Epoch: 0, Step: 1748, Batch(micro): 1748, Batch (considering grad accum): 218,  Loss: 6.9007, Time: 3.52s, Token/s: 145.50
Epoch: 0, Step: 1749, Batch(micro): 1749, Batch (considering grad accum): 218,  Loss: 6.9952, Time: 3.49s, Token/s: 146.50
Epoch: 0, Step: 1750, Batch(micro): 1750, Batch (considering grad accum): 218,  Loss: 7.8093, Time: 3.66s, Token/s: 139.94
Epoch: 0, Step: 1751, Batch(micro): 1751, Batch (considering grad accum): 218,  Loss: 7.4812, Time: 19.88s, Token/s: 25.76
Epoch: 0, Step: 1752, Batch(micro): 1752, Batch (considering grad accum): 219,  Loss: 7.1612, Time: 7.35s, Token/s: 69.62
Epoch: 0, Step: 1753, Batch(micro): 1753, Batch (considering grad accum): 219,  Loss: 7.3043, Time: 5.04s, Token/s: 101.67
Epoch: 0, Step: 1754, Batch(micro): 1754, Batch (considering grad accum): 219,  Loss: 7.4802, Time: 3.87s, Token/s: 132.35
Epoch: 0, Step: 1755, Batch(micro): 1755, Batch (considering grad accum): 219,  Loss: 7.3394, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 1756, Batch(micro): 1756, Batch (considering grad accum): 219,  Loss: 7.7553, Time: 3.55s, Token/s: 144.22
Epoch: 0, Step: 1757, Batch(micro): 1757, Batch (considering grad accum): 219,  Loss: 7.7671, Time: 3.69s, Token/s: 138.62
Epoch: 0, Step: 1758, Batch(micro): 1758, Batch (considering grad accum): 219,  Loss: 7.5558, Time: 3.60s, Token/s: 142.11
Epoch: 0, Step: 1759, Batch(micro): 1759, Batch (considering grad accum): 219,  Loss: 7.2366, Time: 17.52s, Token/s: 29.23
Epoch: 0, Step: 1760, Batch(micro): 1760, Batch (considering grad accum): 220,  Loss: 7.6628, Time: 5.30s, Token/s: 96.52
Epoch: 0, Step: 1761, Batch(micro): 1761, Batch (considering grad accum): 220,  Loss: 8.2310, Time: 4.31s, Token/s: 118.83
Epoch: 0, Step: 1762, Batch(micro): 1762, Batch (considering grad accum): 220,  Loss: 8.1064, Time: 3.77s, Token/s: 135.69
Epoch: 0, Step: 1763, Batch(micro): 1763, Batch (considering grad accum): 220,  Loss: 7.8029, Time: 3.60s, Token/s: 142.29
Epoch: 0, Step: 1764, Batch(micro): 1764, Batch (considering grad accum): 220,  Loss: 7.6473, Time: 3.57s, Token/s: 143.57
Epoch: 0, Step: 1765, Batch(micro): 1765, Batch (considering grad accum): 220,  Loss: 8.3273, Time: 4.33s, Token/s: 118.17
Epoch: 0, Step: 1766, Batch(micro): 1766, Batch (considering grad accum): 220,  Loss: 7.3802, Time: 3.63s, Token/s: 141.11
Epoch: 0, Step: 1767, Batch(micro): 1767, Batch (considering grad accum): 220,  Loss: 7.5483, Time: 18.36s, Token/s: 27.89
Epoch: 0, Step: 1768, Batch(micro): 1768, Batch (considering grad accum): 221,  Loss: 7.4664, Time: 6.21s, Token/s: 82.51
Epoch: 0, Step: 1769, Batch(micro): 1769, Batch (considering grad accum): 221,  Loss: 7.6328, Time: 4.00s, Token/s: 127.91
Epoch: 0, Step: 1770, Batch(micro): 1770, Batch (considering grad accum): 221,  Loss: 7.2625, Time: 3.76s, Token/s: 136.16
Epoch: 0, Step: 1771, Batch(micro): 1771, Batch (considering grad accum): 221,  Loss: 7.3887, Time: 3.89s, Token/s: 131.60
Epoch: 0, Step: 1772, Batch(micro): 1772, Batch (considering grad accum): 221,  Loss: 7.2181, Time: 4.22s, Token/s: 121.29
Epoch: 0, Step: 1773, Batch(micro): 1773, Batch (considering grad accum): 221,  Loss: 8.1493, Time: 3.46s, Token/s: 148.05
Epoch: 0, Step: 1774, Batch(micro): 1774, Batch (considering grad accum): 221,  Loss: 8.2250, Time: 3.77s, Token/s: 135.88
Epoch: 0, Step: 1775, Batch(micro): 1775, Batch (considering grad accum): 221,  Loss: 7.6220, Time: 23.13s, Token/s: 22.14
Epoch: 0, Step: 1776, Batch(micro): 1776, Batch (considering grad accum): 222,  Loss: 7.3814, Time: 7.96s, Token/s: 64.34
Epoch: 0, Step: 1777, Batch(micro): 1777, Batch (considering grad accum): 222,  Loss: 7.0876, Time: 3.77s, Token/s: 135.91
Epoch: 0, Step: 1778, Batch(micro): 1778, Batch (considering grad accum): 222,  Loss: 7.2762, Time: 3.32s, Token/s: 154.34
Epoch: 0, Step: 1779, Batch(micro): 1779, Batch (considering grad accum): 222,  Loss: 7.2158, Time: 3.25s, Token/s: 157.65
Epoch: 0, Step: 1780, Batch(micro): 1780, Batch (considering grad accum): 222,  Loss: 7.3458, Time: 3.69s, Token/s: 138.80
Epoch: 0, Step: 1781, Batch(micro): 1781, Batch (considering grad accum): 222,  Loss: 7.2740, Time: 3.45s, Token/s: 148.23
Epoch: 0, Step: 1782, Batch(micro): 1782, Batch (considering grad accum): 222,  Loss: 7.6977, Time: 3.42s, Token/s: 149.61
Epoch: 0, Step: 1783, Batch(micro): 1783, Batch (considering grad accum): 222,  Loss: 7.6051, Time: 19.79s, Token/s: 25.87
Epoch: 0, Step: 1784, Batch(micro): 1784, Batch (considering grad accum): 223,  Loss: 7.3907, Time: 8.13s, Token/s: 62.98
Epoch: 0, Step: 1785, Batch(micro): 1785, Batch (considering grad accum): 223,  Loss: 7.2835, Time: 3.80s, Token/s: 134.66
Epoch: 0, Step: 1786, Batch(micro): 1786, Batch (considering grad accum): 223,  Loss: 7.5317, Time: 3.44s, Token/s: 148.75
Epoch: 0, Step: 1787, Batch(micro): 1787, Batch (considering grad accum): 223,  Loss: 7.5971, Time: 4.14s, Token/s: 123.69
Epoch: 0, Step: 1788, Batch(micro): 1788, Batch (considering grad accum): 223,  Loss: 7.2930, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 1789, Batch(micro): 1789, Batch (considering grad accum): 223,  Loss: 7.4310, Time: 3.35s, Token/s: 152.69
Epoch: 0, Step: 1790, Batch(micro): 1790, Batch (considering grad accum): 223,  Loss: 7.7251, Time: 3.40s, Token/s: 150.63
Epoch: 0, Step: 1791, Batch(micro): 1791, Batch (considering grad accum): 223,  Loss: 7.4073, Time: 19.68s, Token/s: 26.02
Epoch: 0, Step: 1792, Batch(micro): 1792, Batch (considering grad accum): 224,  Loss: 7.6997, Time: 7.75s, Token/s: 66.06
Epoch: 0, Step: 1793, Batch(micro): 1793, Batch (considering grad accum): 224,  Loss: 7.7206, Time: 3.69s, Token/s: 138.60
Epoch: 0, Step: 1794, Batch(micro): 1794, Batch (considering grad accum): 224,  Loss: 7.9454, Time: 3.11s, Token/s: 164.59
Epoch: 0, Step: 1795, Batch(micro): 1795, Batch (considering grad accum): 224,  Loss: 7.4646, Time: 3.05s, Token/s: 167.82
Epoch: 0, Step: 1796, Batch(micro): 1796, Batch (considering grad accum): 224,  Loss: 7.3312, Time: 3.28s, Token/s: 155.97
Epoch: 0, Step: 1797, Batch(micro): 1797, Batch (considering grad accum): 224,  Loss: 7.1646, Time: 3.25s, Token/s: 157.45
Epoch: 0, Step: 1798, Batch(micro): 1798, Batch (considering grad accum): 224,  Loss: 7.0574, Time: 3.25s, Token/s: 157.68
Epoch: 0, Step: 1799, Batch(micro): 1799, Batch (considering grad accum): 224,  Loss: 7.3769, Time: 18.05s, Token/s: 28.36
Updating MLP bias
Epoch: 0, Step: 1800, Batch(micro): 1800, Batch (considering grad accum): 225,  Loss: 7.7296, Time: 7.00s, Token/s: 73.18
Epoch: 0, Step: 1801, Batch(micro): 1801, Batch (considering grad accum): 225,  Loss: 7.6005, Time: 3.94s, Token/s: 130.09
Epoch: 0, Step: 1802, Batch(micro): 1802, Batch (considering grad accum): 225,  Loss: 8.2716, Time: 3.67s, Token/s: 139.45
Epoch: 0, Step: 1803, Batch(micro): 1803, Batch (considering grad accum): 225,  Loss: 7.5654, Time: 3.57s, Token/s: 143.58
Epoch: 0, Step: 1804, Batch(micro): 1804, Batch (considering grad accum): 225,  Loss: 7.5653, Time: 3.59s, Token/s: 142.74
Epoch: 0, Step: 1805, Batch(micro): 1805, Batch (considering grad accum): 225,  Loss: 8.0177, Time: 3.29s, Token/s: 155.58
Epoch: 0, Step: 1806, Batch(micro): 1806, Batch (considering grad accum): 225,  Loss: 7.5805, Time: 3.67s, Token/s: 139.52
Epoch: 0, Step: 1807, Batch(micro): 1807, Batch (considering grad accum): 225,  Loss: 7.3818, Time: 20.99s, Token/s: 24.39
Epoch: 0, Step: 1808, Batch(micro): 1808, Batch (considering grad accum): 226,  Loss: 7.2902, Time: 8.98s, Token/s: 57.04
Epoch: 0, Step: 1809, Batch(micro): 1809, Batch (considering grad accum): 226,  Loss: 7.5205, Time: 3.98s, Token/s: 128.58
Epoch: 0, Step: 1810, Batch(micro): 1810, Batch (considering grad accum): 226,  Loss: 7.2107, Time: 3.71s, Token/s: 137.96
Epoch: 0, Step: 1811, Batch(micro): 1811, Batch (considering grad accum): 226,  Loss: 7.2258, Time: 3.59s, Token/s: 142.78
Epoch: 0, Step: 1812, Batch(micro): 1812, Batch (considering grad accum): 226,  Loss: 7.6365, Time: 3.21s, Token/s: 159.75
Epoch: 0, Step: 1813, Batch(micro): 1813, Batch (considering grad accum): 226,  Loss: 7.0942, Time: 3.18s, Token/s: 161.18
Epoch: 0, Step: 1814, Batch(micro): 1814, Batch (considering grad accum): 226,  Loss: 7.1112, Time: 3.35s, Token/s: 152.73
Epoch: 0, Step: 1815, Batch(micro): 1815, Batch (considering grad accum): 226,  Loss: 7.5754, Time: 24.41s, Token/s: 20.97
Epoch: 0, Step: 1816, Batch(micro): 1816, Batch (considering grad accum): 227,  Loss: 7.6887, Time: 9.15s, Token/s: 55.94
Epoch: 0, Step: 1817, Batch(micro): 1817, Batch (considering grad accum): 227,  Loss: 8.0451, Time: 4.21s, Token/s: 121.60
Epoch: 0, Step: 1818, Batch(micro): 1818, Batch (considering grad accum): 227,  Loss: 7.4407, Time: 3.38s, Token/s: 151.59
Epoch: 0, Step: 1819, Batch(micro): 1819, Batch (considering grad accum): 227,  Loss: 7.6246, Time: 3.22s, Token/s: 159.05
Epoch: 0, Step: 1820, Batch(micro): 1820, Batch (considering grad accum): 227,  Loss: 6.9783, Time: 4.09s, Token/s: 125.28
Epoch: 0, Step: 1821, Batch(micro): 1821, Batch (considering grad accum): 227,  Loss: 7.3366, Time: 3.65s, Token/s: 140.40
Epoch: 0, Step: 1822, Batch(micro): 1822, Batch (considering grad accum): 227,  Loss: 7.1207, Time: 3.65s, Token/s: 140.18
Epoch: 0, Step: 1823, Batch(micro): 1823, Batch (considering grad accum): 227,  Loss: 7.3677, Time: 26.52s, Token/s: 19.30
Epoch: 0, Step: 1824, Batch(micro): 1824, Batch (considering grad accum): 228,  Loss: 7.2746, Time: 9.85s, Token/s: 51.99
Epoch: 0, Step: 1825, Batch(micro): 1825, Batch (considering grad accum): 228,  Loss: 7.4241, Time: 4.33s, Token/s: 118.20
Epoch: 0, Step: 1826, Batch(micro): 1826, Batch (considering grad accum): 228,  Loss: 7.0121, Time: 3.32s, Token/s: 154.15
Epoch: 0, Step: 1827, Batch(micro): 1827, Batch (considering grad accum): 228,  Loss: 7.8712, Time: 3.39s, Token/s: 151.07
Epoch: 0, Step: 1828, Batch(micro): 1828, Batch (considering grad accum): 228,  Loss: 7.5268, Time: 3.51s, Token/s: 145.79
Epoch: 0, Step: 1829, Batch(micro): 1829, Batch (considering grad accum): 228,  Loss: 7.9972, Time: 3.37s, Token/s: 152.10
Epoch: 0, Step: 1830, Batch(micro): 1830, Batch (considering grad accum): 228,  Loss: 7.6807, Time: 3.44s, Token/s: 148.91
Epoch: 0, Step: 1831, Batch(micro): 1831, Batch (considering grad accum): 228,  Loss: 7.1538, Time: 25.96s, Token/s: 19.72
Epoch: 0, Step: 1832, Batch(micro): 1832, Batch (considering grad accum): 229,  Loss: 7.8051, Time: 8.05s, Token/s: 63.60
Epoch: 0, Step: 1833, Batch(micro): 1833, Batch (considering grad accum): 229,  Loss: 7.3037, Time: 3.67s, Token/s: 139.38
Epoch: 0, Step: 1834, Batch(micro): 1834, Batch (considering grad accum): 229,  Loss: 8.0637, Time: 3.71s, Token/s: 137.99
Epoch: 0, Step: 1835, Batch(micro): 1835, Batch (considering grad accum): 229,  Loss: 7.3031, Time: 3.68s, Token/s: 139.24
Epoch: 0, Step: 1836, Batch(micro): 1836, Batch (considering grad accum): 229,  Loss: 7.0964, Time: 3.43s, Token/s: 149.34
Epoch: 0, Step: 1837, Batch(micro): 1837, Batch (considering grad accum): 229,  Loss: 7.3797, Time: 3.36s, Token/s: 152.17
Epoch: 0, Step: 1838, Batch(micro): 1838, Batch (considering grad accum): 229,  Loss: 7.1238, Time: 3.57s, Token/s: 143.33
Epoch: 0, Step: 1839, Batch(micro): 1839, Batch (considering grad accum): 229,  Loss: 7.6495, Time: 25.73s, Token/s: 19.90
Epoch: 0, Step: 1840, Batch(micro): 1840, Batch (considering grad accum): 230,  Loss: 7.4827, Time: 7.45s, Token/s: 68.69
Epoch: 0, Step: 1841, Batch(micro): 1841, Batch (considering grad accum): 230,  Loss: 7.5070, Time: 3.67s, Token/s: 139.43
Epoch: 0, Step: 1842, Batch(micro): 1842, Batch (considering grad accum): 230,  Loss: 7.3100, Time: 3.06s, Token/s: 167.19
Epoch: 0, Step: 1843, Batch(micro): 1843, Batch (considering grad accum): 230,  Loss: 7.3897, Time: 3.37s, Token/s: 151.99
Epoch: 0, Step: 1844, Batch(micro): 1844, Batch (considering grad accum): 230,  Loss: 7.2231, Time: 3.06s, Token/s: 167.45
Epoch: 0, Step: 1845, Batch(micro): 1845, Batch (considering grad accum): 230,  Loss: 7.9643, Time: 2.88s, Token/s: 177.60
Epoch: 0, Step: 1846, Batch(micro): 1846, Batch (considering grad accum): 230,  Loss: 7.2469, Time: 3.02s, Token/s: 169.75
Epoch: 0, Step: 1847, Batch(micro): 1847, Batch (considering grad accum): 230,  Loss: 7.2167, Time: 26.83s, Token/s: 19.09
Epoch: 0, Step: 1848, Batch(micro): 1848, Batch (considering grad accum): 231,  Loss: 8.2666, Time: 9.07s, Token/s: 56.47
Epoch: 0, Step: 1849, Batch(micro): 1849, Batch (considering grad accum): 231,  Loss: 7.9592, Time: 4.11s, Token/s: 124.67
Epoch: 0, Step: 1850, Batch(micro): 1850, Batch (considering grad accum): 231,  Loss: 7.2695, Time: 3.59s, Token/s: 142.58
Epoch: 0, Step: 1851, Batch(micro): 1851, Batch (considering grad accum): 231,  Loss: 7.2563, Time: 3.67s, Token/s: 139.66
Epoch: 0, Step: 1852, Batch(micro): 1852, Batch (considering grad accum): 231,  Loss: 7.4815, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 1853, Batch(micro): 1853, Batch (considering grad accum): 231,  Loss: 7.7529, Time: 3.77s, Token/s: 135.75
Epoch: 0, Step: 1854, Batch(micro): 1854, Batch (considering grad accum): 231,  Loss: 8.0071, Time: 3.45s, Token/s: 148.24
Epoch: 0, Step: 1855, Batch(micro): 1855, Batch (considering grad accum): 231,  Loss: 7.4483, Time: 24.01s, Token/s: 21.32
Epoch: 0, Step: 1856, Batch(micro): 1856, Batch (considering grad accum): 232,  Loss: 6.8759, Time: 8.84s, Token/s: 57.93
Epoch: 0, Step: 1857, Batch(micro): 1857, Batch (considering grad accum): 232,  Loss: 7.6174, Time: 3.79s, Token/s: 135.17
Epoch: 0, Step: 1858, Batch(micro): 1858, Batch (considering grad accum): 232,  Loss: 7.9490, Time: 3.39s, Token/s: 150.85
Epoch: 0, Step: 1859, Batch(micro): 1859, Batch (considering grad accum): 232,  Loss: 7.6736, Time: 3.94s, Token/s: 129.95
Epoch: 0, Step: 1860, Batch(micro): 1860, Batch (considering grad accum): 232,  Loss: 7.8328, Time: 3.80s, Token/s: 134.69
Epoch: 0, Step: 1861, Batch(micro): 1861, Batch (considering grad accum): 232,  Loss: 7.5346, Time: 3.36s, Token/s: 152.35
Epoch: 0, Step: 1862, Batch(micro): 1862, Batch (considering grad accum): 232,  Loss: 7.3462, Time: 3.74s, Token/s: 136.89
Epoch: 0, Step: 1863, Batch(micro): 1863, Batch (considering grad accum): 232,  Loss: 7.6134, Time: 25.47s, Token/s: 20.10
Epoch: 0, Step: 1864, Batch(micro): 1864, Batch (considering grad accum): 233,  Loss: 7.4704, Time: 12.09s, Token/s: 42.34
Epoch: 0, Step: 1865, Batch(micro): 1865, Batch (considering grad accum): 233,  Loss: 7.9988, Time: 3.88s, Token/s: 131.83
Epoch: 0, Step: 1866, Batch(micro): 1866, Batch (considering grad accum): 233,  Loss: 7.7037, Time: 3.57s, Token/s: 143.54
Epoch: 0, Step: 1867, Batch(micro): 1867, Batch (considering grad accum): 233,  Loss: 8.0083, Time: 3.55s, Token/s: 144.24
Epoch: 0, Step: 1868, Batch(micro): 1868, Batch (considering grad accum): 233,  Loss: 7.5911, Time: 3.84s, Token/s: 133.31
Epoch: 0, Step: 1869, Batch(micro): 1869, Batch (considering grad accum): 233,  Loss: 7.4640, Time: 3.80s, Token/s: 134.75
Epoch: 0, Step: 1870, Batch(micro): 1870, Batch (considering grad accum): 233,  Loss: 7.7963, Time: 3.62s, Token/s: 141.37
Epoch: 0, Step: 1871, Batch(micro): 1871, Batch (considering grad accum): 233,  Loss: 8.3979, Time: 25.06s, Token/s: 20.43
Epoch: 0, Step: 1872, Batch(micro): 1872, Batch (considering grad accum): 234,  Loss: 7.3127, Time: 6.63s, Token/s: 77.19
Epoch: 0, Step: 1873, Batch(micro): 1873, Batch (considering grad accum): 234,  Loss: 7.9315, Time: 3.76s, Token/s: 136.35
Epoch: 0, Step: 1874, Batch(micro): 1874, Batch (considering grad accum): 234,  Loss: 7.8298, Time: 3.21s, Token/s: 159.68
Epoch: 0, Step: 1875, Batch(micro): 1875, Batch (considering grad accum): 234,  Loss: 7.7477, Time: 3.23s, Token/s: 158.52
Epoch: 0, Step: 1876, Batch(micro): 1876, Batch (considering grad accum): 234,  Loss: 7.2745, Time: 3.59s, Token/s: 142.57
Epoch: 0, Step: 1877, Batch(micro): 1877, Batch (considering grad accum): 234,  Loss: 7.4940, Time: 3.56s, Token/s: 143.64
Epoch: 0, Step: 1878, Batch(micro): 1878, Batch (considering grad accum): 234,  Loss: 7.0075, Time: 3.27s, Token/s: 156.76
Epoch: 0, Step: 1879, Batch(micro): 1879, Batch (considering grad accum): 234,  Loss: 7.4958, Time: 25.14s, Token/s: 20.37
Epoch: 0, Step: 1880, Batch(micro): 1880, Batch (considering grad accum): 235,  Loss: 7.3709, Time: 7.40s, Token/s: 69.23
Epoch: 0, Step: 1881, Batch(micro): 1881, Batch (considering grad accum): 235,  Loss: 7.2765, Time: 4.48s, Token/s: 114.16
Epoch: 0, Step: 1882, Batch(micro): 1882, Batch (considering grad accum): 235,  Loss: 7.5092, Time: 3.90s, Token/s: 131.19
Epoch: 0, Step: 1883, Batch(micro): 1883, Batch (considering grad accum): 235,  Loss: 6.9974, Time: 3.55s, Token/s: 144.16
Epoch: 0, Step: 1884, Batch(micro): 1884, Batch (considering grad accum): 235,  Loss: 7.1269, Time: 3.96s, Token/s: 129.18
Epoch: 0, Step: 1885, Batch(micro): 1885, Batch (considering grad accum): 235,  Loss: 6.9166, Time: 3.71s, Token/s: 138.10
Epoch: 0, Step: 1886, Batch(micro): 1886, Batch (considering grad accum): 235,  Loss: 7.2968, Time: 3.47s, Token/s: 147.65
Epoch: 0, Step: 1887, Batch(micro): 1887, Batch (considering grad accum): 235,  Loss: 7.2711, Time: 25.61s, Token/s: 19.99
Epoch: 0, Step: 1888, Batch(micro): 1888, Batch (considering grad accum): 236,  Loss: 8.2085, Time: 7.84s, Token/s: 65.33
Epoch: 0, Step: 1889, Batch(micro): 1889, Batch (considering grad accum): 236,  Loss: 7.5295, Time: 3.93s, Token/s: 130.12
Epoch: 0, Step: 1890, Batch(micro): 1890, Batch (considering grad accum): 236,  Loss: 7.3624, Time: 3.44s, Token/s: 148.68
Epoch: 0, Step: 1891, Batch(micro): 1891, Batch (considering grad accum): 236,  Loss: 7.4338, Time: 3.99s, Token/s: 128.45
Epoch: 0, Step: 1892, Batch(micro): 1892, Batch (considering grad accum): 236,  Loss: 7.6093, Time: 3.55s, Token/s: 144.11
Epoch: 0, Step: 1893, Batch(micro): 1893, Batch (considering grad accum): 236,  Loss: 7.2517, Time: 3.24s, Token/s: 158.19
Epoch: 0, Step: 1894, Batch(micro): 1894, Batch (considering grad accum): 236,  Loss: 7.1143, Time: 3.15s, Token/s: 162.43
Epoch: 0, Step: 1895, Batch(micro): 1895, Batch (considering grad accum): 236,  Loss: 8.0144, Time: 20.15s, Token/s: 25.41
Epoch: 0, Step: 1896, Batch(micro): 1896, Batch (considering grad accum): 237,  Loss: 6.8609, Time: 6.41s, Token/s: 79.84
Epoch: 0, Step: 1897, Batch(micro): 1897, Batch (considering grad accum): 237,  Loss: 7.2539, Time: 3.65s, Token/s: 140.16
Epoch: 0, Step: 1898, Batch(micro): 1898, Batch (considering grad accum): 237,  Loss: 7.3641, Time: 3.64s, Token/s: 140.81
Epoch: 0, Step: 1899, Batch(micro): 1899, Batch (considering grad accum): 237,  Loss: 7.4640, Time: 3.56s, Token/s: 143.83
Updating MLP bias
Epoch: 0, Step: 1900, Batch(micro): 1900, Batch (considering grad accum): 237,  Loss: 7.5096, Time: 3.53s, Token/s: 145.08
Epoch: 0, Step: 1901, Batch(micro): 1901, Batch (considering grad accum): 237,  Loss: 7.3379, Time: 3.40s, Token/s: 150.57
Epoch: 0, Step: 1902, Batch(micro): 1902, Batch (considering grad accum): 237,  Loss: 7.3480, Time: 3.55s, Token/s: 144.33
Epoch: 0, Step: 1903, Batch(micro): 1903, Batch (considering grad accum): 237,  Loss: 7.2744, Time: 22.70s, Token/s: 22.56
Epoch: 0, Step: 1904, Batch(micro): 1904, Batch (considering grad accum): 238,  Loss: 7.2625, Time: 7.24s, Token/s: 70.70
Epoch: 0, Step: 1905, Batch(micro): 1905, Batch (considering grad accum): 238,  Loss: 7.4166, Time: 3.84s, Token/s: 133.49
Epoch: 0, Step: 1906, Batch(micro): 1906, Batch (considering grad accum): 238,  Loss: 7.1992, Time: 4.43s, Token/s: 115.60
Epoch: 0, Step: 1907, Batch(micro): 1907, Batch (considering grad accum): 238,  Loss: 7.7759, Time: 3.56s, Token/s: 143.85
Epoch: 0, Step: 1908, Batch(micro): 1908, Batch (considering grad accum): 238,  Loss: 7.6167, Time: 3.41s, Token/s: 150.34
Epoch: 0, Step: 1909, Batch(micro): 1909, Batch (considering grad accum): 238,  Loss: 7.3198, Time: 3.86s, Token/s: 132.52
Epoch: 0, Step: 1910, Batch(micro): 1910, Batch (considering grad accum): 238,  Loss: 7.2660, Time: 3.68s, Token/s: 139.00
Epoch: 0, Step: 1911, Batch(micro): 1911, Batch (considering grad accum): 238,  Loss: 7.1946, Time: 22.51s, Token/s: 22.74
Epoch: 0, Step: 1912, Batch(micro): 1912, Batch (considering grad accum): 239,  Loss: 7.5168, Time: 7.25s, Token/s: 70.65
Epoch: 0, Step: 1913, Batch(micro): 1913, Batch (considering grad accum): 239,  Loss: 7.5686, Time: 4.10s, Token/s: 124.85
Epoch: 0, Step: 1914, Batch(micro): 1914, Batch (considering grad accum): 239,  Loss: 7.4198, Time: 3.55s, Token/s: 144.15
Epoch: 0, Step: 1915, Batch(micro): 1915, Batch (considering grad accum): 239,  Loss: 8.0159, Time: 3.64s, Token/s: 140.83
Epoch: 0, Step: 1916, Batch(micro): 1916, Batch (considering grad accum): 239,  Loss: 8.5262, Time: 3.52s, Token/s: 145.30
Epoch: 0, Step: 1917, Batch(micro): 1917, Batch (considering grad accum): 239,  Loss: 7.4419, Time: 3.55s, Token/s: 144.04
Epoch: 0, Step: 1918, Batch(micro): 1918, Batch (considering grad accum): 239,  Loss: 7.8980, Time: 3.17s, Token/s: 161.45
Epoch: 0, Step: 1919, Batch(micro): 1919, Batch (considering grad accum): 239,  Loss: 8.2336, Time: 25.04s, Token/s: 20.44
Epoch: 0, Step: 1920, Batch(micro): 1920, Batch (considering grad accum): 240,  Loss: 7.4592, Time: 8.22s, Token/s: 62.27
Epoch: 0, Step: 1921, Batch(micro): 1921, Batch (considering grad accum): 240,  Loss: 8.1478, Time: 3.70s, Token/s: 138.46
Epoch: 0, Step: 1922, Batch(micro): 1922, Batch (considering grad accum): 240,  Loss: 7.1214, Time: 3.24s, Token/s: 158.06
Epoch: 0, Step: 1923, Batch(micro): 1923, Batch (considering grad accum): 240,  Loss: 7.4563, Time: 3.37s, Token/s: 152.07
Epoch: 0, Step: 1924, Batch(micro): 1924, Batch (considering grad accum): 240,  Loss: 7.4369, Time: 3.47s, Token/s: 147.70
Epoch: 0, Step: 1925, Batch(micro): 1925, Batch (considering grad accum): 240,  Loss: 7.6315, Time: 3.56s, Token/s: 143.84
Epoch: 0, Step: 1926, Batch(micro): 1926, Batch (considering grad accum): 240,  Loss: 7.0753, Time: 3.49s, Token/s: 146.86
Epoch: 0, Step: 1927, Batch(micro): 1927, Batch (considering grad accum): 240,  Loss: 7.6980, Time: 23.54s, Token/s: 21.75
Epoch: 0, Step: 1928, Batch(micro): 1928, Batch (considering grad accum): 241,  Loss: 8.1939, Time: 6.53s, Token/s: 78.41
Epoch: 0, Step: 1929, Batch(micro): 1929, Batch (considering grad accum): 241,  Loss: 7.7598, Time: 3.73s, Token/s: 137.36
Epoch: 0, Step: 1930, Batch(micro): 1930, Batch (considering grad accum): 241,  Loss: 7.4714, Time: 3.44s, Token/s: 148.79
Epoch: 0, Step: 1931, Batch(micro): 1931, Batch (considering grad accum): 241,  Loss: 7.2951, Time: 3.43s, Token/s: 149.13
Epoch: 0, Step: 1932, Batch(micro): 1932, Batch (considering grad accum): 241,  Loss: 7.4854, Time: 3.88s, Token/s: 131.88
Epoch: 0, Step: 1933, Batch(micro): 1933, Batch (considering grad accum): 241,  Loss: 7.9524, Time: 3.23s, Token/s: 158.43
Epoch: 0, Step: 1934, Batch(micro): 1934, Batch (considering grad accum): 241,  Loss: 7.0909, Time: 3.31s, Token/s: 154.49
Epoch: 0, Step: 1935, Batch(micro): 1935, Batch (considering grad accum): 241,  Loss: 7.2972, Time: 22.78s, Token/s: 22.47
Epoch: 0, Step: 1936, Batch(micro): 1936, Batch (considering grad accum): 242,  Loss: 7.8994, Time: 7.79s, Token/s: 65.75
Epoch: 0, Step: 1937, Batch(micro): 1937, Batch (considering grad accum): 242,  Loss: 7.8166, Time: 3.97s, Token/s: 129.00
Epoch: 0, Step: 1938, Batch(micro): 1938, Batch (considering grad accum): 242,  Loss: 7.3749, Time: 3.69s, Token/s: 138.64
Epoch: 0, Step: 1939, Batch(micro): 1939, Batch (considering grad accum): 242,  Loss: 7.3359, Time: 3.49s, Token/s: 146.85
Epoch: 0, Step: 1940, Batch(micro): 1940, Batch (considering grad accum): 242,  Loss: 7.5134, Time: 3.63s, Token/s: 140.86
Epoch: 0, Step: 1941, Batch(micro): 1941, Batch (considering grad accum): 242,  Loss: 7.1773, Time: 3.45s, Token/s: 148.39
Epoch: 0, Step: 1942, Batch(micro): 1942, Batch (considering grad accum): 242,  Loss: 7.2539, Time: 3.51s, Token/s: 145.88
Epoch: 0, Step: 1943, Batch(micro): 1943, Batch (considering grad accum): 242,  Loss: 7.3220, Time: 26.33s, Token/s: 19.45
Epoch: 0, Step: 1944, Batch(micro): 1944, Batch (considering grad accum): 243,  Loss: 7.3015, Time: 6.87s, Token/s: 74.56
Epoch: 0, Step: 1945, Batch(micro): 1945, Batch (considering grad accum): 243,  Loss: 7.6363, Time: 4.09s, Token/s: 125.08
Epoch: 0, Step: 1946, Batch(micro): 1946, Batch (considering grad accum): 243,  Loss: 7.3900, Time: 3.57s, Token/s: 143.61
Epoch: 0, Step: 1947, Batch(micro): 1947, Batch (considering grad accum): 243,  Loss: 7.4816, Time: 3.98s, Token/s: 128.50
Epoch: 0, Step: 1948, Batch(micro): 1948, Batch (considering grad accum): 243,  Loss: 8.5003, Time: 3.34s, Token/s: 153.18
Epoch: 0, Step: 1949, Batch(micro): 1949, Batch (considering grad accum): 243,  Loss: 7.7678, Time: 3.43s, Token/s: 149.23
Epoch: 0, Step: 1950, Batch(micro): 1950, Batch (considering grad accum): 243,  Loss: 7.3765, Time: 3.59s, Token/s: 142.71
Epoch: 0, Step: 1951, Batch(micro): 1951, Batch (considering grad accum): 243,  Loss: 7.5479, Time: 23.91s, Token/s: 21.42
Epoch: 0, Step: 1952, Batch(micro): 1952, Batch (considering grad accum): 244,  Loss: 7.3711, Time: 8.33s, Token/s: 61.49
Epoch: 0, Step: 1953, Batch(micro): 1953, Batch (considering grad accum): 244,  Loss: 7.0879, Time: 4.15s, Token/s: 123.28
Epoch: 0, Step: 1954, Batch(micro): 1954, Batch (considering grad accum): 244,  Loss: 6.9655, Time: 3.70s, Token/s: 138.34
Epoch: 0, Step: 1955, Batch(micro): 1955, Batch (considering grad accum): 244,  Loss: 7.2353, Time: 4.12s, Token/s: 124.23
Epoch: 0, Step: 1956, Batch(micro): 1956, Batch (considering grad accum): 244,  Loss: 7.2892, Time: 3.58s, Token/s: 143.15
Epoch: 0, Step: 1957, Batch(micro): 1957, Batch (considering grad accum): 244,  Loss: 7.1361, Time: 3.43s, Token/s: 149.43
Epoch: 0, Step: 1958, Batch(micro): 1958, Batch (considering grad accum): 244,  Loss: 7.7960, Time: 3.62s, Token/s: 141.28
Epoch: 0, Step: 1959, Batch(micro): 1959, Batch (considering grad accum): 244,  Loss: 7.3529, Time: 23.33s, Token/s: 21.94
Epoch: 0, Step: 1960, Batch(micro): 1960, Batch (considering grad accum): 245,  Loss: 6.8892, Time: 9.96s, Token/s: 51.40
Epoch: 0, Step: 1961, Batch(micro): 1961, Batch (considering grad accum): 245,  Loss: 7.3295, Time: 4.23s, Token/s: 121.10
Epoch: 0, Step: 1962, Batch(micro): 1962, Batch (considering grad accum): 245,  Loss: 7.6707, Time: 3.54s, Token/s: 144.73
Epoch: 0, Step: 1963, Batch(micro): 1963, Batch (considering grad accum): 245,  Loss: 7.4491, Time: 3.69s, Token/s: 138.76
Epoch: 0, Step: 1964, Batch(micro): 1964, Batch (considering grad accum): 245,  Loss: 7.1443, Time: 3.63s, Token/s: 141.20
Epoch: 0, Step: 1965, Batch(micro): 1965, Batch (considering grad accum): 245,  Loss: 7.1726, Time: 3.39s, Token/s: 150.86
Epoch: 0, Step: 1966, Batch(micro): 1966, Batch (considering grad accum): 245,  Loss: 6.8753, Time: 3.49s, Token/s: 146.91
Epoch: 0, Step: 1967, Batch(micro): 1967, Batch (considering grad accum): 245,  Loss: 7.1416, Time: 22.90s, Token/s: 22.36
Epoch: 0, Step: 1968, Batch(micro): 1968, Batch (considering grad accum): 246,  Loss: 7.1243, Time: 7.73s, Token/s: 66.26
Epoch: 0, Step: 1969, Batch(micro): 1969, Batch (considering grad accum): 246,  Loss: 7.3480, Time: 4.07s, Token/s: 125.75
Epoch: 0, Step: 1970, Batch(micro): 1970, Batch (considering grad accum): 246,  Loss: 6.9797, Time: 3.36s, Token/s: 152.21
Epoch: 0, Step: 1971, Batch(micro): 1971, Batch (considering grad accum): 246,  Loss: 6.9381, Time: 3.81s, Token/s: 134.43
Epoch: 0, Step: 1972, Batch(micro): 1972, Batch (considering grad accum): 246,  Loss: 7.1253, Time: 3.63s, Token/s: 141.21
Epoch: 0, Step: 1973, Batch(micro): 1973, Batch (considering grad accum): 246,  Loss: 6.9920, Time: 3.46s, Token/s: 148.14
Epoch: 0, Step: 1974, Batch(micro): 1974, Batch (considering grad accum): 246,  Loss: 7.0310, Time: 3.41s, Token/s: 150.06
Epoch: 0, Step: 1975, Batch(micro): 1975, Batch (considering grad accum): 246,  Loss: 7.0938, Time: 23.46s, Token/s: 21.83
Epoch: 0, Step: 1976, Batch(micro): 1976, Batch (considering grad accum): 247,  Loss: 7.2890, Time: 9.40s, Token/s: 54.49
Epoch: 0, Step: 1977, Batch(micro): 1977, Batch (considering grad accum): 247,  Loss: 7.2565, Time: 4.23s, Token/s: 120.94
Epoch: 0, Step: 1978, Batch(micro): 1978, Batch (considering grad accum): 247,  Loss: 7.3075, Time: 3.54s, Token/s: 144.66
Epoch: 0, Step: 1979, Batch(micro): 1979, Batch (considering grad accum): 247,  Loss: 8.1269, Time: 3.48s, Token/s: 147.10
Epoch: 0, Step: 1980, Batch(micro): 1980, Batch (considering grad accum): 247,  Loss: 7.2542, Time: 3.42s, Token/s: 149.51
Epoch: 0, Step: 1981, Batch(micro): 1981, Batch (considering grad accum): 247,  Loss: 7.1236, Time: 3.47s, Token/s: 147.36
Epoch: 0, Step: 1982, Batch(micro): 1982, Batch (considering grad accum): 247,  Loss: 7.2929, Time: 3.35s, Token/s: 152.98
Epoch: 0, Step: 1983, Batch(micro): 1983, Batch (considering grad accum): 247,  Loss: 7.4155, Time: 24.58s, Token/s: 20.83
Epoch: 0, Step: 1984, Batch(micro): 1984, Batch (considering grad accum): 248,  Loss: 7.4917, Time: 7.84s, Token/s: 65.27
Epoch: 0, Step: 1985, Batch(micro): 1985, Batch (considering grad accum): 248,  Loss: 7.6974, Time: 4.03s, Token/s: 127.06
Epoch: 0, Step: 1986, Batch(micro): 1986, Batch (considering grad accum): 248,  Loss: 7.3237, Time: 3.67s, Token/s: 139.45
Epoch: 0, Step: 1987, Batch(micro): 1987, Batch (considering grad accum): 248,  Loss: 7.9378, Time: 3.46s, Token/s: 147.93
Epoch: 0, Step: 1988, Batch(micro): 1988, Batch (considering grad accum): 248,  Loss: 7.5591, Time: 3.23s, Token/s: 158.56
Epoch: 0, Step: 1989, Batch(micro): 1989, Batch (considering grad accum): 248,  Loss: 7.3461, Time: 3.65s, Token/s: 140.12
Epoch: 0, Step: 1990, Batch(micro): 1990, Batch (considering grad accum): 248,  Loss: 7.7158, Time: 3.36s, Token/s: 152.45
Epoch: 0, Step: 1991, Batch(micro): 1991, Batch (considering grad accum): 248,  Loss: 6.7807, Time: 18.99s, Token/s: 26.96
Epoch: 0, Step: 1992, Batch(micro): 1992, Batch (considering grad accum): 249,  Loss: 7.2347, Time: 7.89s, Token/s: 64.92
Epoch: 0, Step: 1993, Batch(micro): 1993, Batch (considering grad accum): 249,  Loss: 6.7432, Time: 3.78s, Token/s: 135.40
Epoch: 0, Step: 1994, Batch(micro): 1994, Batch (considering grad accum): 249,  Loss: 7.3583, Time: 3.56s, Token/s: 143.89
Epoch: 0, Step: 1995, Batch(micro): 1995, Batch (considering grad accum): 249,  Loss: 7.8308, Time: 3.65s, Token/s: 140.31
Epoch: 0, Step: 1996, Batch(micro): 1996, Batch (considering grad accum): 249,  Loss: 7.2012, Time: 3.49s, Token/s: 146.68
Epoch: 0, Step: 1997, Batch(micro): 1997, Batch (considering grad accum): 249,  Loss: 6.7395, Time: 3.32s, Token/s: 154.00
Epoch: 0, Step: 1998, Batch(micro): 1998, Batch (considering grad accum): 249,  Loss: 7.0062, Time: 3.58s, Token/s: 143.19
Epoch: 0, Step: 1999, Batch(micro): 1999, Batch (considering grad accum): 249,  Loss: 7.3228, Time: 18.42s, Token/s: 27.79
Updating MLP bias
Epoch: 0, Step: 2000, Batch(micro): 2000, Batch (considering grad accum): 250,  Loss: 7.1523, Time: 5.90s, Token/s: 86.78
Saved checkpoint at step 2000
What is Gravity?4 us-!
Section of the

Imagine you, and on diverse. To and the world to their the the and-Conclusion for a
Epoch: 0, Step: 2001, Batch(micro): 2001, Batch (considering grad accum): 250,  Loss: 7.7359, Time: 14.60s, Token/s: 35.08
Epoch: 0, Step: 2002, Batch(micro): 2002, Batch (considering grad accum): 250,  Loss: 7.5220, Time: 4.11s, Token/s: 124.44
Epoch: 0, Step: 2003, Batch(micro): 2003, Batch (considering grad accum): 250,  Loss: 7.5384, Time: 3.31s, Token/s: 154.51
Epoch: 0, Step: 2004, Batch(micro): 2004, Batch (considering grad accum): 250,  Loss: 7.4678, Time: 3.31s, Token/s: 154.89
Epoch: 0, Step: 2005, Batch(micro): 2005, Batch (considering grad accum): 250,  Loss: 7.4793, Time: 3.62s, Token/s: 141.30
Epoch: 0, Step: 2006, Batch(micro): 2006, Batch (considering grad accum): 250,  Loss: 7.1704, Time: 3.49s, Token/s: 146.85
Epoch: 0, Step: 2007, Batch(micro): 2007, Batch (considering grad accum): 250,  Loss: 7.5373, Time: 21.91s, Token/s: 23.37
Epoch: 0, Step: 2008, Batch(micro): 2008, Batch (considering grad accum): 251,  Loss: 7.6749, Time: 9.10s, Token/s: 56.29
Epoch: 0, Step: 2009, Batch(micro): 2009, Batch (considering grad accum): 251,  Loss: 8.1267, Time: 3.90s, Token/s: 131.22
Epoch: 0, Step: 2010, Batch(micro): 2010, Batch (considering grad accum): 251,  Loss: 7.4430, Time: 4.03s, Token/s: 126.97
Epoch: 0, Step: 2011, Batch(micro): 2011, Batch (considering grad accum): 251,  Loss: 7.2005, Time: 3.25s, Token/s: 157.75
Epoch: 0, Step: 2012, Batch(micro): 2012, Batch (considering grad accum): 251,  Loss: 7.1108, Time: 3.18s, Token/s: 160.96
Epoch: 0, Step: 2013, Batch(micro): 2013, Batch (considering grad accum): 251,  Loss: 7.0744, Time: 3.23s, Token/s: 158.58
Epoch: 0, Step: 2014, Batch(micro): 2014, Batch (considering grad accum): 251,  Loss: 7.8194, Time: 3.16s, Token/s: 161.92
Epoch: 0, Step: 2015, Batch(micro): 2015, Batch (considering grad accum): 251,  Loss: 6.9262, Time: 17.88s, Token/s: 28.64
Epoch: 0, Step: 2016, Batch(micro): 2016, Batch (considering grad accum): 252,  Loss: 6.9245, Time: 6.59s, Token/s: 77.75
Epoch: 0, Step: 2017, Batch(micro): 2017, Batch (considering grad accum): 252,  Loss: 6.9116, Time: 3.71s, Token/s: 137.84
Epoch: 0, Step: 2018, Batch(micro): 2018, Batch (considering grad accum): 252,  Loss: 7.0040, Time: 3.35s, Token/s: 152.70
Epoch: 0, Step: 2019, Batch(micro): 2019, Batch (considering grad accum): 252,  Loss: 8.2791, Time: 3.42s, Token/s: 149.74
Epoch: 0, Step: 2020, Batch(micro): 2020, Batch (considering grad accum): 252,  Loss: 7.1397, Time: 3.48s, Token/s: 147.02
Epoch: 0, Step: 2021, Batch(micro): 2021, Batch (considering grad accum): 252,  Loss: 7.0966, Time: 3.44s, Token/s: 148.75
Epoch: 0, Step: 2022, Batch(micro): 2022, Batch (considering grad accum): 252,  Loss: 7.4581, Time: 3.52s, Token/s: 145.35
Epoch: 0, Step: 2023, Batch(micro): 2023, Batch (considering grad accum): 252,  Loss: 7.3610, Time: 19.58s, Token/s: 26.15
Epoch: 0, Step: 2024, Batch(micro): 2024, Batch (considering grad accum): 253,  Loss: 6.9389, Time: 7.51s, Token/s: 68.19
Epoch: 0, Step: 2025, Batch(micro): 2025, Batch (considering grad accum): 253,  Loss: 6.5800, Time: 3.90s, Token/s: 131.13
Epoch: 0, Step: 2026, Batch(micro): 2026, Batch (considering grad accum): 253,  Loss: 7.1106, Time: 3.95s, Token/s: 129.69
Epoch: 0, Step: 2027, Batch(micro): 2027, Batch (considering grad accum): 253,  Loss: 7.3467, Time: 3.88s, Token/s: 131.92
Epoch: 0, Step: 2028, Batch(micro): 2028, Batch (considering grad accum): 253,  Loss: 7.3357, Time: 3.55s, Token/s: 144.09
Epoch: 0, Step: 2029, Batch(micro): 2029, Batch (considering grad accum): 253,  Loss: 6.8316, Time: 3.92s, Token/s: 130.64
Epoch: 0, Step: 2030, Batch(micro): 2030, Batch (considering grad accum): 253,  Loss: 7.0743, Time: 3.66s, Token/s: 139.97
Epoch: 0, Step: 2031, Batch(micro): 2031, Batch (considering grad accum): 253,  Loss: 7.4816, Time: 21.93s, Token/s: 23.34
Epoch: 0, Step: 2032, Batch(micro): 2032, Batch (considering grad accum): 254,  Loss: 7.2620, Time: 8.12s, Token/s: 63.05
Epoch: 0, Step: 2033, Batch(micro): 2033, Batch (considering grad accum): 254,  Loss: 6.9191, Time: 3.94s, Token/s: 129.87
Epoch: 0, Step: 2034, Batch(micro): 2034, Batch (considering grad accum): 254,  Loss: 7.0671, Time: 3.79s, Token/s: 134.93
Epoch: 0, Step: 2035, Batch(micro): 2035, Batch (considering grad accum): 254,  Loss: 6.7936, Time: 3.50s, Token/s: 146.30
Epoch: 0, Step: 2036, Batch(micro): 2036, Batch (considering grad accum): 254,  Loss: 7.4952, Time: 3.55s, Token/s: 144.32
Epoch: 0, Step: 2037, Batch(micro): 2037, Batch (considering grad accum): 254,  Loss: 7.1282, Time: 3.76s, Token/s: 136.25
Epoch: 0, Step: 2038, Batch(micro): 2038, Batch (considering grad accum): 254,  Loss: 6.3556, Time: 3.94s, Token/s: 129.91
Epoch: 0, Step: 2039, Batch(micro): 2039, Batch (considering grad accum): 254,  Loss: 7.2747, Time: 25.10s, Token/s: 20.40
Epoch: 0, Step: 2040, Batch(micro): 2040, Batch (considering grad accum): 255,  Loss: 8.1439, Time: 9.30s, Token/s: 55.08
Epoch: 0, Step: 2041, Batch(micro): 2041, Batch (considering grad accum): 255,  Loss: 7.9110, Time: 3.89s, Token/s: 131.55
Epoch: 0, Step: 2042, Batch(micro): 2042, Batch (considering grad accum): 255,  Loss: 7.5588, Time: 3.73s, Token/s: 137.18
Epoch: 0, Step: 2043, Batch(micro): 2043, Batch (considering grad accum): 255,  Loss: 7.2025, Time: 3.69s, Token/s: 138.65
Epoch: 0, Step: 2044, Batch(micro): 2044, Batch (considering grad accum): 255,  Loss: 7.3495, Time: 3.88s, Token/s: 132.11
Epoch: 0, Step: 2045, Batch(micro): 2045, Batch (considering grad accum): 255,  Loss: 7.0667, Time: 3.36s, Token/s: 152.44
Epoch: 0, Step: 2046, Batch(micro): 2046, Batch (considering grad accum): 255,  Loss: 6.9482, Time: 3.13s, Token/s: 163.37
Epoch: 0, Step: 2047, Batch(micro): 2047, Batch (considering grad accum): 255,  Loss: 6.8236, Time: 23.13s, Token/s: 22.13
Epoch: 0, Step: 2048, Batch(micro): 2048, Batch (considering grad accum): 256,  Loss: 6.9783, Time: 8.56s, Token/s: 59.83
Epoch: 0, Step: 2049, Batch(micro): 2049, Batch (considering grad accum): 256,  Loss: 7.2769, Time: 3.73s, Token/s: 137.17
Epoch: 0, Step: 2050, Batch(micro): 2050, Batch (considering grad accum): 256,  Loss: 7.1722, Time: 3.65s, Token/s: 140.42
Epoch: 0, Step: 2051, Batch(micro): 2051, Batch (considering grad accum): 256,  Loss: 7.5436, Time: 3.62s, Token/s: 141.50
Epoch: 0, Step: 2052, Batch(micro): 2052, Batch (considering grad accum): 256,  Loss: 7.3662, Time: 3.57s, Token/s: 143.39
Epoch: 0, Step: 2053, Batch(micro): 2053, Batch (considering grad accum): 256,  Loss: 7.0315, Time: 3.49s, Token/s: 146.73
Epoch: 0, Step: 2054, Batch(micro): 2054, Batch (considering grad accum): 256,  Loss: 6.5862, Time: 3.36s, Token/s: 152.42
Epoch: 0, Step: 2055, Batch(micro): 2055, Batch (considering grad accum): 256,  Loss: 7.0435, Time: 23.38s, Token/s: 21.89
Epoch: 0, Step: 2056, Batch(micro): 2056, Batch (considering grad accum): 257,  Loss: 7.5191, Time: 6.67s, Token/s: 76.78
Epoch: 0, Step: 2057, Batch(micro): 2057, Batch (considering grad accum): 257,  Loss: 7.2851, Time: 4.08s, Token/s: 125.64
Epoch: 0, Step: 2058, Batch(micro): 2058, Batch (considering grad accum): 257,  Loss: 7.0702, Time: 3.64s, Token/s: 140.60
Epoch: 0, Step: 2059, Batch(micro): 2059, Batch (considering grad accum): 257,  Loss: 7.2500, Time: 3.61s, Token/s: 141.80
Epoch: 0, Step: 2060, Batch(micro): 2060, Batch (considering grad accum): 257,  Loss: 7.3436, Time: 3.66s, Token/s: 139.71
Epoch: 0, Step: 2061, Batch(micro): 2061, Batch (considering grad accum): 257,  Loss: 7.6194, Time: 3.51s, Token/s: 145.70
Epoch: 0, Step: 2062, Batch(micro): 2062, Batch (considering grad accum): 257,  Loss: 7.0120, Time: 3.55s, Token/s: 144.13
Epoch: 0, Step: 2063, Batch(micro): 2063, Batch (considering grad accum): 257,  Loss: 7.0628, Time: 22.03s, Token/s: 23.24
Epoch: 0, Step: 2064, Batch(micro): 2064, Batch (considering grad accum): 258,  Loss: 7.6703, Time: 7.16s, Token/s: 71.52
Epoch: 0, Step: 2065, Batch(micro): 2065, Batch (considering grad accum): 258,  Loss: 7.3693, Time: 3.97s, Token/s: 129.05
Epoch: 0, Step: 2066, Batch(micro): 2066, Batch (considering grad accum): 258,  Loss: 7.3016, Time: 3.54s, Token/s: 144.71
Epoch: 0, Step: 2067, Batch(micro): 2067, Batch (considering grad accum): 258,  Loss: 7.0556, Time: 3.45s, Token/s: 148.40
Epoch: 0, Step: 2068, Batch(micro): 2068, Batch (considering grad accum): 258,  Loss: 7.6074, Time: 3.56s, Token/s: 143.76
Epoch: 0, Step: 2069, Batch(micro): 2069, Batch (considering grad accum): 258,  Loss: 7.7133, Time: 3.60s, Token/s: 142.08
Epoch: 0, Step: 2070, Batch(micro): 2070, Batch (considering grad accum): 258,  Loss: 7.3406, Time: 3.79s, Token/s: 135.22
Epoch: 0, Step: 2071, Batch(micro): 2071, Batch (considering grad accum): 258,  Loss: 6.9208, Time: 22.88s, Token/s: 22.38
Epoch: 0, Step: 2072, Batch(micro): 2072, Batch (considering grad accum): 259,  Loss: 7.1569, Time: 7.78s, Token/s: 65.83
Epoch: 0, Step: 2073, Batch(micro): 2073, Batch (considering grad accum): 259,  Loss: 7.4192, Time: 3.46s, Token/s: 147.95
Epoch: 0, Step: 2074, Batch(micro): 2074, Batch (considering grad accum): 259,  Loss: 6.8856, Time: 3.33s, Token/s: 153.79
Epoch: 0, Step: 2075, Batch(micro): 2075, Batch (considering grad accum): 259,  Loss: 7.1859, Time: 3.33s, Token/s: 153.69
Epoch: 0, Step: 2076, Batch(micro): 2076, Batch (considering grad accum): 259,  Loss: 7.7973, Time: 3.53s, Token/s: 145.24
Epoch: 0, Step: 2077, Batch(micro): 2077, Batch (considering grad accum): 259,  Loss: 7.4013, Time: 3.65s, Token/s: 140.30
Epoch: 0, Step: 2078, Batch(micro): 2078, Batch (considering grad accum): 259,  Loss: 7.6140, Time: 3.55s, Token/s: 144.15
Epoch: 0, Step: 2079, Batch(micro): 2079, Batch (considering grad accum): 259,  Loss: 7.5573, Time: 23.27s, Token/s: 22.00
Epoch: 0, Step: 2080, Batch(micro): 2080, Batch (considering grad accum): 260,  Loss: 7.4257, Time: 7.62s, Token/s: 67.23
Epoch: 0, Step: 2081, Batch(micro): 2081, Batch (considering grad accum): 260,  Loss: 7.4004, Time: 4.00s, Token/s: 127.91
Epoch: 0, Step: 2082, Batch(micro): 2082, Batch (considering grad accum): 260,  Loss: 7.3324, Time: 3.31s, Token/s: 154.56
Epoch: 0, Step: 2083, Batch(micro): 2083, Batch (considering grad accum): 260,  Loss: 7.2213, Time: 3.51s, Token/s: 145.67
Epoch: 0, Step: 2084, Batch(micro): 2084, Batch (considering grad accum): 260,  Loss: 6.7299, Time: 3.52s, Token/s: 145.51
Epoch: 0, Step: 2085, Batch(micro): 2085, Batch (considering grad accum): 260,  Loss: 6.4388, Time: 3.36s, Token/s: 152.41
Epoch: 0, Step: 2086, Batch(micro): 2086, Batch (considering grad accum): 260,  Loss: 7.1499, Time: 3.37s, Token/s: 151.76
Epoch: 0, Step: 2087, Batch(micro): 2087, Batch (considering grad accum): 260,  Loss: 7.7673, Time: 23.97s, Token/s: 21.36
Epoch: 0, Step: 2088, Batch(micro): 2088, Batch (considering grad accum): 261,  Loss: 7.1425, Time: 6.97s, Token/s: 73.45
Epoch: 0, Step: 2089, Batch(micro): 2089, Batch (considering grad accum): 261,  Loss: 7.4695, Time: 3.54s, Token/s: 144.82
Epoch: 0, Step: 2090, Batch(micro): 2090, Batch (considering grad accum): 261,  Loss: 7.1634, Time: 3.75s, Token/s: 136.53
Epoch: 0, Step: 2091, Batch(micro): 2091, Batch (considering grad accum): 261,  Loss: 6.8498, Time: 3.57s, Token/s: 143.26
Epoch: 0, Step: 2092, Batch(micro): 2092, Batch (considering grad accum): 261,  Loss: 6.7913, Time: 3.69s, Token/s: 138.60
Epoch: 0, Step: 2093, Batch(micro): 2093, Batch (considering grad accum): 261,  Loss: 7.4069, Time: 3.97s, Token/s: 128.87
Epoch: 0, Step: 2094, Batch(micro): 2094, Batch (considering grad accum): 261,  Loss: 7.7605, Time: 3.26s, Token/s: 157.01
Epoch: 0, Step: 2095, Batch(micro): 2095, Batch (considering grad accum): 261,  Loss: 7.3301, Time: 22.72s, Token/s: 22.53
Epoch: 0, Step: 2096, Batch(micro): 2096, Batch (considering grad accum): 262,  Loss: 7.6862, Time: 6.57s, Token/s: 77.95
Epoch: 0, Step: 2097, Batch(micro): 2097, Batch (considering grad accum): 262,  Loss: 7.0404, Time: 4.41s, Token/s: 116.21
Epoch: 0, Step: 2098, Batch(micro): 2098, Batch (considering grad accum): 262,  Loss: 7.8124, Time: 3.69s, Token/s: 138.94
Epoch: 0, Step: 2099, Batch(micro): 2099, Batch (considering grad accum): 262,  Loss: 6.8885, Time: 3.92s, Token/s: 130.45
Updating MLP bias
Epoch: 0, Step: 2100, Batch(micro): 2100, Batch (considering grad accum): 262,  Loss: 7.1036, Time: 3.47s, Token/s: 147.53
Epoch: 0, Step: 2101, Batch(micro): 2101, Batch (considering grad accum): 262,  Loss: 7.0562, Time: 3.49s, Token/s: 146.84
Epoch: 0, Step: 2102, Batch(micro): 2102, Batch (considering grad accum): 262,  Loss: 7.3388, Time: 3.64s, Token/s: 140.48
Epoch: 0, Step: 2103, Batch(micro): 2103, Batch (considering grad accum): 262,  Loss: 7.1758, Time: 23.36s, Token/s: 21.92
Epoch: 0, Step: 2104, Batch(micro): 2104, Batch (considering grad accum): 263,  Loss: 6.8846, Time: 7.07s, Token/s: 72.39
Epoch: 0, Step: 2105, Batch(micro): 2105, Batch (considering grad accum): 263,  Loss: 7.0086, Time: 3.90s, Token/s: 131.28
Epoch: 0, Step: 2106, Batch(micro): 2106, Batch (considering grad accum): 263,  Loss: 6.7721, Time: 3.46s, Token/s: 147.91
Epoch: 0, Step: 2107, Batch(micro): 2107, Batch (considering grad accum): 263,  Loss: 7.2768, Time: 3.43s, Token/s: 149.09
Epoch: 0, Step: 2108, Batch(micro): 2108, Batch (considering grad accum): 263,  Loss: 7.0322, Time: 3.30s, Token/s: 155.33
Epoch: 0, Step: 2109, Batch(micro): 2109, Batch (considering grad accum): 263,  Loss: 6.9915, Time: 3.95s, Token/s: 129.64
Epoch: 0, Step: 2110, Batch(micro): 2110, Batch (considering grad accum): 263,  Loss: 7.0819, Time: 3.25s, Token/s: 157.65
Epoch: 0, Step: 2111, Batch(micro): 2111, Batch (considering grad accum): 263,  Loss: 7.0676, Time: 24.91s, Token/s: 20.55
Epoch: 0, Step: 2112, Batch(micro): 2112, Batch (considering grad accum): 264,  Loss: 7.2025, Time: 9.27s, Token/s: 55.23
Epoch: 0, Step: 2113, Batch(micro): 2113, Batch (considering grad accum): 264,  Loss: 7.5329, Time: 4.01s, Token/s: 127.56
Epoch: 0, Step: 2114, Batch(micro): 2114, Batch (considering grad accum): 264,  Loss: 7.1911, Time: 4.54s, Token/s: 112.74
Epoch: 0, Step: 2115, Batch(micro): 2115, Batch (considering grad accum): 264,  Loss: 8.0770, Time: 3.27s, Token/s: 156.46
Epoch: 0, Step: 2116, Batch(micro): 2116, Batch (considering grad accum): 264,  Loss: 7.9148, Time: 3.48s, Token/s: 147.11
Epoch: 0, Step: 2117, Batch(micro): 2117, Batch (considering grad accum): 264,  Loss: 7.0773, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 2118, Batch(micro): 2118, Batch (considering grad accum): 264,  Loss: 7.4289, Time: 3.86s, Token/s: 132.48
Epoch: 0, Step: 2119, Batch(micro): 2119, Batch (considering grad accum): 264,  Loss: 7.0703, Time: 22.71s, Token/s: 22.55
Epoch: 0, Step: 2120, Batch(micro): 2120, Batch (considering grad accum): 265,  Loss: 7.0754, Time: 6.24s, Token/s: 82.10
Epoch: 0, Step: 2121, Batch(micro): 2121, Batch (considering grad accum): 265,  Loss: 6.3225, Time: 3.76s, Token/s: 136.21
Epoch: 0, Step: 2122, Batch(micro): 2122, Batch (considering grad accum): 265,  Loss: 6.9481, Time: 3.21s, Token/s: 159.59
Epoch: 0, Step: 2123, Batch(micro): 2123, Batch (considering grad accum): 265,  Loss: 6.7846, Time: 3.27s, Token/s: 156.61
Epoch: 0, Step: 2124, Batch(micro): 2124, Batch (considering grad accum): 265,  Loss: 7.2016, Time: 3.25s, Token/s: 157.76
Epoch: 0, Step: 2125, Batch(micro): 2125, Batch (considering grad accum): 265,  Loss: 7.6222, Time: 3.42s, Token/s: 149.67
Epoch: 0, Step: 2126, Batch(micro): 2126, Batch (considering grad accum): 265,  Loss: 6.8034, Time: 3.69s, Token/s: 138.90
Epoch: 0, Step: 2127, Batch(micro): 2127, Batch (considering grad accum): 265,  Loss: 6.5046, Time: 22.72s, Token/s: 22.53
Epoch: 0, Step: 2128, Batch(micro): 2128, Batch (considering grad accum): 266,  Loss: 7.2955, Time: 7.86s, Token/s: 65.18
Epoch: 0, Step: 2129, Batch(micro): 2129, Batch (considering grad accum): 266,  Loss: 6.9998, Time: 3.64s, Token/s: 140.80
Epoch: 0, Step: 2130, Batch(micro): 2130, Batch (considering grad accum): 266,  Loss: 7.5309, Time: 3.76s, Token/s: 136.06
Epoch: 0, Step: 2131, Batch(micro): 2131, Batch (considering grad accum): 266,  Loss: 6.9192, Time: 3.92s, Token/s: 130.57
Epoch: 0, Step: 2132, Batch(micro): 2132, Batch (considering grad accum): 266,  Loss: 7.1148, Time: 3.97s, Token/s: 129.08
Epoch: 0, Step: 2133, Batch(micro): 2133, Batch (considering grad accum): 266,  Loss: 6.8577, Time: 3.59s, Token/s: 142.66
Epoch: 0, Step: 2134, Batch(micro): 2134, Batch (considering grad accum): 266,  Loss: 7.2886, Time: 3.80s, Token/s: 134.79
Epoch: 0, Step: 2135, Batch(micro): 2135, Batch (considering grad accum): 266,  Loss: 6.6995, Time: 22.80s, Token/s: 22.46
Epoch: 0, Step: 2136, Batch(micro): 2136, Batch (considering grad accum): 267,  Loss: 7.0876, Time: 6.01s, Token/s: 85.22
Epoch: 0, Step: 2137, Batch(micro): 2137, Batch (considering grad accum): 267,  Loss: 7.7469, Time: 4.05s, Token/s: 126.43
Epoch: 0, Step: 2138, Batch(micro): 2138, Batch (considering grad accum): 267,  Loss: 6.8937, Time: 3.72s, Token/s: 137.54
Epoch: 0, Step: 2139, Batch(micro): 2139, Batch (considering grad accum): 267,  Loss: 7.2279, Time: 3.42s, Token/s: 149.80
Epoch: 0, Step: 2140, Batch(micro): 2140, Batch (considering grad accum): 267,  Loss: 7.1121, Time: 3.35s, Token/s: 152.65
Epoch: 0, Step: 2141, Batch(micro): 2141, Batch (considering grad accum): 267,  Loss: 6.9852, Time: 3.52s, Token/s: 145.38
Epoch: 0, Step: 2142, Batch(micro): 2142, Batch (considering grad accum): 267,  Loss: 7.1535, Time: 3.71s, Token/s: 137.87
Epoch: 0, Step: 2143, Batch(micro): 2143, Batch (considering grad accum): 267,  Loss: 7.1647, Time: 23.42s, Token/s: 21.86
Epoch: 0, Step: 2144, Batch(micro): 2144, Batch (considering grad accum): 268,  Loss: 6.8335, Time: 7.23s, Token/s: 70.84
Epoch: 0, Step: 2145, Batch(micro): 2145, Batch (considering grad accum): 268,  Loss: 7.4951, Time: 3.93s, Token/s: 130.25
Epoch: 0, Step: 2146, Batch(micro): 2146, Batch (considering grad accum): 268,  Loss: 7.3336, Time: 3.59s, Token/s: 142.69
Epoch: 0, Step: 2147, Batch(micro): 2147, Batch (considering grad accum): 268,  Loss: 7.3972, Time: 3.17s, Token/s: 161.76
Epoch: 0, Step: 2148, Batch(micro): 2148, Batch (considering grad accum): 268,  Loss: 7.2596, Time: 3.34s, Token/s: 153.39
Epoch: 0, Step: 2149, Batch(micro): 2149, Batch (considering grad accum): 268,  Loss: 7.4834, Time: 3.28s, Token/s: 156.03
Epoch: 0, Step: 2150, Batch(micro): 2150, Batch (considering grad accum): 268,  Loss: 6.7182, Time: 3.34s, Token/s: 153.09
Epoch: 0, Step: 2151, Batch(micro): 2151, Batch (considering grad accum): 268,  Loss: 7.0484, Time: 23.09s, Token/s: 22.17
Epoch: 0, Step: 2152, Batch(micro): 2152, Batch (considering grad accum): 269,  Loss: 6.8421, Time: 7.05s, Token/s: 72.61
Epoch: 0, Step: 2153, Batch(micro): 2153, Batch (considering grad accum): 269,  Loss: 6.9329, Time: 3.74s, Token/s: 137.01
Epoch: 0, Step: 2154, Batch(micro): 2154, Batch (considering grad accum): 269,  Loss: 7.6464, Time: 3.19s, Token/s: 160.31
Epoch: 0, Step: 2155, Batch(micro): 2155, Batch (considering grad accum): 269,  Loss: 7.4714, Time: 3.43s, Token/s: 149.31
Epoch: 0, Step: 2156, Batch(micro): 2156, Batch (considering grad accum): 269,  Loss: 7.1289, Time: 3.52s, Token/s: 145.34
Epoch: 0, Step: 2157, Batch(micro): 2157, Batch (considering grad accum): 269,  Loss: 7.5465, Time: 3.37s, Token/s: 151.82
Epoch: 0, Step: 2158, Batch(micro): 2158, Batch (considering grad accum): 269,  Loss: 7.3057, Time: 3.63s, Token/s: 140.99
Epoch: 0, Step: 2159, Batch(micro): 2159, Batch (considering grad accum): 269,  Loss: 7.5231, Time: 24.60s, Token/s: 20.81
Epoch: 0, Step: 2160, Batch(micro): 2160, Batch (considering grad accum): 270,  Loss: 6.9402, Time: 6.76s, Token/s: 75.73
Epoch: 0, Step: 2161, Batch(micro): 2161, Batch (considering grad accum): 270,  Loss: 7.0126, Time: 4.06s, Token/s: 126.19
Epoch: 0, Step: 2162, Batch(micro): 2162, Batch (considering grad accum): 270,  Loss: 6.8812, Time: 3.51s, Token/s: 146.05
Epoch: 0, Step: 2163, Batch(micro): 2163, Batch (considering grad accum): 270,  Loss: 6.7983, Time: 3.60s, Token/s: 142.24
Epoch: 0, Step: 2164, Batch(micro): 2164, Batch (considering grad accum): 270,  Loss: 7.1489, Time: 3.55s, Token/s: 144.17
Epoch: 0, Step: 2165, Batch(micro): 2165, Batch (considering grad accum): 270,  Loss: 8.1728, Time: 3.66s, Token/s: 139.83
Epoch: 0, Step: 2166, Batch(micro): 2166, Batch (considering grad accum): 270,  Loss: 7.9169, Time: 3.84s, Token/s: 133.16
Epoch: 0, Step: 2167, Batch(micro): 2167, Batch (considering grad accum): 270,  Loss: 7.9289, Time: 24.05s, Token/s: 21.29
Epoch: 0, Step: 2168, Batch(micro): 2168, Batch (considering grad accum): 271,  Loss: 7.2832, Time: 6.76s, Token/s: 75.72
Epoch: 0, Step: 2169, Batch(micro): 2169, Batch (considering grad accum): 271,  Loss: 6.9735, Time: 3.83s, Token/s: 133.63
Epoch: 0, Step: 2170, Batch(micro): 2170, Batch (considering grad accum): 271,  Loss: 7.4724, Time: 3.57s, Token/s: 143.37
Epoch: 0, Step: 2171, Batch(micro): 2171, Batch (considering grad accum): 271,  Loss: 7.1551, Time: 3.51s, Token/s: 146.05
Epoch: 0, Step: 2172, Batch(micro): 2172, Batch (considering grad accum): 271,  Loss: 7.1019, Time: 3.39s, Token/s: 151.17
Epoch: 0, Step: 2173, Batch(micro): 2173, Batch (considering grad accum): 271,  Loss: 7.1825, Time: 3.19s, Token/s: 160.55
Epoch: 0, Step: 2174, Batch(micro): 2174, Batch (considering grad accum): 271,  Loss: 7.4206, Time: 3.53s, Token/s: 145.12
Epoch: 0, Step: 2175, Batch(micro): 2175, Batch (considering grad accum): 271,  Loss: 6.6835, Time: 21.79s, Token/s: 23.50
Epoch: 0, Step: 2176, Batch(micro): 2176, Batch (considering grad accum): 272,  Loss: 6.9844, Time: 6.97s, Token/s: 73.44
Epoch: 0, Step: 2177, Batch(micro): 2177, Batch (considering grad accum): 272,  Loss: 7.1584, Time: 4.08s, Token/s: 125.47
Epoch: 0, Step: 2178, Batch(micro): 2178, Batch (considering grad accum): 272,  Loss: 6.7341, Time: 3.48s, Token/s: 147.22
Epoch: 0, Step: 2179, Batch(micro): 2179, Batch (considering grad accum): 272,  Loss: 7.2373, Time: 3.40s, Token/s: 150.56
Epoch: 0, Step: 2180, Batch(micro): 2180, Batch (considering grad accum): 272,  Loss: 7.7089, Time: 3.51s, Token/s: 146.05
Epoch: 0, Step: 2181, Batch(micro): 2181, Batch (considering grad accum): 272,  Loss: 7.3722, Time: 3.55s, Token/s: 144.16
Epoch: 0, Step: 2182, Batch(micro): 2182, Batch (considering grad accum): 272,  Loss: 6.8228, Time: 3.51s, Token/s: 145.70
Epoch: 0, Step: 2183, Batch(micro): 2183, Batch (considering grad accum): 272,  Loss: 6.8648, Time: 19.70s, Token/s: 25.99
Epoch: 0, Step: 2184, Batch(micro): 2184, Batch (considering grad accum): 273,  Loss: 7.3058, Time: 6.70s, Token/s: 76.44
Epoch: 0, Step: 2185, Batch(micro): 2185, Batch (considering grad accum): 273,  Loss: 7.6052, Time: 4.08s, Token/s: 125.61
Epoch: 0, Step: 2186, Batch(micro): 2186, Batch (considering grad accum): 273,  Loss: 7.2314, Time: 3.38s, Token/s: 151.56
Epoch: 0, Step: 2187, Batch(micro): 2187, Batch (considering grad accum): 273,  Loss: 6.9505, Time: 3.72s, Token/s: 137.77
Epoch: 0, Step: 2188, Batch(micro): 2188, Batch (considering grad accum): 273,  Loss: 6.6887, Time: 3.46s, Token/s: 148.16
Epoch: 0, Step: 2189, Batch(micro): 2189, Batch (considering grad accum): 273,  Loss: 6.8700, Time: 3.45s, Token/s: 148.55
Epoch: 0, Step: 2190, Batch(micro): 2190, Batch (considering grad accum): 273,  Loss: 6.8383, Time: 3.28s, Token/s: 156.10
Epoch: 0, Step: 2191, Batch(micro): 2191, Batch (considering grad accum): 273,  Loss: 7.3520, Time: 17.98s, Token/s: 28.48
Epoch: 0, Step: 2192, Batch(micro): 2192, Batch (considering grad accum): 274,  Loss: 7.2488, Time: 5.87s, Token/s: 87.16
Epoch: 0, Step: 2193, Batch(micro): 2193, Batch (considering grad accum): 274,  Loss: 6.8718, Time: 3.76s, Token/s: 136.09
Epoch: 0, Step: 2194, Batch(micro): 2194, Batch (considering grad accum): 274,  Loss: 7.0595, Time: 3.48s, Token/s: 146.92
Epoch: 0, Step: 2195, Batch(micro): 2195, Batch (considering grad accum): 274,  Loss: 7.0172, Time: 4.15s, Token/s: 123.25
Epoch: 0, Step: 2196, Batch(micro): 2196, Batch (considering grad accum): 274,  Loss: 7.0374, Time: 3.61s, Token/s: 141.73
Epoch: 0, Step: 2197, Batch(micro): 2197, Batch (considering grad accum): 274,  Loss: 7.4215, Time: 3.59s, Token/s: 142.51
Epoch: 0, Step: 2198, Batch(micro): 2198, Batch (considering grad accum): 274,  Loss: 7.1491, Time: 3.23s, Token/s: 158.61
Epoch: 0, Step: 2199, Batch(micro): 2199, Batch (considering grad accum): 274,  Loss: 7.2145, Time: 18.90s, Token/s: 27.09
Updating MLP bias
Epoch: 0, Step: 2200, Batch(micro): 2200, Batch (considering grad accum): 275,  Loss: 7.8899, Time: 7.02s, Token/s: 72.95
Epoch: 0, Step: 2201, Batch(micro): 2201, Batch (considering grad accum): 275,  Loss: 6.9752, Time: 3.86s, Token/s: 132.76
Epoch: 0, Step: 2202, Batch(micro): 2202, Batch (considering grad accum): 275,  Loss: 7.0931, Time: 3.67s, Token/s: 139.34
Epoch: 0, Step: 2203, Batch(micro): 2203, Batch (considering grad accum): 275,  Loss: 6.8778, Time: 3.66s, Token/s: 139.87
Epoch: 0, Step: 2204, Batch(micro): 2204, Batch (considering grad accum): 275,  Loss: 7.0282, Time: 3.42s, Token/s: 149.57
Epoch: 0, Step: 2205, Batch(micro): 2205, Batch (considering grad accum): 275,  Loss: 7.3419, Time: 3.71s, Token/s: 138.15
Epoch: 0, Step: 2206, Batch(micro): 2206, Batch (considering grad accum): 275,  Loss: 6.7916, Time: 3.44s, Token/s: 148.73
Epoch: 0, Step: 2207, Batch(micro): 2207, Batch (considering grad accum): 275,  Loss: 6.9232, Time: 18.60s, Token/s: 27.52
Epoch: 0, Step: 2208, Batch(micro): 2208, Batch (considering grad accum): 276,  Loss: 7.2758, Time: 5.68s, Token/s: 90.17
Epoch: 0, Step: 2209, Batch(micro): 2209, Batch (considering grad accum): 276,  Loss: 7.1803, Time: 3.81s, Token/s: 134.25
Epoch: 0, Step: 2210, Batch(micro): 2210, Batch (considering grad accum): 276,  Loss: 7.4566, Time: 3.58s, Token/s: 143.14
Epoch: 0, Step: 2211, Batch(micro): 2211, Batch (considering grad accum): 276,  Loss: 6.9260, Time: 3.61s, Token/s: 141.89
Epoch: 0, Step: 2212, Batch(micro): 2212, Batch (considering grad accum): 276,  Loss: 6.7908, Time: 3.35s, Token/s: 152.81
Epoch: 0, Step: 2213, Batch(micro): 2213, Batch (considering grad accum): 276,  Loss: 6.4257, Time: 3.65s, Token/s: 140.45
Epoch: 0, Step: 2214, Batch(micro): 2214, Batch (considering grad accum): 276,  Loss: 7.0806, Time: 3.28s, Token/s: 156.05
Epoch: 0, Step: 2215, Batch(micro): 2215, Batch (considering grad accum): 276,  Loss: 6.6788, Time: 18.88s, Token/s: 27.12
Epoch: 0, Step: 2216, Batch(micro): 2216, Batch (considering grad accum): 277,  Loss: 6.8585, Time: 7.63s, Token/s: 67.07
Epoch: 0, Step: 2217, Batch(micro): 2217, Batch (considering grad accum): 277,  Loss: 7.1812, Time: 3.72s, Token/s: 137.71
Epoch: 0, Step: 2218, Batch(micro): 2218, Batch (considering grad accum): 277,  Loss: 6.4047, Time: 3.37s, Token/s: 151.95
Epoch: 0, Step: 2219, Batch(micro): 2219, Batch (considering grad accum): 277,  Loss: 7.1395, Time: 3.41s, Token/s: 150.04
Epoch: 0, Step: 2220, Batch(micro): 2220, Batch (considering grad accum): 277,  Loss: 6.4217, Time: 3.48s, Token/s: 147.13
Epoch: 0, Step: 2221, Batch(micro): 2221, Batch (considering grad accum): 277,  Loss: 6.8485, Time: 3.13s, Token/s: 163.33
Epoch: 0, Step: 2222, Batch(micro): 2222, Batch (considering grad accum): 277,  Loss: 7.2113, Time: 3.61s, Token/s: 141.86
Epoch: 0, Step: 2223, Batch(micro): 2223, Batch (considering grad accum): 277,  Loss: 7.4292, Time: 18.69s, Token/s: 27.39
Epoch: 0, Step: 2224, Batch(micro): 2224, Batch (considering grad accum): 278,  Loss: 7.3570, Time: 6.66s, Token/s: 76.84
Epoch: 0, Step: 2225, Batch(micro): 2225, Batch (considering grad accum): 278,  Loss: 7.9095, Time: 3.70s, Token/s: 138.50
Epoch: 0, Step: 2226, Batch(micro): 2226, Batch (considering grad accum): 278,  Loss: 6.7136, Time: 3.21s, Token/s: 159.69
Epoch: 0, Step: 2227, Batch(micro): 2227, Batch (considering grad accum): 278,  Loss: 6.9720, Time: 3.55s, Token/s: 144.38
Epoch: 0, Step: 2228, Batch(micro): 2228, Batch (considering grad accum): 278,  Loss: 7.6312, Time: 3.51s, Token/s: 145.91
Epoch: 0, Step: 2229, Batch(micro): 2229, Batch (considering grad accum): 278,  Loss: 7.8120, Time: 3.27s, Token/s: 156.66
Epoch: 0, Step: 2230, Batch(micro): 2230, Batch (considering grad accum): 278,  Loss: 7.2720, Time: 3.48s, Token/s: 147.29
Epoch: 0, Step: 2231, Batch(micro): 2231, Batch (considering grad accum): 278,  Loss: 6.5814, Time: 19.05s, Token/s: 26.88
Epoch: 0, Step: 2232, Batch(micro): 2232, Batch (considering grad accum): 279,  Loss: 6.7587, Time: 6.58s, Token/s: 77.86
Epoch: 0, Step: 2233, Batch(micro): 2233, Batch (considering grad accum): 279,  Loss: 7.1499, Time: 4.06s, Token/s: 126.24
Epoch: 0, Step: 2234, Batch(micro): 2234, Batch (considering grad accum): 279,  Loss: 7.2053, Time: 3.70s, Token/s: 138.54
Epoch: 0, Step: 2235, Batch(micro): 2235, Batch (considering grad accum): 279,  Loss: 8.0520, Time: 3.67s, Token/s: 139.42
Epoch: 0, Step: 2236, Batch(micro): 2236, Batch (considering grad accum): 279,  Loss: 7.0417, Time: 3.45s, Token/s: 148.37
Epoch: 0, Step: 2237, Batch(micro): 2237, Batch (considering grad accum): 279,  Loss: 7.4401, Time: 3.69s, Token/s: 138.93
Epoch: 0, Step: 2238, Batch(micro): 2238, Batch (considering grad accum): 279,  Loss: 6.9129, Time: 3.45s, Token/s: 148.62
Epoch: 0, Step: 2239, Batch(micro): 2239, Batch (considering grad accum): 279,  Loss: 7.3217, Time: 18.19s, Token/s: 28.15
Epoch: 0, Step: 2240, Batch(micro): 2240, Batch (considering grad accum): 280,  Loss: 7.6146, Time: 6.44s, Token/s: 79.46
Epoch: 0, Step: 2241, Batch(micro): 2241, Batch (considering grad accum): 280,  Loss: 6.5942, Time: 3.92s, Token/s: 130.69
Epoch: 0, Step: 2242, Batch(micro): 2242, Batch (considering grad accum): 280,  Loss: 6.8700, Time: 3.65s, Token/s: 140.14
Epoch: 0, Step: 2243, Batch(micro): 2243, Batch (considering grad accum): 280,  Loss: 7.5455, Time: 4.29s, Token/s: 119.42
Epoch: 0, Step: 2244, Batch(micro): 2244, Batch (considering grad accum): 280,  Loss: 7.6253, Time: 3.67s, Token/s: 139.63
Epoch: 0, Step: 2245, Batch(micro): 2245, Batch (considering grad accum): 280,  Loss: 6.7426, Time: 3.77s, Token/s: 135.99
Epoch: 0, Step: 2246, Batch(micro): 2246, Batch (considering grad accum): 280,  Loss: 6.9565, Time: 3.43s, Token/s: 149.40
Epoch: 0, Step: 2247, Batch(micro): 2247, Batch (considering grad accum): 280,  Loss: 6.9016, Time: 18.99s, Token/s: 26.97
Epoch: 0, Step: 2248, Batch(micro): 2248, Batch (considering grad accum): 281,  Loss: 6.4238, Time: 7.00s, Token/s: 73.17
Epoch: 0, Step: 2249, Batch(micro): 2249, Batch (considering grad accum): 281,  Loss: 6.8140, Time: 3.70s, Token/s: 138.53
Epoch: 0, Step: 2250, Batch(micro): 2250, Batch (considering grad accum): 281,  Loss: 7.3165, Time: 3.60s, Token/s: 142.24
Epoch: 0, Step: 2251, Batch(micro): 2251, Batch (considering grad accum): 281,  Loss: 7.3505, Time: 3.31s, Token/s: 154.91
Epoch: 0, Step: 2252, Batch(micro): 2252, Batch (considering grad accum): 281,  Loss: 7.4361, Time: 4.14s, Token/s: 123.61
Epoch: 0, Step: 2253, Batch(micro): 2253, Batch (considering grad accum): 281,  Loss: 6.9581, Time: 3.86s, Token/s: 132.81
Epoch: 0, Step: 2254, Batch(micro): 2254, Batch (considering grad accum): 281,  Loss: 7.4050, Time: 3.50s, Token/s: 146.34
Epoch: 0, Step: 2255, Batch(micro): 2255, Batch (considering grad accum): 281,  Loss: 6.9398, Time: 18.89s, Token/s: 27.10
Epoch: 0, Step: 2256, Batch(micro): 2256, Batch (considering grad accum): 282,  Loss: 7.0806, Time: 6.95s, Token/s: 73.67
Epoch: 0, Step: 2257, Batch(micro): 2257, Batch (considering grad accum): 282,  Loss: 7.2098, Time: 3.92s, Token/s: 130.55
Epoch: 0, Step: 2258, Batch(micro): 2258, Batch (considering grad accum): 282,  Loss: 6.9368, Time: 3.93s, Token/s: 130.20
Epoch: 0, Step: 2259, Batch(micro): 2259, Batch (considering grad accum): 282,  Loss: 7.2783, Time: 3.72s, Token/s: 137.52
Epoch: 0, Step: 2260, Batch(micro): 2260, Batch (considering grad accum): 282,  Loss: 7.6502, Time: 3.49s, Token/s: 146.84
Epoch: 0, Step: 2261, Batch(micro): 2261, Batch (considering grad accum): 282,  Loss: 7.6571, Time: 3.26s, Token/s: 157.08
Epoch: 0, Step: 2262, Batch(micro): 2262, Batch (considering grad accum): 282,  Loss: 7.0226, Time: 2.97s, Token/s: 172.28
Epoch: 0, Step: 2263, Batch(micro): 2263, Batch (considering grad accum): 282,  Loss: 7.3805, Time: 18.21s, Token/s: 28.11
Epoch: 0, Step: 2264, Batch(micro): 2264, Batch (considering grad accum): 283,  Loss: 6.7412, Time: 6.73s, Token/s: 76.05
Epoch: 0, Step: 2265, Batch(micro): 2265, Batch (considering grad accum): 283,  Loss: 6.7049, Time: 4.22s, Token/s: 121.19
Epoch: 0, Step: 2266, Batch(micro): 2266, Batch (considering grad accum): 283,  Loss: 7.1148, Time: 3.81s, Token/s: 134.26
Epoch: 0, Step: 2267, Batch(micro): 2267, Batch (considering grad accum): 283,  Loss: 7.1754, Time: 3.50s, Token/s: 146.36
Epoch: 0, Step: 2268, Batch(micro): 2268, Batch (considering grad accum): 283,  Loss: 6.6665, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 2269, Batch(micro): 2269, Batch (considering grad accum): 283,  Loss: 6.9566, Time: 3.39s, Token/s: 151.23
Epoch: 0, Step: 2270, Batch(micro): 2270, Batch (considering grad accum): 283,  Loss: 6.5698, Time: 3.39s, Token/s: 151.18
Epoch: 0, Step: 2271, Batch(micro): 2271, Batch (considering grad accum): 283,  Loss: 7.2618, Time: 18.56s, Token/s: 27.58
Epoch: 0, Step: 2272, Batch(micro): 2272, Batch (considering grad accum): 284,  Loss: 7.5427, Time: 6.17s, Token/s: 82.94
Epoch: 0, Step: 2273, Batch(micro): 2273, Batch (considering grad accum): 284,  Loss: 7.3840, Time: 4.18s, Token/s: 122.60
Epoch: 0, Step: 2274, Batch(micro): 2274, Batch (considering grad accum): 284,  Loss: 7.1141, Time: 3.31s, Token/s: 154.87
Epoch: 0, Step: 2275, Batch(micro): 2275, Batch (considering grad accum): 284,  Loss: 7.0611, Time: 3.72s, Token/s: 137.80
Epoch: 0, Step: 2276, Batch(micro): 2276, Batch (considering grad accum): 284,  Loss: 7.4769, Time: 3.47s, Token/s: 147.44
Epoch: 0, Step: 2277, Batch(micro): 2277, Batch (considering grad accum): 284,  Loss: 7.7148, Time: 3.44s, Token/s: 148.94
Epoch: 0, Step: 2278, Batch(micro): 2278, Batch (considering grad accum): 284,  Loss: 7.1331, Time: 3.22s, Token/s: 158.83
Epoch: 0, Step: 2279, Batch(micro): 2279, Batch (considering grad accum): 284,  Loss: 7.4095, Time: 18.53s, Token/s: 27.63
Epoch: 0, Step: 2280, Batch(micro): 2280, Batch (considering grad accum): 285,  Loss: 7.1355, Time: 7.02s, Token/s: 72.98
Epoch: 0, Step: 2281, Batch(micro): 2281, Batch (considering grad accum): 285,  Loss: 6.7296, Time: 3.96s, Token/s: 129.41
Epoch: 0, Step: 2282, Batch(micro): 2282, Batch (considering grad accum): 285,  Loss: 7.1165, Time: 3.33s, Token/s: 153.60
Epoch: 0, Step: 2283, Batch(micro): 2283, Batch (considering grad accum): 285,  Loss: 7.0716, Time: 3.85s, Token/s: 132.96
Epoch: 0, Step: 2284, Batch(micro): 2284, Batch (considering grad accum): 285,  Loss: 7.5957, Time: 3.45s, Token/s: 148.61
Epoch: 0, Step: 2285, Batch(micro): 2285, Batch (considering grad accum): 285,  Loss: 7.1173, Time: 3.45s, Token/s: 148.51
Epoch: 0, Step: 2286, Batch(micro): 2286, Batch (considering grad accum): 285,  Loss: 6.9454, Time: 3.30s, Token/s: 154.95
Epoch: 0, Step: 2287, Batch(micro): 2287, Batch (considering grad accum): 285,  Loss: 7.1076, Time: 22.78s, Token/s: 22.48
Epoch: 0, Step: 2288, Batch(micro): 2288, Batch (considering grad accum): 286,  Loss: 7.0624, Time: 6.77s, Token/s: 75.65
Epoch: 0, Step: 2289, Batch(micro): 2289, Batch (considering grad accum): 286,  Loss: 7.1537, Time: 3.88s, Token/s: 131.88
Epoch: 0, Step: 2290, Batch(micro): 2290, Batch (considering grad accum): 286,  Loss: 6.3177, Time: 3.50s, Token/s: 146.32
Epoch: 0, Step: 2291, Batch(micro): 2291, Batch (considering grad accum): 286,  Loss: 7.2058, Time: 3.68s, Token/s: 138.95
Epoch: 0, Step: 2292, Batch(micro): 2292, Batch (considering grad accum): 286,  Loss: 7.1872, Time: 3.57s, Token/s: 143.52
Epoch: 0, Step: 2293, Batch(micro): 2293, Batch (considering grad accum): 286,  Loss: 7.4583, Time: 3.23s, Token/s: 158.46
Epoch: 0, Step: 2294, Batch(micro): 2294, Batch (considering grad accum): 286,  Loss: 7.2425, Time: 3.63s, Token/s: 141.19
Epoch: 0, Step: 2295, Batch(micro): 2295, Batch (considering grad accum): 286,  Loss: 6.8870, Time: 22.93s, Token/s: 22.33
Epoch: 0, Step: 2296, Batch(micro): 2296, Batch (considering grad accum): 287,  Loss: 6.7164, Time: 7.49s, Token/s: 68.37
Epoch: 0, Step: 2297, Batch(micro): 2297, Batch (considering grad accum): 287,  Loss: 6.6878, Time: 4.20s, Token/s: 121.82
Epoch: 0, Step: 2298, Batch(micro): 2298, Batch (considering grad accum): 287,  Loss: 6.6807, Time: 3.75s, Token/s: 136.56
Epoch: 0, Step: 2299, Batch(micro): 2299, Batch (considering grad accum): 287,  Loss: 8.8642, Time: 3.62s, Token/s: 141.35
Updating MLP bias
Epoch: 0, Step: 2300, Batch(micro): 2300, Batch (considering grad accum): 287,  Loss: 7.5747, Time: 3.65s, Token/s: 140.29
Epoch: 0, Step: 2301, Batch(micro): 2301, Batch (considering grad accum): 287,  Loss: 6.6914, Time: 3.44s, Token/s: 148.81
Epoch: 0, Step: 2302, Batch(micro): 2302, Batch (considering grad accum): 287,  Loss: 6.6811, Time: 3.32s, Token/s: 154.28
Epoch: 0, Step: 2303, Batch(micro): 2303, Batch (considering grad accum): 287,  Loss: 7.1099, Time: 21.42s, Token/s: 23.91
Epoch: 0, Step: 2304, Batch(micro): 2304, Batch (considering grad accum): 288,  Loss: 6.5344, Time: 6.93s, Token/s: 73.91
Epoch: 0, Step: 2305, Batch(micro): 2305, Batch (considering grad accum): 288,  Loss: 7.4796, Time: 4.30s, Token/s: 118.97
Epoch: 0, Step: 2306, Batch(micro): 2306, Batch (considering grad accum): 288,  Loss: 7.6331, Time: 3.64s, Token/s: 140.84
Epoch: 0, Step: 2307, Batch(micro): 2307, Batch (considering grad accum): 288,  Loss: 7.5902, Time: 3.23s, Token/s: 158.68
Epoch: 0, Step: 2308, Batch(micro): 2308, Batch (considering grad accum): 288,  Loss: 7.1118, Time: 3.75s, Token/s: 136.48
Epoch: 0, Step: 2309, Batch(micro): 2309, Batch (considering grad accum): 288,  Loss: 7.3739, Time: 3.61s, Token/s: 141.84
Epoch: 0, Step: 2310, Batch(micro): 2310, Batch (considering grad accum): 288,  Loss: 7.2319, Time: 3.25s, Token/s: 157.44
Epoch: 0, Step: 2311, Batch(micro): 2311, Batch (considering grad accum): 288,  Loss: 7.4694, Time: 25.21s, Token/s: 20.31
Epoch: 0, Step: 2312, Batch(micro): 2312, Batch (considering grad accum): 289,  Loss: 6.2369, Time: 5.95s, Token/s: 86.07
Epoch: 0, Step: 2313, Batch(micro): 2313, Batch (considering grad accum): 289,  Loss: 6.9229, Time: 4.05s, Token/s: 126.41
Epoch: 0, Step: 2314, Batch(micro): 2314, Batch (considering grad accum): 289,  Loss: 6.4835, Time: 3.37s, Token/s: 151.71
Epoch: 0, Step: 2315, Batch(micro): 2315, Batch (considering grad accum): 289,  Loss: 7.2555, Time: 3.38s, Token/s: 151.42
Epoch: 0, Step: 2316, Batch(micro): 2316, Batch (considering grad accum): 289,  Loss: 6.9728, Time: 3.32s, Token/s: 154.36
Epoch: 0, Step: 2317, Batch(micro): 2317, Batch (considering grad accum): 289,  Loss: 7.2330, Time: 3.33s, Token/s: 153.88
Epoch: 0, Step: 2318, Batch(micro): 2318, Batch (considering grad accum): 289,  Loss: 7.3522, Time: 3.43s, Token/s: 149.35
Epoch: 0, Step: 2319, Batch(micro): 2319, Batch (considering grad accum): 289,  Loss: 6.9457, Time: 24.32s, Token/s: 21.06
Epoch: 0, Step: 2320, Batch(micro): 2320, Batch (considering grad accum): 290,  Loss: 6.7102, Time: 7.26s, Token/s: 70.54
Epoch: 0, Step: 2321, Batch(micro): 2321, Batch (considering grad accum): 290,  Loss: 7.4330, Time: 3.70s, Token/s: 138.54
Epoch: 0, Step: 2322, Batch(micro): 2322, Batch (considering grad accum): 290,  Loss: 7.4483, Time: 3.52s, Token/s: 145.63
Epoch: 0, Step: 2323, Batch(micro): 2323, Batch (considering grad accum): 290,  Loss: 8.3241, Time: 3.61s, Token/s: 141.89
Epoch: 0, Step: 2324, Batch(micro): 2324, Batch (considering grad accum): 290,  Loss: 7.7835, Time: 3.49s, Token/s: 146.50
Epoch: 0, Step: 2325, Batch(micro): 2325, Batch (considering grad accum): 290,  Loss: 7.2809, Time: 3.25s, Token/s: 157.34
Epoch: 0, Step: 2326, Batch(micro): 2326, Batch (considering grad accum): 290,  Loss: 7.2950, Time: 3.47s, Token/s: 147.60
Epoch: 0, Step: 2327, Batch(micro): 2327, Batch (considering grad accum): 290,  Loss: 7.3138, Time: 24.68s, Token/s: 20.74
Epoch: 0, Step: 2328, Batch(micro): 2328, Batch (considering grad accum): 291,  Loss: 7.3016, Time: 7.83s, Token/s: 65.40
Epoch: 0, Step: 2329, Batch(micro): 2329, Batch (considering grad accum): 291,  Loss: 8.0277, Time: 4.51s, Token/s: 113.50
Epoch: 0, Step: 2330, Batch(micro): 2330, Batch (considering grad accum): 291,  Loss: 8.0386, Time: 3.44s, Token/s: 148.90
Epoch: 0, Step: 2331, Batch(micro): 2331, Batch (considering grad accum): 291,  Loss: 6.6545, Time: 3.53s, Token/s: 145.00
Epoch: 0, Step: 2332, Batch(micro): 2332, Batch (considering grad accum): 291,  Loss: 7.1470, Time: 3.27s, Token/s: 156.59
Epoch: 0, Step: 2333, Batch(micro): 2333, Batch (considering grad accum): 291,  Loss: 7.1160, Time: 3.43s, Token/s: 149.17
Epoch: 0, Step: 2334, Batch(micro): 2334, Batch (considering grad accum): 291,  Loss: 6.7684, Time: 3.68s, Token/s: 139.02
Epoch: 0, Step: 2335, Batch(micro): 2335, Batch (considering grad accum): 291,  Loss: 7.0214, Time: 24.33s, Token/s: 21.05
Epoch: 0, Step: 2336, Batch(micro): 2336, Batch (considering grad accum): 292,  Loss: 7.7841, Time: 7.64s, Token/s: 67.02
Epoch: 0, Step: 2337, Batch(micro): 2337, Batch (considering grad accum): 292,  Loss: 7.6029, Time: 4.29s, Token/s: 119.48
Epoch: 0, Step: 2338, Batch(micro): 2338, Batch (considering grad accum): 292,  Loss: 6.6329, Time: 3.74s, Token/s: 136.90
Epoch: 0, Step: 2339, Batch(micro): 2339, Batch (considering grad accum): 292,  Loss: 7.2118, Time: 3.56s, Token/s: 143.66
Epoch: 0, Step: 2340, Batch(micro): 2340, Batch (considering grad accum): 292,  Loss: 7.4900, Time: 3.49s, Token/s: 146.72
Epoch: 0, Step: 2341, Batch(micro): 2341, Batch (considering grad accum): 292,  Loss: 7.4639, Time: 3.50s, Token/s: 146.28
Epoch: 0, Step: 2342, Batch(micro): 2342, Batch (considering grad accum): 292,  Loss: 7.4906, Time: 3.48s, Token/s: 147.16
Epoch: 0, Step: 2343, Batch(micro): 2343, Batch (considering grad accum): 292,  Loss: 7.6174, Time: 22.06s, Token/s: 23.21
Epoch: 0, Step: 2344, Batch(micro): 2344, Batch (considering grad accum): 293,  Loss: 7.0912, Time: 7.18s, Token/s: 71.35
Epoch: 0, Step: 2345, Batch(micro): 2345, Batch (considering grad accum): 293,  Loss: 7.0630, Time: 4.09s, Token/s: 125.18
Epoch: 0, Step: 2346, Batch(micro): 2346, Batch (considering grad accum): 293,  Loss: 6.7321, Time: 4.08s, Token/s: 125.39
Epoch: 0, Step: 2347, Batch(micro): 2347, Batch (considering grad accum): 293,  Loss: 7.0046, Time: 3.43s, Token/s: 149.47
Epoch: 0, Step: 2348, Batch(micro): 2348, Batch (considering grad accum): 293,  Loss: 7.0656, Time: 3.58s, Token/s: 142.82
Epoch: 0, Step: 2349, Batch(micro): 2349, Batch (considering grad accum): 293,  Loss: 6.6780, Time: 3.35s, Token/s: 152.82
Epoch: 0, Step: 2350, Batch(micro): 2350, Batch (considering grad accum): 293,  Loss: 7.0513, Time: 3.81s, Token/s: 134.42
Epoch: 0, Step: 2351, Batch(micro): 2351, Batch (considering grad accum): 293,  Loss: 7.2199, Time: 22.97s, Token/s: 22.29
Epoch: 0, Step: 2352, Batch(micro): 2352, Batch (considering grad accum): 294,  Loss: 6.8516, Time: 8.47s, Token/s: 60.44
Epoch: 0, Step: 2353, Batch(micro): 2353, Batch (considering grad accum): 294,  Loss: 6.8817, Time: 3.72s, Token/s: 137.61
Epoch: 0, Step: 2354, Batch(micro): 2354, Batch (considering grad accum): 294,  Loss: 7.8303, Time: 3.45s, Token/s: 148.26
Epoch: 0, Step: 2355, Batch(micro): 2355, Batch (considering grad accum): 294,  Loss: 8.5877, Time: 3.47s, Token/s: 147.54
Epoch: 0, Step: 2356, Batch(micro): 2356, Batch (considering grad accum): 294,  Loss: 7.6631, Time: 3.50s, Token/s: 146.20
Epoch: 0, Step: 2357, Batch(micro): 2357, Batch (considering grad accum): 294,  Loss: 6.9892, Time: 3.24s, Token/s: 158.13
Epoch: 0, Step: 2358, Batch(micro): 2358, Batch (considering grad accum): 294,  Loss: 7.2564, Time: 3.18s, Token/s: 161.16
Epoch: 0, Step: 2359, Batch(micro): 2359, Batch (considering grad accum): 294,  Loss: 6.4945, Time: 22.13s, Token/s: 23.14
Epoch: 0, Step: 2360, Batch(micro): 2360, Batch (considering grad accum): 295,  Loss: 6.6871, Time: 7.54s, Token/s: 67.94
Epoch: 0, Step: 2361, Batch(micro): 2361, Batch (considering grad accum): 295,  Loss: 6.9767, Time: 4.34s, Token/s: 117.87
Epoch: 0, Step: 2362, Batch(micro): 2362, Batch (considering grad accum): 295,  Loss: 7.1028, Time: 3.70s, Token/s: 138.37
Epoch: 0, Step: 2363, Batch(micro): 2363, Batch (considering grad accum): 295,  Loss: 7.9174, Time: 3.61s, Token/s: 141.78
Epoch: 0, Step: 2364, Batch(micro): 2364, Batch (considering grad accum): 295,  Loss: 6.7751, Time: 3.71s, Token/s: 138.01
Epoch: 0, Step: 2365, Batch(micro): 2365, Batch (considering grad accum): 295,  Loss: 7.1737, Time: 3.70s, Token/s: 138.21
Epoch: 0, Step: 2366, Batch(micro): 2366, Batch (considering grad accum): 295,  Loss: 7.0241, Time: 3.70s, Token/s: 138.45
Epoch: 0, Step: 2367, Batch(micro): 2367, Batch (considering grad accum): 295,  Loss: 7.5141, Time: 22.88s, Token/s: 22.38
Epoch: 0, Step: 2368, Batch(micro): 2368, Batch (considering grad accum): 296,  Loss: 6.9661, Time: 6.90s, Token/s: 74.23
Epoch: 0, Step: 2369, Batch(micro): 2369, Batch (considering grad accum): 296,  Loss: 7.4815, Time: 3.81s, Token/s: 134.53
Epoch: 0, Step: 2370, Batch(micro): 2370, Batch (considering grad accum): 296,  Loss: 6.8821, Time: 3.93s, Token/s: 130.42
Epoch: 0, Step: 2371, Batch(micro): 2371, Batch (considering grad accum): 296,  Loss: 6.6062, Time: 3.36s, Token/s: 152.20
Epoch: 0, Step: 2372, Batch(micro): 2372, Batch (considering grad accum): 296,  Loss: 7.2813, Time: 3.60s, Token/s: 142.16
Epoch: 0, Step: 2373, Batch(micro): 2373, Batch (considering grad accum): 296,  Loss: 6.8499, Time: 3.50s, Token/s: 146.38
Epoch: 0, Step: 2374, Batch(micro): 2374, Batch (considering grad accum): 296,  Loss: 6.7355, Time: 3.32s, Token/s: 154.13
Epoch: 0, Step: 2375, Batch(micro): 2375, Batch (considering grad accum): 296,  Loss: 7.2443, Time: 25.02s, Token/s: 20.47
Epoch: 0, Step: 2376, Batch(micro): 2376, Batch (considering grad accum): 297,  Loss: 7.9892, Time: 7.87s, Token/s: 65.08
Epoch: 0, Step: 2377, Batch(micro): 2377, Batch (considering grad accum): 297,  Loss: 7.7115, Time: 4.06s, Token/s: 125.96
Epoch: 0, Step: 2378, Batch(micro): 2378, Batch (considering grad accum): 297,  Loss: 7.2476, Time: 3.59s, Token/s: 142.61
Epoch: 0, Step: 2379, Batch(micro): 2379, Batch (considering grad accum): 297,  Loss: 6.2489, Time: 3.38s, Token/s: 151.60
Epoch: 0, Step: 2380, Batch(micro): 2380, Batch (considering grad accum): 297,  Loss: 6.7724, Time: 3.44s, Token/s: 148.87
Epoch: 0, Step: 2381, Batch(micro): 2381, Batch (considering grad accum): 297,  Loss: 6.7203, Time: 3.31s, Token/s: 154.73
Epoch: 0, Step: 2382, Batch(micro): 2382, Batch (considering grad accum): 297,  Loss: 7.1411, Time: 3.26s, Token/s: 156.89
Epoch: 0, Step: 2383, Batch(micro): 2383, Batch (considering grad accum): 297,  Loss: 6.4392, Time: 24.37s, Token/s: 21.01
Epoch: 0, Step: 2384, Batch(micro): 2384, Batch (considering grad accum): 298,  Loss: 6.8438, Time: 12.10s, Token/s: 42.33
Epoch: 0, Step: 2385, Batch(micro): 2385, Batch (considering grad accum): 298,  Loss: 7.0548, Time: 3.83s, Token/s: 133.68
Epoch: 0, Step: 2386, Batch(micro): 2386, Batch (considering grad accum): 298,  Loss: 7.1128, Time: 3.45s, Token/s: 148.31
Epoch: 0, Step: 2387, Batch(micro): 2387, Batch (considering grad accum): 298,  Loss: 7.9026, Time: 3.59s, Token/s: 142.47
Epoch: 0, Step: 2388, Batch(micro): 2388, Batch (considering grad accum): 298,  Loss: 6.2723, Time: 3.31s, Token/s: 154.86
Epoch: 0, Step: 2389, Batch(micro): 2389, Batch (considering grad accum): 298,  Loss: 6.9522, Time: 3.30s, Token/s: 155.29
Epoch: 0, Step: 2390, Batch(micro): 2390, Batch (considering grad accum): 298,  Loss: 7.0139, Time: 3.36s, Token/s: 152.33
Epoch: 0, Step: 2391, Batch(micro): 2391, Batch (considering grad accum): 298,  Loss: 7.2952, Time: 25.23s, Token/s: 20.30
Epoch: 0, Step: 2392, Batch(micro): 2392, Batch (considering grad accum): 299,  Loss: 6.4695, Time: 6.86s, Token/s: 74.68
Epoch: 0, Step: 2393, Batch(micro): 2393, Batch (considering grad accum): 299,  Loss: 6.6656, Time: 3.95s, Token/s: 129.66
Epoch: 0, Step: 2394, Batch(micro): 2394, Batch (considering grad accum): 299,  Loss: 6.5890, Time: 3.47s, Token/s: 147.69
Epoch: 0, Step: 2395, Batch(micro): 2395, Batch (considering grad accum): 299,  Loss: 7.4338, Time: 3.59s, Token/s: 142.58
Epoch: 0, Step: 2396, Batch(micro): 2396, Batch (considering grad accum): 299,  Loss: 7.0144, Time: 3.25s, Token/s: 157.66
Epoch: 0, Step: 2397, Batch(micro): 2397, Batch (considering grad accum): 299,  Loss: 7.1652, Time: 3.41s, Token/s: 150.16
Epoch: 0, Step: 2398, Batch(micro): 2398, Batch (considering grad accum): 299,  Loss: 7.4245, Time: 3.50s, Token/s: 146.38
Epoch: 0, Step: 2399, Batch(micro): 2399, Batch (considering grad accum): 299,  Loss: 7.0707, Time: 20.90s, Token/s: 24.50
Updating MLP bias
Epoch: 0, Step: 2400, Batch(micro): 2400, Batch (considering grad accum): 300,  Loss: 7.1303, Time: 6.01s, Token/s: 85.15
Epoch: 0, Step: 2401, Batch(micro): 2401, Batch (considering grad accum): 300,  Loss: 6.6299, Time: 4.26s, Token/s: 120.20
Epoch: 0, Step: 2402, Batch(micro): 2402, Batch (considering grad accum): 300,  Loss: 7.2197, Time: 3.51s, Token/s: 145.74
Epoch: 0, Step: 2403, Batch(micro): 2403, Batch (considering grad accum): 300,  Loss: 8.3233, Time: 3.74s, Token/s: 136.75
Epoch: 0, Step: 2404, Batch(micro): 2404, Batch (considering grad accum): 300,  Loss: 7.0549, Time: 3.24s, Token/s: 158.04
Epoch: 0, Step: 2405, Batch(micro): 2405, Batch (considering grad accum): 300,  Loss: 6.4565, Time: 3.79s, Token/s: 135.05
Epoch: 0, Step: 2406, Batch(micro): 2406, Batch (considering grad accum): 300,  Loss: 6.9524, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 2407, Batch(micro): 2407, Batch (considering grad accum): 300,  Loss: 6.8886, Time: 23.84s, Token/s: 21.48
Epoch: 0, Step: 2408, Batch(micro): 2408, Batch (considering grad accum): 301,  Loss: 7.1927, Time: 7.15s, Token/s: 71.60
Epoch: 0, Step: 2409, Batch(micro): 2409, Batch (considering grad accum): 301,  Loss: 7.4820, Time: 3.76s, Token/s: 136.23
Epoch: 0, Step: 2410, Batch(micro): 2410, Batch (considering grad accum): 301,  Loss: 6.9922, Time: 4.07s, Token/s: 125.95
Epoch: 0, Step: 2411, Batch(micro): 2411, Batch (considering grad accum): 301,  Loss: 6.8744, Time: 4.20s, Token/s: 121.90
Epoch: 0, Step: 2412, Batch(micro): 2412, Batch (considering grad accum): 301,  Loss: 6.7377, Time: 3.24s, Token/s: 158.21
Epoch: 0, Step: 2413, Batch(micro): 2413, Batch (considering grad accum): 301,  Loss: 7.2598, Time: 3.22s, Token/s: 159.16
Epoch: 0, Step: 2414, Batch(micro): 2414, Batch (considering grad accum): 301,  Loss: 7.3335, Time: 3.42s, Token/s: 149.80
Epoch: 0, Step: 2415, Batch(micro): 2415, Batch (considering grad accum): 301,  Loss: 6.9263, Time: 24.11s, Token/s: 21.23
Epoch: 0, Step: 2416, Batch(micro): 2416, Batch (considering grad accum): 302,  Loss: 6.6808, Time: 8.22s, Token/s: 62.31
Epoch: 0, Step: 2417, Batch(micro): 2417, Batch (considering grad accum): 302,  Loss: 6.5110, Time: 3.90s, Token/s: 131.23
Epoch: 0, Step: 2418, Batch(micro): 2418, Batch (considering grad accum): 302,  Loss: 6.9136, Time: 3.65s, Token/s: 140.21
Epoch: 0, Step: 2419, Batch(micro): 2419, Batch (considering grad accum): 302,  Loss: 7.6470, Time: 3.35s, Token/s: 152.97
Epoch: 0, Step: 2420, Batch(micro): 2420, Batch (considering grad accum): 302,  Loss: 7.8207, Time: 3.43s, Token/s: 149.41
Epoch: 0, Step: 2421, Batch(micro): 2421, Batch (considering grad accum): 302,  Loss: 6.6816, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 2422, Batch(micro): 2422, Batch (considering grad accum): 302,  Loss: 6.4591, Time: 4.04s, Token/s: 126.83
Epoch: 0, Step: 2423, Batch(micro): 2423, Batch (considering grad accum): 302,  Loss: 6.3862, Time: 23.20s, Token/s: 22.06
Epoch: 0, Step: 2424, Batch(micro): 2424, Batch (considering grad accum): 303,  Loss: 7.3539, Time: 8.45s, Token/s: 60.61
Epoch: 0, Step: 2425, Batch(micro): 2425, Batch (considering grad accum): 303,  Loss: 6.8574, Time: 4.85s, Token/s: 105.47
Epoch: 0, Step: 2426, Batch(micro): 2426, Batch (considering grad accum): 303,  Loss: 7.0312, Time: 3.29s, Token/s: 155.75
Epoch: 0, Step: 2427, Batch(micro): 2427, Batch (considering grad accum): 303,  Loss: 7.2486, Time: 3.26s, Token/s: 157.03
Epoch: 0, Step: 2428, Batch(micro): 2428, Batch (considering grad accum): 303,  Loss: 6.7604, Time: 3.23s, Token/s: 158.29
Epoch: 0, Step: 2429, Batch(micro): 2429, Batch (considering grad accum): 303,  Loss: 7.2498, Time: 5.23s, Token/s: 97.92
Epoch: 0, Step: 2430, Batch(micro): 2430, Batch (considering grad accum): 303,  Loss: 7.7179, Time: 3.72s, Token/s: 137.64
Epoch: 0, Step: 2431, Batch(micro): 2431, Batch (considering grad accum): 303,  Loss: 7.1031, Time: 25.35s, Token/s: 20.20
Epoch: 0, Step: 2432, Batch(micro): 2432, Batch (considering grad accum): 304,  Loss: 7.8483, Time: 9.03s, Token/s: 56.71
Epoch: 0, Step: 2433, Batch(micro): 2433, Batch (considering grad accum): 304,  Loss: 7.6907, Time: 4.15s, Token/s: 123.32
Epoch: 0, Step: 2434, Batch(micro): 2434, Batch (considering grad accum): 304,  Loss: 7.1928, Time: 3.66s, Token/s: 139.84
Epoch: 0, Step: 2435, Batch(micro): 2435, Batch (considering grad accum): 304,  Loss: 7.5013, Time: 3.52s, Token/s: 145.44
Epoch: 0, Step: 2436, Batch(micro): 2436, Batch (considering grad accum): 304,  Loss: 7.3307, Time: 3.40s, Token/s: 150.75
Epoch: 0, Step: 2437, Batch(micro): 2437, Batch (considering grad accum): 304,  Loss: 7.2038, Time: 3.29s, Token/s: 155.54
Epoch: 0, Step: 2438, Batch(micro): 2438, Batch (considering grad accum): 304,  Loss: 7.0486, Time: 3.31s, Token/s: 154.49
Epoch: 0, Step: 2439, Batch(micro): 2439, Batch (considering grad accum): 304,  Loss: 7.1542, Time: 22.40s, Token/s: 22.86
Epoch: 0, Step: 2440, Batch(micro): 2440, Batch (considering grad accum): 305,  Loss: 7.2374, Time: 8.41s, Token/s: 60.84
Epoch: 0, Step: 2441, Batch(micro): 2441, Batch (considering grad accum): 305,  Loss: 7.9515, Time: 3.79s, Token/s: 135.26
Epoch: 0, Step: 2442, Batch(micro): 2442, Batch (considering grad accum): 305,  Loss: 7.2699, Time: 3.66s, Token/s: 140.07
Epoch: 0, Step: 2443, Batch(micro): 2443, Batch (considering grad accum): 305,  Loss: 7.2409, Time: 3.59s, Token/s: 142.78
Epoch: 0, Step: 2444, Batch(micro): 2444, Batch (considering grad accum): 305,  Loss: 7.4405, Time: 3.46s, Token/s: 147.78
Epoch: 0, Step: 2445, Batch(micro): 2445, Batch (considering grad accum): 305,  Loss: 7.0322, Time: 3.61s, Token/s: 141.87
Epoch: 0, Step: 2446, Batch(micro): 2446, Batch (considering grad accum): 305,  Loss: 6.9423, Time: 3.40s, Token/s: 150.81
Epoch: 0, Step: 2447, Batch(micro): 2447, Batch (considering grad accum): 305,  Loss: 6.5626, Time: 24.82s, Token/s: 20.63
Epoch: 0, Step: 2448, Batch(micro): 2448, Batch (considering grad accum): 306,  Loss: 7.0274, Time: 7.09s, Token/s: 72.25
Epoch: 0, Step: 2449, Batch(micro): 2449, Batch (considering grad accum): 306,  Loss: 6.8102, Time: 3.91s, Token/s: 131.00
Epoch: 0, Step: 2450, Batch(micro): 2450, Batch (considering grad accum): 306,  Loss: 7.5640, Time: 3.48s, Token/s: 147.03
Epoch: 0, Step: 2451, Batch(micro): 2451, Batch (considering grad accum): 306,  Loss: 6.6511, Time: 3.53s, Token/s: 144.87
Epoch: 0, Step: 2452, Batch(micro): 2452, Batch (considering grad accum): 306,  Loss: 7.1085, Time: 3.49s, Token/s: 146.77
Epoch: 0, Step: 2453, Batch(micro): 2453, Batch (considering grad accum): 306,  Loss: 7.5104, Time: 3.31s, Token/s: 154.55
Epoch: 0, Step: 2454, Batch(micro): 2454, Batch (considering grad accum): 306,  Loss: 7.3591, Time: 3.27s, Token/s: 156.55
Epoch: 0, Step: 2455, Batch(micro): 2455, Batch (considering grad accum): 306,  Loss: 7.6769, Time: 22.32s, Token/s: 22.94
Epoch: 0, Step: 2456, Batch(micro): 2456, Batch (considering grad accum): 307,  Loss: 7.1300, Time: 7.11s, Token/s: 72.05
Epoch: 0, Step: 2457, Batch(micro): 2457, Batch (considering grad accum): 307,  Loss: 8.1124, Time: 3.76s, Token/s: 136.22
Epoch: 0, Step: 2458, Batch(micro): 2458, Batch (considering grad accum): 307,  Loss: 6.7865, Time: 3.54s, Token/s: 144.44
Epoch: 0, Step: 2459, Batch(micro): 2459, Batch (considering grad accum): 307,  Loss: 7.8999, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 2460, Batch(micro): 2460, Batch (considering grad accum): 307,  Loss: 7.1423, Time: 3.48s, Token/s: 147.08
Epoch: 0, Step: 2461, Batch(micro): 2461, Batch (considering grad accum): 307,  Loss: 7.0090, Time: 3.38s, Token/s: 151.49
Epoch: 0, Step: 2462, Batch(micro): 2462, Batch (considering grad accum): 307,  Loss: 7.5052, Time: 3.31s, Token/s: 154.58
Epoch: 0, Step: 2463, Batch(micro): 2463, Batch (considering grad accum): 307,  Loss: 7.1598, Time: 22.86s, Token/s: 22.40
Epoch: 0, Step: 2464, Batch(micro): 2464, Batch (considering grad accum): 308,  Loss: 6.8985, Time: 7.14s, Token/s: 71.67
Epoch: 0, Step: 2465, Batch(micro): 2465, Batch (considering grad accum): 308,  Loss: 7.2640, Time: 4.05s, Token/s: 126.30
Epoch: 0, Step: 2466, Batch(micro): 2466, Batch (considering grad accum): 308,  Loss: 6.9131, Time: 3.54s, Token/s: 144.44
Epoch: 0, Step: 2467, Batch(micro): 2467, Batch (considering grad accum): 308,  Loss: 6.8983, Time: 3.67s, Token/s: 139.68
Epoch: 0, Step: 2468, Batch(micro): 2468, Batch (considering grad accum): 308,  Loss: 7.0760, Time: 3.34s, Token/s: 153.45
Epoch: 0, Step: 2469, Batch(micro): 2469, Batch (considering grad accum): 308,  Loss: 7.4761, Time: 3.53s, Token/s: 145.15
Epoch: 0, Step: 2470, Batch(micro): 2470, Batch (considering grad accum): 308,  Loss: 6.9704, Time: 3.43s, Token/s: 149.25
Epoch: 0, Step: 2471, Batch(micro): 2471, Batch (considering grad accum): 308,  Loss: 7.3300, Time: 25.23s, Token/s: 20.29
Epoch: 0, Step: 2472, Batch(micro): 2472, Batch (considering grad accum): 309,  Loss: 8.1479, Time: 9.25s, Token/s: 55.33
Epoch: 0, Step: 2473, Batch(micro): 2473, Batch (considering grad accum): 309,  Loss: 6.9835, Time: 3.80s, Token/s: 134.56
Epoch: 0, Step: 2474, Batch(micro): 2474, Batch (considering grad accum): 309,  Loss: 7.1501, Time: 3.17s, Token/s: 161.33
Epoch: 0, Step: 2475, Batch(micro): 2475, Batch (considering grad accum): 309,  Loss: 7.1848, Time: 3.15s, Token/s: 162.74
Epoch: 0, Step: 2476, Batch(micro): 2476, Batch (considering grad accum): 309,  Loss: 7.2963, Time: 3.19s, Token/s: 160.39
Epoch: 0, Step: 2477, Batch(micro): 2477, Batch (considering grad accum): 309,  Loss: 6.4877, Time: 3.23s, Token/s: 158.49
Epoch: 0, Step: 2478, Batch(micro): 2478, Batch (considering grad accum): 309,  Loss: 7.1168, Time: 3.56s, Token/s: 143.89
Epoch: 0, Step: 2479, Batch(micro): 2479, Batch (considering grad accum): 309,  Loss: 7.7088, Time: 21.33s, Token/s: 24.01
Epoch: 0, Step: 2480, Batch(micro): 2480, Batch (considering grad accum): 310,  Loss: 7.4327, Time: 7.12s, Token/s: 71.95
Epoch: 0, Step: 2481, Batch(micro): 2481, Batch (considering grad accum): 310,  Loss: 7.1606, Time: 3.94s, Token/s: 129.92
Epoch: 0, Step: 2482, Batch(micro): 2482, Batch (considering grad accum): 310,  Loss: 7.0895, Time: 3.20s, Token/s: 159.98
Epoch: 0, Step: 2483, Batch(micro): 2483, Batch (considering grad accum): 310,  Loss: 7.2804, Time: 3.16s, Token/s: 161.81
Epoch: 0, Step: 2484, Batch(micro): 2484, Batch (considering grad accum): 310,  Loss: 6.7655, Time: 3.18s, Token/s: 160.96
Epoch: 0, Step: 2485, Batch(micro): 2485, Batch (considering grad accum): 310,  Loss: 7.5797, Time: 3.28s, Token/s: 155.91
Epoch: 0, Step: 2486, Batch(micro): 2486, Batch (considering grad accum): 310,  Loss: 6.6381, Time: 3.24s, Token/s: 158.13
Epoch: 0, Step: 2487, Batch(micro): 2487, Batch (considering grad accum): 310,  Loss: 7.4452, Time: 20.17s, Token/s: 25.38
Epoch: 0, Step: 2488, Batch(micro): 2488, Batch (considering grad accum): 311,  Loss: 7.9401, Time: 7.68s, Token/s: 66.68
Epoch: 0, Step: 2489, Batch(micro): 2489, Batch (considering grad accum): 311,  Loss: 6.6666, Time: 3.79s, Token/s: 135.04
Epoch: 0, Step: 2490, Batch(micro): 2490, Batch (considering grad accum): 311,  Loss: 7.2146, Time: 3.29s, Token/s: 155.55
Epoch: 0, Step: 2491, Batch(micro): 2491, Batch (considering grad accum): 311,  Loss: 7.2989, Time: 3.45s, Token/s: 148.31
Epoch: 0, Step: 2492, Batch(micro): 2492, Batch (considering grad accum): 311,  Loss: 6.6673, Time: 3.29s, Token/s: 155.46
Epoch: 0, Step: 2493, Batch(micro): 2493, Batch (considering grad accum): 311,  Loss: 6.5111, Time: 4.37s, Token/s: 117.15
Epoch: 0, Step: 2494, Batch(micro): 2494, Batch (considering grad accum): 311,  Loss: 7.0816, Time: 3.50s, Token/s: 146.49
Epoch: 0, Step: 2495, Batch(micro): 2495, Batch (considering grad accum): 311,  Loss: 7.0449, Time: 20.41s, Token/s: 25.09
Epoch: 0, Step: 2496, Batch(micro): 2496, Batch (considering grad accum): 312,  Loss: 6.5403, Time: 6.41s, Token/s: 79.88
Epoch: 0, Step: 2497, Batch(micro): 2497, Batch (considering grad accum): 312,  Loss: 7.1368, Time: 4.04s, Token/s: 126.62
Epoch: 0, Step: 2498, Batch(micro): 2498, Batch (considering grad accum): 312,  Loss: 6.8297, Time: 3.83s, Token/s: 133.75
Epoch: 0, Step: 2499, Batch(micro): 2499, Batch (considering grad accum): 312,  Loss: 6.3873, Time: 3.68s, Token/s: 139.06
Updating MLP bias
Epoch: 0, Step: 2500, Batch(micro): 2500, Batch (considering grad accum): 312,  Loss: 7.5379, Time: 3.37s, Token/s: 151.90
Epoch: 0, Step: 2501, Batch(micro): 2501, Batch (considering grad accum): 312,  Loss: 7.9835, Time: 3.81s, Token/s: 134.46
Epoch: 0, Step: 2502, Batch(micro): 2502, Batch (considering grad accum): 312,  Loss: 7.5982, Time: 3.24s, Token/s: 157.95
Epoch: 0, Step: 2503, Batch(micro): 2503, Batch (considering grad accum): 312,  Loss: 7.2819, Time: 18.28s, Token/s: 28.02
Epoch: 0, Step: 2504, Batch(micro): 2504, Batch (considering grad accum): 313,  Loss: 6.7410, Time: 6.63s, Token/s: 77.19
Epoch: 0, Step: 2505, Batch(micro): 2505, Batch (considering grad accum): 313,  Loss: 7.3605, Time: 4.00s, Token/s: 128.09
Epoch: 0, Step: 2506, Batch(micro): 2506, Batch (considering grad accum): 313,  Loss: 8.5847, Time: 3.55s, Token/s: 144.35
Epoch: 0, Step: 2507, Batch(micro): 2507, Batch (considering grad accum): 313,  Loss: 7.3631, Time: 3.56s, Token/s: 143.99
Epoch: 0, Step: 2508, Batch(micro): 2508, Batch (considering grad accum): 313,  Loss: 6.8586, Time: 3.33s, Token/s: 153.75
Epoch: 0, Step: 2509, Batch(micro): 2509, Batch (considering grad accum): 313,  Loss: 6.9611, Time: 3.28s, Token/s: 156.21
Epoch: 0, Step: 2510, Batch(micro): 2510, Batch (considering grad accum): 313,  Loss: 7.3211, Time: 3.30s, Token/s: 155.04
Epoch: 0, Step: 2511, Batch(micro): 2511, Batch (considering grad accum): 313,  Loss: 6.8293, Time: 19.10s, Token/s: 26.81
Epoch: 0, Step: 2512, Batch(micro): 2512, Batch (considering grad accum): 314,  Loss: 7.5338, Time: 6.30s, Token/s: 81.26
Epoch: 0, Step: 2513, Batch(micro): 2513, Batch (considering grad accum): 314,  Loss: 7.5332, Time: 4.13s, Token/s: 124.08
Epoch: 0, Step: 2514, Batch(micro): 2514, Batch (considering grad accum): 314,  Loss: 7.6811, Time: 3.68s, Token/s: 139.18
Epoch: 0, Step: 2515, Batch(micro): 2515, Batch (considering grad accum): 314,  Loss: 9.2266, Time: 3.70s, Token/s: 138.21
Epoch: 0, Step: 2516, Batch(micro): 2516, Batch (considering grad accum): 314,  Loss: 9.8636, Time: 3.76s, Token/s: 136.20
Epoch: 0, Step: 2517, Batch(micro): 2517, Batch (considering grad accum): 314,  Loss: 9.3027, Time: 3.33s, Token/s: 153.82
Epoch: 0, Step: 2518, Batch(micro): 2518, Batch (considering grad accum): 314,  Loss: 9.0159, Time: 3.55s, Token/s: 144.27
Epoch: 0, Step: 2519, Batch(micro): 2519, Batch (considering grad accum): 314,  Loss: 7.0201, Time: 19.45s, Token/s: 26.32
Epoch: 0, Step: 2520, Batch(micro): 2520, Batch (considering grad accum): 315,  Loss: 7.0288, Time: 7.32s, Token/s: 69.91
Epoch: 0, Step: 2521, Batch(micro): 2521, Batch (considering grad accum): 315,  Loss: 7.4293, Time: 3.98s, Token/s: 128.68
Epoch: 0, Step: 2522, Batch(micro): 2522, Batch (considering grad accum): 315,  Loss: 7.1897, Time: 3.65s, Token/s: 140.34
Epoch: 0, Step: 2523, Batch(micro): 2523, Batch (considering grad accum): 315,  Loss: 6.9302, Time: 4.00s, Token/s: 128.03
Epoch: 0, Step: 2524, Batch(micro): 2524, Batch (considering grad accum): 315,  Loss: 7.0732, Time: 3.92s, Token/s: 130.53
Epoch: 0, Step: 2525, Batch(micro): 2525, Batch (considering grad accum): 315,  Loss: 7.1199, Time: 3.25s, Token/s: 157.76
Epoch: 0, Step: 2526, Batch(micro): 2526, Batch (considering grad accum): 315,  Loss: 7.2200, Time: 3.54s, Token/s: 144.75
Epoch: 0, Step: 2527, Batch(micro): 2527, Batch (considering grad accum): 315,  Loss: 7.3456, Time: 18.70s, Token/s: 27.39
Epoch: 0, Step: 2528, Batch(micro): 2528, Batch (considering grad accum): 316,  Loss: 6.7424, Time: 6.88s, Token/s: 74.38
Epoch: 0, Step: 2529, Batch(micro): 2529, Batch (considering grad accum): 316,  Loss: 6.6332, Time: 4.17s, Token/s: 122.75
Epoch: 0, Step: 2530, Batch(micro): 2530, Batch (considering grad accum): 316,  Loss: 6.7022, Time: 3.47s, Token/s: 147.36
Epoch: 0, Step: 2531, Batch(micro): 2531, Batch (considering grad accum): 316,  Loss: 7.3071, Time: 3.98s, Token/s: 128.63
Epoch: 0, Step: 2532, Batch(micro): 2532, Batch (considering grad accum): 316,  Loss: 7.0263, Time: 3.83s, Token/s: 133.54
Epoch: 0, Step: 2533, Batch(micro): 2533, Batch (considering grad accum): 316,  Loss: 6.8972, Time: 3.45s, Token/s: 148.37
Epoch: 0, Step: 2534, Batch(micro): 2534, Batch (considering grad accum): 316,  Loss: 6.8888, Time: 3.42s, Token/s: 149.50
Epoch: 0, Step: 2535, Batch(micro): 2535, Batch (considering grad accum): 316,  Loss: 7.0051, Time: 23.87s, Token/s: 21.45
Epoch: 0, Step: 2536, Batch(micro): 2536, Batch (considering grad accum): 317,  Loss: 6.7083, Time: 6.46s, Token/s: 79.29
Epoch: 0, Step: 2537, Batch(micro): 2537, Batch (considering grad accum): 317,  Loss: 7.1143, Time: 4.04s, Token/s: 126.61
Epoch: 0, Step: 2538, Batch(micro): 2538, Batch (considering grad accum): 317,  Loss: 7.0684, Time: 3.62s, Token/s: 141.25
Epoch: 0, Step: 2539, Batch(micro): 2539, Batch (considering grad accum): 317,  Loss: 7.2447, Time: 3.63s, Token/s: 141.21
Epoch: 0, Step: 2540, Batch(micro): 2540, Batch (considering grad accum): 317,  Loss: 7.2079, Time: 3.36s, Token/s: 152.21
Epoch: 0, Step: 2541, Batch(micro): 2541, Batch (considering grad accum): 317,  Loss: 7.3369, Time: 3.42s, Token/s: 149.92
Epoch: 0, Step: 2542, Batch(micro): 2542, Batch (considering grad accum): 317,  Loss: 7.1671, Time: 3.31s, Token/s: 154.76
Epoch: 0, Step: 2543, Batch(micro): 2543, Batch (considering grad accum): 317,  Loss: 6.6850, Time: 24.91s, Token/s: 20.56
Epoch: 0, Step: 2544, Batch(micro): 2544, Batch (considering grad accum): 318,  Loss: 6.6212, Time: 8.73s, Token/s: 58.61
Epoch: 0, Step: 2545, Batch(micro): 2545, Batch (considering grad accum): 318,  Loss: 7.3735, Time: 3.96s, Token/s: 129.33
Epoch: 0, Step: 2546, Batch(micro): 2546, Batch (considering grad accum): 318,  Loss: 6.9116, Time: 3.40s, Token/s: 150.39
Epoch: 0, Step: 2547, Batch(micro): 2547, Batch (considering grad accum): 318,  Loss: 6.6114, Time: 3.34s, Token/s: 153.10
Epoch: 0, Step: 2548, Batch(micro): 2548, Batch (considering grad accum): 318,  Loss: 6.5196, Time: 3.21s, Token/s: 159.66
Epoch: 0, Step: 2549, Batch(micro): 2549, Batch (considering grad accum): 318,  Loss: 6.2668, Time: 3.36s, Token/s: 152.56
Epoch: 0, Step: 2550, Batch(micro): 2550, Batch (considering grad accum): 318,  Loss: 6.6801, Time: 3.62s, Token/s: 141.51
Epoch: 0, Step: 2551, Batch(micro): 2551, Batch (considering grad accum): 318,  Loss: 7.0983, Time: 25.11s, Token/s: 20.39
Epoch: 0, Step: 2552, Batch(micro): 2552, Batch (considering grad accum): 319,  Loss: 7.8309, Time: 8.21s, Token/s: 62.37
Epoch: 0, Step: 2553, Batch(micro): 2553, Batch (considering grad accum): 319,  Loss: 7.5671, Time: 3.79s, Token/s: 135.19
Epoch: 0, Step: 2554, Batch(micro): 2554, Batch (considering grad accum): 319,  Loss: 7.0338, Time: 3.25s, Token/s: 157.73
Epoch: 0, Step: 2555, Batch(micro): 2555, Batch (considering grad accum): 319,  Loss: 7.1703, Time: 3.26s, Token/s: 156.99
Epoch: 0, Step: 2556, Batch(micro): 2556, Batch (considering grad accum): 319,  Loss: 8.0087, Time: 3.20s, Token/s: 159.78
Epoch: 0, Step: 2557, Batch(micro): 2557, Batch (considering grad accum): 319,  Loss: 7.2854, Time: 3.24s, Token/s: 157.91
Epoch: 0, Step: 2558, Batch(micro): 2558, Batch (considering grad accum): 319,  Loss: 6.8973, Time: 3.45s, Token/s: 148.45
Epoch: 0, Step: 2559, Batch(micro): 2559, Batch (considering grad accum): 319,  Loss: 7.0697, Time: 24.66s, Token/s: 20.76
Epoch: 0, Step: 2560, Batch(micro): 2560, Batch (considering grad accum): 320,  Loss: 6.9851, Time: 7.07s, Token/s: 72.40
Epoch: 0, Step: 2561, Batch(micro): 2561, Batch (considering grad accum): 320,  Loss: 7.7628, Time: 4.31s, Token/s: 118.85
Epoch: 0, Step: 2562, Batch(micro): 2562, Batch (considering grad accum): 320,  Loss: 7.0164, Time: 3.62s, Token/s: 141.63
Epoch: 0, Step: 2563, Batch(micro): 2563, Batch (considering grad accum): 320,  Loss: 6.5584, Time: 3.98s, Token/s: 128.54
Epoch: 0, Step: 2564, Batch(micro): 2564, Batch (considering grad accum): 320,  Loss: 7.4354, Time: 3.46s, Token/s: 147.87
Epoch: 0, Step: 2565, Batch(micro): 2565, Batch (considering grad accum): 320,  Loss: 6.8689, Time: 3.47s, Token/s: 147.37
Epoch: 0, Step: 2566, Batch(micro): 2566, Batch (considering grad accum): 320,  Loss: 6.7121, Time: 3.42s, Token/s: 149.50
Epoch: 0, Step: 2567, Batch(micro): 2567, Batch (considering grad accum): 320,  Loss: 6.8942, Time: 23.94s, Token/s: 21.39
Epoch: 0, Step: 2568, Batch(micro): 2568, Batch (considering grad accum): 321,  Loss: 6.6239, Time: 7.87s, Token/s: 65.02
Epoch: 0, Step: 2569, Batch(micro): 2569, Batch (considering grad accum): 321,  Loss: 6.9269, Time: 3.80s, Token/s: 134.79
Epoch: 0, Step: 2570, Batch(micro): 2570, Batch (considering grad accum): 321,  Loss: 7.0461, Time: 3.38s, Token/s: 151.37
Epoch: 0, Step: 2571, Batch(micro): 2571, Batch (considering grad accum): 321,  Loss: 6.6842, Time: 3.53s, Token/s: 145.20
Epoch: 0, Step: 2572, Batch(micro): 2572, Batch (considering grad accum): 321,  Loss: 6.5482, Time: 3.54s, Token/s: 144.61
Epoch: 0, Step: 2573, Batch(micro): 2573, Batch (considering grad accum): 321,  Loss: 7.1362, Time: 3.45s, Token/s: 148.25
Epoch: 0, Step: 2574, Batch(micro): 2574, Batch (considering grad accum): 321,  Loss: 7.1617, Time: 3.56s, Token/s: 143.67
Epoch: 0, Step: 2575, Batch(micro): 2575, Batch (considering grad accum): 321,  Loss: 6.7128, Time: 25.15s, Token/s: 20.36
Epoch: 0, Step: 2576, Batch(micro): 2576, Batch (considering grad accum): 322,  Loss: 6.9560, Time: 7.29s, Token/s: 70.21
Epoch: 0, Step: 2577, Batch(micro): 2577, Batch (considering grad accum): 322,  Loss: 7.2621, Time: 3.80s, Token/s: 134.91
Epoch: 0, Step: 2578, Batch(micro): 2578, Batch (considering grad accum): 322,  Loss: 7.2459, Time: 3.51s, Token/s: 145.93
Epoch: 0, Step: 2579, Batch(micro): 2579, Batch (considering grad accum): 322,  Loss: 6.9584, Time: 3.63s, Token/s: 141.03
Epoch: 0, Step: 2580, Batch(micro): 2580, Batch (considering grad accum): 322,  Loss: 6.6760, Time: 3.50s, Token/s: 146.18
Epoch: 0, Step: 2581, Batch(micro): 2581, Batch (considering grad accum): 322,  Loss: 6.6247, Time: 3.45s, Token/s: 148.30
Epoch: 0, Step: 2582, Batch(micro): 2582, Batch (considering grad accum): 322,  Loss: 6.7833, Time: 3.64s, Token/s: 140.82
Epoch: 0, Step: 2583, Batch(micro): 2583, Batch (considering grad accum): 322,  Loss: 6.7689, Time: 22.96s, Token/s: 22.30
Epoch: 0, Step: 2584, Batch(micro): 2584, Batch (considering grad accum): 323,  Loss: 6.8052, Time: 6.24s, Token/s: 82.09
Epoch: 0, Step: 2585, Batch(micro): 2585, Batch (considering grad accum): 323,  Loss: 7.0726, Time: 3.58s, Token/s: 142.87
Epoch: 0, Step: 2586, Batch(micro): 2586, Batch (considering grad accum): 323,  Loss: 7.1564, Time: 3.44s, Token/s: 148.87
Epoch: 0, Step: 2587, Batch(micro): 2587, Batch (considering grad accum): 323,  Loss: 7.5900, Time: 3.69s, Token/s: 138.60
Epoch: 0, Step: 2588, Batch(micro): 2588, Batch (considering grad accum): 323,  Loss: 8.0253, Time: 3.66s, Token/s: 139.78
Epoch: 0, Step: 2589, Batch(micro): 2589, Batch (considering grad accum): 323,  Loss: 7.3474, Time: 3.61s, Token/s: 141.67
Epoch: 0, Step: 2590, Batch(micro): 2590, Batch (considering grad accum): 323,  Loss: 7.2945, Time: 3.46s, Token/s: 148.05
Epoch: 0, Step: 2591, Batch(micro): 2591, Batch (considering grad accum): 323,  Loss: 7.2027, Time: 24.70s, Token/s: 20.73
Epoch: 0, Step: 2592, Batch(micro): 2592, Batch (considering grad accum): 324,  Loss: 6.7343, Time: 8.70s, Token/s: 58.83
Epoch: 0, Step: 2593, Batch(micro): 2593, Batch (considering grad accum): 324,  Loss: 7.1747, Time: 3.65s, Token/s: 140.33
Epoch: 0, Step: 2594, Batch(micro): 2594, Batch (considering grad accum): 324,  Loss: 6.7665, Time: 3.64s, Token/s: 140.61
Epoch: 0, Step: 2595, Batch(micro): 2595, Batch (considering grad accum): 324,  Loss: 6.8222, Time: 3.58s, Token/s: 143.08
Epoch: 0, Step: 2596, Batch(micro): 2596, Batch (considering grad accum): 324,  Loss: 7.3774, Time: 3.65s, Token/s: 140.20
Epoch: 0, Step: 2597, Batch(micro): 2597, Batch (considering grad accum): 324,  Loss: 7.1064, Time: 3.56s, Token/s: 143.91
Epoch: 0, Step: 2598, Batch(micro): 2598, Batch (considering grad accum): 324,  Loss: 6.8205, Time: 3.49s, Token/s: 146.59
Epoch: 0, Step: 2599, Batch(micro): 2599, Batch (considering grad accum): 324,  Loss: 7.2794, Time: 24.59s, Token/s: 20.82
Updating MLP bias
Epoch: 0, Step: 2600, Batch(micro): 2600, Batch (considering grad accum): 325,  Loss: 6.8579, Time: 8.23s, Token/s: 62.21
Epoch: 0, Step: 2601, Batch(micro): 2601, Batch (considering grad accum): 325,  Loss: 6.8722, Time: 3.83s, Token/s: 133.57
Epoch: 0, Step: 2602, Batch(micro): 2602, Batch (considering grad accum): 325,  Loss: 7.0221, Time: 3.76s, Token/s: 136.17
Epoch: 0, Step: 2603, Batch(micro): 2603, Batch (considering grad accum): 325,  Loss: 6.9049, Time: 3.60s, Token/s: 142.10
Epoch: 0, Step: 2604, Batch(micro): 2604, Batch (considering grad accum): 325,  Loss: 7.2190, Time: 3.52s, Token/s: 145.37
Epoch: 0, Step: 2605, Batch(micro): 2605, Batch (considering grad accum): 325,  Loss: 6.6635, Time: 3.45s, Token/s: 148.26
Epoch: 0, Step: 2606, Batch(micro): 2606, Batch (considering grad accum): 325,  Loss: 7.3685, Time: 3.28s, Token/s: 156.12
Epoch: 0, Step: 2607, Batch(micro): 2607, Batch (considering grad accum): 325,  Loss: 6.8153, Time: 23.80s, Token/s: 21.51
Epoch: 0, Step: 2608, Batch(micro): 2608, Batch (considering grad accum): 326,  Loss: 6.5786, Time: 8.74s, Token/s: 58.58
Epoch: 0, Step: 2609, Batch(micro): 2609, Batch (considering grad accum): 326,  Loss: 6.1210, Time: 4.25s, Token/s: 120.57
Epoch: 0, Step: 2610, Batch(micro): 2610, Batch (considering grad accum): 326,  Loss: 6.6140, Time: 4.65s, Token/s: 110.01
Epoch: 0, Step: 2611, Batch(micro): 2611, Batch (considering grad accum): 326,  Loss: 6.5617, Time: 3.31s, Token/s: 154.56
Epoch: 0, Step: 2612, Batch(micro): 2612, Batch (considering grad accum): 326,  Loss: 6.6905, Time: 3.63s, Token/s: 141.19
Epoch: 0, Step: 2613, Batch(micro): 2613, Batch (considering grad accum): 326,  Loss: 7.6448, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 2614, Batch(micro): 2614, Batch (considering grad accum): 326,  Loss: 6.4039, Time: 3.49s, Token/s: 146.91
Epoch: 0, Step: 2615, Batch(micro): 2615, Batch (considering grad accum): 326,  Loss: 7.2187, Time: 24.23s, Token/s: 21.13
Epoch: 0, Step: 2616, Batch(micro): 2616, Batch (considering grad accum): 327,  Loss: 6.2512, Time: 6.60s, Token/s: 77.62
Epoch: 0, Step: 2617, Batch(micro): 2617, Batch (considering grad accum): 327,  Loss: 7.4459, Time: 3.93s, Token/s: 130.31
Epoch: 0, Step: 2618, Batch(micro): 2618, Batch (considering grad accum): 327,  Loss: 6.7075, Time: 3.79s, Token/s: 135.14
Epoch: 0, Step: 2619, Batch(micro): 2619, Batch (considering grad accum): 327,  Loss: 6.9707, Time: 3.56s, Token/s: 143.80
Epoch: 0, Step: 2620, Batch(micro): 2620, Batch (considering grad accum): 327,  Loss: 6.8514, Time: 3.53s, Token/s: 145.21
Epoch: 0, Step: 2621, Batch(micro): 2621, Batch (considering grad accum): 327,  Loss: 6.7312, Time: 3.48s, Token/s: 147.14
Epoch: 0, Step: 2622, Batch(micro): 2622, Batch (considering grad accum): 327,  Loss: 6.8187, Time: 3.48s, Token/s: 147.27
Epoch: 0, Step: 2623, Batch(micro): 2623, Batch (considering grad accum): 327,  Loss: 7.2050, Time: 23.10s, Token/s: 22.16
Epoch: 0, Step: 2624, Batch(micro): 2624, Batch (considering grad accum): 328,  Loss: 7.1125, Time: 7.43s, Token/s: 68.95
Epoch: 0, Step: 2625, Batch(micro): 2625, Batch (considering grad accum): 328,  Loss: 6.9789, Time: 4.28s, Token/s: 119.76
Epoch: 0, Step: 2626, Batch(micro): 2626, Batch (considering grad accum): 328,  Loss: 6.3965, Time: 3.54s, Token/s: 144.64
Epoch: 0, Step: 2627, Batch(micro): 2627, Batch (considering grad accum): 328,  Loss: 7.0244, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 2628, Batch(micro): 2628, Batch (considering grad accum): 328,  Loss: 7.5963, Time: 3.67s, Token/s: 139.47
Epoch: 0, Step: 2629, Batch(micro): 2629, Batch (considering grad accum): 328,  Loss: 6.9301, Time: 3.58s, Token/s: 142.91
Epoch: 0, Step: 2630, Batch(micro): 2630, Batch (considering grad accum): 328,  Loss: 6.5555, Time: 3.44s, Token/s: 149.01
Epoch: 0, Step: 2631, Batch(micro): 2631, Batch (considering grad accum): 328,  Loss: 7.1279, Time: 21.35s, Token/s: 23.98
Epoch: 0, Step: 2632, Batch(micro): 2632, Batch (considering grad accum): 329,  Loss: 7.9193, Time: 10.40s, Token/s: 49.21
Epoch: 0, Step: 2633, Batch(micro): 2633, Batch (considering grad accum): 329,  Loss: 6.7057, Time: 3.29s, Token/s: 155.71
Epoch: 0, Step: 2634, Batch(micro): 2634, Batch (considering grad accum): 329,  Loss: 6.9970, Time: 3.17s, Token/s: 161.53
Epoch: 0, Step: 2635, Batch(micro): 2635, Batch (considering grad accum): 329,  Loss: 6.4880, Time: 3.17s, Token/s: 161.66
Epoch: 0, Step: 2636, Batch(micro): 2636, Batch (considering grad accum): 329,  Loss: 6.8790, Time: 3.48s, Token/s: 146.96
Epoch: 0, Step: 2637, Batch(micro): 2637, Batch (considering grad accum): 329,  Loss: 7.3780, Time: 3.65s, Token/s: 140.32
Epoch: 0, Step: 2638, Batch(micro): 2638, Batch (considering grad accum): 329,  Loss: 7.0559, Time: 3.46s, Token/s: 147.77
Epoch: 0, Step: 2639, Batch(micro): 2639, Batch (considering grad accum): 329,  Loss: 6.2364, Time: 22.92s, Token/s: 22.34
Epoch: 0, Step: 2640, Batch(micro): 2640, Batch (considering grad accum): 330,  Loss: 7.2822, Time: 8.12s, Token/s: 63.06
Epoch: 0, Step: 2641, Batch(micro): 2641, Batch (considering grad accum): 330,  Loss: 7.2567, Time: 4.13s, Token/s: 123.96
Epoch: 0, Step: 2642, Batch(micro): 2642, Batch (considering grad accum): 330,  Loss: 7.4159, Time: 3.57s, Token/s: 143.37
Epoch: 0, Step: 2643, Batch(micro): 2643, Batch (considering grad accum): 330,  Loss: 7.1658, Time: 3.63s, Token/s: 141.24
Epoch: 0, Step: 2644, Batch(micro): 2644, Batch (considering grad accum): 330,  Loss: 6.5446, Time: 3.54s, Token/s: 144.72
Epoch: 0, Step: 2645, Batch(micro): 2645, Batch (considering grad accum): 330,  Loss: 6.9227, Time: 3.69s, Token/s: 138.79
Epoch: 0, Step: 2646, Batch(micro): 2646, Batch (considering grad accum): 330,  Loss: 7.0896, Time: 3.42s, Token/s: 149.81
Epoch: 0, Step: 2647, Batch(micro): 2647, Batch (considering grad accum): 330,  Loss: 7.0814, Time: 21.97s, Token/s: 23.30
Epoch: 0, Step: 2648, Batch(micro): 2648, Batch (considering grad accum): 331,  Loss: 8.3983, Time: 9.33s, Token/s: 54.88
Epoch: 0, Step: 2649, Batch(micro): 2649, Batch (considering grad accum): 331,  Loss: 7.5804, Time: 3.98s, Token/s: 128.72
Epoch: 0, Step: 2650, Batch(micro): 2650, Batch (considering grad accum): 331,  Loss: 7.0034, Time: 3.53s, Token/s: 145.04
Epoch: 0, Step: 2651, Batch(micro): 2651, Batch (considering grad accum): 331,  Loss: 6.9834, Time: 3.49s, Token/s: 146.69
Epoch: 0, Step: 2652, Batch(micro): 2652, Batch (considering grad accum): 331,  Loss: 6.7181, Time: 3.43s, Token/s: 149.42
Epoch: 0, Step: 2653, Batch(micro): 2653, Batch (considering grad accum): 331,  Loss: 7.3262, Time: 3.52s, Token/s: 145.31
Epoch: 0, Step: 2654, Batch(micro): 2654, Batch (considering grad accum): 331,  Loss: 6.8594, Time: 3.65s, Token/s: 140.26
Epoch: 0, Step: 2655, Batch(micro): 2655, Batch (considering grad accum): 331,  Loss: 6.4353, Time: 24.89s, Token/s: 20.57
Epoch: 0, Step: 2656, Batch(micro): 2656, Batch (considering grad accum): 332,  Loss: 6.7713, Time: 6.75s, Token/s: 75.88
Epoch: 0, Step: 2657, Batch(micro): 2657, Batch (considering grad accum): 332,  Loss: 6.7056, Time: 3.73s, Token/s: 137.30
Epoch: 0, Step: 2658, Batch(micro): 2658, Batch (considering grad accum): 332,  Loss: 6.8919, Time: 3.29s, Token/s: 155.54
Epoch: 0, Step: 2659, Batch(micro): 2659, Batch (considering grad accum): 332,  Loss: 7.1849, Time: 3.28s, Token/s: 156.14
Epoch: 0, Step: 2660, Batch(micro): 2660, Batch (considering grad accum): 332,  Loss: 7.2213, Time: 3.26s, Token/s: 157.17
Epoch: 0, Step: 2661, Batch(micro): 2661, Batch (considering grad accum): 332,  Loss: 7.4804, Time: 3.42s, Token/s: 149.86
Epoch: 0, Step: 2662, Batch(micro): 2662, Batch (considering grad accum): 332,  Loss: 7.2994, Time: 3.46s, Token/s: 147.91
Epoch: 0, Step: 2663, Batch(micro): 2663, Batch (considering grad accum): 332,  Loss: 7.6323, Time: 23.72s, Token/s: 21.58
Epoch: 0, Step: 2664, Batch(micro): 2664, Batch (considering grad accum): 333,  Loss: 7.1449, Time: 7.30s, Token/s: 70.14
Epoch: 0, Step: 2665, Batch(micro): 2665, Batch (considering grad accum): 333,  Loss: 7.0238, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 2666, Batch(micro): 2666, Batch (considering grad accum): 333,  Loss: 6.8785, Time: 3.73s, Token/s: 137.44
Epoch: 0, Step: 2667, Batch(micro): 2667, Batch (considering grad accum): 333,  Loss: 7.5959, Time: 3.78s, Token/s: 135.48
Epoch: 0, Step: 2668, Batch(micro): 2668, Batch (considering grad accum): 333,  Loss: 6.9600, Time: 3.60s, Token/s: 142.39
Epoch: 0, Step: 2669, Batch(micro): 2669, Batch (considering grad accum): 333,  Loss: 7.4425, Time: 3.54s, Token/s: 144.78
Epoch: 0, Step: 2670, Batch(micro): 2670, Batch (considering grad accum): 333,  Loss: 6.5331, Time: 3.27s, Token/s: 156.64
Epoch: 0, Step: 2671, Batch(micro): 2671, Batch (considering grad accum): 333,  Loss: 6.2174, Time: 21.41s, Token/s: 23.91
Epoch: 0, Step: 2672, Batch(micro): 2672, Batch (considering grad accum): 334,  Loss: 6.9154, Time: 8.25s, Token/s: 62.07
Epoch: 0, Step: 2673, Batch(micro): 2673, Batch (considering grad accum): 334,  Loss: 7.1031, Time: 3.27s, Token/s: 156.38
Epoch: 0, Step: 2674, Batch(micro): 2674, Batch (considering grad accum): 334,  Loss: 6.9964, Time: 3.19s, Token/s: 160.33
Epoch: 0, Step: 2675, Batch(micro): 2675, Batch (considering grad accum): 334,  Loss: 7.3753, Time: 3.09s, Token/s: 165.51
Epoch: 0, Step: 2676, Batch(micro): 2676, Batch (considering grad accum): 334,  Loss: 6.2374, Time: 3.17s, Token/s: 161.68
Epoch: 0, Step: 2677, Batch(micro): 2677, Batch (considering grad accum): 334,  Loss: 6.7664, Time: 3.25s, Token/s: 157.67
Epoch: 0, Step: 2678, Batch(micro): 2678, Batch (considering grad accum): 334,  Loss: 7.1951, Time: 3.20s, Token/s: 160.04
Epoch: 0, Step: 2679, Batch(micro): 2679, Batch (considering grad accum): 334,  Loss: 6.7078, Time: 23.20s, Token/s: 22.07
Epoch: 0, Step: 2680, Batch(micro): 2680, Batch (considering grad accum): 335,  Loss: 6.6522, Time: 7.15s, Token/s: 71.57
Epoch: 0, Step: 2681, Batch(micro): 2681, Batch (considering grad accum): 335,  Loss: 7.1093, Time: 3.99s, Token/s: 128.19
Epoch: 0, Step: 2682, Batch(micro): 2682, Batch (considering grad accum): 335,  Loss: 7.2118, Time: 3.88s, Token/s: 131.89
Epoch: 0, Step: 2683, Batch(micro): 2683, Batch (considering grad accum): 335,  Loss: 7.3394, Time: 3.73s, Token/s: 137.26
Epoch: 0, Step: 2684, Batch(micro): 2684, Batch (considering grad accum): 335,  Loss: 6.8378, Time: 3.57s, Token/s: 143.31
Epoch: 0, Step: 2685, Batch(micro): 2685, Batch (considering grad accum): 335,  Loss: 6.9583, Time: 3.44s, Token/s: 148.84
Epoch: 0, Step: 2686, Batch(micro): 2686, Batch (considering grad accum): 335,  Loss: 6.8517, Time: 3.28s, Token/s: 156.13
Epoch: 0, Step: 2687, Batch(micro): 2687, Batch (considering grad accum): 335,  Loss: 6.6843, Time: 18.76s, Token/s: 27.29
Epoch: 0, Step: 2688, Batch(micro): 2688, Batch (considering grad accum): 336,  Loss: 6.8604, Time: 6.44s, Token/s: 79.49
Epoch: 0, Step: 2689, Batch(micro): 2689, Batch (considering grad accum): 336,  Loss: 6.9272, Time: 4.25s, Token/s: 120.49
Epoch: 0, Step: 2690, Batch(micro): 2690, Batch (considering grad accum): 336,  Loss: 7.1506, Time: 3.63s, Token/s: 141.18
Epoch: 0, Step: 2691, Batch(micro): 2691, Batch (considering grad accum): 336,  Loss: 6.4204, Time: 3.66s, Token/s: 139.93
Epoch: 0, Step: 2692, Batch(micro): 2692, Batch (considering grad accum): 336,  Loss: 7.4246, Time: 3.32s, Token/s: 154.07
Epoch: 0, Step: 2693, Batch(micro): 2693, Batch (considering grad accum): 336,  Loss: 7.1000, Time: 3.30s, Token/s: 155.06
Epoch: 0, Step: 2694, Batch(micro): 2694, Batch (considering grad accum): 336,  Loss: 6.5817, Time: 3.06s, Token/s: 167.27
Epoch: 0, Step: 2695, Batch(micro): 2695, Batch (considering grad accum): 336,  Loss: 6.3587, Time: 17.89s, Token/s: 28.62
Epoch: 0, Step: 2696, Batch(micro): 2696, Batch (considering grad accum): 337,  Loss: 7.0986, Time: 7.73s, Token/s: 66.24
Epoch: 0, Step: 2697, Batch(micro): 2697, Batch (considering grad accum): 337,  Loss: 6.4048, Time: 3.94s, Token/s: 129.93
Epoch: 0, Step: 2698, Batch(micro): 2698, Batch (considering grad accum): 337,  Loss: 6.8432, Time: 3.56s, Token/s: 143.87
Epoch: 0, Step: 2699, Batch(micro): 2699, Batch (considering grad accum): 337,  Loss: 6.8382, Time: 3.31s, Token/s: 154.88
Updating MLP bias
Epoch: 0, Step: 2700, Batch(micro): 2700, Batch (considering grad accum): 337,  Loss: 7.3361, Time: 3.70s, Token/s: 138.40
Epoch: 0, Step: 2701, Batch(micro): 2701, Batch (considering grad accum): 337,  Loss: 6.8698, Time: 3.67s, Token/s: 139.33
Epoch: 0, Step: 2702, Batch(micro): 2702, Batch (considering grad accum): 337,  Loss: 8.0546, Time: 3.28s, Token/s: 156.31
Epoch: 0, Step: 2703, Batch(micro): 2703, Batch (considering grad accum): 337,  Loss: 8.1134, Time: 17.91s, Token/s: 28.59
Epoch: 0, Step: 2704, Batch(micro): 2704, Batch (considering grad accum): 338,  Loss: 7.0197, Time: 7.74s, Token/s: 66.13
Epoch: 0, Step: 2705, Batch(micro): 2705, Batch (considering grad accum): 338,  Loss: 6.4762, Time: 3.53s, Token/s: 145.18
Epoch: 0, Step: 2706, Batch(micro): 2706, Batch (considering grad accum): 338,  Loss: 7.1251, Time: 3.05s, Token/s: 168.11
Epoch: 0, Step: 2707, Batch(micro): 2707, Batch (considering grad accum): 338,  Loss: 7.1660, Time: 3.61s, Token/s: 141.97
Epoch: 0, Step: 2708, Batch(micro): 2708, Batch (considering grad accum): 338,  Loss: 6.8874, Time: 3.33s, Token/s: 153.57
Epoch: 0, Step: 2709, Batch(micro): 2709, Batch (considering grad accum): 338,  Loss: 7.0328, Time: 3.24s, Token/s: 158.12
Epoch: 0, Step: 2710, Batch(micro): 2710, Batch (considering grad accum): 338,  Loss: 6.8110, Time: 2.98s, Token/s: 171.84
Epoch: 0, Step: 2711, Batch(micro): 2711, Batch (considering grad accum): 338,  Loss: 6.9674, Time: 18.09s, Token/s: 28.30
Epoch: 0, Step: 2712, Batch(micro): 2712, Batch (considering grad accum): 339,  Loss: 6.5837, Time: 6.71s, Token/s: 76.33
Epoch: 0, Step: 2713, Batch(micro): 2713, Batch (considering grad accum): 339,  Loss: 6.3387, Time: 3.91s, Token/s: 130.95
Epoch: 0, Step: 2714, Batch(micro): 2714, Batch (considering grad accum): 339,  Loss: 6.8946, Time: 3.28s, Token/s: 155.97
Epoch: 0, Step: 2715, Batch(micro): 2715, Batch (considering grad accum): 339,  Loss: 6.6347, Time: 3.35s, Token/s: 152.80
Epoch: 0, Step: 2716, Batch(micro): 2716, Batch (considering grad accum): 339,  Loss: 6.7794, Time: 3.44s, Token/s: 148.70
Epoch: 0, Step: 2717, Batch(micro): 2717, Batch (considering grad accum): 339,  Loss: 6.7401, Time: 3.12s, Token/s: 164.15
Epoch: 0, Step: 2718, Batch(micro): 2718, Batch (considering grad accum): 339,  Loss: 6.8394, Time: 2.98s, Token/s: 171.93
Epoch: 0, Step: 2719, Batch(micro): 2719, Batch (considering grad accum): 339,  Loss: 6.5743, Time: 20.73s, Token/s: 24.70
Epoch: 0, Step: 2720, Batch(micro): 2720, Batch (considering grad accum): 340,  Loss: 6.6775, Time: 6.59s, Token/s: 77.75
Epoch: 0, Step: 2721, Batch(micro): 2721, Batch (considering grad accum): 340,  Loss: 6.6574, Time: 3.69s, Token/s: 138.75
Epoch: 0, Step: 2722, Batch(micro): 2722, Batch (considering grad accum): 340,  Loss: 7.2625, Time: 3.29s, Token/s: 155.48
Epoch: 0, Step: 2723, Batch(micro): 2723, Batch (considering grad accum): 340,  Loss: 7.4030, Time: 3.23s, Token/s: 158.41
Epoch: 0, Step: 2724, Batch(micro): 2724, Batch (considering grad accum): 340,  Loss: 7.2879, Time: 3.19s, Token/s: 160.62
Epoch: 0, Step: 2725, Batch(micro): 2725, Batch (considering grad accum): 340,  Loss: 7.0332, Time: 3.18s, Token/s: 160.95
Epoch: 0, Step: 2726, Batch(micro): 2726, Batch (considering grad accum): 340,  Loss: 6.3573, Time: 3.40s, Token/s: 150.51
Epoch: 0, Step: 2727, Batch(micro): 2727, Batch (considering grad accum): 340,  Loss: 6.7920, Time: 18.70s, Token/s: 27.38
Epoch: 0, Step: 2728, Batch(micro): 2728, Batch (considering grad accum): 341,  Loss: 6.4490, Time: 5.93s, Token/s: 86.28
Epoch: 0, Step: 2729, Batch(micro): 2729, Batch (considering grad accum): 341,  Loss: 6.8038, Time: 4.00s, Token/s: 127.92
Epoch: 0, Step: 2730, Batch(micro): 2730, Batch (considering grad accum): 341,  Loss: 7.6015, Time: 3.37s, Token/s: 152.03
Epoch: 0, Step: 2731, Batch(micro): 2731, Batch (considering grad accum): 341,  Loss: 8.1073, Time: 3.42s, Token/s: 149.72
Epoch: 0, Step: 2732, Batch(micro): 2732, Batch (considering grad accum): 341,  Loss: 6.9897, Time: 3.20s, Token/s: 159.75
Epoch: 0, Step: 2733, Batch(micro): 2733, Batch (considering grad accum): 341,  Loss: 6.7778, Time: 3.12s, Token/s: 164.06
Epoch: 0, Step: 2734, Batch(micro): 2734, Batch (considering grad accum): 341,  Loss: 6.6281, Time: 2.86s, Token/s: 178.88
Epoch: 0, Step: 2735, Batch(micro): 2735, Batch (considering grad accum): 341,  Loss: 6.5341, Time: 17.89s, Token/s: 28.62
Epoch: 0, Step: 2736, Batch(micro): 2736, Batch (considering grad accum): 342,  Loss: 6.0476, Time: 6.19s, Token/s: 82.67
Epoch: 0, Step: 2737, Batch(micro): 2737, Batch (considering grad accum): 342,  Loss: 7.5003, Time: 3.47s, Token/s: 147.65
Epoch: 0, Step: 2738, Batch(micro): 2738, Batch (considering grad accum): 342,  Loss: 6.7101, Time: 3.28s, Token/s: 156.21
Epoch: 0, Step: 2739, Batch(micro): 2739, Batch (considering grad accum): 342,  Loss: 6.5046, Time: 2.88s, Token/s: 178.04
Epoch: 0, Step: 2740, Batch(micro): 2740, Batch (considering grad accum): 342,  Loss: 6.7898, Time: 3.06s, Token/s: 167.26
Epoch: 0, Step: 2741, Batch(micro): 2741, Batch (considering grad accum): 342,  Loss: 7.3753, Time: 2.96s, Token/s: 173.26
Epoch: 0, Step: 2742, Batch(micro): 2742, Batch (considering grad accum): 342,  Loss: 7.2526, Time: 4.74s, Token/s: 108.00
Epoch: 0, Step: 2743, Batch(micro): 2743, Batch (considering grad accum): 342,  Loss: 7.5380, Time: 18.29s, Token/s: 28.00
Epoch: 0, Step: 2744, Batch(micro): 2744, Batch (considering grad accum): 343,  Loss: 7.1103, Time: 7.43s, Token/s: 68.91
Epoch: 0, Step: 2745, Batch(micro): 2745, Batch (considering grad accum): 343,  Loss: 7.0646, Time: 3.93s, Token/s: 130.23
Epoch: 0, Step: 2746, Batch(micro): 2746, Batch (considering grad accum): 343,  Loss: 7.1954, Time: 3.32s, Token/s: 154.31
Epoch: 0, Step: 2747, Batch(micro): 2747, Batch (considering grad accum): 343,  Loss: 7.4149, Time: 3.05s, Token/s: 167.78
Epoch: 0, Step: 2748, Batch(micro): 2748, Batch (considering grad accum): 343,  Loss: 6.7834, Time: 3.01s, Token/s: 170.02
Epoch: 0, Step: 2749, Batch(micro): 2749, Batch (considering grad accum): 343,  Loss: 6.6501, Time: 3.06s, Token/s: 167.33
Epoch: 0, Step: 2750, Batch(micro): 2750, Batch (considering grad accum): 343,  Loss: 7.5788, Time: 2.93s, Token/s: 174.68
Epoch: 0, Step: 2751, Batch(micro): 2751, Batch (considering grad accum): 343,  Loss: 8.0338, Time: 18.33s, Token/s: 27.93
Epoch: 0, Step: 2752, Batch(micro): 2752, Batch (considering grad accum): 344,  Loss: 7.8888, Time: 6.42s, Token/s: 79.81
Epoch: 0, Step: 2753, Batch(micro): 2753, Batch (considering grad accum): 344,  Loss: 6.9163, Time: 3.59s, Token/s: 142.77
Epoch: 0, Step: 2754, Batch(micro): 2754, Batch (considering grad accum): 344,  Loss: 6.5613, Time: 2.97s, Token/s: 172.65
Epoch: 0, Step: 2755, Batch(micro): 2755, Batch (considering grad accum): 344,  Loss: 6.8843, Time: 3.03s, Token/s: 168.75
Epoch: 0, Step: 2756, Batch(micro): 2756, Batch (considering grad accum): 344,  Loss: 7.7982, Time: 3.03s, Token/s: 169.02
Epoch: 0, Step: 2757, Batch(micro): 2757, Batch (considering grad accum): 344,  Loss: 6.6404, Time: 3.27s, Token/s: 156.73
Epoch: 0, Step: 2758, Batch(micro): 2758, Batch (considering grad accum): 344,  Loss: 6.4306, Time: 3.23s, Token/s: 158.75
Epoch: 0, Step: 2759, Batch(micro): 2759, Batch (considering grad accum): 344,  Loss: 6.7404, Time: 18.77s, Token/s: 27.28
Epoch: 0, Step: 2760, Batch(micro): 2760, Batch (considering grad accum): 345,  Loss: 7.0834, Time: 6.77s, Token/s: 75.65
Epoch: 0, Step: 2761, Batch(micro): 2761, Batch (considering grad accum): 345,  Loss: 6.4143, Time: 3.63s, Token/s: 140.88
Epoch: 0, Step: 2762, Batch(micro): 2762, Batch (considering grad accum): 345,  Loss: 6.3936, Time: 3.07s, Token/s: 166.86
Epoch: 0, Step: 2763, Batch(micro): 2763, Batch (considering grad accum): 345,  Loss: 6.8263, Time: 3.35s, Token/s: 153.02
Epoch: 0, Step: 2764, Batch(micro): 2764, Batch (considering grad accum): 345,  Loss: 6.9801, Time: 3.23s, Token/s: 158.28
Epoch: 0, Step: 2765, Batch(micro): 2765, Batch (considering grad accum): 345,  Loss: 6.9689, Time: 3.27s, Token/s: 156.66
Epoch: 0, Step: 2766, Batch(micro): 2766, Batch (considering grad accum): 345,  Loss: 7.1672, Time: 3.25s, Token/s: 157.38
Epoch: 0, Step: 2767, Batch(micro): 2767, Batch (considering grad accum): 345,  Loss: 6.7373, Time: 17.60s, Token/s: 29.08
Epoch: 0, Step: 2768, Batch(micro): 2768, Batch (considering grad accum): 346,  Loss: 6.5460, Time: 6.23s, Token/s: 82.20
Epoch: 0, Step: 2769, Batch(micro): 2769, Batch (considering grad accum): 346,  Loss: 6.8767, Time: 3.72s, Token/s: 137.81
Epoch: 0, Step: 2770, Batch(micro): 2770, Batch (considering grad accum): 346,  Loss: 6.5678, Time: 3.23s, Token/s: 158.70
Epoch: 0, Step: 2771, Batch(micro): 2771, Batch (considering grad accum): 346,  Loss: 6.7680, Time: 3.11s, Token/s: 164.59
Epoch: 0, Step: 2772, Batch(micro): 2772, Batch (considering grad accum): 346,  Loss: 7.0098, Time: 3.79s, Token/s: 135.21
Epoch: 0, Step: 2773, Batch(micro): 2773, Batch (considering grad accum): 346,  Loss: 7.0183, Time: 3.23s, Token/s: 158.52
Epoch: 0, Step: 2774, Batch(micro): 2774, Batch (considering grad accum): 346,  Loss: 6.8916, Time: 3.08s, Token/s: 166.06
Epoch: 0, Step: 2775, Batch(micro): 2775, Batch (considering grad accum): 346,  Loss: 6.5159, Time: 19.29s, Token/s: 26.55
Epoch: 0, Step: 2776, Batch(micro): 2776, Batch (considering grad accum): 347,  Loss: 6.6342, Time: 7.09s, Token/s: 72.25
Epoch: 0, Step: 2777, Batch(micro): 2777, Batch (considering grad accum): 347,  Loss: 6.7135, Time: 4.07s, Token/s: 125.89
Epoch: 0, Step: 2778, Batch(micro): 2778, Batch (considering grad accum): 347,  Loss: 6.3663, Time: 3.40s, Token/s: 150.76
Epoch: 0, Step: 2779, Batch(micro): 2779, Batch (considering grad accum): 347,  Loss: 6.6040, Time: 3.51s, Token/s: 145.70
Epoch: 0, Step: 2780, Batch(micro): 2780, Batch (considering grad accum): 347,  Loss: 6.5693, Time: 3.44s, Token/s: 148.89
Epoch: 0, Step: 2781, Batch(micro): 2781, Batch (considering grad accum): 347,  Loss: 6.9392, Time: 3.59s, Token/s: 142.73
Epoch: 0, Step: 2782, Batch(micro): 2782, Batch (considering grad accum): 347,  Loss: 7.0130, Time: 3.60s, Token/s: 142.33
Epoch: 0, Step: 2783, Batch(micro): 2783, Batch (considering grad accum): 347,  Loss: 6.9687, Time: 23.40s, Token/s: 21.88
Epoch: 0, Step: 2784, Batch(micro): 2784, Batch (considering grad accum): 348,  Loss: 6.8345, Time: 7.07s, Token/s: 72.38
Epoch: 0, Step: 2785, Batch(micro): 2785, Batch (considering grad accum): 348,  Loss: 7.4677, Time: 3.77s, Token/s: 135.95
Epoch: 0, Step: 2786, Batch(micro): 2786, Batch (considering grad accum): 348,  Loss: 7.4163, Time: 3.49s, Token/s: 146.63
Epoch: 0, Step: 2787, Batch(micro): 2787, Batch (considering grad accum): 348,  Loss: 7.3718, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 2788, Batch(micro): 2788, Batch (considering grad accum): 348,  Loss: 6.7054, Time: 3.48s, Token/s: 147.33
Epoch: 0, Step: 2789, Batch(micro): 2789, Batch (considering grad accum): 348,  Loss: 6.9850, Time: 2.92s, Token/s: 175.27
Epoch: 0, Step: 2790, Batch(micro): 2790, Batch (considering grad accum): 348,  Loss: 6.8225, Time: 3.15s, Token/s: 162.50
Epoch: 0, Step: 2791, Batch(micro): 2791, Batch (considering grad accum): 348,  Loss: 6.6919, Time: 25.66s, Token/s: 19.96
Epoch: 0, Step: 2792, Batch(micro): 2792, Batch (considering grad accum): 349,  Loss: 6.8769, Time: 6.90s, Token/s: 74.25
Epoch: 0, Step: 2793, Batch(micro): 2793, Batch (considering grad accum): 349,  Loss: 6.9016, Time: 3.59s, Token/s: 142.69
Epoch: 0, Step: 2794, Batch(micro): 2794, Batch (considering grad accum): 349,  Loss: 6.9082, Time: 3.99s, Token/s: 128.30
Epoch: 0, Step: 2795, Batch(micro): 2795, Batch (considering grad accum): 349,  Loss: 6.7937, Time: 3.51s, Token/s: 145.75
Epoch: 0, Step: 2796, Batch(micro): 2796, Batch (considering grad accum): 349,  Loss: 6.9887, Time: 3.10s, Token/s: 165.02
Epoch: 0, Step: 2797, Batch(micro): 2797, Batch (considering grad accum): 349,  Loss: 6.7618, Time: 3.10s, Token/s: 165.15
Epoch: 0, Step: 2798, Batch(micro): 2798, Batch (considering grad accum): 349,  Loss: 6.8788, Time: 3.16s, Token/s: 162.00
Epoch: 0, Step: 2799, Batch(micro): 2799, Batch (considering grad accum): 349,  Loss: 7.0158, Time: 22.59s, Token/s: 22.66
Updating MLP bias
Epoch: 0, Step: 2800, Batch(micro): 2800, Batch (considering grad accum): 350,  Loss: 6.6553, Time: 8.06s, Token/s: 63.51
Epoch: 0, Step: 2801, Batch(micro): 2801, Batch (considering grad accum): 350,  Loss: 7.7590, Time: 3.56s, Token/s: 143.68
Epoch: 0, Step: 2802, Batch(micro): 2802, Batch (considering grad accum): 350,  Loss: 7.2849, Time: 2.85s, Token/s: 179.50
Epoch: 0, Step: 2803, Batch(micro): 2803, Batch (considering grad accum): 350,  Loss: 7.4705, Time: 3.11s, Token/s: 164.57
Epoch: 0, Step: 2804, Batch(micro): 2804, Batch (considering grad accum): 350,  Loss: 7.3042, Time: 2.93s, Token/s: 174.87
Epoch: 0, Step: 2805, Batch(micro): 2805, Batch (considering grad accum): 350,  Loss: 7.2595, Time: 2.94s, Token/s: 174.39
Epoch: 0, Step: 2806, Batch(micro): 2806, Batch (considering grad accum): 350,  Loss: 7.7667, Time: 2.91s, Token/s: 176.04
Epoch: 0, Step: 2807, Batch(micro): 2807, Batch (considering grad accum): 350,  Loss: 7.7361, Time: 22.29s, Token/s: 22.97
Epoch: 0, Step: 2808, Batch(micro): 2808, Batch (considering grad accum): 351,  Loss: 6.8249, Time: 6.89s, Token/s: 74.32
Epoch: 0, Step: 2809, Batch(micro): 2809, Batch (considering grad accum): 351,  Loss: 6.8327, Time: 3.47s, Token/s: 147.75
Epoch: 0, Step: 2810, Batch(micro): 2810, Batch (considering grad accum): 351,  Loss: 7.0899, Time: 2.95s, Token/s: 173.64
Epoch: 0, Step: 2811, Batch(micro): 2811, Batch (considering grad accum): 351,  Loss: 7.1957, Time: 2.94s, Token/s: 174.23
Epoch: 0, Step: 2812, Batch(micro): 2812, Batch (considering grad accum): 351,  Loss: 7.2167, Time: 2.95s, Token/s: 173.70
Epoch: 0, Step: 2813, Batch(micro): 2813, Batch (considering grad accum): 351,  Loss: 7.1197, Time: 3.01s, Token/s: 169.86
Epoch: 0, Step: 2814, Batch(micro): 2814, Batch (considering grad accum): 351,  Loss: 6.6711, Time: 2.92s, Token/s: 175.14
Epoch: 0, Step: 2815, Batch(micro): 2815, Batch (considering grad accum): 351,  Loss: 6.9424, Time: 23.53s, Token/s: 21.76
Epoch: 0, Step: 2816, Batch(micro): 2816, Batch (considering grad accum): 352,  Loss: 6.6164, Time: 7.11s, Token/s: 72.01
Epoch: 0, Step: 2817, Batch(micro): 2817, Batch (considering grad accum): 352,  Loss: 6.6610, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 2818, Batch(micro): 2818, Batch (considering grad accum): 352,  Loss: 7.1649, Time: 3.30s, Token/s: 155.37
Epoch: 0, Step: 2819, Batch(micro): 2819, Batch (considering grad accum): 352,  Loss: 6.7278, Time: 3.14s, Token/s: 162.93
Epoch: 0, Step: 2820, Batch(micro): 2820, Batch (considering grad accum): 352,  Loss: 6.9495, Time: 2.97s, Token/s: 172.29
Epoch: 0, Step: 2821, Batch(micro): 2821, Batch (considering grad accum): 352,  Loss: 7.7331, Time: 2.96s, Token/s: 172.73
Epoch: 0, Step: 2822, Batch(micro): 2822, Batch (considering grad accum): 352,  Loss: 6.4532, Time: 2.96s, Token/s: 172.93
Epoch: 0, Step: 2823, Batch(micro): 2823, Batch (considering grad accum): 352,  Loss: 6.9765, Time: 23.10s, Token/s: 22.17
Epoch: 0, Step: 2824, Batch(micro): 2824, Batch (considering grad accum): 353,  Loss: 7.4024, Time: 6.71s, Token/s: 76.36
Epoch: 0, Step: 2825, Batch(micro): 2825, Batch (considering grad accum): 353,  Loss: 6.5963, Time: 3.91s, Token/s: 130.85
Epoch: 0, Step: 2826, Batch(micro): 2826, Batch (considering grad accum): 353,  Loss: 6.1508, Time: 3.60s, Token/s: 142.05
Epoch: 0, Step: 2827, Batch(micro): 2827, Batch (considering grad accum): 353,  Loss: 6.1379, Time: 3.53s, Token/s: 145.07
Epoch: 0, Step: 2828, Batch(micro): 2828, Batch (considering grad accum): 353,  Loss: 6.4936, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 2829, Batch(micro): 2829, Batch (considering grad accum): 353,  Loss: 6.6541, Time: 3.01s, Token/s: 170.36
Epoch: 0, Step: 2830, Batch(micro): 2830, Batch (considering grad accum): 353,  Loss: 6.8951, Time: 3.19s, Token/s: 160.45
Epoch: 0, Step: 2831, Batch(micro): 2831, Batch (considering grad accum): 353,  Loss: 6.1587, Time: 25.53s, Token/s: 20.05
Epoch: 0, Step: 2832, Batch(micro): 2832, Batch (considering grad accum): 354,  Loss: 6.4171, Time: 7.36s, Token/s: 69.60
Epoch: 0, Step: 2833, Batch(micro): 2833, Batch (considering grad accum): 354,  Loss: 6.6185, Time: 4.17s, Token/s: 122.79
Epoch: 0, Step: 2834, Batch(micro): 2834, Batch (considering grad accum): 354,  Loss: 6.1161, Time: 3.79s, Token/s: 134.95
Epoch: 0, Step: 2835, Batch(micro): 2835, Batch (considering grad accum): 354,  Loss: 6.5148, Time: 3.50s, Token/s: 146.30
Epoch: 0, Step: 2836, Batch(micro): 2836, Batch (considering grad accum): 354,  Loss: 7.0977, Time: 3.50s, Token/s: 146.42
Epoch: 0, Step: 2837, Batch(micro): 2837, Batch (considering grad accum): 354,  Loss: 6.6552, Time: 3.58s, Token/s: 142.88
Epoch: 0, Step: 2838, Batch(micro): 2838, Batch (considering grad accum): 354,  Loss: 6.9144, Time: 3.47s, Token/s: 147.66
Epoch: 0, Step: 2839, Batch(micro): 2839, Batch (considering grad accum): 354,  Loss: 6.6129, Time: 24.63s, Token/s: 20.79
Epoch: 0, Step: 2840, Batch(micro): 2840, Batch (considering grad accum): 355,  Loss: 6.8824, Time: 7.90s, Token/s: 64.79
Epoch: 0, Step: 2841, Batch(micro): 2841, Batch (considering grad accum): 355,  Loss: 7.2003, Time: 3.92s, Token/s: 130.50
Epoch: 0, Step: 2842, Batch(micro): 2842, Batch (considering grad accum): 355,  Loss: 7.2049, Time: 3.53s, Token/s: 144.85
Epoch: 0, Step: 2843, Batch(micro): 2843, Batch (considering grad accum): 355,  Loss: 6.7040, Time: 3.56s, Token/s: 143.95
Epoch: 0, Step: 2844, Batch(micro): 2844, Batch (considering grad accum): 355,  Loss: 7.0011, Time: 3.35s, Token/s: 152.94
Epoch: 0, Step: 2845, Batch(micro): 2845, Batch (considering grad accum): 355,  Loss: 6.8157, Time: 3.23s, Token/s: 158.46
Epoch: 0, Step: 2846, Batch(micro): 2846, Batch (considering grad accum): 355,  Loss: 7.2455, Time: 2.93s, Token/s: 175.00
Epoch: 0, Step: 2847, Batch(micro): 2847, Batch (considering grad accum): 355,  Loss: 7.3446, Time: 23.74s, Token/s: 21.56
Epoch: 0, Step: 2848, Batch(micro): 2848, Batch (considering grad accum): 356,  Loss: 6.8215, Time: 7.10s, Token/s: 72.13
Epoch: 0, Step: 2849, Batch(micro): 2849, Batch (considering grad accum): 356,  Loss: 6.4916, Time: 3.77s, Token/s: 135.65
Epoch: 0, Step: 2850, Batch(micro): 2850, Batch (considering grad accum): 356,  Loss: 6.3259, Time: 3.30s, Token/s: 155.37
Epoch: 0, Step: 2851, Batch(micro): 2851, Batch (considering grad accum): 356,  Loss: 6.9100, Time: 3.37s, Token/s: 151.87
Epoch: 0, Step: 2852, Batch(micro): 2852, Batch (considering grad accum): 356,  Loss: 6.7925, Time: 3.10s, Token/s: 165.04
Epoch: 0, Step: 2853, Batch(micro): 2853, Batch (considering grad accum): 356,  Loss: 6.9974, Time: 2.94s, Token/s: 174.25
Epoch: 0, Step: 2854, Batch(micro): 2854, Batch (considering grad accum): 356,  Loss: 6.8637, Time: 2.93s, Token/s: 174.96
Epoch: 0, Step: 2855, Batch(micro): 2855, Batch (considering grad accum): 356,  Loss: 6.8053, Time: 22.19s, Token/s: 23.08
Epoch: 0, Step: 2856, Batch(micro): 2856, Batch (considering grad accum): 357,  Loss: 6.5862, Time: 6.63s, Token/s: 77.23
Epoch: 0, Step: 2857, Batch(micro): 2857, Batch (considering grad accum): 357,  Loss: 5.9662, Time: 3.64s, Token/s: 140.56
Epoch: 0, Step: 2858, Batch(micro): 2858, Batch (considering grad accum): 357,  Loss: 6.6366, Time: 3.20s, Token/s: 160.20
Epoch: 0, Step: 2859, Batch(micro): 2859, Batch (considering grad accum): 357,  Loss: 7.1424, Time: 3.13s, Token/s: 163.48
Epoch: 0, Step: 2860, Batch(micro): 2860, Batch (considering grad accum): 357,  Loss: 6.6317, Time: 3.23s, Token/s: 158.41
Epoch: 0, Step: 2861, Batch(micro): 2861, Batch (considering grad accum): 357,  Loss: 7.2400, Time: 3.35s, Token/s: 152.81
Epoch: 0, Step: 2862, Batch(micro): 2862, Batch (considering grad accum): 357,  Loss: 6.7002, Time: 2.98s, Token/s: 171.99
Epoch: 0, Step: 2863, Batch(micro): 2863, Batch (considering grad accum): 357,  Loss: 6.1228, Time: 24.67s, Token/s: 20.76
Epoch: 0, Step: 2864, Batch(micro): 2864, Batch (considering grad accum): 358,  Loss: 6.2821, Time: 7.24s, Token/s: 70.68
Epoch: 0, Step: 2865, Batch(micro): 2865, Batch (considering grad accum): 358,  Loss: 6.4088, Time: 4.93s, Token/s: 103.89
Epoch: 0, Step: 2866, Batch(micro): 2866, Batch (considering grad accum): 358,  Loss: 7.0944, Time: 3.48s, Token/s: 146.95
Epoch: 0, Step: 2867, Batch(micro): 2867, Batch (considering grad accum): 358,  Loss: 7.3133, Time: 3.02s, Token/s: 169.52
Epoch: 0, Step: 2868, Batch(micro): 2868, Batch (considering grad accum): 358,  Loss: 6.8304, Time: 2.88s, Token/s: 177.50
Epoch: 0, Step: 2869, Batch(micro): 2869, Batch (considering grad accum): 358,  Loss: 7.3272, Time: 3.01s, Token/s: 170.19
Epoch: 0, Step: 2870, Batch(micro): 2870, Batch (considering grad accum): 358,  Loss: 6.4555, Time: 3.30s, Token/s: 154.96
Epoch: 0, Step: 2871, Batch(micro): 2871, Batch (considering grad accum): 358,  Loss: 7.0508, Time: 26.13s, Token/s: 19.59
Epoch: 0, Step: 2872, Batch(micro): 2872, Batch (considering grad accum): 359,  Loss: 6.8994, Time: 8.00s, Token/s: 64.04
Epoch: 0, Step: 2873, Batch(micro): 2873, Batch (considering grad accum): 359,  Loss: 7.2486, Time: 3.66s, Token/s: 139.76
Epoch: 0, Step: 2874, Batch(micro): 2874, Batch (considering grad accum): 359,  Loss: 6.8809, Time: 2.94s, Token/s: 174.16
Epoch: 0, Step: 2875, Batch(micro): 2875, Batch (considering grad accum): 359,  Loss: 6.6582, Time: 3.06s, Token/s: 167.35
Epoch: 0, Step: 2876, Batch(micro): 2876, Batch (considering grad accum): 359,  Loss: 6.6606, Time: 3.04s, Token/s: 168.18
Epoch: 0, Step: 2877, Batch(micro): 2877, Batch (considering grad accum): 359,  Loss: 7.3413, Time: 3.24s, Token/s: 158.22
Epoch: 0, Step: 2878, Batch(micro): 2878, Batch (considering grad accum): 359,  Loss: 6.1695, Time: 3.14s, Token/s: 163.06
Epoch: 0, Step: 2879, Batch(micro): 2879, Batch (considering grad accum): 359,  Loss: 6.0749, Time: 22.17s, Token/s: 23.09
Epoch: 0, Step: 2880, Batch(micro): 2880, Batch (considering grad accum): 360,  Loss: 6.8118, Time: 6.99s, Token/s: 73.29
Epoch: 0, Step: 2881, Batch(micro): 2881, Batch (considering grad accum): 360,  Loss: 7.0125, Time: 3.72s, Token/s: 137.70
Epoch: 0, Step: 2882, Batch(micro): 2882, Batch (considering grad accum): 360,  Loss: 7.0688, Time: 3.25s, Token/s: 157.42
Epoch: 0, Step: 2883, Batch(micro): 2883, Batch (considering grad accum): 360,  Loss: 7.8393, Time: 3.06s, Token/s: 167.49
Epoch: 0, Step: 2884, Batch(micro): 2884, Batch (considering grad accum): 360,  Loss: 7.4986, Time: 3.03s, Token/s: 169.14
Epoch: 0, Step: 2885, Batch(micro): 2885, Batch (considering grad accum): 360,  Loss: 6.4871, Time: 2.94s, Token/s: 174.21
Epoch: 0, Step: 2886, Batch(micro): 2886, Batch (considering grad accum): 360,  Loss: 6.8680, Time: 2.91s, Token/s: 175.82
Epoch: 0, Step: 2887, Batch(micro): 2887, Batch (considering grad accum): 360,  Loss: 6.4294, Time: 24.37s, Token/s: 21.01
Epoch: 0, Step: 2888, Batch(micro): 2888, Batch (considering grad accum): 361,  Loss: 6.4708, Time: 8.23s, Token/s: 62.19
Epoch: 0, Step: 2889, Batch(micro): 2889, Batch (considering grad accum): 361,  Loss: 7.4984, Time: 3.35s, Token/s: 152.65
Epoch: 0, Step: 2890, Batch(micro): 2890, Batch (considering grad accum): 361,  Loss: 6.9064, Time: 3.08s, Token/s: 166.29
Epoch: 0, Step: 2891, Batch(micro): 2891, Batch (considering grad accum): 361,  Loss: 6.5431, Time: 3.10s, Token/s: 165.40
Epoch: 0, Step: 2892, Batch(micro): 2892, Batch (considering grad accum): 361,  Loss: 6.3499, Time: 2.89s, Token/s: 177.25
Epoch: 0, Step: 2893, Batch(micro): 2893, Batch (considering grad accum): 361,  Loss: 6.8533, Time: 2.91s, Token/s: 176.10
Epoch: 0, Step: 2894, Batch(micro): 2894, Batch (considering grad accum): 361,  Loss: 6.6685, Time: 2.88s, Token/s: 177.78
Epoch: 0, Step: 2895, Batch(micro): 2895, Batch (considering grad accum): 361,  Loss: 7.6104, Time: 23.92s, Token/s: 21.41
Epoch: 0, Step: 2896, Batch(micro): 2896, Batch (considering grad accum): 362,  Loss: 7.7694, Time: 6.76s, Token/s: 75.74
Epoch: 0, Step: 2897, Batch(micro): 2897, Batch (considering grad accum): 362,  Loss: 7.1666, Time: 4.03s, Token/s: 127.16
Epoch: 0, Step: 2898, Batch(micro): 2898, Batch (considering grad accum): 362,  Loss: 6.5655, Time: 3.37s, Token/s: 151.79
Epoch: 0, Step: 2899, Batch(micro): 2899, Batch (considering grad accum): 362,  Loss: 6.0568, Time: 3.13s, Token/s: 163.34
Updating MLP bias
Epoch: 0, Step: 2900, Batch(micro): 2900, Batch (considering grad accum): 362,  Loss: 6.3296, Time: 3.26s, Token/s: 157.20
Epoch: 0, Step: 2901, Batch(micro): 2901, Batch (considering grad accum): 362,  Loss: 6.7032, Time: 3.15s, Token/s: 162.29
Epoch: 0, Step: 2902, Batch(micro): 2902, Batch (considering grad accum): 362,  Loss: 6.9929, Time: 2.89s, Token/s: 176.96
Epoch: 0, Step: 2903, Batch(micro): 2903, Batch (considering grad accum): 362,  Loss: 6.9105, Time: 21.49s, Token/s: 23.82
Epoch: 0, Step: 2904, Batch(micro): 2904, Batch (considering grad accum): 363,  Loss: 6.7772, Time: 6.26s, Token/s: 81.75
Epoch: 0, Step: 2905, Batch(micro): 2905, Batch (considering grad accum): 363,  Loss: 6.7503, Time: 3.78s, Token/s: 135.63
Epoch: 0, Step: 2906, Batch(micro): 2906, Batch (considering grad accum): 363,  Loss: 7.1697, Time: 3.23s, Token/s: 158.53
Epoch: 0, Step: 2907, Batch(micro): 2907, Batch (considering grad accum): 363,  Loss: 7.2762, Time: 3.36s, Token/s: 152.19
Epoch: 0, Step: 2908, Batch(micro): 2908, Batch (considering grad accum): 363,  Loss: 7.3980, Time: 3.83s, Token/s: 133.64
Epoch: 0, Step: 2909, Batch(micro): 2909, Batch (considering grad accum): 363,  Loss: 7.0667, Time: 3.37s, Token/s: 151.77
Epoch: 0, Step: 2910, Batch(micro): 2910, Batch (considering grad accum): 363,  Loss: 6.4043, Time: 2.94s, Token/s: 174.17
Epoch: 0, Step: 2911, Batch(micro): 2911, Batch (considering grad accum): 363,  Loss: 6.9817, Time: 25.31s, Token/s: 20.23
Epoch: 0, Step: 2912, Batch(micro): 2912, Batch (considering grad accum): 364,  Loss: 7.3900, Time: 7.43s, Token/s: 68.94
Epoch: 0, Step: 2913, Batch(micro): 2913, Batch (considering grad accum): 364,  Loss: 7.3638, Time: 3.87s, Token/s: 132.17
Epoch: 0, Step: 2914, Batch(micro): 2914, Batch (considering grad accum): 364,  Loss: 6.8127, Time: 3.06s, Token/s: 167.35
Epoch: 0, Step: 2915, Batch(micro): 2915, Batch (considering grad accum): 364,  Loss: 7.4805, Time: 3.30s, Token/s: 155.27
Epoch: 0, Step: 2916, Batch(micro): 2916, Batch (considering grad accum): 364,  Loss: 7.8251, Time: 3.36s, Token/s: 152.61
Epoch: 0, Step: 2917, Batch(micro): 2917, Batch (considering grad accum): 364,  Loss: 6.9464, Time: 3.35s, Token/s: 152.77
Epoch: 0, Step: 2918, Batch(micro): 2918, Batch (considering grad accum): 364,  Loss: 6.5860, Time: 3.13s, Token/s: 163.75
Epoch: 0, Step: 2919, Batch(micro): 2919, Batch (considering grad accum): 364,  Loss: 6.5889, Time: 23.91s, Token/s: 21.41
Epoch: 0, Step: 2920, Batch(micro): 2920, Batch (considering grad accum): 365,  Loss: 7.2078, Time: 9.00s, Token/s: 56.88
Epoch: 0, Step: 2921, Batch(micro): 2921, Batch (considering grad accum): 365,  Loss: 6.5470, Time: 3.96s, Token/s: 129.39
Epoch: 0, Step: 2922, Batch(micro): 2922, Batch (considering grad accum): 365,  Loss: 6.8100, Time: 3.55s, Token/s: 144.21
Epoch: 0, Step: 2923, Batch(micro): 2923, Batch (considering grad accum): 365,  Loss: 6.6064, Time: 3.04s, Token/s: 168.18
Epoch: 0, Step: 2924, Batch(micro): 2924, Batch (considering grad accum): 365,  Loss: 7.2057, Time: 3.89s, Token/s: 131.74
Epoch: 0, Step: 2925, Batch(micro): 2925, Batch (considering grad accum): 365,  Loss: 6.6150, Time: 3.31s, Token/s: 154.55
Epoch: 0, Step: 2926, Batch(micro): 2926, Batch (considering grad accum): 365,  Loss: 6.6431, Time: 3.17s, Token/s: 161.36
Epoch: 0, Step: 2927, Batch(micro): 2927, Batch (considering grad accum): 365,  Loss: 6.7284, Time: 22.07s, Token/s: 23.20
Epoch: 0, Step: 2928, Batch(micro): 2928, Batch (considering grad accum): 366,  Loss: 7.1290, Time: 6.83s, Token/s: 74.98
Epoch: 0, Step: 2929, Batch(micro): 2929, Batch (considering grad accum): 366,  Loss: 7.0040, Time: 3.77s, Token/s: 135.87
Epoch: 0, Step: 2930, Batch(micro): 2930, Batch (considering grad accum): 366,  Loss: 6.7838, Time: 3.55s, Token/s: 144.40
Epoch: 0, Step: 2931, Batch(micro): 2931, Batch (considering grad accum): 366,  Loss: 6.2772, Time: 3.58s, Token/s: 143.06
Epoch: 0, Step: 2932, Batch(micro): 2932, Batch (considering grad accum): 366,  Loss: 6.5507, Time: 3.31s, Token/s: 154.52
Epoch: 0, Step: 2933, Batch(micro): 2933, Batch (considering grad accum): 366,  Loss: 6.8247, Time: 3.43s, Token/s: 149.49
Epoch: 0, Step: 2934, Batch(micro): 2934, Batch (considering grad accum): 366,  Loss: 7.8675, Time: 3.28s, Token/s: 156.17
Epoch: 0, Step: 2935, Batch(micro): 2935, Batch (considering grad accum): 366,  Loss: 6.3443, Time: 23.01s, Token/s: 22.25
Epoch: 0, Step: 2936, Batch(micro): 2936, Batch (considering grad accum): 367,  Loss: 6.6755, Time: 8.15s, Token/s: 62.81
Epoch: 0, Step: 2937, Batch(micro): 2937, Batch (considering grad accum): 367,  Loss: 7.0651, Time: 3.53s, Token/s: 145.01
Epoch: 0, Step: 2938, Batch(micro): 2938, Batch (considering grad accum): 367,  Loss: 6.8737, Time: 3.15s, Token/s: 162.37
Epoch: 0, Step: 2939, Batch(micro): 2939, Batch (considering grad accum): 367,  Loss: 7.1699, Time: 3.41s, Token/s: 150.29
Epoch: 0, Step: 2940, Batch(micro): 2940, Batch (considering grad accum): 367,  Loss: 7.1115, Time: 3.36s, Token/s: 152.40
Epoch: 0, Step: 2941, Batch(micro): 2941, Batch (considering grad accum): 367,  Loss: 7.3512, Time: 3.47s, Token/s: 147.63
Epoch: 0, Step: 2942, Batch(micro): 2942, Batch (considering grad accum): 367,  Loss: 6.3517, Time: 3.01s, Token/s: 169.91
Epoch: 0, Step: 2943, Batch(micro): 2943, Batch (considering grad accum): 367,  Loss: 6.7955, Time: 23.91s, Token/s: 21.41
Epoch: 0, Step: 2944, Batch(micro): 2944, Batch (considering grad accum): 368,  Loss: 6.2859, Time: 8.54s, Token/s: 59.95
Epoch: 0, Step: 2945, Batch(micro): 2945, Batch (considering grad accum): 368,  Loss: 7.0484, Time: 3.58s, Token/s: 143.14
Epoch: 0, Step: 2946, Batch(micro): 2946, Batch (considering grad accum): 368,  Loss: 7.6831, Time: 2.95s, Token/s: 173.74
Epoch: 0, Step: 2947, Batch(micro): 2947, Batch (considering grad accum): 368,  Loss: 6.6930, Time: 2.88s, Token/s: 177.97
Epoch: 0, Step: 2948, Batch(micro): 2948, Batch (considering grad accum): 368,  Loss: 6.9184, Time: 2.94s, Token/s: 174.08
Epoch: 0, Step: 2949, Batch(micro): 2949, Batch (considering grad accum): 368,  Loss: 7.6246, Time: 2.98s, Token/s: 171.83
Epoch: 0, Step: 2950, Batch(micro): 2950, Batch (considering grad accum): 368,  Loss: 7.1554, Time: 3.40s, Token/s: 150.70
Epoch: 0, Step: 2951, Batch(micro): 2951, Batch (considering grad accum): 368,  Loss: 7.0208, Time: 21.93s, Token/s: 23.35
Epoch: 0, Step: 2952, Batch(micro): 2952, Batch (considering grad accum): 369,  Loss: 7.2353, Time: 8.31s, Token/s: 61.61
Epoch: 0, Step: 2953, Batch(micro): 2953, Batch (considering grad accum): 369,  Loss: 6.9252, Time: 3.77s, Token/s: 135.70
Epoch: 0, Step: 2954, Batch(micro): 2954, Batch (considering grad accum): 369,  Loss: 6.8995, Time: 3.07s, Token/s: 166.63
Epoch: 0, Step: 2955, Batch(micro): 2955, Batch (considering grad accum): 369,  Loss: 6.8327, Time: 3.32s, Token/s: 154.45
Epoch: 0, Step: 2956, Batch(micro): 2956, Batch (considering grad accum): 369,  Loss: 7.0908, Time: 3.10s, Token/s: 165.39
Epoch: 0, Step: 2957, Batch(micro): 2957, Batch (considering grad accum): 369,  Loss: 7.0630, Time: 2.94s, Token/s: 174.12
Epoch: 0, Step: 2958, Batch(micro): 2958, Batch (considering grad accum): 369,  Loss: 7.2718, Time: 2.89s, Token/s: 177.29
Epoch: 0, Step: 2959, Batch(micro): 2959, Batch (considering grad accum): 369,  Loss: 6.9343, Time: 22.83s, Token/s: 22.43
Epoch: 0, Step: 2960, Batch(micro): 2960, Batch (considering grad accum): 370,  Loss: 6.7110, Time: 7.70s, Token/s: 66.54
Epoch: 0, Step: 2961, Batch(micro): 2961, Batch (considering grad accum): 370,  Loss: 7.0693, Time: 4.02s, Token/s: 127.46
Epoch: 0, Step: 2962, Batch(micro): 2962, Batch (considering grad accum): 370,  Loss: 7.0881, Time: 3.37s, Token/s: 152.10
Epoch: 0, Step: 2963, Batch(micro): 2963, Batch (considering grad accum): 370,  Loss: 6.6570, Time: 2.91s, Token/s: 175.72
Epoch: 0, Step: 2964, Batch(micro): 2964, Batch (considering grad accum): 370,  Loss: 6.9344, Time: 2.94s, Token/s: 174.14
Epoch: 0, Step: 2965, Batch(micro): 2965, Batch (considering grad accum): 370,  Loss: 6.8553, Time: 2.92s, Token/s: 175.05
Epoch: 0, Step: 2966, Batch(micro): 2966, Batch (considering grad accum): 370,  Loss: 6.9483, Time: 2.93s, Token/s: 174.49
Epoch: 0, Step: 2967, Batch(micro): 2967, Batch (considering grad accum): 370,  Loss: 7.2461, Time: 19.21s, Token/s: 26.65
Epoch: 0, Step: 2968, Batch(micro): 2968, Batch (considering grad accum): 371,  Loss: 6.4575, Time: 5.57s, Token/s: 92.00
Epoch: 0, Step: 2969, Batch(micro): 2969, Batch (considering grad accum): 371,  Loss: 6.7860, Time: 3.95s, Token/s: 129.53
Epoch: 0, Step: 2970, Batch(micro): 2970, Batch (considering grad accum): 371,  Loss: 6.1902, Time: 3.35s, Token/s: 152.80
Epoch: 0, Step: 2971, Batch(micro): 2971, Batch (considering grad accum): 371,  Loss: 6.7821, Time: 3.27s, Token/s: 156.38
Epoch: 0, Step: 2972, Batch(micro): 2972, Batch (considering grad accum): 371,  Loss: 6.7396, Time: 3.23s, Token/s: 158.45
Epoch: 0, Step: 2973, Batch(micro): 2973, Batch (considering grad accum): 371,  Loss: 6.7796, Time: 2.93s, Token/s: 174.80
Epoch: 0, Step: 2974, Batch(micro): 2974, Batch (considering grad accum): 371,  Loss: 7.0109, Time: 2.95s, Token/s: 173.43
Epoch: 0, Step: 2975, Batch(micro): 2975, Batch (considering grad accum): 371,  Loss: 8.2470, Time: 18.33s, Token/s: 27.93
Epoch: 0, Step: 2976, Batch(micro): 2976, Batch (considering grad accum): 372,  Loss: 6.3353, Time: 5.80s, Token/s: 88.25
Epoch: 0, Step: 2977, Batch(micro): 2977, Batch (considering grad accum): 372,  Loss: 7.2310, Time: 3.53s, Token/s: 145.23
Epoch: 0, Step: 2978, Batch(micro): 2978, Batch (considering grad accum): 372,  Loss: 6.7081, Time: 3.36s, Token/s: 152.31
Epoch: 0, Step: 2979, Batch(micro): 2979, Batch (considering grad accum): 372,  Loss: 7.1065, Time: 3.38s, Token/s: 151.42
Epoch: 0, Step: 2980, Batch(micro): 2980, Batch (considering grad accum): 372,  Loss: 6.8533, Time: 3.43s, Token/s: 149.26
Epoch: 0, Step: 2981, Batch(micro): 2981, Batch (considering grad accum): 372,  Loss: 6.6674, Time: 3.18s, Token/s: 161.12
Epoch: 0, Step: 2982, Batch(micro): 2982, Batch (considering grad accum): 372,  Loss: 6.7299, Time: 3.20s, Token/s: 160.24
Epoch: 0, Step: 2983, Batch(micro): 2983, Batch (considering grad accum): 372,  Loss: 7.5479, Time: 18.49s, Token/s: 27.69
Epoch: 0, Step: 2984, Batch(micro): 2984, Batch (considering grad accum): 373,  Loss: 6.9998, Time: 6.43s, Token/s: 79.68
Epoch: 0, Step: 2985, Batch(micro): 2985, Batch (considering grad accum): 373,  Loss: 7.0863, Time: 3.77s, Token/s: 135.77
Epoch: 0, Step: 2986, Batch(micro): 2986, Batch (considering grad accum): 373,  Loss: 6.9200, Time: 3.64s, Token/s: 140.65
Epoch: 0, Step: 2987, Batch(micro): 2987, Batch (considering grad accum): 373,  Loss: 7.4445, Time: 3.49s, Token/s: 146.77
Epoch: 0, Step: 2988, Batch(micro): 2988, Batch (considering grad accum): 373,  Loss: 7.4376, Time: 3.47s, Token/s: 147.64
Epoch: 0, Step: 2989, Batch(micro): 2989, Batch (considering grad accum): 373,  Loss: 7.1863, Time: 3.43s, Token/s: 149.31
Epoch: 0, Step: 2990, Batch(micro): 2990, Batch (considering grad accum): 373,  Loss: 6.7896, Time: 3.22s, Token/s: 158.94
Epoch: 0, Step: 2991, Batch(micro): 2991, Batch (considering grad accum): 373,  Loss: 6.9521, Time: 20.64s, Token/s: 24.81
Epoch: 0, Step: 2992, Batch(micro): 2992, Batch (considering grad accum): 374,  Loss: 7.6184, Time: 7.69s, Token/s: 66.56
Epoch: 0, Step: 2993, Batch(micro): 2993, Batch (considering grad accum): 374,  Loss: 7.0229, Time: 3.90s, Token/s: 131.21
Epoch: 0, Step: 2994, Batch(micro): 2994, Batch (considering grad accum): 374,  Loss: 6.6847, Time: 3.22s, Token/s: 159.04
Epoch: 0, Step: 2995, Batch(micro): 2995, Batch (considering grad accum): 374,  Loss: 6.7877, Time: 3.24s, Token/s: 157.91
Epoch: 0, Step: 2996, Batch(micro): 2996, Batch (considering grad accum): 374,  Loss: 6.1008, Time: 3.09s, Token/s: 165.96
Epoch: 0, Step: 2997, Batch(micro): 2997, Batch (considering grad accum): 374,  Loss: 6.9137, Time: 3.12s, Token/s: 163.87
Epoch: 0, Step: 2998, Batch(micro): 2998, Batch (considering grad accum): 374,  Loss: 6.7928, Time: 3.02s, Token/s: 169.50
Epoch: 0, Step: 2999, Batch(micro): 2999, Batch (considering grad accum): 374,  Loss: 6.2859, Time: 18.51s, Token/s: 27.66
Updating MLP bias
Epoch: 0, Step: 3000, Batch(micro): 3000, Batch (considering grad accum): 375,  Loss: 7.3758, Time: 6.34s, Token/s: 80.81
Saved checkpoint at step 3000
What is Gravity?
**. This process to the importance of high- advanced of the, some ways in our, ensuring you can lead to our that's look for
Epoch: 0, Step: 3001, Batch(micro): 3001, Batch (considering grad accum): 375,  Loss: 7.0000, Time: 17.87s, Token/s: 28.64
Epoch: 0, Step: 3002, Batch(micro): 3002, Batch (considering grad accum): 375,  Loss: 7.5546, Time: 3.30s, Token/s: 155.03
Epoch: 0, Step: 3003, Batch(micro): 3003, Batch (considering grad accum): 375,  Loss: 6.7976, Time: 3.18s, Token/s: 161.07
Epoch: 0, Step: 3004, Batch(micro): 3004, Batch (considering grad accum): 375,  Loss: 6.5098, Time: 2.98s, Token/s: 171.64
Epoch: 0, Step: 3005, Batch(micro): 3005, Batch (considering grad accum): 375,  Loss: 7.3746, Time: 3.01s, Token/s: 169.85
Epoch: 0, Step: 3006, Batch(micro): 3006, Batch (considering grad accum): 375,  Loss: 7.3305, Time: 2.91s, Token/s: 175.67
Epoch: 0, Step: 3007, Batch(micro): 3007, Batch (considering grad accum): 375,  Loss: 6.8472, Time: 24.39s, Token/s: 20.99
Epoch: 0, Step: 3008, Batch(micro): 3008, Batch (considering grad accum): 376,  Loss: 7.0887, Time: 10.68s, Token/s: 47.94
Epoch: 0, Step: 3009, Batch(micro): 3009, Batch (considering grad accum): 376,  Loss: 6.8484, Time: 3.86s, Token/s: 132.75
Epoch: 0, Step: 3010, Batch(micro): 3010, Batch (considering grad accum): 376,  Loss: 6.9247, Time: 3.30s, Token/s: 155.01
Epoch: 0, Step: 3011, Batch(micro): 3011, Batch (considering grad accum): 376,  Loss: 7.3102, Time: 3.50s, Token/s: 146.16
Epoch: 0, Step: 3012, Batch(micro): 3012, Batch (considering grad accum): 376,  Loss: 7.1924, Time: 3.43s, Token/s: 149.29
Epoch: 0, Step: 3013, Batch(micro): 3013, Batch (considering grad accum): 376,  Loss: 7.4219, Time: 3.01s, Token/s: 170.27
Epoch: 0, Step: 3014, Batch(micro): 3014, Batch (considering grad accum): 376,  Loss: 7.6614, Time: 3.26s, Token/s: 156.83
Epoch: 0, Step: 3015, Batch(micro): 3015, Batch (considering grad accum): 376,  Loss: 6.6700, Time: 24.90s, Token/s: 20.56
Epoch: 0, Step: 3016, Batch(micro): 3016, Batch (considering grad accum): 377,  Loss: 6.9140, Time: 9.34s, Token/s: 54.84
Epoch: 0, Step: 3017, Batch(micro): 3017, Batch (considering grad accum): 377,  Loss: 6.3432, Time: 3.81s, Token/s: 134.21
Epoch: 0, Step: 3018, Batch(micro): 3018, Batch (considering grad accum): 377,  Loss: 6.3152, Time: 3.16s, Token/s: 161.79
Epoch: 0, Step: 3019, Batch(micro): 3019, Batch (considering grad accum): 377,  Loss: 6.4061, Time: 3.01s, Token/s: 169.92
Epoch: 0, Step: 3020, Batch(micro): 3020, Batch (considering grad accum): 377,  Loss: 6.9130, Time: 3.00s, Token/s: 170.44
Epoch: 0, Step: 3021, Batch(micro): 3021, Batch (considering grad accum): 377,  Loss: 6.6388, Time: 3.01s, Token/s: 169.86
Epoch: 0, Step: 3022, Batch(micro): 3022, Batch (considering grad accum): 377,  Loss: 6.1334, Time: 2.97s, Token/s: 172.41
Epoch: 0, Step: 3023, Batch(micro): 3023, Batch (considering grad accum): 377,  Loss: 6.7431, Time: 23.71s, Token/s: 21.59
Epoch: 0, Step: 3024, Batch(micro): 3024, Batch (considering grad accum): 378,  Loss: 7.0067, Time: 7.57s, Token/s: 67.67
Epoch: 0, Step: 3025, Batch(micro): 3025, Batch (considering grad accum): 378,  Loss: 6.2828, Time: 4.07s, Token/s: 125.71
Epoch: 0, Step: 3026, Batch(micro): 3026, Batch (considering grad accum): 378,  Loss: 7.3723, Time: 3.39s, Token/s: 151.11
Epoch: 0, Step: 3027, Batch(micro): 3027, Batch (considering grad accum): 378,  Loss: 7.0067, Time: 3.37s, Token/s: 151.78
Epoch: 0, Step: 3028, Batch(micro): 3028, Batch (considering grad accum): 378,  Loss: 6.4341, Time: 3.37s, Token/s: 151.96
Epoch: 0, Step: 3029, Batch(micro): 3029, Batch (considering grad accum): 378,  Loss: 6.8748, Time: 3.20s, Token/s: 160.21
Epoch: 0, Step: 3030, Batch(micro): 3030, Batch (considering grad accum): 378,  Loss: 7.1733, Time: 3.05s, Token/s: 167.96
Epoch: 0, Step: 3031, Batch(micro): 3031, Batch (considering grad accum): 378,  Loss: 6.7494, Time: 24.11s, Token/s: 21.23
Epoch: 0, Step: 3032, Batch(micro): 3032, Batch (considering grad accum): 379,  Loss: 6.6849, Time: 7.10s, Token/s: 72.10
Epoch: 0, Step: 3033, Batch(micro): 3033, Batch (considering grad accum): 379,  Loss: 6.2672, Time: 3.95s, Token/s: 129.77
Epoch: 0, Step: 3034, Batch(micro): 3034, Batch (considering grad accum): 379,  Loss: 6.7460, Time: 3.49s, Token/s: 146.78
Epoch: 0, Step: 3035, Batch(micro): 3035, Batch (considering grad accum): 379,  Loss: 6.6339, Time: 3.29s, Token/s: 155.64
Epoch: 0, Step: 3036, Batch(micro): 3036, Batch (considering grad accum): 379,  Loss: 6.3420, Time: 3.14s, Token/s: 163.09
Epoch: 0, Step: 3037, Batch(micro): 3037, Batch (considering grad accum): 379,  Loss: 7.0352, Time: 3.32s, Token/s: 154.01
Epoch: 0, Step: 3038, Batch(micro): 3038, Batch (considering grad accum): 379,  Loss: 6.9301, Time: 3.73s, Token/s: 137.44
Epoch: 0, Step: 3039, Batch(micro): 3039, Batch (considering grad accum): 379,  Loss: 6.9078, Time: 23.36s, Token/s: 21.92
Epoch: 0, Step: 3040, Batch(micro): 3040, Batch (considering grad accum): 380,  Loss: 6.0881, Time: 7.36s, Token/s: 69.59
Epoch: 0, Step: 3041, Batch(micro): 3041, Batch (considering grad accum): 380,  Loss: 6.8596, Time: 3.66s, Token/s: 140.06
Epoch: 0, Step: 3042, Batch(micro): 3042, Batch (considering grad accum): 380,  Loss: 7.0870, Time: 3.93s, Token/s: 130.15
Epoch: 0, Step: 3043, Batch(micro): 3043, Batch (considering grad accum): 380,  Loss: 6.7485, Time: 3.46s, Token/s: 148.15
Epoch: 0, Step: 3044, Batch(micro): 3044, Batch (considering grad accum): 380,  Loss: 6.7680, Time: 3.54s, Token/s: 144.62
Epoch: 0, Step: 3045, Batch(micro): 3045, Batch (considering grad accum): 380,  Loss: 6.7845, Time: 3.46s, Token/s: 147.79
Epoch: 0, Step: 3046, Batch(micro): 3046, Batch (considering grad accum): 380,  Loss: 7.1035, Time: 3.21s, Token/s: 159.69
Epoch: 0, Step: 3047, Batch(micro): 3047, Batch (considering grad accum): 380,  Loss: 7.3230, Time: 21.93s, Token/s: 23.35
Epoch: 0, Step: 3048, Batch(micro): 3048, Batch (considering grad accum): 381,  Loss: 7.0030, Time: 6.96s, Token/s: 73.55
Epoch: 0, Step: 3049, Batch(micro): 3049, Batch (considering grad accum): 381,  Loss: 7.1630, Time: 3.81s, Token/s: 134.56
Epoch: 0, Step: 3050, Batch(micro): 3050, Batch (considering grad accum): 381,  Loss: 6.5832, Time: 3.44s, Token/s: 148.96
Epoch: 0, Step: 3051, Batch(micro): 3051, Batch (considering grad accum): 381,  Loss: 6.6820, Time: 3.37s, Token/s: 152.02
Epoch: 0, Step: 3052, Batch(micro): 3052, Batch (considering grad accum): 381,  Loss: 6.6043, Time: 3.74s, Token/s: 136.85
Epoch: 0, Step: 3053, Batch(micro): 3053, Batch (considering grad accum): 381,  Loss: 6.8976, Time: 3.16s, Token/s: 161.82
Epoch: 0, Step: 3054, Batch(micro): 3054, Batch (considering grad accum): 381,  Loss: 7.1091, Time: 2.97s, Token/s: 172.61
Epoch: 0, Step: 3055, Batch(micro): 3055, Batch (considering grad accum): 381,  Loss: 7.5616, Time: 23.07s, Token/s: 22.19
Epoch: 0, Step: 3056, Batch(micro): 3056, Batch (considering grad accum): 382,  Loss: 6.9637, Time: 8.22s, Token/s: 62.28
Epoch: 0, Step: 3057, Batch(micro): 3057, Batch (considering grad accum): 382,  Loss: 6.8848, Time: 3.86s, Token/s: 132.67
Epoch: 0, Step: 3058, Batch(micro): 3058, Batch (considering grad accum): 382,  Loss: 7.1141, Time: 3.80s, Token/s: 134.90
Epoch: 0, Step: 3059, Batch(micro): 3059, Batch (considering grad accum): 382,  Loss: 7.4532, Time: 3.58s, Token/s: 142.90
Epoch: 0, Step: 3060, Batch(micro): 3060, Batch (considering grad accum): 382,  Loss: 6.9061, Time: 3.12s, Token/s: 163.97
Epoch: 0, Step: 3061, Batch(micro): 3061, Batch (considering grad accum): 382,  Loss: 7.0463, Time: 3.36s, Token/s: 152.16
Epoch: 0, Step: 3062, Batch(micro): 3062, Batch (considering grad accum): 382,  Loss: 7.3937, Time: 3.39s, Token/s: 150.84
Epoch: 0, Step: 3063, Batch(micro): 3063, Batch (considering grad accum): 382,  Loss: 6.4015, Time: 23.15s, Token/s: 22.12
Epoch: 0, Step: 3064, Batch(micro): 3064, Batch (considering grad accum): 383,  Loss: 6.5485, Time: 8.40s, Token/s: 60.95
Epoch: 0, Step: 3065, Batch(micro): 3065, Batch (considering grad accum): 383,  Loss: 6.9080, Time: 3.83s, Token/s: 133.78
Epoch: 0, Step: 3066, Batch(micro): 3066, Batch (considering grad accum): 383,  Loss: 7.7977, Time: 3.40s, Token/s: 150.72
Epoch: 0, Step: 3067, Batch(micro): 3067, Batch (considering grad accum): 383,  Loss: 6.7351, Time: 3.48s, Token/s: 147.30
Epoch: 0, Step: 3068, Batch(micro): 3068, Batch (considering grad accum): 383,  Loss: 6.8225, Time: 3.22s, Token/s: 159.09
Epoch: 0, Step: 3069, Batch(micro): 3069, Batch (considering grad accum): 383,  Loss: 6.3352, Time: 2.90s, Token/s: 176.30
Epoch: 0, Step: 3070, Batch(micro): 3070, Batch (considering grad accum): 383,  Loss: 5.7818, Time: 2.99s, Token/s: 171.52
Epoch: 0, Step: 3071, Batch(micro): 3071, Batch (considering grad accum): 383,  Loss: 6.2543, Time: 24.02s, Token/s: 21.31
Epoch: 0, Step: 3072, Batch(micro): 3072, Batch (considering grad accum): 384,  Loss: 7.1859, Time: 7.85s, Token/s: 65.21
Epoch: 0, Step: 3073, Batch(micro): 3073, Batch (considering grad accum): 384,  Loss: 6.3782, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 3074, Batch(micro): 3074, Batch (considering grad accum): 384,  Loss: 5.9877, Time: 3.30s, Token/s: 155.07
Epoch: 0, Step: 3075, Batch(micro): 3075, Batch (considering grad accum): 384,  Loss: 6.5251, Time: 3.40s, Token/s: 150.71
Epoch: 0, Step: 3076, Batch(micro): 3076, Batch (considering grad accum): 384,  Loss: 6.6221, Time: 3.39s, Token/s: 151.15
Epoch: 0, Step: 3077, Batch(micro): 3077, Batch (considering grad accum): 384,  Loss: 6.6719, Time: 3.11s, Token/s: 164.61
Epoch: 0, Step: 3078, Batch(micro): 3078, Batch (considering grad accum): 384,  Loss: 6.6939, Time: 3.13s, Token/s: 163.77
Epoch: 0, Step: 3079, Batch(micro): 3079, Batch (considering grad accum): 384,  Loss: 6.8596, Time: 23.14s, Token/s: 22.13
Epoch: 0, Step: 3080, Batch(micro): 3080, Batch (considering grad accum): 385,  Loss: 6.6474, Time: 8.24s, Token/s: 62.15
Epoch: 0, Step: 3081, Batch(micro): 3081, Batch (considering grad accum): 385,  Loss: 6.2395, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 3082, Batch(micro): 3082, Batch (considering grad accum): 385,  Loss: 6.0883, Time: 3.53s, Token/s: 144.93
Epoch: 0, Step: 3083, Batch(micro): 3083, Batch (considering grad accum): 385,  Loss: 6.9615, Time: 3.40s, Token/s: 150.55
Epoch: 0, Step: 3084, Batch(micro): 3084, Batch (considering grad accum): 385,  Loss: 7.2784, Time: 3.28s, Token/s: 156.13
Epoch: 0, Step: 3085, Batch(micro): 3085, Batch (considering grad accum): 385,  Loss: 7.4055, Time: 3.06s, Token/s: 167.37
Epoch: 0, Step: 3086, Batch(micro): 3086, Batch (considering grad accum): 385,  Loss: 7.0500, Time: 3.12s, Token/s: 164.36
Epoch: 0, Step: 3087, Batch(micro): 3087, Batch (considering grad accum): 385,  Loss: 6.5179, Time: 26.01s, Token/s: 19.69
Epoch: 0, Step: 3088, Batch(micro): 3088, Batch (considering grad accum): 386,  Loss: 6.7985, Time: 7.27s, Token/s: 70.45
Epoch: 0, Step: 3089, Batch(micro): 3089, Batch (considering grad accum): 386,  Loss: 6.9258, Time: 3.83s, Token/s: 133.61
Epoch: 0, Step: 3090, Batch(micro): 3090, Batch (considering grad accum): 386,  Loss: 7.2166, Time: 3.16s, Token/s: 161.95
Epoch: 0, Step: 3091, Batch(micro): 3091, Batch (considering grad accum): 386,  Loss: 6.6212, Time: 3.15s, Token/s: 162.67
Epoch: 0, Step: 3092, Batch(micro): 3092, Batch (considering grad accum): 386,  Loss: 7.3274, Time: 3.17s, Token/s: 161.34
Epoch: 0, Step: 3093, Batch(micro): 3093, Batch (considering grad accum): 386,  Loss: 6.1578, Time: 3.01s, Token/s: 169.87
Epoch: 0, Step: 3094, Batch(micro): 3094, Batch (considering grad accum): 386,  Loss: 6.6062, Time: 3.26s, Token/s: 157.27
Epoch: 0, Step: 3095, Batch(micro): 3095, Batch (considering grad accum): 386,  Loss: 6.5119, Time: 24.79s, Token/s: 20.66
Epoch: 0, Step: 3096, Batch(micro): 3096, Batch (considering grad accum): 387,  Loss: 6.3779, Time: 7.50s, Token/s: 68.29
Epoch: 0, Step: 3097, Batch(micro): 3097, Batch (considering grad accum): 387,  Loss: 6.6046, Time: 3.82s, Token/s: 134.08
Epoch: 0, Step: 3098, Batch(micro): 3098, Batch (considering grad accum): 387,  Loss: 6.6151, Time: 4.01s, Token/s: 127.82
Epoch: 0, Step: 3099, Batch(micro): 3099, Batch (considering grad accum): 387,  Loss: 7.0591, Time: 3.80s, Token/s: 134.73
Updating MLP bias
Epoch: 0, Step: 3100, Batch(micro): 3100, Batch (considering grad accum): 387,  Loss: 7.4094, Time: 2.98s, Token/s: 171.96
Epoch: 0, Step: 3101, Batch(micro): 3101, Batch (considering grad accum): 387,  Loss: 7.4804, Time: 2.95s, Token/s: 173.29
Epoch: 0, Step: 3102, Batch(micro): 3102, Batch (considering grad accum): 387,  Loss: 6.9322, Time: 3.46s, Token/s: 148.00
Epoch: 0, Step: 3103, Batch(micro): 3103, Batch (considering grad accum): 387,  Loss: 6.7636, Time: 25.25s, Token/s: 20.28
Epoch: 0, Step: 3104, Batch(micro): 3104, Batch (considering grad accum): 388,  Loss: 6.3742, Time: 7.56s, Token/s: 67.72
Epoch: 0, Step: 3105, Batch(micro): 3105, Batch (considering grad accum): 388,  Loss: 6.9570, Time: 3.89s, Token/s: 131.54
Epoch: 0, Step: 3106, Batch(micro): 3106, Batch (considering grad accum): 388,  Loss: 7.6587, Time: 3.18s, Token/s: 160.99
Epoch: 0, Step: 3107, Batch(micro): 3107, Batch (considering grad accum): 388,  Loss: 6.9433, Time: 3.51s, Token/s: 145.98
Epoch: 0, Step: 3108, Batch(micro): 3108, Batch (considering grad accum): 388,  Loss: 6.4674, Time: 3.60s, Token/s: 142.22
Epoch: 0, Step: 3109, Batch(micro): 3109, Batch (considering grad accum): 388,  Loss: 7.1515, Time: 3.37s, Token/s: 151.90
Epoch: 0, Step: 3110, Batch(micro): 3110, Batch (considering grad accum): 388,  Loss: 6.4419, Time: 2.89s, Token/s: 177.05
Epoch: 0, Step: 3111, Batch(micro): 3111, Batch (considering grad accum): 388,  Loss: 6.9094, Time: 23.19s, Token/s: 22.08
Epoch: 0, Step: 3112, Batch(micro): 3112, Batch (considering grad accum): 389,  Loss: 6.9455, Time: 8.79s, Token/s: 58.27
Epoch: 0, Step: 3113, Batch(micro): 3113, Batch (considering grad accum): 389,  Loss: 8.0296, Time: 4.06s, Token/s: 126.26
Epoch: 0, Step: 3114, Batch(micro): 3114, Batch (considering grad accum): 389,  Loss: 7.8755, Time: 3.44s, Token/s: 148.62
Epoch: 0, Step: 3115, Batch(micro): 3115, Batch (considering grad accum): 389,  Loss: 6.3857, Time: 3.47s, Token/s: 147.68
Epoch: 0, Step: 3116, Batch(micro): 3116, Batch (considering grad accum): 389,  Loss: 6.4136, Time: 3.09s, Token/s: 165.85
Epoch: 0, Step: 3117, Batch(micro): 3117, Batch (considering grad accum): 389,  Loss: 7.0817, Time: 3.00s, Token/s: 170.43
Epoch: 0, Step: 3118, Batch(micro): 3118, Batch (considering grad accum): 389,  Loss: 7.5864, Time: 3.00s, Token/s: 170.64
Epoch: 0, Step: 3119, Batch(micro): 3119, Batch (considering grad accum): 389,  Loss: 7.9799, Time: 21.56s, Token/s: 23.74
Epoch: 0, Step: 3120, Batch(micro): 3120, Batch (considering grad accum): 390,  Loss: 6.5149, Time: 6.75s, Token/s: 75.83
Epoch: 0, Step: 3121, Batch(micro): 3121, Batch (considering grad accum): 390,  Loss: 7.2042, Time: 3.83s, Token/s: 133.84
Epoch: 0, Step: 3122, Batch(micro): 3122, Batch (considering grad accum): 390,  Loss: 6.7202, Time: 3.55s, Token/s: 144.28
Epoch: 0, Step: 3123, Batch(micro): 3123, Batch (considering grad accum): 390,  Loss: 6.7445, Time: 3.37s, Token/s: 151.94
Epoch: 0, Step: 3124, Batch(micro): 3124, Batch (considering grad accum): 390,  Loss: 6.5451, Time: 3.27s, Token/s: 156.55
Epoch: 0, Step: 3125, Batch(micro): 3125, Batch (considering grad accum): 390,  Loss: 6.4141, Time: 3.22s, Token/s: 159.17
Epoch: 0, Step: 3126, Batch(micro): 3126, Batch (considering grad accum): 390,  Loss: 6.5725, Time: 3.23s, Token/s: 158.34
Epoch: 0, Step: 3127, Batch(micro): 3127, Batch (considering grad accum): 390,  Loss: 6.5142, Time: 21.89s, Token/s: 23.39
Epoch: 0, Step: 3128, Batch(micro): 3128, Batch (considering grad accum): 391,  Loss: 8.0986, Time: 8.12s, Token/s: 63.09
Epoch: 0, Step: 3129, Batch(micro): 3129, Batch (considering grad accum): 391,  Loss: 6.6637, Time: 3.65s, Token/s: 140.13
Epoch: 0, Step: 3130, Batch(micro): 3130, Batch (considering grad accum): 391,  Loss: 6.5982, Time: 2.88s, Token/s: 177.83
Epoch: 0, Step: 3131, Batch(micro): 3131, Batch (considering grad accum): 391,  Loss: 6.6157, Time: 3.03s, Token/s: 168.77
Epoch: 0, Step: 3132, Batch(micro): 3132, Batch (considering grad accum): 391,  Loss: 6.3047, Time: 3.39s, Token/s: 151.10
Epoch: 0, Step: 3133, Batch(micro): 3133, Batch (considering grad accum): 391,  Loss: 6.3328, Time: 3.26s, Token/s: 157.20
Epoch: 0, Step: 3134, Batch(micro): 3134, Batch (considering grad accum): 391,  Loss: 6.7631, Time: 2.91s, Token/s: 176.19
Epoch: 0, Step: 3135, Batch(micro): 3135, Batch (considering grad accum): 391,  Loss: 6.2504, Time: 23.06s, Token/s: 22.20
Epoch: 0, Step: 3136, Batch(micro): 3136, Batch (considering grad accum): 392,  Loss: 6.5989, Time: 6.48s, Token/s: 78.98
Epoch: 0, Step: 3137, Batch(micro): 3137, Batch (considering grad accum): 392,  Loss: 7.1883, Time: 3.67s, Token/s: 139.62
Epoch: 0, Step: 3138, Batch(micro): 3138, Batch (considering grad accum): 392,  Loss: 6.9167, Time: 3.65s, Token/s: 140.26
Epoch: 0, Step: 3139, Batch(micro): 3139, Batch (considering grad accum): 392,  Loss: 7.0815, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 3140, Batch(micro): 3140, Batch (considering grad accum): 392,  Loss: 6.4466, Time: 2.94s, Token/s: 174.12
Epoch: 0, Step: 3141, Batch(micro): 3141, Batch (considering grad accum): 392,  Loss: 6.0894, Time: 2.96s, Token/s: 173.10
Epoch: 0, Step: 3142, Batch(micro): 3142, Batch (considering grad accum): 392,  Loss: 6.1876, Time: 3.21s, Token/s: 159.43
Epoch: 0, Step: 3143, Batch(micro): 3143, Batch (considering grad accum): 392,  Loss: 6.4183, Time: 25.72s, Token/s: 19.90
Epoch: 0, Step: 3144, Batch(micro): 3144, Batch (considering grad accum): 393,  Loss: 6.7921, Time: 7.11s, Token/s: 71.96
Epoch: 0, Step: 3145, Batch(micro): 3145, Batch (considering grad accum): 393,  Loss: 6.6881, Time: 3.84s, Token/s: 133.33
Epoch: 0, Step: 3146, Batch(micro): 3146, Batch (considering grad accum): 393,  Loss: 6.4951, Time: 3.81s, Token/s: 134.26
Epoch: 0, Step: 3147, Batch(micro): 3147, Batch (considering grad accum): 393,  Loss: 6.9965, Time: 3.34s, Token/s: 153.22
Epoch: 0, Step: 3148, Batch(micro): 3148, Batch (considering grad accum): 393,  Loss: 7.6131, Time: 3.13s, Token/s: 163.53
Epoch: 0, Step: 3149, Batch(micro): 3149, Batch (considering grad accum): 393,  Loss: 6.4946, Time: 3.23s, Token/s: 158.65
Epoch: 0, Step: 3150, Batch(micro): 3150, Batch (considering grad accum): 393,  Loss: 6.7283, Time: 3.14s, Token/s: 162.93
Epoch: 0, Step: 3151, Batch(micro): 3151, Batch (considering grad accum): 393,  Loss: 8.3264, Time: 25.33s, Token/s: 20.21
Epoch: 0, Step: 3152, Batch(micro): 3152, Batch (considering grad accum): 394,  Loss: 6.7484, Time: 7.39s, Token/s: 69.25
Epoch: 0, Step: 3153, Batch(micro): 3153, Batch (considering grad accum): 394,  Loss: 6.6777, Time: 3.59s, Token/s: 142.71
Epoch: 0, Step: 3154, Batch(micro): 3154, Batch (considering grad accum): 394,  Loss: 6.8439, Time: 2.88s, Token/s: 177.88
Epoch: 0, Step: 3155, Batch(micro): 3155, Batch (considering grad accum): 394,  Loss: 6.5182, Time: 3.01s, Token/s: 170.25
Epoch: 0, Step: 3156, Batch(micro): 3156, Batch (considering grad accum): 394,  Loss: 6.2801, Time: 3.00s, Token/s: 170.44
Epoch: 0, Step: 3157, Batch(micro): 3157, Batch (considering grad accum): 394,  Loss: 6.8902, Time: 3.38s, Token/s: 151.61
Epoch: 0, Step: 3158, Batch(micro): 3158, Batch (considering grad accum): 394,  Loss: 6.9354, Time: 3.22s, Token/s: 159.10
Epoch: 0, Step: 3159, Batch(micro): 3159, Batch (considering grad accum): 394,  Loss: 6.4960, Time: 19.43s, Token/s: 26.35
Epoch: 0, Step: 3160, Batch(micro): 3160, Batch (considering grad accum): 395,  Loss: 6.8642, Time: 6.84s, Token/s: 74.83
Epoch: 0, Step: 3161, Batch(micro): 3161, Batch (considering grad accum): 395,  Loss: 6.3499, Time: 3.46s, Token/s: 147.99
Epoch: 0, Step: 3162, Batch(micro): 3162, Batch (considering grad accum): 395,  Loss: 6.6789, Time: 2.94s, Token/s: 174.17
Epoch: 0, Step: 3163, Batch(micro): 3163, Batch (considering grad accum): 395,  Loss: 7.1430, Time: 2.97s, Token/s: 172.56
Epoch: 0, Step: 3164, Batch(micro): 3164, Batch (considering grad accum): 395,  Loss: 6.5237, Time: 2.86s, Token/s: 178.89
Epoch: 0, Step: 3165, Batch(micro): 3165, Batch (considering grad accum): 395,  Loss: 6.2516, Time: 2.92s, Token/s: 175.62
Epoch: 0, Step: 3166, Batch(micro): 3166, Batch (considering grad accum): 395,  Loss: 6.7256, Time: 3.07s, Token/s: 166.90
Epoch: 0, Step: 3167, Batch(micro): 3167, Batch (considering grad accum): 395,  Loss: 6.5495, Time: 18.66s, Token/s: 27.43
Epoch: 0, Step: 3168, Batch(micro): 3168, Batch (considering grad accum): 396,  Loss: 6.9871, Time: 5.94s, Token/s: 86.23
Epoch: 0, Step: 3169, Batch(micro): 3169, Batch (considering grad accum): 396,  Loss: 6.7335, Time: 3.81s, Token/s: 134.26
Epoch: 0, Step: 3170, Batch(micro): 3170, Batch (considering grad accum): 396,  Loss: 7.2615, Time: 3.04s, Token/s: 168.68
Epoch: 0, Step: 3171, Batch(micro): 3171, Batch (considering grad accum): 396,  Loss: 6.6870, Time: 3.04s, Token/s: 168.33
Epoch: 0, Step: 3172, Batch(micro): 3172, Batch (considering grad accum): 396,  Loss: 6.9722, Time: 3.26s, Token/s: 157.13
Epoch: 0, Step: 3173, Batch(micro): 3173, Batch (considering grad accum): 396,  Loss: 6.8757, Time: 3.45s, Token/s: 148.28
Epoch: 0, Step: 3174, Batch(micro): 3174, Batch (considering grad accum): 396,  Loss: 6.1547, Time: 3.06s, Token/s: 167.45
Epoch: 0, Step: 3175, Batch(micro): 3175, Batch (considering grad accum): 396,  Loss: 6.5185, Time: 18.43s, Token/s: 27.78
Epoch: 0, Step: 3176, Batch(micro): 3176, Batch (considering grad accum): 397,  Loss: 7.0393, Time: 6.42s, Token/s: 79.80
Epoch: 0, Step: 3177, Batch(micro): 3177, Batch (considering grad accum): 397,  Loss: 6.6833, Time: 3.58s, Token/s: 142.93
Epoch: 0, Step: 3178, Batch(micro): 3178, Batch (considering grad accum): 397,  Loss: 6.6589, Time: 3.72s, Token/s: 137.66
Epoch: 0, Step: 3179, Batch(micro): 3179, Batch (considering grad accum): 397,  Loss: 6.7617, Time: 3.35s, Token/s: 152.93
Epoch: 0, Step: 3180, Batch(micro): 3180, Batch (considering grad accum): 397,  Loss: 7.8015, Time: 3.31s, Token/s: 154.72
Epoch: 0, Step: 3181, Batch(micro): 3181, Batch (considering grad accum): 397,  Loss: 7.0380, Time: 3.27s, Token/s: 156.46
Epoch: 0, Step: 3182, Batch(micro): 3182, Batch (considering grad accum): 397,  Loss: 8.1243, Time: 3.17s, Token/s: 161.44
Epoch: 0, Step: 3183, Batch(micro): 3183, Batch (considering grad accum): 397,  Loss: 6.4970, Time: 17.56s, Token/s: 29.16
Epoch: 0, Step: 3184, Batch(micro): 3184, Batch (considering grad accum): 398,  Loss: 6.1908, Time: 5.78s, Token/s: 88.53
Epoch: 0, Step: 3185, Batch(micro): 3185, Batch (considering grad accum): 398,  Loss: 6.6131, Time: 3.75s, Token/s: 136.39
Epoch: 0, Step: 3186, Batch(micro): 3186, Batch (considering grad accum): 398,  Loss: 6.5238, Time: 3.66s, Token/s: 140.06
Epoch: 0, Step: 3187, Batch(micro): 3187, Batch (considering grad accum): 398,  Loss: 6.4378, Time: 3.30s, Token/s: 155.22
Epoch: 0, Step: 3188, Batch(micro): 3188, Batch (considering grad accum): 398,  Loss: 6.9302, Time: 3.37s, Token/s: 151.73
Epoch: 0, Step: 3189, Batch(micro): 3189, Batch (considering grad accum): 398,  Loss: 7.1754, Time: 3.19s, Token/s: 160.28
Epoch: 0, Step: 3190, Batch(micro): 3190, Batch (considering grad accum): 398,  Loss: 7.0032, Time: 3.03s, Token/s: 169.10
Epoch: 0, Step: 3191, Batch(micro): 3191, Batch (considering grad accum): 398,  Loss: 6.5764, Time: 18.49s, Token/s: 27.69
Epoch: 0, Step: 3192, Batch(micro): 3192, Batch (considering grad accum): 399,  Loss: 6.7078, Time: 6.17s, Token/s: 82.93
Epoch: 0, Step: 3193, Batch(micro): 3193, Batch (considering grad accum): 399,  Loss: 6.8802, Time: 3.48s, Token/s: 147.03
Epoch: 0, Step: 3194, Batch(micro): 3194, Batch (considering grad accum): 399,  Loss: 6.9807, Time: 3.38s, Token/s: 151.36
Epoch: 0, Step: 3195, Batch(micro): 3195, Batch (considering grad accum): 399,  Loss: 6.1760, Time: 3.51s, Token/s: 145.71
Epoch: 0, Step: 3196, Batch(micro): 3196, Batch (considering grad accum): 399,  Loss: 6.5870, Time: 3.45s, Token/s: 148.54
Epoch: 0, Step: 3197, Batch(micro): 3197, Batch (considering grad accum): 399,  Loss: 6.1481, Time: 3.19s, Token/s: 160.44
Epoch: 0, Step: 3198, Batch(micro): 3198, Batch (considering grad accum): 399,  Loss: 6.4223, Time: 3.31s, Token/s: 154.49
Epoch: 0, Step: 3199, Batch(micro): 3199, Batch (considering grad accum): 399,  Loss: 7.0710, Time: 20.73s, Token/s: 24.70
Updating MLP bias
Epoch: 0, Step: 3200, Batch(micro): 3200, Batch (considering grad accum): 400,  Loss: 6.6366, Time: 6.95s, Token/s: 73.65
Epoch: 0, Step: 3201, Batch(micro): 3201, Batch (considering grad accum): 400,  Loss: 6.2961, Time: 3.69s, Token/s: 138.72
Epoch: 0, Step: 3202, Batch(micro): 3202, Batch (considering grad accum): 400,  Loss: 6.4364, Time: 3.29s, Token/s: 155.77
Epoch: 0, Step: 3203, Batch(micro): 3203, Batch (considering grad accum): 400,  Loss: 7.5737, Time: 3.43s, Token/s: 149.10
Epoch: 0, Step: 3204, Batch(micro): 3204, Batch (considering grad accum): 400,  Loss: 7.5551, Time: 3.33s, Token/s: 153.60
Epoch: 0, Step: 3205, Batch(micro): 3205, Batch (considering grad accum): 400,  Loss: 8.0945, Time: 3.07s, Token/s: 166.77
Epoch: 0, Step: 3206, Batch(micro): 3206, Batch (considering grad accum): 400,  Loss: 6.7908, Time: 3.03s, Token/s: 168.72
Epoch: 0, Step: 3207, Batch(micro): 3207, Batch (considering grad accum): 400,  Loss: 7.0632, Time: 17.73s, Token/s: 28.88
Epoch: 0, Step: 3208, Batch(micro): 3208, Batch (considering grad accum): 401,  Loss: 6.6637, Time: 6.54s, Token/s: 78.29
Epoch: 0, Step: 3209, Batch(micro): 3209, Batch (considering grad accum): 401,  Loss: 6.8904, Time: 4.03s, Token/s: 127.16
Epoch: 0, Step: 3210, Batch(micro): 3210, Batch (considering grad accum): 401,  Loss: 7.1969, Time: 3.57s, Token/s: 143.60
Epoch: 0, Step: 3211, Batch(micro): 3211, Batch (considering grad accum): 401,  Loss: 6.6091, Time: 3.08s, Token/s: 166.26
Epoch: 0, Step: 3212, Batch(micro): 3212, Batch (considering grad accum): 401,  Loss: 7.0704, Time: 3.02s, Token/s: 169.49
Epoch: 0, Step: 3213, Batch(micro): 3213, Batch (considering grad accum): 401,  Loss: 7.6040, Time: 2.96s, Token/s: 172.87
Epoch: 0, Step: 3214, Batch(micro): 3214, Batch (considering grad accum): 401,  Loss: 7.5504, Time: 3.07s, Token/s: 166.88
Epoch: 0, Step: 3215, Batch(micro): 3215, Batch (considering grad accum): 401,  Loss: 7.1126, Time: 17.80s, Token/s: 28.76
Epoch: 0, Step: 3216, Batch(micro): 3216, Batch (considering grad accum): 402,  Loss: 6.2615, Time: 6.51s, Token/s: 78.66
Epoch: 0, Step: 3217, Batch(micro): 3217, Batch (considering grad accum): 402,  Loss: 6.2619, Time: 3.42s, Token/s: 149.89
Epoch: 0, Step: 3218, Batch(micro): 3218, Batch (considering grad accum): 402,  Loss: 6.3036, Time: 3.34s, Token/s: 153.40
Epoch: 0, Step: 3219, Batch(micro): 3219, Batch (considering grad accum): 402,  Loss: 6.6016, Time: 3.29s, Token/s: 155.67
Epoch: 0, Step: 3220, Batch(micro): 3220, Batch (considering grad accum): 402,  Loss: 7.3014, Time: 3.54s, Token/s: 144.68
Epoch: 0, Step: 3221, Batch(micro): 3221, Batch (considering grad accum): 402,  Loss: 7.8066, Time: 3.30s, Token/s: 155.15
Epoch: 0, Step: 3222, Batch(micro): 3222, Batch (considering grad accum): 402,  Loss: 7.1347, Time: 3.75s, Token/s: 136.48
Epoch: 0, Step: 3223, Batch(micro): 3223, Batch (considering grad accum): 402,  Loss: 7.2730, Time: 18.56s, Token/s: 27.59
Epoch: 0, Step: 3224, Batch(micro): 3224, Batch (considering grad accum): 403,  Loss: 7.2207, Time: 7.04s, Token/s: 72.68
Epoch: 0, Step: 3225, Batch(micro): 3225, Batch (considering grad accum): 403,  Loss: 6.3986, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 3226, Batch(micro): 3226, Batch (considering grad accum): 403,  Loss: 6.4153, Time: 3.69s, Token/s: 138.69
Epoch: 0, Step: 3227, Batch(micro): 3227, Batch (considering grad accum): 403,  Loss: 7.1922, Time: 3.27s, Token/s: 156.45
Epoch: 0, Step: 3228, Batch(micro): 3228, Batch (considering grad accum): 403,  Loss: 6.2167, Time: 3.33s, Token/s: 153.93
Epoch: 0, Step: 3229, Batch(micro): 3229, Batch (considering grad accum): 403,  Loss: 6.0899, Time: 3.59s, Token/s: 142.62
Epoch: 0, Step: 3230, Batch(micro): 3230, Batch (considering grad accum): 403,  Loss: 6.1710, Time: 4.29s, Token/s: 119.39
Epoch: 0, Step: 3231, Batch(micro): 3231, Batch (considering grad accum): 403,  Loss: 5.8995, Time: 21.31s, Token/s: 24.03
Epoch: 0, Step: 3232, Batch(micro): 3232, Batch (considering grad accum): 404,  Loss: 6.7482, Time: 7.16s, Token/s: 71.53
Epoch: 0, Step: 3233, Batch(micro): 3233, Batch (considering grad accum): 404,  Loss: 7.3007, Time: 3.75s, Token/s: 136.71
Epoch: 0, Step: 3234, Batch(micro): 3234, Batch (considering grad accum): 404,  Loss: 6.8277, Time: 3.20s, Token/s: 159.99
Epoch: 0, Step: 3235, Batch(micro): 3235, Batch (considering grad accum): 404,  Loss: 6.2811, Time: 3.23s, Token/s: 158.55
Epoch: 0, Step: 3236, Batch(micro): 3236, Batch (considering grad accum): 404,  Loss: 6.6298, Time: 3.23s, Token/s: 158.43
Epoch: 0, Step: 3237, Batch(micro): 3237, Batch (considering grad accum): 404,  Loss: 6.6266, Time: 3.20s, Token/s: 160.13
Epoch: 0, Step: 3238, Batch(micro): 3238, Batch (considering grad accum): 404,  Loss: 6.3888, Time: 3.21s, Token/s: 159.71
Epoch: 0, Step: 3239, Batch(micro): 3239, Batch (considering grad accum): 404,  Loss: 6.4805, Time: 20.09s, Token/s: 25.49
Epoch: 0, Step: 3240, Batch(micro): 3240, Batch (considering grad accum): 405,  Loss: 7.0239, Time: 7.95s, Token/s: 64.36
Epoch: 0, Step: 3241, Batch(micro): 3241, Batch (considering grad accum): 405,  Loss: 7.1199, Time: 3.84s, Token/s: 133.23
Epoch: 0, Step: 3242, Batch(micro): 3242, Batch (considering grad accum): 405,  Loss: 6.9532, Time: 3.57s, Token/s: 143.36
Epoch: 0, Step: 3243, Batch(micro): 3243, Batch (considering grad accum): 405,  Loss: 6.7012, Time: 3.40s, Token/s: 150.58
Epoch: 0, Step: 3244, Batch(micro): 3244, Batch (considering grad accum): 405,  Loss: 7.1234, Time: 3.92s, Token/s: 130.70
Epoch: 0, Step: 3245, Batch(micro): 3245, Batch (considering grad accum): 405,  Loss: 7.4702, Time: 3.67s, Token/s: 139.60
Epoch: 0, Step: 3246, Batch(micro): 3246, Batch (considering grad accum): 405,  Loss: 6.4220, Time: 3.55s, Token/s: 144.28
Epoch: 0, Step: 3247, Batch(micro): 3247, Batch (considering grad accum): 405,  Loss: 7.0941, Time: 19.39s, Token/s: 26.41
Epoch: 0, Step: 3248, Batch(micro): 3248, Batch (considering grad accum): 406,  Loss: 6.0916, Time: 7.55s, Token/s: 67.81
Epoch: 0, Step: 3249, Batch(micro): 3249, Batch (considering grad accum): 406,  Loss: 6.9242, Time: 3.80s, Token/s: 134.70
Epoch: 0, Step: 3250, Batch(micro): 3250, Batch (considering grad accum): 406,  Loss: 6.7851, Time: 3.58s, Token/s: 142.84
Epoch: 0, Step: 3251, Batch(micro): 3251, Batch (considering grad accum): 406,  Loss: 6.9307, Time: 3.65s, Token/s: 140.19
Epoch: 0, Step: 3252, Batch(micro): 3252, Batch (considering grad accum): 406,  Loss: 6.9531, Time: 3.75s, Token/s: 136.53
Epoch: 0, Step: 3253, Batch(micro): 3253, Batch (considering grad accum): 406,  Loss: 7.2300, Time: 3.57s, Token/s: 143.44
Epoch: 0, Step: 3254, Batch(micro): 3254, Batch (considering grad accum): 406,  Loss: 7.0852, Time: 3.27s, Token/s: 156.80
Epoch: 0, Step: 3255, Batch(micro): 3255, Batch (considering grad accum): 406,  Loss: 7.2030, Time: 21.98s, Token/s: 23.30
Epoch: 0, Step: 3256, Batch(micro): 3256, Batch (considering grad accum): 407,  Loss: 6.4252, Time: 7.06s, Token/s: 72.56
Epoch: 0, Step: 3257, Batch(micro): 3257, Batch (considering grad accum): 407,  Loss: 6.4513, Time: 4.26s, Token/s: 120.18
Epoch: 0, Step: 3258, Batch(micro): 3258, Batch (considering grad accum): 407,  Loss: 6.9934, Time: 3.70s, Token/s: 138.55
Epoch: 0, Step: 3259, Batch(micro): 3259, Batch (considering grad accum): 407,  Loss: 6.9653, Time: 3.48s, Token/s: 147.24
Epoch: 0, Step: 3260, Batch(micro): 3260, Batch (considering grad accum): 407,  Loss: 6.3460, Time: 3.73s, Token/s: 137.28
Epoch: 0, Step: 3261, Batch(micro): 3261, Batch (considering grad accum): 407,  Loss: 6.7457, Time: 3.56s, Token/s: 143.65
Epoch: 0, Step: 3262, Batch(micro): 3262, Batch (considering grad accum): 407,  Loss: 6.5996, Time: 3.37s, Token/s: 152.07
Epoch: 0, Step: 3263, Batch(micro): 3263, Batch (considering grad accum): 407,  Loss: 6.7490, Time: 22.75s, Token/s: 22.51
Epoch: 0, Step: 3264, Batch(micro): 3264, Batch (considering grad accum): 408,  Loss: 6.9026, Time: 6.61s, Token/s: 77.45
Epoch: 0, Step: 3265, Batch(micro): 3265, Batch (considering grad accum): 408,  Loss: 6.3261, Time: 3.99s, Token/s: 128.45
Epoch: 0, Step: 3266, Batch(micro): 3266, Batch (considering grad accum): 408,  Loss: 6.9150, Time: 4.10s, Token/s: 124.96
Epoch: 0, Step: 3267, Batch(micro): 3267, Batch (considering grad accum): 408,  Loss: 8.1375, Time: 3.92s, Token/s: 130.45
Epoch: 0, Step: 3268, Batch(micro): 3268, Batch (considering grad accum): 408,  Loss: 6.4634, Time: 3.63s, Token/s: 140.89
Epoch: 0, Step: 3269, Batch(micro): 3269, Batch (considering grad accum): 408,  Loss: 6.3384, Time: 3.52s, Token/s: 145.39
Epoch: 0, Step: 3270, Batch(micro): 3270, Batch (considering grad accum): 408,  Loss: 6.9785, Time: 3.62s, Token/s: 141.52
Epoch: 0, Step: 3271, Batch(micro): 3271, Batch (considering grad accum): 408,  Loss: 7.6146, Time: 23.78s, Token/s: 21.53
Epoch: 0, Step: 3272, Batch(micro): 3272, Batch (considering grad accum): 409,  Loss: 7.9319, Time: 7.42s, Token/s: 69.01
Epoch: 0, Step: 3273, Batch(micro): 3273, Batch (considering grad accum): 409,  Loss: 6.1718, Time: 4.12s, Token/s: 124.12
Epoch: 0, Step: 3274, Batch(micro): 3274, Batch (considering grad accum): 409,  Loss: 6.2636, Time: 3.70s, Token/s: 138.53
Epoch: 0, Step: 3275, Batch(micro): 3275, Batch (considering grad accum): 409,  Loss: 6.4483, Time: 3.45s, Token/s: 148.44
Epoch: 0, Step: 3276, Batch(micro): 3276, Batch (considering grad accum): 409,  Loss: 6.3722, Time: 3.97s, Token/s: 129.06
Epoch: 0, Step: 3277, Batch(micro): 3277, Batch (considering grad accum): 409,  Loss: 6.8367, Time: 3.29s, Token/s: 155.77
Epoch: 0, Step: 3278, Batch(micro): 3278, Batch (considering grad accum): 409,  Loss: 7.3527, Time: 3.55s, Token/s: 144.42
Epoch: 0, Step: 3279, Batch(micro): 3279, Batch (considering grad accum): 409,  Loss: 6.8273, Time: 25.78s, Token/s: 19.86
Epoch: 0, Step: 3280, Batch(micro): 3280, Batch (considering grad accum): 410,  Loss: 6.5636, Time: 7.54s, Token/s: 67.87
Epoch: 0, Step: 3281, Batch(micro): 3281, Batch (considering grad accum): 410,  Loss: 6.5612, Time: 4.16s, Token/s: 123.16
Epoch: 0, Step: 3282, Batch(micro): 3282, Batch (considering grad accum): 410,  Loss: 7.0939, Time: 3.56s, Token/s: 143.88
Epoch: 0, Step: 3283, Batch(micro): 3283, Batch (considering grad accum): 410,  Loss: 6.8522, Time: 3.60s, Token/s: 142.08
Epoch: 0, Step: 3284, Batch(micro): 3284, Batch (considering grad accum): 410,  Loss: 6.3853, Time: 3.82s, Token/s: 134.20
Epoch: 0, Step: 3285, Batch(micro): 3285, Batch (considering grad accum): 410,  Loss: 6.4253, Time: 3.57s, Token/s: 143.61
Epoch: 0, Step: 3286, Batch(micro): 3286, Batch (considering grad accum): 410,  Loss: 6.6093, Time: 3.20s, Token/s: 160.07
Epoch: 0, Step: 3287, Batch(micro): 3287, Batch (considering grad accum): 410,  Loss: 7.0750, Time: 23.68s, Token/s: 21.62
Epoch: 0, Step: 3288, Batch(micro): 3288, Batch (considering grad accum): 411,  Loss: 7.0249, Time: 8.02s, Token/s: 63.88
Epoch: 0, Step: 3289, Batch(micro): 3289, Batch (considering grad accum): 411,  Loss: 7.4256, Time: 3.95s, Token/s: 129.48
Epoch: 0, Step: 3290, Batch(micro): 3290, Batch (considering grad accum): 411,  Loss: 6.4253, Time: 3.64s, Token/s: 140.77
Epoch: 0, Step: 3291, Batch(micro): 3291, Batch (considering grad accum): 411,  Loss: 7.5474, Time: 3.60s, Token/s: 142.06
Epoch: 0, Step: 3292, Batch(micro): 3292, Batch (considering grad accum): 411,  Loss: 6.9481, Time: 3.67s, Token/s: 139.52
Epoch: 0, Step: 3293, Batch(micro): 3293, Batch (considering grad accum): 411,  Loss: 6.8587, Time: 3.47s, Token/s: 147.70
Epoch: 0, Step: 3294, Batch(micro): 3294, Batch (considering grad accum): 411,  Loss: 6.6291, Time: 3.62s, Token/s: 141.48
Epoch: 0, Step: 3295, Batch(micro): 3295, Batch (considering grad accum): 411,  Loss: 6.6983, Time: 23.67s, Token/s: 21.63
Epoch: 0, Step: 3296, Batch(micro): 3296, Batch (considering grad accum): 412,  Loss: 6.8867, Time: 7.15s, Token/s: 71.61
Epoch: 0, Step: 3297, Batch(micro): 3297, Batch (considering grad accum): 412,  Loss: 6.1324, Time: 3.88s, Token/s: 132.00
Epoch: 0, Step: 3298, Batch(micro): 3298, Batch (considering grad accum): 412,  Loss: 6.6043, Time: 3.61s, Token/s: 141.68
Epoch: 0, Step: 3299, Batch(micro): 3299, Batch (considering grad accum): 412,  Loss: 6.8311, Time: 3.60s, Token/s: 142.25
Updating MLP bias
Epoch: 0, Step: 3300, Batch(micro): 3300, Batch (considering grad accum): 412,  Loss: 7.6524, Time: 3.57s, Token/s: 143.55
Epoch: 0, Step: 3301, Batch(micro): 3301, Batch (considering grad accum): 412,  Loss: 6.7157, Time: 3.56s, Token/s: 144.01
Epoch: 0, Step: 3302, Batch(micro): 3302, Batch (considering grad accum): 412,  Loss: 6.5355, Time: 3.96s, Token/s: 129.15
Epoch: 0, Step: 3303, Batch(micro): 3303, Batch (considering grad accum): 412,  Loss: 6.8525, Time: 24.06s, Token/s: 21.28
Epoch: 0, Step: 3304, Batch(micro): 3304, Batch (considering grad accum): 413,  Loss: 7.2496, Time: 6.37s, Token/s: 80.35
Epoch: 0, Step: 3305, Batch(micro): 3305, Batch (considering grad accum): 413,  Loss: 7.7985, Time: 3.94s, Token/s: 129.98
Epoch: 0, Step: 3306, Batch(micro): 3306, Batch (considering grad accum): 413,  Loss: 7.0794, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 3307, Batch(micro): 3307, Batch (considering grad accum): 413,  Loss: 6.7250, Time: 3.51s, Token/s: 145.82
Epoch: 0, Step: 3308, Batch(micro): 3308, Batch (considering grad accum): 413,  Loss: 7.2774, Time: 3.53s, Token/s: 144.94
Epoch: 0, Step: 3309, Batch(micro): 3309, Batch (considering grad accum): 413,  Loss: 6.3611, Time: 3.37s, Token/s: 152.04
Epoch: 0, Step: 3310, Batch(micro): 3310, Batch (considering grad accum): 413,  Loss: 7.0895, Time: 3.76s, Token/s: 136.18
Epoch: 0, Step: 3311, Batch(micro): 3311, Batch (considering grad accum): 413,  Loss: 7.2854, Time: 22.29s, Token/s: 22.97
Epoch: 0, Step: 3312, Batch(micro): 3312, Batch (considering grad accum): 414,  Loss: 8.2306, Time: 7.70s, Token/s: 66.45
Epoch: 0, Step: 3313, Batch(micro): 3313, Batch (considering grad accum): 414,  Loss: 7.2187, Time: 4.08s, Token/s: 125.54
Epoch: 0, Step: 3314, Batch(micro): 3314, Batch (considering grad accum): 414,  Loss: 6.4621, Time: 3.94s, Token/s: 130.01
Epoch: 0, Step: 3315, Batch(micro): 3315, Batch (considering grad accum): 414,  Loss: 6.4484, Time: 3.54s, Token/s: 144.81
Epoch: 0, Step: 3316, Batch(micro): 3316, Batch (considering grad accum): 414,  Loss: 7.2899, Time: 3.44s, Token/s: 148.65
Epoch: 0, Step: 3317, Batch(micro): 3317, Batch (considering grad accum): 414,  Loss: 7.0884, Time: 3.64s, Token/s: 140.67
Epoch: 0, Step: 3318, Batch(micro): 3318, Batch (considering grad accum): 414,  Loss: 6.1996, Time: 3.47s, Token/s: 147.56
Epoch: 0, Step: 3319, Batch(micro): 3319, Batch (considering grad accum): 414,  Loss: 7.0766, Time: 24.21s, Token/s: 21.15
Epoch: 0, Step: 3320, Batch(micro): 3320, Batch (considering grad accum): 415,  Loss: 7.0502, Time: 6.90s, Token/s: 74.26
Epoch: 0, Step: 3321, Batch(micro): 3321, Batch (considering grad accum): 415,  Loss: 6.5717, Time: 3.93s, Token/s: 130.27
Epoch: 0, Step: 3322, Batch(micro): 3322, Batch (considering grad accum): 415,  Loss: 6.5638, Time: 3.60s, Token/s: 142.32
Epoch: 0, Step: 3323, Batch(micro): 3323, Batch (considering grad accum): 415,  Loss: 6.2767, Time: 3.53s, Token/s: 145.23
Epoch: 0, Step: 3324, Batch(micro): 3324, Batch (considering grad accum): 415,  Loss: 6.5555, Time: 3.71s, Token/s: 138.13
Epoch: 0, Step: 3325, Batch(micro): 3325, Batch (considering grad accum): 415,  Loss: 5.8912, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 3326, Batch(micro): 3326, Batch (considering grad accum): 415,  Loss: 6.2177, Time: 3.73s, Token/s: 137.17
Epoch: 0, Step: 3327, Batch(micro): 3327, Batch (considering grad accum): 415,  Loss: 6.7927, Time: 25.47s, Token/s: 20.10
Epoch: 0, Step: 3328, Batch(micro): 3328, Batch (considering grad accum): 416,  Loss: 6.6177, Time: 8.13s, Token/s: 62.94
Epoch: 0, Step: 3329, Batch(micro): 3329, Batch (considering grad accum): 416,  Loss: 6.5635, Time: 3.88s, Token/s: 131.92
Epoch: 0, Step: 3330, Batch(micro): 3330, Batch (considering grad accum): 416,  Loss: 6.3964, Time: 3.62s, Token/s: 141.36
Epoch: 0, Step: 3331, Batch(micro): 3331, Batch (considering grad accum): 416,  Loss: 6.6029, Time: 3.55s, Token/s: 144.32
Epoch: 0, Step: 3332, Batch(micro): 3332, Batch (considering grad accum): 416,  Loss: 6.3406, Time: 4.47s, Token/s: 114.54
Epoch: 0, Step: 3333, Batch(micro): 3333, Batch (considering grad accum): 416,  Loss: 6.3125, Time: 3.87s, Token/s: 132.32
Epoch: 0, Step: 3334, Batch(micro): 3334, Batch (considering grad accum): 416,  Loss: 6.8032, Time: 3.33s, Token/s: 153.95
Epoch: 0, Step: 3335, Batch(micro): 3335, Batch (considering grad accum): 416,  Loss: 6.8611, Time: 24.24s, Token/s: 21.13
Epoch: 0, Step: 3336, Batch(micro): 3336, Batch (considering grad accum): 417,  Loss: 6.9417, Time: 6.89s, Token/s: 74.29
Epoch: 0, Step: 3337, Batch(micro): 3337, Batch (considering grad accum): 417,  Loss: 6.9093, Time: 3.77s, Token/s: 135.86
Epoch: 0, Step: 3338, Batch(micro): 3338, Batch (considering grad accum): 417,  Loss: 6.9052, Time: 3.88s, Token/s: 132.13
Epoch: 0, Step: 3339, Batch(micro): 3339, Batch (considering grad accum): 417,  Loss: 7.1917, Time: 3.85s, Token/s: 133.14
Epoch: 0, Step: 3340, Batch(micro): 3340, Batch (considering grad accum): 417,  Loss: 7.1571, Time: 3.50s, Token/s: 146.26
Epoch: 0, Step: 3341, Batch(micro): 3341, Batch (considering grad accum): 417,  Loss: 6.6742, Time: 3.57s, Token/s: 143.59
Epoch: 0, Step: 3342, Batch(micro): 3342, Batch (considering grad accum): 417,  Loss: 6.4176, Time: 3.42s, Token/s: 149.53
Epoch: 0, Step: 3343, Batch(micro): 3343, Batch (considering grad accum): 417,  Loss: 6.4813, Time: 23.07s, Token/s: 22.19
Epoch: 0, Step: 3344, Batch(micro): 3344, Batch (considering grad accum): 418,  Loss: 6.1289, Time: 8.47s, Token/s: 60.42
Epoch: 0, Step: 3345, Batch(micro): 3345, Batch (considering grad accum): 418,  Loss: 6.1183, Time: 3.88s, Token/s: 131.93
Epoch: 0, Step: 3346, Batch(micro): 3346, Batch (considering grad accum): 418,  Loss: 7.2195, Time: 3.55s, Token/s: 144.39
Epoch: 0, Step: 3347, Batch(micro): 3347, Batch (considering grad accum): 418,  Loss: 7.2915, Time: 4.42s, Token/s: 115.91
Epoch: 0, Step: 3348, Batch(micro): 3348, Batch (considering grad accum): 418,  Loss: 6.3603, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 3349, Batch(micro): 3349, Batch (considering grad accum): 418,  Loss: 6.8487, Time: 3.61s, Token/s: 141.69
Epoch: 0, Step: 3350, Batch(micro): 3350, Batch (considering grad accum): 418,  Loss: 6.6193, Time: 3.35s, Token/s: 152.85
Epoch: 0, Step: 3351, Batch(micro): 3351, Batch (considering grad accum): 418,  Loss: 7.0589, Time: 22.45s, Token/s: 22.80
Epoch: 0, Step: 3352, Batch(micro): 3352, Batch (considering grad accum): 419,  Loss: 6.6934, Time: 9.32s, Token/s: 54.91
Epoch: 0, Step: 3353, Batch(micro): 3353, Batch (considering grad accum): 419,  Loss: 6.4787, Time: 3.81s, Token/s: 134.21
Epoch: 0, Step: 3354, Batch(micro): 3354, Batch (considering grad accum): 419,  Loss: 6.2390, Time: 3.46s, Token/s: 147.90
Epoch: 0, Step: 3355, Batch(micro): 3355, Batch (considering grad accum): 419,  Loss: 6.8918, Time: 3.55s, Token/s: 144.05
Epoch: 0, Step: 3356, Batch(micro): 3356, Batch (considering grad accum): 419,  Loss: 7.7071, Time: 3.74s, Token/s: 136.72
Epoch: 0, Step: 3357, Batch(micro): 3357, Batch (considering grad accum): 419,  Loss: 7.5856, Time: 3.73s, Token/s: 137.35
Epoch: 0, Step: 3358, Batch(micro): 3358, Batch (considering grad accum): 419,  Loss: 6.3167, Time: 3.34s, Token/s: 153.38
Epoch: 0, Step: 3359, Batch(micro): 3359, Batch (considering grad accum): 419,  Loss: 6.1955, Time: 24.06s, Token/s: 21.28
Epoch: 0, Step: 3360, Batch(micro): 3360, Batch (considering grad accum): 420,  Loss: 6.4720, Time: 7.54s, Token/s: 67.91
Epoch: 0, Step: 3361, Batch(micro): 3361, Batch (considering grad accum): 420,  Loss: 6.5109, Time: 4.09s, Token/s: 125.33
Epoch: 0, Step: 3362, Batch(micro): 3362, Batch (considering grad accum): 420,  Loss: 6.2808, Time: 3.72s, Token/s: 137.67
Epoch: 0, Step: 3363, Batch(micro): 3363, Batch (considering grad accum): 420,  Loss: 6.4530, Time: 3.40s, Token/s: 150.41
Epoch: 0, Step: 3364, Batch(micro): 3364, Batch (considering grad accum): 420,  Loss: 6.4961, Time: 3.28s, Token/s: 156.33
Epoch: 0, Step: 3365, Batch(micro): 3365, Batch (considering grad accum): 420,  Loss: 7.3185, Time: 3.26s, Token/s: 157.10
Epoch: 0, Step: 3366, Batch(micro): 3366, Batch (considering grad accum): 420,  Loss: 6.1881, Time: 3.56s, Token/s: 143.76
Epoch: 0, Step: 3367, Batch(micro): 3367, Batch (considering grad accum): 420,  Loss: 6.3252, Time: 25.41s, Token/s: 20.15
Epoch: 0, Step: 3368, Batch(micro): 3368, Batch (considering grad accum): 421,  Loss: 6.4618, Time: 7.94s, Token/s: 64.47
Epoch: 0, Step: 3369, Batch(micro): 3369, Batch (considering grad accum): 421,  Loss: 6.7430, Time: 4.21s, Token/s: 121.73
Epoch: 0, Step: 3370, Batch(micro): 3370, Batch (considering grad accum): 421,  Loss: 6.7206, Time: 3.55s, Token/s: 144.22
Epoch: 0, Step: 3371, Batch(micro): 3371, Batch (considering grad accum): 421,  Loss: 5.7522, Time: 3.47s, Token/s: 147.71
Epoch: 0, Step: 3372, Batch(micro): 3372, Batch (considering grad accum): 421,  Loss: 6.3764, Time: 3.43s, Token/s: 149.08
Epoch: 0, Step: 3373, Batch(micro): 3373, Batch (considering grad accum): 421,  Loss: 7.8811, Time: 3.43s, Token/s: 149.08
Epoch: 0, Step: 3374, Batch(micro): 3374, Batch (considering grad accum): 421,  Loss: 7.4832, Time: 3.51s, Token/s: 145.83
Epoch: 0, Step: 3375, Batch(micro): 3375, Batch (considering grad accum): 421,  Loss: 7.4696, Time: 25.49s, Token/s: 20.08
Epoch: 0, Step: 3376, Batch(micro): 3376, Batch (considering grad accum): 422,  Loss: 6.5269, Time: 7.74s, Token/s: 66.14
Epoch: 0, Step: 3377, Batch(micro): 3377, Batch (considering grad accum): 422,  Loss: 6.7084, Time: 4.04s, Token/s: 126.88
Epoch: 0, Step: 3378, Batch(micro): 3378, Batch (considering grad accum): 422,  Loss: 6.5909, Time: 3.52s, Token/s: 145.39
Epoch: 0, Step: 3379, Batch(micro): 3379, Batch (considering grad accum): 422,  Loss: 6.5001, Time: 3.54s, Token/s: 144.61
Epoch: 0, Step: 3380, Batch(micro): 3380, Batch (considering grad accum): 422,  Loss: 6.4984, Time: 3.22s, Token/s: 159.14
Epoch: 0, Step: 3381, Batch(micro): 3381, Batch (considering grad accum): 422,  Loss: 7.2783, Time: 3.18s, Token/s: 160.89
Epoch: 0, Step: 3382, Batch(micro): 3382, Batch (considering grad accum): 422,  Loss: 6.6857, Time: 3.25s, Token/s: 157.38
Epoch: 0, Step: 3383, Batch(micro): 3383, Batch (considering grad accum): 422,  Loss: 6.2629, Time: 24.38s, Token/s: 21.00
Epoch: 0, Step: 3384, Batch(micro): 3384, Batch (considering grad accum): 423,  Loss: 6.4052, Time: 6.74s, Token/s: 75.96
Epoch: 0, Step: 3385, Batch(micro): 3385, Batch (considering grad accum): 423,  Loss: 6.6553, Time: 3.94s, Token/s: 129.86
Epoch: 0, Step: 3386, Batch(micro): 3386, Batch (considering grad accum): 423,  Loss: 7.4926, Time: 3.48s, Token/s: 147.01
Epoch: 0, Step: 3387, Batch(micro): 3387, Batch (considering grad accum): 423,  Loss: 7.5642, Time: 3.64s, Token/s: 140.62
Epoch: 0, Step: 3388, Batch(micro): 3388, Batch (considering grad accum): 423,  Loss: 7.0526, Time: 3.23s, Token/s: 158.74
Epoch: 0, Step: 3389, Batch(micro): 3389, Batch (considering grad accum): 423,  Loss: 6.5266, Time: 3.54s, Token/s: 144.74
Epoch: 0, Step: 3390, Batch(micro): 3390, Batch (considering grad accum): 423,  Loss: 6.6680, Time: 3.27s, Token/s: 156.49
Epoch: 0, Step: 3391, Batch(micro): 3391, Batch (considering grad accum): 423,  Loss: 7.0242, Time: 23.75s, Token/s: 21.56
Epoch: 0, Step: 3392, Batch(micro): 3392, Batch (considering grad accum): 424,  Loss: 6.1977, Time: 6.77s, Token/s: 75.58
Epoch: 0, Step: 3393, Batch(micro): 3393, Batch (considering grad accum): 424,  Loss: 6.4699, Time: 3.70s, Token/s: 138.38
Epoch: 0, Step: 3394, Batch(micro): 3394, Batch (considering grad accum): 424,  Loss: 6.6845, Time: 3.23s, Token/s: 158.73
Epoch: 0, Step: 3395, Batch(micro): 3395, Batch (considering grad accum): 424,  Loss: 6.8916, Time: 3.40s, Token/s: 150.39
Epoch: 0, Step: 3396, Batch(micro): 3396, Batch (considering grad accum): 424,  Loss: 6.5850, Time: 3.58s, Token/s: 143.03
Epoch: 0, Step: 3397, Batch(micro): 3397, Batch (considering grad accum): 424,  Loss: 6.5515, Time: 3.65s, Token/s: 140.18
Epoch: 0, Step: 3398, Batch(micro): 3398, Batch (considering grad accum): 424,  Loss: 5.9672, Time: 3.45s, Token/s: 148.39
Epoch: 0, Step: 3399, Batch(micro): 3399, Batch (considering grad accum): 424,  Loss: 7.0685, Time: 23.66s, Token/s: 21.64
Updating MLP bias
Epoch: 0, Step: 3400, Batch(micro): 3400, Batch (considering grad accum): 425,  Loss: 5.8894, Time: 7.08s, Token/s: 72.27
Epoch: 0, Step: 3401, Batch(micro): 3401, Batch (considering grad accum): 425,  Loss: 6.5620, Time: 3.92s, Token/s: 130.55
Epoch: 0, Step: 3402, Batch(micro): 3402, Batch (considering grad accum): 425,  Loss: 7.1397, Time: 3.60s, Token/s: 142.19
Epoch: 0, Step: 3403, Batch(micro): 3403, Batch (considering grad accum): 425,  Loss: 6.7300, Time: 3.74s, Token/s: 136.74
Epoch: 0, Step: 3404, Batch(micro): 3404, Batch (considering grad accum): 425,  Loss: 7.2046, Time: 3.63s, Token/s: 141.23
Epoch: 0, Step: 3405, Batch(micro): 3405, Batch (considering grad accum): 425,  Loss: 6.8066, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 3406, Batch(micro): 3406, Batch (considering grad accum): 425,  Loss: 6.2111, Time: 3.46s, Token/s: 148.12
Epoch: 0, Step: 3407, Batch(micro): 3407, Batch (considering grad accum): 425,  Loss: 6.9005, Time: 22.30s, Token/s: 22.96
Epoch: 0, Step: 3408, Batch(micro): 3408, Batch (considering grad accum): 426,  Loss: 6.1335, Time: 8.13s, Token/s: 63.01
Epoch: 0, Step: 3409, Batch(micro): 3409, Batch (considering grad accum): 426,  Loss: 6.8548, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 3410, Batch(micro): 3410, Batch (considering grad accum): 426,  Loss: 6.4036, Time: 3.17s, Token/s: 161.70
Epoch: 0, Step: 3411, Batch(micro): 3411, Batch (considering grad accum): 426,  Loss: 6.4755, Time: 3.38s, Token/s: 151.40
Epoch: 0, Step: 3412, Batch(micro): 3412, Batch (considering grad accum): 426,  Loss: 6.2449, Time: 3.54s, Token/s: 144.56
Epoch: 0, Step: 3413, Batch(micro): 3413, Batch (considering grad accum): 426,  Loss: 5.6699, Time: 3.60s, Token/s: 142.05
Epoch: 0, Step: 3414, Batch(micro): 3414, Batch (considering grad accum): 426,  Loss: 6.3254, Time: 3.44s, Token/s: 148.69
Epoch: 0, Step: 3415, Batch(micro): 3415, Batch (considering grad accum): 426,  Loss: 6.6664, Time: 24.46s, Token/s: 20.93
Epoch: 0, Step: 3416, Batch(micro): 3416, Batch (considering grad accum): 427,  Loss: 6.9927, Time: 9.39s, Token/s: 54.50
Epoch: 0, Step: 3417, Batch(micro): 3417, Batch (considering grad accum): 427,  Loss: 6.5261, Time: 4.01s, Token/s: 127.79
Epoch: 0, Step: 3418, Batch(micro): 3418, Batch (considering grad accum): 427,  Loss: 6.5255, Time: 3.42s, Token/s: 149.64
Epoch: 0, Step: 3419, Batch(micro): 3419, Batch (considering grad accum): 427,  Loss: 6.9162, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 3420, Batch(micro): 3420, Batch (considering grad accum): 427,  Loss: 6.6907, Time: 3.36s, Token/s: 152.46
Epoch: 0, Step: 3421, Batch(micro): 3421, Batch (considering grad accum): 427,  Loss: 6.7143, Time: 3.54s, Token/s: 144.72
Epoch: 0, Step: 3422, Batch(micro): 3422, Batch (considering grad accum): 427,  Loss: 6.0669, Time: 3.43s, Token/s: 149.20
Epoch: 0, Step: 3423, Batch(micro): 3423, Batch (considering grad accum): 427,  Loss: 6.3504, Time: 23.07s, Token/s: 22.20
Epoch: 0, Step: 3424, Batch(micro): 3424, Batch (considering grad accum): 428,  Loss: 6.3154, Time: 7.54s, Token/s: 67.91
Epoch: 0, Step: 3425, Batch(micro): 3425, Batch (considering grad accum): 428,  Loss: 6.9991, Time: 4.00s, Token/s: 128.03
Epoch: 0, Step: 3426, Batch(micro): 3426, Batch (considering grad accum): 428,  Loss: 6.9121, Time: 3.71s, Token/s: 137.95
Epoch: 0, Step: 3427, Batch(micro): 3427, Batch (considering grad accum): 428,  Loss: 7.2128, Time: 3.65s, Token/s: 140.25
Epoch: 0, Step: 3428, Batch(micro): 3428, Batch (considering grad accum): 428,  Loss: 6.7029, Time: 3.74s, Token/s: 137.08
Epoch: 0, Step: 3429, Batch(micro): 3429, Batch (considering grad accum): 428,  Loss: 6.6874, Time: 3.47s, Token/s: 147.63
Epoch: 0, Step: 3430, Batch(micro): 3430, Batch (considering grad accum): 428,  Loss: 5.8108, Time: 3.24s, Token/s: 157.82
Epoch: 0, Step: 3431, Batch(micro): 3431, Batch (considering grad accum): 428,  Loss: 6.2877, Time: 21.94s, Token/s: 23.33
Epoch: 0, Step: 3432, Batch(micro): 3432, Batch (considering grad accum): 429,  Loss: 6.7821, Time: 8.44s, Token/s: 60.65
Epoch: 0, Step: 3433, Batch(micro): 3433, Batch (considering grad accum): 429,  Loss: 7.1646, Time: 3.99s, Token/s: 128.23
Epoch: 0, Step: 3434, Batch(micro): 3434, Batch (considering grad accum): 429,  Loss: 6.5511, Time: 3.60s, Token/s: 142.36
Epoch: 0, Step: 3435, Batch(micro): 3435, Batch (considering grad accum): 429,  Loss: 6.0451, Time: 3.55s, Token/s: 144.06
Epoch: 0, Step: 3436, Batch(micro): 3436, Batch (considering grad accum): 429,  Loss: 6.3762, Time: 3.18s, Token/s: 161.18
Epoch: 0, Step: 3437, Batch(micro): 3437, Batch (considering grad accum): 429,  Loss: 6.0877, Time: 3.23s, Token/s: 158.70
Epoch: 0, Step: 3438, Batch(micro): 3438, Batch (considering grad accum): 429,  Loss: 6.6232, Time: 3.27s, Token/s: 156.42
Epoch: 0, Step: 3439, Batch(micro): 3439, Batch (considering grad accum): 429,  Loss: 6.0013, Time: 23.39s, Token/s: 21.89
Epoch: 0, Step: 3440, Batch(micro): 3440, Batch (considering grad accum): 430,  Loss: 7.0338, Time: 7.74s, Token/s: 66.16
Epoch: 0, Step: 3441, Batch(micro): 3441, Batch (considering grad accum): 430,  Loss: 6.7494, Time: 4.15s, Token/s: 123.24
Epoch: 0, Step: 3442, Batch(micro): 3442, Batch (considering grad accum): 430,  Loss: 6.6738, Time: 3.76s, Token/s: 136.35
Epoch: 0, Step: 3443, Batch(micro): 3443, Batch (considering grad accum): 430,  Loss: 7.0347, Time: 3.53s, Token/s: 144.94
Epoch: 0, Step: 3444, Batch(micro): 3444, Batch (considering grad accum): 430,  Loss: 6.8899, Time: 3.54s, Token/s: 144.59
Epoch: 0, Step: 3445, Batch(micro): 3445, Batch (considering grad accum): 430,  Loss: 7.1963, Time: 3.46s, Token/s: 148.10
Epoch: 0, Step: 3446, Batch(micro): 3446, Batch (considering grad accum): 430,  Loss: 6.7853, Time: 3.48s, Token/s: 147.11
Epoch: 0, Step: 3447, Batch(micro): 3447, Batch (considering grad accum): 430,  Loss: 6.7236, Time: 20.97s, Token/s: 24.42
Epoch: 0, Step: 3448, Batch(micro): 3448, Batch (considering grad accum): 431,  Loss: 6.7199, Time: 7.17s, Token/s: 71.42
Epoch: 0, Step: 3449, Batch(micro): 3449, Batch (considering grad accum): 431,  Loss: 6.9758, Time: 3.64s, Token/s: 140.65
Epoch: 0, Step: 3450, Batch(micro): 3450, Batch (considering grad accum): 431,  Loss: 7.1718, Time: 3.18s, Token/s: 160.79
Epoch: 0, Step: 3451, Batch(micro): 3451, Batch (considering grad accum): 431,  Loss: 6.8407, Time: 3.12s, Token/s: 164.28
Epoch: 0, Step: 3452, Batch(micro): 3452, Batch (considering grad accum): 431,  Loss: 6.6891, Time: 3.20s, Token/s: 160.14
Epoch: 0, Step: 3453, Batch(micro): 3453, Batch (considering grad accum): 431,  Loss: 7.3563, Time: 3.22s, Token/s: 158.92
Epoch: 0, Step: 3454, Batch(micro): 3454, Batch (considering grad accum): 431,  Loss: 6.4397, Time: 3.52s, Token/s: 145.32
Epoch: 0, Step: 3455, Batch(micro): 3455, Batch (considering grad accum): 431,  Loss: 6.6789, Time: 17.97s, Token/s: 28.49
Epoch: 0, Step: 3456, Batch(micro): 3456, Batch (considering grad accum): 432,  Loss: 7.1818, Time: 6.14s, Token/s: 83.42
Epoch: 0, Step: 3457, Batch(micro): 3457, Batch (considering grad accum): 432,  Loss: 6.7390, Time: 3.99s, Token/s: 128.40
Epoch: 0, Step: 3458, Batch(micro): 3458, Batch (considering grad accum): 432,  Loss: 6.6109, Time: 3.55s, Token/s: 144.03
Epoch: 0, Step: 3459, Batch(micro): 3459, Batch (considering grad accum): 432,  Loss: 6.6007, Time: 3.49s, Token/s: 146.56
Epoch: 0, Step: 3460, Batch(micro): 3460, Batch (considering grad accum): 432,  Loss: 6.7115, Time: 3.69s, Token/s: 138.68
Epoch: 0, Step: 3461, Batch(micro): 3461, Batch (considering grad accum): 432,  Loss: 6.8240, Time: 3.52s, Token/s: 145.58
Epoch: 0, Step: 3462, Batch(micro): 3462, Batch (considering grad accum): 432,  Loss: 6.5252, Time: 3.22s, Token/s: 158.99
Epoch: 0, Step: 3463, Batch(micro): 3463, Batch (considering grad accum): 432,  Loss: 6.3589, Time: 18.26s, Token/s: 28.04
Epoch: 0, Step: 3464, Batch(micro): 3464, Batch (considering grad accum): 433,  Loss: 7.2207, Time: 7.07s, Token/s: 72.42
Epoch: 0, Step: 3465, Batch(micro): 3465, Batch (considering grad accum): 433,  Loss: 6.8235, Time: 3.88s, Token/s: 131.82
Epoch: 0, Step: 3466, Batch(micro): 3466, Batch (considering grad accum): 433,  Loss: 7.0086, Time: 3.95s, Token/s: 129.72
Epoch: 0, Step: 3467, Batch(micro): 3467, Batch (considering grad accum): 433,  Loss: 6.6506, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 3468, Batch(micro): 3468, Batch (considering grad accum): 433,  Loss: 6.9082, Time: 3.43s, Token/s: 149.06
Epoch: 0, Step: 3469, Batch(micro): 3469, Batch (considering grad accum): 433,  Loss: 6.5159, Time: 3.54s, Token/s: 144.67
Epoch: 0, Step: 3470, Batch(micro): 3470, Batch (considering grad accum): 433,  Loss: 7.0943, Time: 3.61s, Token/s: 141.65
Epoch: 0, Step: 3471, Batch(micro): 3471, Batch (considering grad accum): 433,  Loss: 7.0125, Time: 18.60s, Token/s: 27.53
Epoch: 0, Step: 3472, Batch(micro): 3472, Batch (considering grad accum): 434,  Loss: 6.7859, Time: 7.53s, Token/s: 67.98
Epoch: 0, Step: 3473, Batch(micro): 3473, Batch (considering grad accum): 434,  Loss: 6.0115, Time: 3.84s, Token/s: 133.47
Epoch: 0, Step: 3474, Batch(micro): 3474, Batch (considering grad accum): 434,  Loss: 6.1981, Time: 3.76s, Token/s: 136.03
Epoch: 0, Step: 3475, Batch(micro): 3475, Batch (considering grad accum): 434,  Loss: 6.8120, Time: 3.64s, Token/s: 140.50
Epoch: 0, Step: 3476, Batch(micro): 3476, Batch (considering grad accum): 434,  Loss: 7.3611, Time: 3.42s, Token/s: 149.61
Epoch: 0, Step: 3477, Batch(micro): 3477, Batch (considering grad accum): 434,  Loss: 6.2492, Time: 3.33s, Token/s: 153.89
Epoch: 0, Step: 3478, Batch(micro): 3478, Batch (considering grad accum): 434,  Loss: 6.6042, Time: 3.45s, Token/s: 148.42
Epoch: 0, Step: 3479, Batch(micro): 3479, Batch (considering grad accum): 434,  Loss: 7.0083, Time: 19.51s, Token/s: 26.24
Epoch: 0, Step: 3480, Batch(micro): 3480, Batch (considering grad accum): 435,  Loss: 6.2714, Time: 6.74s, Token/s: 76.01
Epoch: 0, Step: 3481, Batch(micro): 3481, Batch (considering grad accum): 435,  Loss: 6.4595, Time: 3.91s, Token/s: 130.83
Epoch: 0, Step: 3482, Batch(micro): 3482, Batch (considering grad accum): 435,  Loss: 6.6416, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 3483, Batch(micro): 3483, Batch (considering grad accum): 435,  Loss: 6.8121, Time: 3.58s, Token/s: 142.96
Epoch: 0, Step: 3484, Batch(micro): 3484, Batch (considering grad accum): 435,  Loss: 6.1509, Time: 3.76s, Token/s: 136.14
Epoch: 0, Step: 3485, Batch(micro): 3485, Batch (considering grad accum): 435,  Loss: 6.5464, Time: 3.45s, Token/s: 148.48
Epoch: 0, Step: 3486, Batch(micro): 3486, Batch (considering grad accum): 435,  Loss: 5.7987, Time: 3.36s, Token/s: 152.24
Epoch: 0, Step: 3487, Batch(micro): 3487, Batch (considering grad accum): 435,  Loss: 6.6497, Time: 18.99s, Token/s: 26.96
Epoch: 0, Step: 3488, Batch(micro): 3488, Batch (considering grad accum): 436,  Loss: 5.6499, Time: 7.28s, Token/s: 70.33
Epoch: 0, Step: 3489, Batch(micro): 3489, Batch (considering grad accum): 436,  Loss: 6.9606, Time: 3.81s, Token/s: 134.46
Epoch: 0, Step: 3490, Batch(micro): 3490, Batch (considering grad accum): 436,  Loss: 6.8860, Time: 3.37s, Token/s: 151.77
Epoch: 0, Step: 3491, Batch(micro): 3491, Batch (considering grad accum): 436,  Loss: 7.0358, Time: 3.43s, Token/s: 149.25
Epoch: 0, Step: 3492, Batch(micro): 3492, Batch (considering grad accum): 436,  Loss: 7.0174, Time: 3.54s, Token/s: 144.48
Epoch: 0, Step: 3493, Batch(micro): 3493, Batch (considering grad accum): 436,  Loss: 6.2842, Time: 3.57s, Token/s: 143.59
Epoch: 0, Step: 3494, Batch(micro): 3494, Batch (considering grad accum): 436,  Loss: 6.8567, Time: 3.28s, Token/s: 155.93
Epoch: 0, Step: 3495, Batch(micro): 3495, Batch (considering grad accum): 436,  Loss: 7.1843, Time: 20.07s, Token/s: 25.52
Epoch: 0, Step: 3496, Batch(micro): 3496, Batch (considering grad accum): 437,  Loss: 6.4318, Time: 7.21s, Token/s: 71.03
Epoch: 0, Step: 3497, Batch(micro): 3497, Batch (considering grad accum): 437,  Loss: 6.3386, Time: 3.66s, Token/s: 139.70
Epoch: 0, Step: 3498, Batch(micro): 3498, Batch (considering grad accum): 437,  Loss: 6.3341, Time: 3.84s, Token/s: 133.47
Epoch: 0, Step: 3499, Batch(micro): 3499, Batch (considering grad accum): 437,  Loss: 6.6920, Time: 3.63s, Token/s: 141.15
Updating MLP bias
Epoch: 0, Step: 3500, Batch(micro): 3500, Batch (considering grad accum): 437,  Loss: 6.2523, Time: 3.55s, Token/s: 144.19
Epoch: 0, Step: 3501, Batch(micro): 3501, Batch (considering grad accum): 437,  Loss: 7.0396, Time: 3.46s, Token/s: 147.95
Epoch: 0, Step: 3502, Batch(micro): 3502, Batch (considering grad accum): 437,  Loss: 6.2392, Time: 3.58s, Token/s: 142.92
Epoch: 0, Step: 3503, Batch(micro): 3503, Batch (considering grad accum): 437,  Loss: 5.8962, Time: 19.18s, Token/s: 26.69
Epoch: 0, Step: 3504, Batch(micro): 3504, Batch (considering grad accum): 438,  Loss: 6.7747, Time: 6.73s, Token/s: 76.04
Epoch: 0, Step: 3505, Batch(micro): 3505, Batch (considering grad accum): 438,  Loss: 6.8469, Time: 3.96s, Token/s: 129.44
Epoch: 0, Step: 3506, Batch(micro): 3506, Batch (considering grad accum): 438,  Loss: 6.4316, Time: 3.89s, Token/s: 131.58
Epoch: 0, Step: 3507, Batch(micro): 3507, Batch (considering grad accum): 438,  Loss: 6.8275, Time: 4.21s, Token/s: 121.54
Epoch: 0, Step: 3508, Batch(micro): 3508, Batch (considering grad accum): 438,  Loss: 6.7723, Time: 3.31s, Token/s: 154.79
Epoch: 0, Step: 3509, Batch(micro): 3509, Batch (considering grad accum): 438,  Loss: 6.7072, Time: 3.18s, Token/s: 160.86
Epoch: 0, Step: 3510, Batch(micro): 3510, Batch (considering grad accum): 438,  Loss: 6.5861, Time: 3.20s, Token/s: 160.00
Epoch: 0, Step: 3511, Batch(micro): 3511, Batch (considering grad accum): 438,  Loss: 6.6015, Time: 21.41s, Token/s: 23.92
Epoch: 0, Step: 3512, Batch(micro): 3512, Batch (considering grad accum): 439,  Loss: 6.4853, Time: 7.86s, Token/s: 65.15
Epoch: 0, Step: 3513, Batch(micro): 3513, Batch (considering grad accum): 439,  Loss: 6.8105, Time: 3.99s, Token/s: 128.26
Epoch: 0, Step: 3514, Batch(micro): 3514, Batch (considering grad accum): 439,  Loss: 7.4377, Time: 3.71s, Token/s: 137.84
Epoch: 0, Step: 3515, Batch(micro): 3515, Batch (considering grad accum): 439,  Loss: 7.2730, Time: 3.61s, Token/s: 141.85
Epoch: 0, Step: 3516, Batch(micro): 3516, Batch (considering grad accum): 439,  Loss: 6.7280, Time: 3.62s, Token/s: 141.27
Epoch: 0, Step: 3517, Batch(micro): 3517, Batch (considering grad accum): 439,  Loss: 6.4143, Time: 3.40s, Token/s: 150.41
Epoch: 0, Step: 3518, Batch(micro): 3518, Batch (considering grad accum): 439,  Loss: 6.8284, Time: 3.35s, Token/s: 152.88
Epoch: 0, Step: 3519, Batch(micro): 3519, Batch (considering grad accum): 439,  Loss: 6.6283, Time: 22.27s, Token/s: 22.99
Epoch: 0, Step: 3520, Batch(micro): 3520, Batch (considering grad accum): 440,  Loss: 6.4850, Time: 7.71s, Token/s: 66.39
Epoch: 0, Step: 3521, Batch(micro): 3521, Batch (considering grad accum): 440,  Loss: 7.3898, Time: 3.87s, Token/s: 132.26
Epoch: 0, Step: 3522, Batch(micro): 3522, Batch (considering grad accum): 440,  Loss: 6.4915, Time: 3.58s, Token/s: 142.97
Epoch: 0, Step: 3523, Batch(micro): 3523, Batch (considering grad accum): 440,  Loss: 6.7306, Time: 3.21s, Token/s: 159.28
Epoch: 0, Step: 3524, Batch(micro): 3524, Batch (considering grad accum): 440,  Loss: 6.8762, Time: 3.26s, Token/s: 156.88
Epoch: 0, Step: 3525, Batch(micro): 3525, Batch (considering grad accum): 440,  Loss: 6.7766, Time: 3.22s, Token/s: 159.18
Epoch: 0, Step: 3526, Batch(micro): 3526, Batch (considering grad accum): 440,  Loss: 6.8657, Time: 3.93s, Token/s: 130.38
Epoch: 0, Step: 3527, Batch(micro): 3527, Batch (considering grad accum): 440,  Loss: 6.6327, Time: 23.09s, Token/s: 22.18
Epoch: 0, Step: 3528, Batch(micro): 3528, Batch (considering grad accum): 441,  Loss: 6.9304, Time: 6.59s, Token/s: 77.72
Epoch: 0, Step: 3529, Batch(micro): 3529, Batch (considering grad accum): 441,  Loss: 7.4108, Time: 3.93s, Token/s: 130.28
Epoch: 0, Step: 3530, Batch(micro): 3530, Batch (considering grad accum): 441,  Loss: 6.5117, Time: 3.50s, Token/s: 146.32
Epoch: 0, Step: 3531, Batch(micro): 3531, Batch (considering grad accum): 441,  Loss: 5.8065, Time: 3.48s, Token/s: 147.08
Epoch: 0, Step: 3532, Batch(micro): 3532, Batch (considering grad accum): 441,  Loss: 6.1156, Time: 3.64s, Token/s: 140.55
Epoch: 0, Step: 3533, Batch(micro): 3533, Batch (considering grad accum): 441,  Loss: 6.2870, Time: 3.40s, Token/s: 150.58
Epoch: 0, Step: 3534, Batch(micro): 3534, Batch (considering grad accum): 441,  Loss: 6.8252, Time: 3.24s, Token/s: 157.99
Epoch: 0, Step: 3535, Batch(micro): 3535, Batch (considering grad accum): 441,  Loss: 6.8647, Time: 21.98s, Token/s: 23.29
Epoch: 0, Step: 3536, Batch(micro): 3536, Batch (considering grad accum): 442,  Loss: 6.5673, Time: 7.01s, Token/s: 73.06
Epoch: 0, Step: 3537, Batch(micro): 3537, Batch (considering grad accum): 442,  Loss: 6.5933, Time: 3.94s, Token/s: 129.85
Epoch: 0, Step: 3538, Batch(micro): 3538, Batch (considering grad accum): 442,  Loss: 7.2101, Time: 3.41s, Token/s: 150.25
Epoch: 0, Step: 3539, Batch(micro): 3539, Batch (considering grad accum): 442,  Loss: 7.2492, Time: 3.46s, Token/s: 148.01
Epoch: 0, Step: 3540, Batch(micro): 3540, Batch (considering grad accum): 442,  Loss: 6.2945, Time: 3.49s, Token/s: 146.67
Epoch: 0, Step: 3541, Batch(micro): 3541, Batch (considering grad accum): 442,  Loss: 6.6865, Time: 3.53s, Token/s: 145.04
Epoch: 0, Step: 3542, Batch(micro): 3542, Batch (considering grad accum): 442,  Loss: 6.6474, Time: 3.38s, Token/s: 151.55
Epoch: 0, Step: 3543, Batch(micro): 3543, Batch (considering grad accum): 442,  Loss: 6.8319, Time: 22.41s, Token/s: 22.85
Epoch: 0, Step: 3544, Batch(micro): 3544, Batch (considering grad accum): 443,  Loss: 7.2296, Time: 8.60s, Token/s: 59.56
Epoch: 0, Step: 3545, Batch(micro): 3545, Batch (considering grad accum): 443,  Loss: 6.7124, Time: 3.95s, Token/s: 129.75
Epoch: 0, Step: 3546, Batch(micro): 3546, Batch (considering grad accum): 443,  Loss: 6.7978, Time: 3.50s, Token/s: 146.10
Epoch: 0, Step: 3547, Batch(micro): 3547, Batch (considering grad accum): 443,  Loss: 6.6711, Time: 3.75s, Token/s: 136.42
Epoch: 0, Step: 3548, Batch(micro): 3548, Batch (considering grad accum): 443,  Loss: 6.8908, Time: 3.41s, Token/s: 150.21
Epoch: 0, Step: 3549, Batch(micro): 3549, Batch (considering grad accum): 443,  Loss: 6.3058, Time: 3.45s, Token/s: 148.40
Epoch: 0, Step: 3550, Batch(micro): 3550, Batch (considering grad accum): 443,  Loss: 6.6204, Time: 3.12s, Token/s: 164.01
Epoch: 0, Step: 3551, Batch(micro): 3551, Batch (considering grad accum): 443,  Loss: 6.7758, Time: 22.85s, Token/s: 22.40
Epoch: 0, Step: 3552, Batch(micro): 3552, Batch (considering grad accum): 444,  Loss: 6.5269, Time: 7.10s, Token/s: 72.11
Epoch: 0, Step: 3553, Batch(micro): 3553, Batch (considering grad accum): 444,  Loss: 7.0406, Time: 3.66s, Token/s: 139.90
Epoch: 0, Step: 3554, Batch(micro): 3554, Batch (considering grad accum): 444,  Loss: 6.3617, Time: 3.35s, Token/s: 152.92
Epoch: 0, Step: 3555, Batch(micro): 3555, Batch (considering grad accum): 444,  Loss: 7.0351, Time: 3.37s, Token/s: 151.76
Epoch: 0, Step: 3556, Batch(micro): 3556, Batch (considering grad accum): 444,  Loss: 6.4984, Time: 3.63s, Token/s: 140.88
Epoch: 0, Step: 3557, Batch(micro): 3557, Batch (considering grad accum): 444,  Loss: 7.7196, Time: 3.65s, Token/s: 140.19
Epoch: 0, Step: 3558, Batch(micro): 3558, Batch (considering grad accum): 444,  Loss: 7.1599, Time: 3.72s, Token/s: 137.60
Epoch: 0, Step: 3559, Batch(micro): 3559, Batch (considering grad accum): 444,  Loss: 6.7700, Time: 23.37s, Token/s: 21.91
Epoch: 0, Step: 3560, Batch(micro): 3560, Batch (considering grad accum): 445,  Loss: 6.3355, Time: 7.30s, Token/s: 70.14
Epoch: 0, Step: 3561, Batch(micro): 3561, Batch (considering grad accum): 445,  Loss: 6.4365, Time: 4.10s, Token/s: 124.76
Epoch: 0, Step: 3562, Batch(micro): 3562, Batch (considering grad accum): 445,  Loss: 7.3024, Time: 3.98s, Token/s: 128.77
Epoch: 0, Step: 3563, Batch(micro): 3563, Batch (considering grad accum): 445,  Loss: 6.7213, Time: 3.64s, Token/s: 140.81
Epoch: 0, Step: 3564, Batch(micro): 3564, Batch (considering grad accum): 445,  Loss: 6.7174, Time: 3.64s, Token/s: 140.80
Epoch: 0, Step: 3565, Batch(micro): 3565, Batch (considering grad accum): 445,  Loss: 7.0193, Time: 3.69s, Token/s: 138.93
Epoch: 0, Step: 3566, Batch(micro): 3566, Batch (considering grad accum): 445,  Loss: 6.9625, Time: 3.45s, Token/s: 148.31
Epoch: 0, Step: 3567, Batch(micro): 3567, Batch (considering grad accum): 445,  Loss: 6.7767, Time: 21.54s, Token/s: 23.77
Epoch: 0, Step: 3568, Batch(micro): 3568, Batch (considering grad accum): 446,  Loss: 5.8388, Time: 7.60s, Token/s: 67.32
Epoch: 0, Step: 3569, Batch(micro): 3569, Batch (considering grad accum): 446,  Loss: 6.4773, Time: 3.85s, Token/s: 132.92
Epoch: 0, Step: 3570, Batch(micro): 3570, Batch (considering grad accum): 446,  Loss: 6.6011, Time: 3.41s, Token/s: 150.16
Epoch: 0, Step: 3571, Batch(micro): 3571, Batch (considering grad accum): 446,  Loss: 6.8032, Time: 3.56s, Token/s: 143.80
Epoch: 0, Step: 3572, Batch(micro): 3572, Batch (considering grad accum): 446,  Loss: 6.4988, Time: 3.49s, Token/s: 146.50
Epoch: 0, Step: 3573, Batch(micro): 3573, Batch (considering grad accum): 446,  Loss: 6.6267, Time: 3.41s, Token/s: 150.29
Epoch: 0, Step: 3574, Batch(micro): 3574, Batch (considering grad accum): 446,  Loss: 7.5699, Time: 3.54s, Token/s: 144.83
Epoch: 0, Step: 3575, Batch(micro): 3575, Batch (considering grad accum): 446,  Loss: 6.4561, Time: 21.22s, Token/s: 24.13
Epoch: 0, Step: 3576, Batch(micro): 3576, Batch (considering grad accum): 447,  Loss: 6.5522, Time: 6.25s, Token/s: 81.86
Epoch: 0, Step: 3577, Batch(micro): 3577, Batch (considering grad accum): 447,  Loss: 7.1478, Time: 3.79s, Token/s: 135.14
Epoch: 0, Step: 3578, Batch(micro): 3578, Batch (considering grad accum): 447,  Loss: 7.2240, Time: 4.34s, Token/s: 117.91
Epoch: 0, Step: 3579, Batch(micro): 3579, Batch (considering grad accum): 447,  Loss: 8.2441, Time: 3.83s, Token/s: 133.65
Epoch: 0, Step: 3580, Batch(micro): 3580, Batch (considering grad accum): 447,  Loss: 7.5526, Time: 3.29s, Token/s: 155.64
Epoch: 0, Step: 3581, Batch(micro): 3581, Batch (considering grad accum): 447,  Loss: 7.7752, Time: 3.44s, Token/s: 148.80
Epoch: 0, Step: 3582, Batch(micro): 3582, Batch (considering grad accum): 447,  Loss: 6.2760, Time: 3.36s, Token/s: 152.45
Epoch: 0, Step: 3583, Batch(micro): 3583, Batch (considering grad accum): 447,  Loss: 6.6116, Time: 20.96s, Token/s: 24.43
Epoch: 0, Step: 3584, Batch(micro): 3584, Batch (considering grad accum): 448,  Loss: 6.9090, Time: 9.55s, Token/s: 53.62
Epoch: 0, Step: 3585, Batch(micro): 3585, Batch (considering grad accum): 448,  Loss: 6.8931, Time: 3.31s, Token/s: 154.72
Epoch: 0, Step: 3586, Batch(micro): 3586, Batch (considering grad accum): 448,  Loss: 6.6809, Time: 3.19s, Token/s: 160.48
Epoch: 0, Step: 3587, Batch(micro): 3587, Batch (considering grad accum): 448,  Loss: 6.7517, Time: 3.21s, Token/s: 159.34
Epoch: 0, Step: 3588, Batch(micro): 3588, Batch (considering grad accum): 448,  Loss: 6.7065, Time: 3.60s, Token/s: 142.16
Epoch: 0, Step: 3589, Batch(micro): 3589, Batch (considering grad accum): 448,  Loss: 6.6158, Time: 3.35s, Token/s: 152.77
Epoch: 0, Step: 3590, Batch(micro): 3590, Batch (considering grad accum): 448,  Loss: 6.2103, Time: 3.37s, Token/s: 152.13
Epoch: 0, Step: 3591, Batch(micro): 3591, Batch (considering grad accum): 448,  Loss: 6.7888, Time: 21.36s, Token/s: 23.97
Epoch: 0, Step: 3592, Batch(micro): 3592, Batch (considering grad accum): 449,  Loss: 6.1361, Time: 7.14s, Token/s: 71.71
Epoch: 0, Step: 3593, Batch(micro): 3593, Batch (considering grad accum): 449,  Loss: 6.0430, Time: 3.78s, Token/s: 135.28
Epoch: 0, Step: 3594, Batch(micro): 3594, Batch (considering grad accum): 449,  Loss: 6.2721, Time: 3.58s, Token/s: 143.10
Epoch: 0, Step: 3595, Batch(micro): 3595, Batch (considering grad accum): 449,  Loss: 6.1996, Time: 3.83s, Token/s: 133.63
Epoch: 0, Step: 3596, Batch(micro): 3596, Batch (considering grad accum): 449,  Loss: 6.0122, Time: 3.65s, Token/s: 140.38
Epoch: 0, Step: 3597, Batch(micro): 3597, Batch (considering grad accum): 449,  Loss: 6.6116, Time: 3.62s, Token/s: 141.40
Epoch: 0, Step: 3598, Batch(micro): 3598, Batch (considering grad accum): 449,  Loss: 6.3653, Time: 3.66s, Token/s: 139.97
Epoch: 0, Step: 3599, Batch(micro): 3599, Batch (considering grad accum): 449,  Loss: 6.2926, Time: 20.93s, Token/s: 24.46
Updating MLP bias
Epoch: 0, Step: 3600, Batch(micro): 3600, Batch (considering grad accum): 450,  Loss: 6.6104, Time: 8.94s, Token/s: 57.30
Epoch: 0, Step: 3601, Batch(micro): 3601, Batch (considering grad accum): 450,  Loss: 7.4042, Time: 3.55s, Token/s: 144.35
Epoch: 0, Step: 3602, Batch(micro): 3602, Batch (considering grad accum): 450,  Loss: 6.9012, Time: 3.22s, Token/s: 159.06
Epoch: 0, Step: 3603, Batch(micro): 3603, Batch (considering grad accum): 450,  Loss: 7.1248, Time: 3.41s, Token/s: 150.34
Epoch: 0, Step: 3604, Batch(micro): 3604, Batch (considering grad accum): 450,  Loss: 7.5594, Time: 3.90s, Token/s: 131.27
Epoch: 0, Step: 3605, Batch(micro): 3605, Batch (considering grad accum): 450,  Loss: 7.2591, Time: 3.88s, Token/s: 132.03
Epoch: 0, Step: 3606, Batch(micro): 3606, Batch (considering grad accum): 450,  Loss: 7.1461, Time: 3.63s, Token/s: 141.17
Epoch: 0, Step: 3607, Batch(micro): 3607, Batch (considering grad accum): 450,  Loss: 6.3080, Time: 23.49s, Token/s: 21.80
Epoch: 0, Step: 3608, Batch(micro): 3608, Batch (considering grad accum): 451,  Loss: 7.6529, Time: 7.16s, Token/s: 71.52
Epoch: 0, Step: 3609, Batch(micro): 3609, Batch (considering grad accum): 451,  Loss: 6.8171, Time: 3.76s, Token/s: 136.00
Epoch: 0, Step: 3610, Batch(micro): 3610, Batch (considering grad accum): 451,  Loss: 6.4996, Time: 3.17s, Token/s: 161.32
Epoch: 0, Step: 3611, Batch(micro): 3611, Batch (considering grad accum): 451,  Loss: 6.4083, Time: 3.34s, Token/s: 153.48
Epoch: 0, Step: 3612, Batch(micro): 3612, Batch (considering grad accum): 451,  Loss: 6.2007, Time: 3.22s, Token/s: 159.02
Epoch: 0, Step: 3613, Batch(micro): 3613, Batch (considering grad accum): 451,  Loss: 6.8017, Time: 3.23s, Token/s: 158.29
Epoch: 0, Step: 3614, Batch(micro): 3614, Batch (considering grad accum): 451,  Loss: 6.6610, Time: 3.62s, Token/s: 141.58
Epoch: 0, Step: 3615, Batch(micro): 3615, Batch (considering grad accum): 451,  Loss: 6.4323, Time: 24.46s, Token/s: 20.93
Epoch: 0, Step: 3616, Batch(micro): 3616, Batch (considering grad accum): 452,  Loss: 6.9053, Time: 6.56s, Token/s: 78.03
Epoch: 0, Step: 3617, Batch(micro): 3617, Batch (considering grad accum): 452,  Loss: 6.4084, Time: 3.76s, Token/s: 136.35
Epoch: 0, Step: 3618, Batch(micro): 3618, Batch (considering grad accum): 452,  Loss: 6.7016, Time: 3.23s, Token/s: 158.75
Epoch: 0, Step: 3619, Batch(micro): 3619, Batch (considering grad accum): 452,  Loss: 6.7401, Time: 3.22s, Token/s: 159.09
Epoch: 0, Step: 3620, Batch(micro): 3620, Batch (considering grad accum): 452,  Loss: 6.2198, Time: 3.65s, Token/s: 140.20
Epoch: 0, Step: 3621, Batch(micro): 3621, Batch (considering grad accum): 452,  Loss: 6.6648, Time: 3.38s, Token/s: 151.42
Epoch: 0, Step: 3622, Batch(micro): 3622, Batch (considering grad accum): 452,  Loss: 6.6924, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 3623, Batch(micro): 3623, Batch (considering grad accum): 452,  Loss: 6.4359, Time: 25.45s, Token/s: 20.12
Epoch: 0, Step: 3624, Batch(micro): 3624, Batch (considering grad accum): 453,  Loss: 6.6660, Time: 7.82s, Token/s: 65.44
Epoch: 0, Step: 3625, Batch(micro): 3625, Batch (considering grad accum): 453,  Loss: 6.9791, Time: 3.83s, Token/s: 133.53
Epoch: 0, Step: 3626, Batch(micro): 3626, Batch (considering grad accum): 453,  Loss: 6.6537, Time: 3.48s, Token/s: 147.07
Epoch: 0, Step: 3627, Batch(micro): 3627, Batch (considering grad accum): 453,  Loss: 7.3700, Time: 3.41s, Token/s: 150.19
Epoch: 0, Step: 3628, Batch(micro): 3628, Batch (considering grad accum): 453,  Loss: 7.0915, Time: 3.63s, Token/s: 141.20
Epoch: 0, Step: 3629, Batch(micro): 3629, Batch (considering grad accum): 453,  Loss: 7.7450, Time: 3.43s, Token/s: 149.40
Epoch: 0, Step: 3630, Batch(micro): 3630, Batch (considering grad accum): 453,  Loss: 8.0896, Time: 3.23s, Token/s: 158.47
Epoch: 0, Step: 3631, Batch(micro): 3631, Batch (considering grad accum): 453,  Loss: 6.6671, Time: 24.19s, Token/s: 21.16
Epoch: 0, Step: 3632, Batch(micro): 3632, Batch (considering grad accum): 454,  Loss: 6.6701, Time: 8.20s, Token/s: 62.43
Epoch: 0, Step: 3633, Batch(micro): 3633, Batch (considering grad accum): 454,  Loss: 6.3944, Time: 3.89s, Token/s: 131.66
Epoch: 0, Step: 3634, Batch(micro): 3634, Batch (considering grad accum): 454,  Loss: 6.4620, Time: 3.47s, Token/s: 147.58
Epoch: 0, Step: 3635, Batch(micro): 3635, Batch (considering grad accum): 454,  Loss: 6.2827, Time: 3.61s, Token/s: 141.97
Epoch: 0, Step: 3636, Batch(micro): 3636, Batch (considering grad accum): 454,  Loss: 6.2932, Time: 3.47s, Token/s: 147.71
Epoch: 0, Step: 3637, Batch(micro): 3637, Batch (considering grad accum): 454,  Loss: 7.5167, Time: 3.51s, Token/s: 145.81
Epoch: 0, Step: 3638, Batch(micro): 3638, Batch (considering grad accum): 454,  Loss: 7.2143, Time: 3.47s, Token/s: 147.73
Epoch: 0, Step: 3639, Batch(micro): 3639, Batch (considering grad accum): 454,  Loss: 6.7363, Time: 24.06s, Token/s: 21.28
Epoch: 0, Step: 3640, Batch(micro): 3640, Batch (considering grad accum): 455,  Loss: 6.3134, Time: 7.81s, Token/s: 65.57
Epoch: 0, Step: 3641, Batch(micro): 3641, Batch (considering grad accum): 455,  Loss: 6.5170, Time: 4.08s, Token/s: 125.48
Epoch: 0, Step: 3642, Batch(micro): 3642, Batch (considering grad accum): 455,  Loss: 7.0207, Time: 3.34s, Token/s: 153.15
Epoch: 0, Step: 3643, Batch(micro): 3643, Batch (considering grad accum): 455,  Loss: 6.7425, Time: 3.33s, Token/s: 153.54
Epoch: 0, Step: 3644, Batch(micro): 3644, Batch (considering grad accum): 455,  Loss: 7.5044, Time: 3.72s, Token/s: 137.62
Epoch: 0, Step: 3645, Batch(micro): 3645, Batch (considering grad accum): 455,  Loss: 8.3623, Time: 3.54s, Token/s: 144.64
Epoch: 0, Step: 3646, Batch(micro): 3646, Batch (considering grad accum): 455,  Loss: 6.7284, Time: 3.58s, Token/s: 142.99
Epoch: 0, Step: 3647, Batch(micro): 3647, Batch (considering grad accum): 455,  Loss: 6.0973, Time: 22.79s, Token/s: 22.47
Epoch: 0, Step: 3648, Batch(micro): 3648, Batch (considering grad accum): 456,  Loss: 6.5951, Time: 8.89s, Token/s: 57.58
Epoch: 0, Step: 3649, Batch(micro): 3649, Batch (considering grad accum): 456,  Loss: 7.4366, Time: 4.03s, Token/s: 127.20
Epoch: 0, Step: 3650, Batch(micro): 3650, Batch (considering grad accum): 456,  Loss: 6.4813, Time: 3.73s, Token/s: 137.08
Epoch: 0, Step: 3651, Batch(micro): 3651, Batch (considering grad accum): 456,  Loss: 6.1754, Time: 3.60s, Token/s: 142.42
Epoch: 0, Step: 3652, Batch(micro): 3652, Batch (considering grad accum): 456,  Loss: 6.5757, Time: 3.30s, Token/s: 155.08
Epoch: 0, Step: 3653, Batch(micro): 3653, Batch (considering grad accum): 456,  Loss: 6.7766, Time: 3.28s, Token/s: 155.92
Epoch: 0, Step: 3654, Batch(micro): 3654, Batch (considering grad accum): 456,  Loss: 6.9174, Time: 3.29s, Token/s: 155.56
Epoch: 0, Step: 3655, Batch(micro): 3655, Batch (considering grad accum): 456,  Loss: 6.6848, Time: 24.98s, Token/s: 20.49
Epoch: 0, Step: 3656, Batch(micro): 3656, Batch (considering grad accum): 457,  Loss: 6.2822, Time: 7.45s, Token/s: 68.74
Epoch: 0, Step: 3657, Batch(micro): 3657, Batch (considering grad accum): 457,  Loss: 6.8089, Time: 3.76s, Token/s: 136.23
Epoch: 0, Step: 3658, Batch(micro): 3658, Batch (considering grad accum): 457,  Loss: 6.8990, Time: 3.66s, Token/s: 140.00
Epoch: 0, Step: 3659, Batch(micro): 3659, Batch (considering grad accum): 457,  Loss: 6.9127, Time: 3.52s, Token/s: 145.37
Epoch: 0, Step: 3660, Batch(micro): 3660, Batch (considering grad accum): 457,  Loss: 6.8566, Time: 3.36s, Token/s: 152.54
Epoch: 0, Step: 3661, Batch(micro): 3661, Batch (considering grad accum): 457,  Loss: 7.0962, Time: 3.35s, Token/s: 152.63
Epoch: 0, Step: 3662, Batch(micro): 3662, Batch (considering grad accum): 457,  Loss: 6.9682, Time: 3.49s, Token/s: 146.57
Epoch: 0, Step: 3663, Batch(micro): 3663, Batch (considering grad accum): 457,  Loss: 6.0659, Time: 19.55s, Token/s: 26.19
Epoch: 0, Step: 3664, Batch(micro): 3664, Batch (considering grad accum): 458,  Loss: 6.8014, Time: 5.96s, Token/s: 85.92
Epoch: 0, Step: 3665, Batch(micro): 3665, Batch (considering grad accum): 458,  Loss: 6.3041, Time: 3.62s, Token/s: 141.32
Epoch: 0, Step: 3666, Batch(micro): 3666, Batch (considering grad accum): 458,  Loss: 6.2976, Time: 3.20s, Token/s: 159.77
Epoch: 0, Step: 3667, Batch(micro): 3667, Batch (considering grad accum): 458,  Loss: 5.9004, Time: 3.58s, Token/s: 143.04
Epoch: 0, Step: 3668, Batch(micro): 3668, Batch (considering grad accum): 458,  Loss: 6.6649, Time: 3.31s, Token/s: 154.48
Epoch: 0, Step: 3669, Batch(micro): 3669, Batch (considering grad accum): 458,  Loss: 6.4074, Time: 3.50s, Token/s: 146.12
Epoch: 0, Step: 3670, Batch(micro): 3670, Batch (considering grad accum): 458,  Loss: 6.3529, Time: 3.36s, Token/s: 152.35
Epoch: 0, Step: 3671, Batch(micro): 3671, Batch (considering grad accum): 458,  Loss: 6.7777, Time: 18.42s, Token/s: 27.80
Epoch: 0, Step: 3672, Batch(micro): 3672, Batch (considering grad accum): 459,  Loss: 6.5905, Time: 6.74s, Token/s: 75.96
Epoch: 0, Step: 3673, Batch(micro): 3673, Batch (considering grad accum): 459,  Loss: 6.7154, Time: 3.36s, Token/s: 152.27
Epoch: 0, Step: 3674, Batch(micro): 3674, Batch (considering grad accum): 459,  Loss: 5.9786, Time: 3.39s, Token/s: 151.11
Epoch: 0, Step: 3675, Batch(micro): 3675, Batch (considering grad accum): 459,  Loss: 6.0950, Time: 3.22s, Token/s: 158.96
Epoch: 0, Step: 3676, Batch(micro): 3676, Batch (considering grad accum): 459,  Loss: 6.2664, Time: 2.85s, Token/s: 179.94
Epoch: 0, Step: 3677, Batch(micro): 3677, Batch (considering grad accum): 459,  Loss: 6.8055, Time: 3.07s, Token/s: 166.78
Epoch: 0, Step: 3678, Batch(micro): 3678, Batch (considering grad accum): 459,  Loss: 6.4999, Time: 3.03s, Token/s: 169.08
Epoch: 0, Step: 3679, Batch(micro): 3679, Batch (considering grad accum): 459,  Loss: 6.9538, Time: 20.40s, Token/s: 25.09
Epoch: 0, Step: 3680, Batch(micro): 3680, Batch (considering grad accum): 460,  Loss: 6.6351, Time: 7.03s, Token/s: 72.88
Epoch: 0, Step: 3681, Batch(micro): 3681, Batch (considering grad accum): 460,  Loss: 6.3050, Time: 3.79s, Token/s: 135.22
Epoch: 0, Step: 3682, Batch(micro): 3682, Batch (considering grad accum): 460,  Loss: 6.2826, Time: 3.19s, Token/s: 160.71
Epoch: 0, Step: 3683, Batch(micro): 3683, Batch (considering grad accum): 460,  Loss: 6.2097, Time: 3.20s, Token/s: 160.13
Epoch: 0, Step: 3684, Batch(micro): 3684, Batch (considering grad accum): 460,  Loss: 6.5298, Time: 3.53s, Token/s: 144.84
Epoch: 0, Step: 3685, Batch(micro): 3685, Batch (considering grad accum): 460,  Loss: 6.4157, Time: 3.50s, Token/s: 146.40
Epoch: 0, Step: 3686, Batch(micro): 3686, Batch (considering grad accum): 460,  Loss: 5.9456, Time: 3.48s, Token/s: 147.15
Epoch: 0, Step: 3687, Batch(micro): 3687, Batch (considering grad accum): 460,  Loss: 6.5293, Time: 19.35s, Token/s: 26.46
Epoch: 0, Step: 3688, Batch(micro): 3688, Batch (considering grad accum): 461,  Loss: 6.0258, Time: 7.24s, Token/s: 70.73
Epoch: 0, Step: 3689, Batch(micro): 3689, Batch (considering grad accum): 461,  Loss: 6.7512, Time: 3.71s, Token/s: 137.92
Epoch: 0, Step: 3690, Batch(micro): 3690, Batch (considering grad accum): 461,  Loss: 6.4449, Time: 3.17s, Token/s: 161.26
Epoch: 0, Step: 3691, Batch(micro): 3691, Batch (considering grad accum): 461,  Loss: 6.2848, Time: 3.48s, Token/s: 147.24
Epoch: 0, Step: 3692, Batch(micro): 3692, Batch (considering grad accum): 461,  Loss: 6.7284, Time: 3.62s, Token/s: 141.61
Epoch: 0, Step: 3693, Batch(micro): 3693, Batch (considering grad accum): 461,  Loss: 7.8191, Time: 3.42s, Token/s: 149.62
Epoch: 0, Step: 3694, Batch(micro): 3694, Batch (considering grad accum): 461,  Loss: 6.0253, Time: 3.52s, Token/s: 145.63
Epoch: 0, Step: 3695, Batch(micro): 3695, Batch (considering grad accum): 461,  Loss: 6.2595, Time: 19.58s, Token/s: 26.14
Epoch: 0, Step: 3696, Batch(micro): 3696, Batch (considering grad accum): 462,  Loss: 6.8668, Time: 6.22s, Token/s: 82.28
Epoch: 0, Step: 3697, Batch(micro): 3697, Batch (considering grad accum): 462,  Loss: 6.8095, Time: 4.11s, Token/s: 124.44
Epoch: 0, Step: 3698, Batch(micro): 3698, Batch (considering grad accum): 462,  Loss: 6.9578, Time: 3.43s, Token/s: 149.36
Epoch: 0, Step: 3699, Batch(micro): 3699, Batch (considering grad accum): 462,  Loss: 6.5572, Time: 3.51s, Token/s: 145.83
Updating MLP bias
Epoch: 0, Step: 3700, Batch(micro): 3700, Batch (considering grad accum): 462,  Loss: 6.9095, Time: 3.50s, Token/s: 146.08
Epoch: 0, Step: 3701, Batch(micro): 3701, Batch (considering grad accum): 462,  Loss: 7.7993, Time: 3.55s, Token/s: 144.37
Epoch: 0, Step: 3702, Batch(micro): 3702, Batch (considering grad accum): 462,  Loss: 7.3461, Time: 3.35s, Token/s: 153.05
Epoch: 0, Step: 3703, Batch(micro): 3703, Batch (considering grad accum): 462,  Loss: 8.4845, Time: 18.81s, Token/s: 27.21
Epoch: 0, Step: 3704, Batch(micro): 3704, Batch (considering grad accum): 463,  Loss: 6.8862, Time: 6.81s, Token/s: 75.22
Epoch: 0, Step: 3705, Batch(micro): 3705, Batch (considering grad accum): 463,  Loss: 6.2839, Time: 3.81s, Token/s: 134.56
Epoch: 0, Step: 3706, Batch(micro): 3706, Batch (considering grad accum): 463,  Loss: 5.7960, Time: 3.80s, Token/s: 134.77
Epoch: 0, Step: 3707, Batch(micro): 3707, Batch (considering grad accum): 463,  Loss: 6.6948, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 3708, Batch(micro): 3708, Batch (considering grad accum): 463,  Loss: 6.4678, Time: 3.20s, Token/s: 159.91
Epoch: 0, Step: 3709, Batch(micro): 3709, Batch (considering grad accum): 463,  Loss: 6.3727, Time: 3.18s, Token/s: 160.76
Epoch: 0, Step: 3710, Batch(micro): 3710, Batch (considering grad accum): 463,  Loss: 6.7438, Time: 3.19s, Token/s: 160.26
Epoch: 0, Step: 3711, Batch(micro): 3711, Batch (considering grad accum): 463,  Loss: 7.0849, Time: 18.03s, Token/s: 28.39
Epoch: 0, Step: 3712, Batch(micro): 3712, Batch (considering grad accum): 464,  Loss: 6.6518, Time: 6.33s, Token/s: 80.87
Epoch: 0, Step: 3713, Batch(micro): 3713, Batch (considering grad accum): 464,  Loss: 6.7767, Time: 3.87s, Token/s: 132.29
Epoch: 0, Step: 3714, Batch(micro): 3714, Batch (considering grad accum): 464,  Loss: 6.8160, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 3715, Batch(micro): 3715, Batch (considering grad accum): 464,  Loss: 6.5441, Time: 3.66s, Token/s: 140.02
Epoch: 0, Step: 3716, Batch(micro): 3716, Batch (considering grad accum): 464,  Loss: 6.3148, Time: 3.56s, Token/s: 143.68
Epoch: 0, Step: 3717, Batch(micro): 3717, Batch (considering grad accum): 464,  Loss: 6.1633, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 3718, Batch(micro): 3718, Batch (considering grad accum): 464,  Loss: 6.4654, Time: 3.47s, Token/s: 147.59
Epoch: 0, Step: 3719, Batch(micro): 3719, Batch (considering grad accum): 464,  Loss: 6.4817, Time: 17.68s, Token/s: 28.96
Epoch: 0, Step: 3720, Batch(micro): 3720, Batch (considering grad accum): 465,  Loss: 6.3987, Time: 6.39s, Token/s: 80.17
Epoch: 0, Step: 3721, Batch(micro): 3721, Batch (considering grad accum): 465,  Loss: 6.4484, Time: 3.77s, Token/s: 135.86
Epoch: 0, Step: 3722, Batch(micro): 3722, Batch (considering grad accum): 465,  Loss: 6.4009, Time: 3.85s, Token/s: 133.05
Epoch: 0, Step: 3723, Batch(micro): 3723, Batch (considering grad accum): 465,  Loss: 6.7215, Time: 3.41s, Token/s: 150.14
Epoch: 0, Step: 3724, Batch(micro): 3724, Batch (considering grad accum): 465,  Loss: 6.3742, Time: 3.87s, Token/s: 132.42
Epoch: 0, Step: 3725, Batch(micro): 3725, Batch (considering grad accum): 465,  Loss: 6.8107, Time: 3.60s, Token/s: 142.15
Epoch: 0, Step: 3726, Batch(micro): 3726, Batch (considering grad accum): 465,  Loss: 6.2851, Time: 4.25s, Token/s: 120.48
Epoch: 0, Step: 3727, Batch(micro): 3727, Batch (considering grad accum): 465,  Loss: 6.7377, Time: 18.86s, Token/s: 27.15
Epoch: 0, Step: 3728, Batch(micro): 3728, Batch (considering grad accum): 466,  Loss: 6.3871, Time: 6.51s, Token/s: 78.69
Epoch: 0, Step: 3729, Batch(micro): 3729, Batch (considering grad accum): 466,  Loss: 6.8304, Time: 3.71s, Token/s: 137.93
Epoch: 0, Step: 3730, Batch(micro): 3730, Batch (considering grad accum): 466,  Loss: 7.2912, Time: 3.38s, Token/s: 151.31
Epoch: 0, Step: 3731, Batch(micro): 3731, Batch (considering grad accum): 466,  Loss: 6.9943, Time: 3.44s, Token/s: 148.87
Epoch: 0, Step: 3732, Batch(micro): 3732, Batch (considering grad accum): 466,  Loss: 7.9577, Time: 3.52s, Token/s: 145.40
Epoch: 0, Step: 3733, Batch(micro): 3733, Batch (considering grad accum): 466,  Loss: 7.7766, Time: 3.60s, Token/s: 142.07
Epoch: 0, Step: 3734, Batch(micro): 3734, Batch (considering grad accum): 466,  Loss: 6.5210, Time: 3.69s, Token/s: 138.70
Epoch: 0, Step: 3735, Batch(micro): 3735, Batch (considering grad accum): 466,  Loss: 6.6569, Time: 19.47s, Token/s: 26.30
Epoch: 0, Step: 3736, Batch(micro): 3736, Batch (considering grad accum): 467,  Loss: 7.1968, Time: 6.82s, Token/s: 75.03
Epoch: 0, Step: 3737, Batch(micro): 3737, Batch (considering grad accum): 467,  Loss: 6.9639, Time: 3.98s, Token/s: 128.68
Epoch: 0, Step: 3738, Batch(micro): 3738, Batch (considering grad accum): 467,  Loss: 6.5434, Time: 3.53s, Token/s: 145.19
Epoch: 0, Step: 3739, Batch(micro): 3739, Batch (considering grad accum): 467,  Loss: 7.2732, Time: 3.32s, Token/s: 154.43
Epoch: 0, Step: 3740, Batch(micro): 3740, Batch (considering grad accum): 467,  Loss: 7.0304, Time: 3.43s, Token/s: 149.11
Epoch: 0, Step: 3741, Batch(micro): 3741, Batch (considering grad accum): 467,  Loss: 7.0570, Time: 3.72s, Token/s: 137.58
Epoch: 0, Step: 3742, Batch(micro): 3742, Batch (considering grad accum): 467,  Loss: 6.7482, Time: 3.58s, Token/s: 142.98
Epoch: 0, Step: 3743, Batch(micro): 3743, Batch (considering grad accum): 467,  Loss: 6.5027, Time: 19.50s, Token/s: 26.26
Epoch: 0, Step: 3744, Batch(micro): 3744, Batch (considering grad accum): 468,  Loss: 7.3726, Time: 6.54s, Token/s: 78.33
Epoch: 0, Step: 3745, Batch(micro): 3745, Batch (considering grad accum): 468,  Loss: 7.2229, Time: 4.25s, Token/s: 120.53
Epoch: 0, Step: 3746, Batch(micro): 3746, Batch (considering grad accum): 468,  Loss: 6.5017, Time: 3.57s, Token/s: 143.49
Epoch: 0, Step: 3747, Batch(micro): 3747, Batch (considering grad accum): 468,  Loss: 6.5672, Time: 3.32s, Token/s: 154.01
Epoch: 0, Step: 3748, Batch(micro): 3748, Batch (considering grad accum): 468,  Loss: 6.3519, Time: 3.27s, Token/s: 156.49
Epoch: 0, Step: 3749, Batch(micro): 3749, Batch (considering grad accum): 468,  Loss: 6.5724, Time: 3.26s, Token/s: 156.99
Epoch: 0, Step: 3750, Batch(micro): 3750, Batch (considering grad accum): 468,  Loss: 6.3950, Time: 3.57s, Token/s: 143.42
Epoch: 0, Step: 3751, Batch(micro): 3751, Batch (considering grad accum): 468,  Loss: 7.0760, Time: 17.89s, Token/s: 28.62
Epoch: 0, Step: 3752, Batch(micro): 3752, Batch (considering grad accum): 469,  Loss: 7.3720, Time: 6.29s, Token/s: 81.41
Epoch: 0, Step: 3753, Batch(micro): 3753, Batch (considering grad accum): 469,  Loss: 6.1652, Time: 4.10s, Token/s: 124.75
Epoch: 0, Step: 3754, Batch(micro): 3754, Batch (considering grad accum): 469,  Loss: 6.3983, Time: 3.48s, Token/s: 147.22
Epoch: 0, Step: 3755, Batch(micro): 3755, Batch (considering grad accum): 469,  Loss: 6.1694, Time: 3.47s, Token/s: 147.74
Epoch: 0, Step: 3756, Batch(micro): 3756, Batch (considering grad accum): 469,  Loss: 6.8079, Time: 3.74s, Token/s: 136.85
Epoch: 0, Step: 3757, Batch(micro): 3757, Batch (considering grad accum): 469,  Loss: 7.1176, Time: 3.50s, Token/s: 146.29
Epoch: 0, Step: 3758, Batch(micro): 3758, Batch (considering grad accum): 469,  Loss: 7.0782, Time: 3.38s, Token/s: 151.67
Epoch: 0, Step: 3759, Batch(micro): 3759, Batch (considering grad accum): 469,  Loss: 7.1736, Time: 20.08s, Token/s: 25.50
Epoch: 0, Step: 3760, Batch(micro): 3760, Batch (considering grad accum): 470,  Loss: 6.9850, Time: 6.98s, Token/s: 73.33
Epoch: 0, Step: 3761, Batch(micro): 3761, Batch (considering grad accum): 470,  Loss: 6.3298, Time: 3.95s, Token/s: 129.52
Epoch: 0, Step: 3762, Batch(micro): 3762, Batch (considering grad accum): 470,  Loss: 6.6095, Time: 3.72s, Token/s: 137.62
Epoch: 0, Step: 3763, Batch(micro): 3763, Batch (considering grad accum): 470,  Loss: 7.3299, Time: 3.37s, Token/s: 152.01
Epoch: 0, Step: 3764, Batch(micro): 3764, Batch (considering grad accum): 470,  Loss: 6.2743, Time: 3.53s, Token/s: 144.94
Epoch: 0, Step: 3765, Batch(micro): 3765, Batch (considering grad accum): 470,  Loss: 6.2464, Time: 3.62s, Token/s: 141.28
Epoch: 0, Step: 3766, Batch(micro): 3766, Batch (considering grad accum): 470,  Loss: 6.4367, Time: 3.35s, Token/s: 152.82
Epoch: 0, Step: 3767, Batch(micro): 3767, Batch (considering grad accum): 470,  Loss: 6.6286, Time: 24.95s, Token/s: 20.52
Epoch: 0, Step: 3768, Batch(micro): 3768, Batch (considering grad accum): 471,  Loss: 6.3914, Time: 8.21s, Token/s: 62.38
Epoch: 0, Step: 3769, Batch(micro): 3769, Batch (considering grad accum): 471,  Loss: 7.3171, Time: 3.89s, Token/s: 131.47
Epoch: 0, Step: 3770, Batch(micro): 3770, Batch (considering grad accum): 471,  Loss: 6.6687, Time: 3.28s, Token/s: 155.90
Epoch: 0, Step: 3771, Batch(micro): 3771, Batch (considering grad accum): 471,  Loss: 6.4656, Time: 3.24s, Token/s: 157.96
Epoch: 0, Step: 3772, Batch(micro): 3772, Batch (considering grad accum): 471,  Loss: 6.5289, Time: 3.25s, Token/s: 157.66
Epoch: 0, Step: 3773, Batch(micro): 3773, Batch (considering grad accum): 471,  Loss: 6.4604, Time: 3.41s, Token/s: 150.20
Epoch: 0, Step: 3774, Batch(micro): 3774, Batch (considering grad accum): 471,  Loss: 6.4275, Time: 3.24s, Token/s: 157.92
Epoch: 0, Step: 3775, Batch(micro): 3775, Batch (considering grad accum): 471,  Loss: 7.2674, Time: 25.33s, Token/s: 20.22
Epoch: 0, Step: 3776, Batch(micro): 3776, Batch (considering grad accum): 472,  Loss: 6.6493, Time: 7.66s, Token/s: 66.86
Epoch: 0, Step: 3777, Batch(micro): 3777, Batch (considering grad accum): 472,  Loss: 6.6947, Time: 4.02s, Token/s: 127.48
Epoch: 0, Step: 3778, Batch(micro): 3778, Batch (considering grad accum): 472,  Loss: 5.9372, Time: 3.53s, Token/s: 145.16
Epoch: 0, Step: 3779, Batch(micro): 3779, Batch (considering grad accum): 472,  Loss: 6.0892, Time: 3.39s, Token/s: 151.26
Epoch: 0, Step: 3780, Batch(micro): 3780, Batch (considering grad accum): 472,  Loss: 6.6713, Time: 3.46s, Token/s: 148.15
Epoch: 0, Step: 3781, Batch(micro): 3781, Batch (considering grad accum): 472,  Loss: 6.4192, Time: 3.58s, Token/s: 143.13
Epoch: 0, Step: 3782, Batch(micro): 3782, Batch (considering grad accum): 472,  Loss: 6.3835, Time: 3.22s, Token/s: 159.06
Epoch: 0, Step: 3783, Batch(micro): 3783, Batch (considering grad accum): 472,  Loss: 5.8391, Time: 23.31s, Token/s: 21.96
Epoch: 0, Step: 3784, Batch(micro): 3784, Batch (considering grad accum): 473,  Loss: 6.5729, Time: 7.58s, Token/s: 67.57
Epoch: 0, Step: 3785, Batch(micro): 3785, Batch (considering grad accum): 473,  Loss: 5.9864, Time: 4.02s, Token/s: 127.49
Epoch: 0, Step: 3786, Batch(micro): 3786, Batch (considering grad accum): 473,  Loss: 6.4548, Time: 3.48s, Token/s: 147.04
Epoch: 0, Step: 3787, Batch(micro): 3787, Batch (considering grad accum): 473,  Loss: 6.8309, Time: 3.46s, Token/s: 148.10
Epoch: 0, Step: 3788, Batch(micro): 3788, Batch (considering grad accum): 473,  Loss: 7.1544, Time: 3.76s, Token/s: 136.09
Epoch: 0, Step: 3789, Batch(micro): 3789, Batch (considering grad accum): 473,  Loss: 6.7949, Time: 3.58s, Token/s: 142.83
Epoch: 0, Step: 3790, Batch(micro): 3790, Batch (considering grad accum): 473,  Loss: 6.7818, Time: 3.44s, Token/s: 148.69
Epoch: 0, Step: 3791, Batch(micro): 3791, Batch (considering grad accum): 473,  Loss: 6.2767, Time: 23.87s, Token/s: 21.45
Epoch: 0, Step: 3792, Batch(micro): 3792, Batch (considering grad accum): 474,  Loss: 6.3044, Time: 7.32s, Token/s: 69.91
Epoch: 0, Step: 3793, Batch(micro): 3793, Batch (considering grad accum): 474,  Loss: 6.6046, Time: 4.01s, Token/s: 127.58
Epoch: 0, Step: 3794, Batch(micro): 3794, Batch (considering grad accum): 474,  Loss: 6.4752, Time: 3.54s, Token/s: 144.83
Epoch: 0, Step: 3795, Batch(micro): 3795, Batch (considering grad accum): 474,  Loss: 6.3142, Time: 3.64s, Token/s: 140.69
Epoch: 0, Step: 3796, Batch(micro): 3796, Batch (considering grad accum): 474,  Loss: 6.4693, Time: 3.50s, Token/s: 146.35
Epoch: 0, Step: 3797, Batch(micro): 3797, Batch (considering grad accum): 474,  Loss: 6.8978, Time: 3.48s, Token/s: 147.26
Epoch: 0, Step: 3798, Batch(micro): 3798, Batch (considering grad accum): 474,  Loss: 7.0527, Time: 3.33s, Token/s: 153.75
Epoch: 0, Step: 3799, Batch(micro): 3799, Batch (considering grad accum): 474,  Loss: 7.3427, Time: 24.07s, Token/s: 21.27
Updating MLP bias
Epoch: 0, Step: 3800, Batch(micro): 3800, Batch (considering grad accum): 475,  Loss: 7.6695, Time: 7.08s, Token/s: 72.34
Epoch: 0, Step: 3801, Batch(micro): 3801, Batch (considering grad accum): 475,  Loss: 7.9439, Time: 3.67s, Token/s: 139.44
Epoch: 0, Step: 3802, Batch(micro): 3802, Batch (considering grad accum): 475,  Loss: 7.9487, Time: 3.53s, Token/s: 145.17
Epoch: 0, Step: 3803, Batch(micro): 3803, Batch (considering grad accum): 475,  Loss: 6.6820, Time: 3.67s, Token/s: 139.50
Epoch: 0, Step: 3804, Batch(micro): 3804, Batch (considering grad accum): 475,  Loss: 6.3020, Time: 3.60s, Token/s: 142.03
Epoch: 0, Step: 3805, Batch(micro): 3805, Batch (considering grad accum): 475,  Loss: 6.9559, Time: 3.48s, Token/s: 147.16
Epoch: 0, Step: 3806, Batch(micro): 3806, Batch (considering grad accum): 475,  Loss: 6.4025, Time: 3.33s, Token/s: 153.55
Epoch: 0, Step: 3807, Batch(micro): 3807, Batch (considering grad accum): 475,  Loss: 7.1131, Time: 24.20s, Token/s: 21.16
Epoch: 0, Step: 3808, Batch(micro): 3808, Batch (considering grad accum): 476,  Loss: 5.8855, Time: 9.59s, Token/s: 53.42
Epoch: 0, Step: 3809, Batch(micro): 3809, Batch (considering grad accum): 476,  Loss: 6.8792, Time: 3.90s, Token/s: 131.30
Epoch: 0, Step: 3810, Batch(micro): 3810, Batch (considering grad accum): 476,  Loss: 6.6073, Time: 3.95s, Token/s: 129.46
Epoch: 0, Step: 3811, Batch(micro): 3811, Batch (considering grad accum): 476,  Loss: 6.9212, Time: 3.53s, Token/s: 144.87
Epoch: 0, Step: 3812, Batch(micro): 3812, Batch (considering grad accum): 476,  Loss: 6.5420, Time: 3.20s, Token/s: 160.03
Epoch: 0, Step: 3813, Batch(micro): 3813, Batch (considering grad accum): 476,  Loss: 6.1380, Time: 3.28s, Token/s: 156.29
Epoch: 0, Step: 3814, Batch(micro): 3814, Batch (considering grad accum): 476,  Loss: 7.0125, Time: 3.23s, Token/s: 158.30
Epoch: 0, Step: 3815, Batch(micro): 3815, Batch (considering grad accum): 476,  Loss: 6.4745, Time: 23.67s, Token/s: 21.63
Epoch: 0, Step: 3816, Batch(micro): 3816, Batch (considering grad accum): 477,  Loss: 6.5453, Time: 6.02s, Token/s: 85.08
Epoch: 0, Step: 3817, Batch(micro): 3817, Batch (considering grad accum): 477,  Loss: 7.2278, Time: 3.78s, Token/s: 135.57
Epoch: 0, Step: 3818, Batch(micro): 3818, Batch (considering grad accum): 477,  Loss: 7.1560, Time: 3.78s, Token/s: 135.47
Epoch: 0, Step: 3819, Batch(micro): 3819, Batch (considering grad accum): 477,  Loss: 6.6614, Time: 3.47s, Token/s: 147.73
Epoch: 0, Step: 3820, Batch(micro): 3820, Batch (considering grad accum): 477,  Loss: 6.1914, Time: 3.55s, Token/s: 144.19
Epoch: 0, Step: 3821, Batch(micro): 3821, Batch (considering grad accum): 477,  Loss: 5.8996, Time: 3.60s, Token/s: 142.35
Epoch: 0, Step: 3822, Batch(micro): 3822, Batch (considering grad accum): 477,  Loss: 7.0992, Time: 3.39s, Token/s: 150.95
Epoch: 0, Step: 3823, Batch(micro): 3823, Batch (considering grad accum): 477,  Loss: 7.1942, Time: 22.40s, Token/s: 22.86
Epoch: 0, Step: 3824, Batch(micro): 3824, Batch (considering grad accum): 478,  Loss: 6.5146, Time: 7.35s, Token/s: 69.69
Epoch: 0, Step: 3825, Batch(micro): 3825, Batch (considering grad accum): 478,  Loss: 6.0963, Time: 3.92s, Token/s: 130.64
Epoch: 0, Step: 3826, Batch(micro): 3826, Batch (considering grad accum): 478,  Loss: 6.4805, Time: 3.45s, Token/s: 148.59
Epoch: 0, Step: 3827, Batch(micro): 3827, Batch (considering grad accum): 478,  Loss: 6.3524, Time: 3.70s, Token/s: 138.49
Epoch: 0, Step: 3828, Batch(micro): 3828, Batch (considering grad accum): 478,  Loss: 6.3765, Time: 3.68s, Token/s: 139.31
Epoch: 0, Step: 3829, Batch(micro): 3829, Batch (considering grad accum): 478,  Loss: 6.6181, Time: 3.56s, Token/s: 143.94
Epoch: 0, Step: 3830, Batch(micro): 3830, Batch (considering grad accum): 478,  Loss: 7.5856, Time: 3.43s, Token/s: 149.21
Epoch: 0, Step: 3831, Batch(micro): 3831, Batch (considering grad accum): 478,  Loss: 6.9049, Time: 21.91s, Token/s: 23.37
Epoch: 0, Step: 3832, Batch(micro): 3832, Batch (considering grad accum): 479,  Loss: 6.2611, Time: 7.55s, Token/s: 67.78
Epoch: 0, Step: 3833, Batch(micro): 3833, Batch (considering grad accum): 479,  Loss: 6.1241, Time: 3.80s, Token/s: 134.65
Epoch: 0, Step: 3834, Batch(micro): 3834, Batch (considering grad accum): 479,  Loss: 7.1382, Time: 3.78s, Token/s: 135.35
Epoch: 0, Step: 3835, Batch(micro): 3835, Batch (considering grad accum): 479,  Loss: 6.4956, Time: 3.68s, Token/s: 139.11
Epoch: 0, Step: 3836, Batch(micro): 3836, Batch (considering grad accum): 479,  Loss: 6.4793, Time: 3.41s, Token/s: 150.34
Epoch: 0, Step: 3837, Batch(micro): 3837, Batch (considering grad accum): 479,  Loss: 5.9117, Time: 3.61s, Token/s: 141.64
Epoch: 0, Step: 3838, Batch(micro): 3838, Batch (considering grad accum): 479,  Loss: 6.6717, Time: 3.37s, Token/s: 152.06
Epoch: 0, Step: 3839, Batch(micro): 3839, Batch (considering grad accum): 479,  Loss: 7.2803, Time: 23.21s, Token/s: 22.06
Epoch: 0, Step: 3840, Batch(micro): 3840, Batch (considering grad accum): 480,  Loss: 6.4625, Time: 6.27s, Token/s: 81.65
Epoch: 0, Step: 3841, Batch(micro): 3841, Batch (considering grad accum): 480,  Loss: 6.0471, Time: 3.71s, Token/s: 138.09
Epoch: 0, Step: 3842, Batch(micro): 3842, Batch (considering grad accum): 480,  Loss: 6.3340, Time: 3.62s, Token/s: 141.29
Epoch: 0, Step: 3843, Batch(micro): 3843, Batch (considering grad accum): 480,  Loss: 6.9571, Time: 3.81s, Token/s: 134.52
Epoch: 0, Step: 3844, Batch(micro): 3844, Batch (considering grad accum): 480,  Loss: 6.6693, Time: 3.51s, Token/s: 145.87
Epoch: 0, Step: 3845, Batch(micro): 3845, Batch (considering grad accum): 480,  Loss: 6.8568, Time: 3.69s, Token/s: 138.59
Epoch: 0, Step: 3846, Batch(micro): 3846, Batch (considering grad accum): 480,  Loss: 6.7166, Time: 3.46s, Token/s: 147.91
Epoch: 0, Step: 3847, Batch(micro): 3847, Batch (considering grad accum): 480,  Loss: 6.6654, Time: 23.40s, Token/s: 21.88
Epoch: 0, Step: 3848, Batch(micro): 3848, Batch (considering grad accum): 481,  Loss: 6.5197, Time: 8.17s, Token/s: 62.70
Epoch: 0, Step: 3849, Batch(micro): 3849, Batch (considering grad accum): 481,  Loss: 6.2067, Time: 3.78s, Token/s: 135.49
Epoch: 0, Step: 3850, Batch(micro): 3850, Batch (considering grad accum): 481,  Loss: 6.9730, Time: 3.21s, Token/s: 159.36
Epoch: 0, Step: 3851, Batch(micro): 3851, Batch (considering grad accum): 481,  Loss: 6.7874, Time: 3.30s, Token/s: 155.29
Epoch: 0, Step: 3852, Batch(micro): 3852, Batch (considering grad accum): 481,  Loss: 6.6093, Time: 3.26s, Token/s: 156.93
Epoch: 0, Step: 3853, Batch(micro): 3853, Batch (considering grad accum): 481,  Loss: 6.4443, Time: 4.46s, Token/s: 114.82
Epoch: 0, Step: 3854, Batch(micro): 3854, Batch (considering grad accum): 481,  Loss: 6.2421, Time: 3.32s, Token/s: 154.10
Epoch: 0, Step: 3855, Batch(micro): 3855, Batch (considering grad accum): 481,  Loss: 6.5514, Time: 21.94s, Token/s: 23.34
Epoch: 0, Step: 3856, Batch(micro): 3856, Batch (considering grad accum): 482,  Loss: 7.2232, Time: 7.18s, Token/s: 71.28
Epoch: 0, Step: 3857, Batch(micro): 3857, Batch (considering grad accum): 482,  Loss: 6.5088, Time: 3.69s, Token/s: 138.67
Epoch: 0, Step: 3858, Batch(micro): 3858, Batch (considering grad accum): 482,  Loss: 6.2309, Time: 3.60s, Token/s: 142.34
Epoch: 0, Step: 3859, Batch(micro): 3859, Batch (considering grad accum): 482,  Loss: 6.7222, Time: 3.64s, Token/s: 140.72
Epoch: 0, Step: 3860, Batch(micro): 3860, Batch (considering grad accum): 482,  Loss: 6.2840, Time: 3.59s, Token/s: 142.79
Epoch: 0, Step: 3861, Batch(micro): 3861, Batch (considering grad accum): 482,  Loss: 6.6592, Time: 3.51s, Token/s: 145.92
Epoch: 0, Step: 3862, Batch(micro): 3862, Batch (considering grad accum): 482,  Loss: 6.5078, Time: 3.73s, Token/s: 137.09
Epoch: 0, Step: 3863, Batch(micro): 3863, Batch (considering grad accum): 482,  Loss: 6.7345, Time: 26.15s, Token/s: 19.58
Epoch: 0, Step: 3864, Batch(micro): 3864, Batch (considering grad accum): 483,  Loss: 6.8726, Time: 8.41s, Token/s: 60.88
Epoch: 0, Step: 3865, Batch(micro): 3865, Batch (considering grad accum): 483,  Loss: 6.5486, Time: 3.67s, Token/s: 139.58
Epoch: 0, Step: 3866, Batch(micro): 3866, Batch (considering grad accum): 483,  Loss: 6.4827, Time: 3.63s, Token/s: 140.93
Epoch: 0, Step: 3867, Batch(micro): 3867, Batch (considering grad accum): 483,  Loss: 6.5928, Time: 3.65s, Token/s: 140.42
Epoch: 0, Step: 3868, Batch(micro): 3868, Batch (considering grad accum): 483,  Loss: 6.5890, Time: 3.45s, Token/s: 148.21
Epoch: 0, Step: 3869, Batch(micro): 3869, Batch (considering grad accum): 483,  Loss: 6.6001, Time: 3.51s, Token/s: 145.95
Epoch: 0, Step: 3870, Batch(micro): 3870, Batch (considering grad accum): 483,  Loss: 7.2936, Time: 3.54s, Token/s: 144.46
Epoch: 0, Step: 3871, Batch(micro): 3871, Batch (considering grad accum): 483,  Loss: 6.3531, Time: 23.77s, Token/s: 21.54
Epoch: 0, Step: 3872, Batch(micro): 3872, Batch (considering grad accum): 484,  Loss: 6.2712, Time: 7.29s, Token/s: 70.27
Epoch: 0, Step: 3873, Batch(micro): 3873, Batch (considering grad accum): 484,  Loss: 6.8283, Time: 3.82s, Token/s: 133.94
Epoch: 0, Step: 3874, Batch(micro): 3874, Batch (considering grad accum): 484,  Loss: 6.9527, Time: 3.54s, Token/s: 144.60
Epoch: 0, Step: 3875, Batch(micro): 3875, Batch (considering grad accum): 484,  Loss: 6.3630, Time: 3.48s, Token/s: 147.05
Epoch: 0, Step: 3876, Batch(micro): 3876, Batch (considering grad accum): 484,  Loss: 6.0261, Time: 3.45s, Token/s: 148.39
Epoch: 0, Step: 3877, Batch(micro): 3877, Batch (considering grad accum): 484,  Loss: 6.6521, Time: 3.66s, Token/s: 139.94
Epoch: 0, Step: 3878, Batch(micro): 3878, Batch (considering grad accum): 484,  Loss: 6.4638, Time: 3.58s, Token/s: 142.90
Epoch: 0, Step: 3879, Batch(micro): 3879, Batch (considering grad accum): 484,  Loss: 5.7454, Time: 23.69s, Token/s: 21.62
Epoch: 0, Step: 3880, Batch(micro): 3880, Batch (considering grad accum): 485,  Loss: 6.7193, Time: 7.02s, Token/s: 72.97
Epoch: 0, Step: 3881, Batch(micro): 3881, Batch (considering grad accum): 485,  Loss: 6.7476, Time: 3.88s, Token/s: 132.04
Epoch: 0, Step: 3882, Batch(micro): 3882, Batch (considering grad accum): 485,  Loss: 7.4163, Time: 3.60s, Token/s: 142.28
Epoch: 0, Step: 3883, Batch(micro): 3883, Batch (considering grad accum): 485,  Loss: 6.9043, Time: 3.66s, Token/s: 139.82
Epoch: 0, Step: 3884, Batch(micro): 3884, Batch (considering grad accum): 485,  Loss: 6.5862, Time: 3.72s, Token/s: 137.58
Epoch: 0, Step: 3885, Batch(micro): 3885, Batch (considering grad accum): 485,  Loss: 6.7259, Time: 3.46s, Token/s: 147.84
Epoch: 0, Step: 3886, Batch(micro): 3886, Batch (considering grad accum): 485,  Loss: 6.8489, Time: 3.29s, Token/s: 155.77
Epoch: 0, Step: 3887, Batch(micro): 3887, Batch (considering grad accum): 485,  Loss: 6.4101, Time: 23.26s, Token/s: 22.01
Epoch: 0, Step: 3888, Batch(micro): 3888, Batch (considering grad accum): 486,  Loss: 6.3221, Time: 6.88s, Token/s: 74.42
Epoch: 0, Step: 3889, Batch(micro): 3889, Batch (considering grad accum): 486,  Loss: 6.2450, Time: 3.68s, Token/s: 139.10
Epoch: 0, Step: 3890, Batch(micro): 3890, Batch (considering grad accum): 486,  Loss: 6.3732, Time: 3.12s, Token/s: 163.86
Epoch: 0, Step: 3891, Batch(micro): 3891, Batch (considering grad accum): 486,  Loss: 6.8529, Time: 3.12s, Token/s: 164.08
Epoch: 0, Step: 3892, Batch(micro): 3892, Batch (considering grad accum): 486,  Loss: 7.5237, Time: 3.16s, Token/s: 162.24
Epoch: 0, Step: 3893, Batch(micro): 3893, Batch (considering grad accum): 486,  Loss: 7.0582, Time: 3.44s, Token/s: 148.65
Epoch: 0, Step: 3894, Batch(micro): 3894, Batch (considering grad accum): 486,  Loss: 6.1800, Time: 3.89s, Token/s: 131.55
Epoch: 0, Step: 3895, Batch(micro): 3895, Batch (considering grad accum): 486,  Loss: 6.0135, Time: 23.75s, Token/s: 21.56
Epoch: 0, Step: 3896, Batch(micro): 3896, Batch (considering grad accum): 487,  Loss: 6.0250, Time: 7.74s, Token/s: 66.17
Epoch: 0, Step: 3897, Batch(micro): 3897, Batch (considering grad accum): 487,  Loss: 6.6064, Time: 3.70s, Token/s: 138.20
Epoch: 0, Step: 3898, Batch(micro): 3898, Batch (considering grad accum): 487,  Loss: 6.6600, Time: 3.18s, Token/s: 160.82
Epoch: 0, Step: 3899, Batch(micro): 3899, Batch (considering grad accum): 487,  Loss: 5.9702, Time: 3.31s, Token/s: 154.65
Updating MLP bias
Epoch: 0, Step: 3900, Batch(micro): 3900, Batch (considering grad accum): 487,  Loss: 6.4415, Time: 3.70s, Token/s: 138.20
Epoch: 0, Step: 3901, Batch(micro): 3901, Batch (considering grad accum): 487,  Loss: 6.2476, Time: 3.66s, Token/s: 139.95
Epoch: 0, Step: 3902, Batch(micro): 3902, Batch (considering grad accum): 487,  Loss: 6.9044, Time: 3.46s, Token/s: 148.00
Epoch: 0, Step: 3903, Batch(micro): 3903, Batch (considering grad accum): 487,  Loss: 6.6833, Time: 22.31s, Token/s: 22.95
Epoch: 0, Step: 3904, Batch(micro): 3904, Batch (considering grad accum): 488,  Loss: 7.1109, Time: 8.07s, Token/s: 63.43
Epoch: 0, Step: 3905, Batch(micro): 3905, Batch (considering grad accum): 488,  Loss: 6.4768, Time: 3.75s, Token/s: 136.63
Epoch: 0, Step: 3906, Batch(micro): 3906, Batch (considering grad accum): 488,  Loss: 6.8702, Time: 3.34s, Token/s: 153.11
Epoch: 0, Step: 3907, Batch(micro): 3907, Batch (considering grad accum): 488,  Loss: 6.5191, Time: 3.31s, Token/s: 154.81
Epoch: 0, Step: 3908, Batch(micro): 3908, Batch (considering grad accum): 488,  Loss: 6.9086, Time: 3.37s, Token/s: 151.84
Epoch: 0, Step: 3909, Batch(micro): 3909, Batch (considering grad accum): 488,  Loss: 6.0360, Time: 3.40s, Token/s: 150.58
Epoch: 0, Step: 3910, Batch(micro): 3910, Batch (considering grad accum): 488,  Loss: 6.3836, Time: 3.54s, Token/s: 144.55
Epoch: 0, Step: 3911, Batch(micro): 3911, Batch (considering grad accum): 488,  Loss: 6.5268, Time: 22.51s, Token/s: 22.75
Epoch: 0, Step: 3912, Batch(micro): 3912, Batch (considering grad accum): 489,  Loss: 7.5234, Time: 6.96s, Token/s: 73.52
Epoch: 0, Step: 3913, Batch(micro): 3913, Batch (considering grad accum): 489,  Loss: 6.4673, Time: 3.73s, Token/s: 137.44
Epoch: 0, Step: 3914, Batch(micro): 3914, Batch (considering grad accum): 489,  Loss: 5.9874, Time: 3.18s, Token/s: 161.23
Epoch: 0, Step: 3915, Batch(micro): 3915, Batch (considering grad accum): 489,  Loss: 6.7273, Time: 3.25s, Token/s: 157.62
Epoch: 0, Step: 3916, Batch(micro): 3916, Batch (considering grad accum): 489,  Loss: 7.1400, Time: 3.22s, Token/s: 159.08
Epoch: 0, Step: 3917, Batch(micro): 3917, Batch (considering grad accum): 489,  Loss: 6.4179, Time: 3.43s, Token/s: 149.10
Epoch: 0, Step: 3918, Batch(micro): 3918, Batch (considering grad accum): 489,  Loss: 6.7337, Time: 3.38s, Token/s: 151.64
Epoch: 0, Step: 3919, Batch(micro): 3919, Batch (considering grad accum): 489,  Loss: 6.6470, Time: 22.72s, Token/s: 22.53
Epoch: 0, Step: 3920, Batch(micro): 3920, Batch (considering grad accum): 490,  Loss: 6.5336, Time: 9.13s, Token/s: 56.06
Epoch: 0, Step: 3921, Batch(micro): 3921, Batch (considering grad accum): 490,  Loss: 6.5430, Time: 4.01s, Token/s: 127.53
Epoch: 0, Step: 3922, Batch(micro): 3922, Batch (considering grad accum): 490,  Loss: 6.5685, Time: 3.61s, Token/s: 141.76
Epoch: 0, Step: 3923, Batch(micro): 3923, Batch (considering grad accum): 490,  Loss: 6.3866, Time: 3.66s, Token/s: 139.91
Epoch: 0, Step: 3924, Batch(micro): 3924, Batch (considering grad accum): 490,  Loss: 6.5662, Time: 3.64s, Token/s: 140.66
Epoch: 0, Step: 3925, Batch(micro): 3925, Batch (considering grad accum): 490,  Loss: 6.2383, Time: 3.62s, Token/s: 141.39
Epoch: 0, Step: 3926, Batch(micro): 3926, Batch (considering grad accum): 490,  Loss: 6.3188, Time: 3.30s, Token/s: 154.99
Epoch: 0, Step: 3927, Batch(micro): 3927, Batch (considering grad accum): 490,  Loss: 6.9579, Time: 24.11s, Token/s: 21.23
Epoch: 0, Step: 3928, Batch(micro): 3928, Batch (considering grad accum): 491,  Loss: 7.3019, Time: 6.44s, Token/s: 79.52
Epoch: 0, Step: 3929, Batch(micro): 3929, Batch (considering grad accum): 491,  Loss: 7.3360, Time: 4.26s, Token/s: 120.26
Epoch: 0, Step: 3930, Batch(micro): 3930, Batch (considering grad accum): 491,  Loss: 6.8573, Time: 4.19s, Token/s: 122.10
Epoch: 0, Step: 3931, Batch(micro): 3931, Batch (considering grad accum): 491,  Loss: 6.5482, Time: 3.88s, Token/s: 132.01
Epoch: 0, Step: 3932, Batch(micro): 3932, Batch (considering grad accum): 491,  Loss: 6.6331, Time: 3.47s, Token/s: 147.70
Epoch: 0, Step: 3933, Batch(micro): 3933, Batch (considering grad accum): 491,  Loss: 7.3699, Time: 4.09s, Token/s: 125.25
Epoch: 0, Step: 3934, Batch(micro): 3934, Batch (considering grad accum): 491,  Loss: 6.5854, Time: 3.54s, Token/s: 144.52
Epoch: 0, Step: 3935, Batch(micro): 3935, Batch (considering grad accum): 491,  Loss: 6.2334, Time: 23.69s, Token/s: 21.61
Epoch: 0, Step: 3936, Batch(micro): 3936, Batch (considering grad accum): 492,  Loss: 6.5648, Time: 7.05s, Token/s: 72.65
Epoch: 0, Step: 3937, Batch(micro): 3937, Batch (considering grad accum): 492,  Loss: 6.2673, Time: 3.83s, Token/s: 133.67
Epoch: 0, Step: 3938, Batch(micro): 3938, Batch (considering grad accum): 492,  Loss: 6.6126, Time: 3.78s, Token/s: 135.38
Epoch: 0, Step: 3939, Batch(micro): 3939, Batch (considering grad accum): 492,  Loss: 6.1392, Time: 3.31s, Token/s: 154.70
Epoch: 0, Step: 3940, Batch(micro): 3940, Batch (considering grad accum): 492,  Loss: 6.5788, Time: 3.57s, Token/s: 143.24
Epoch: 0, Step: 3941, Batch(micro): 3941, Batch (considering grad accum): 492,  Loss: 7.7591, Time: 3.21s, Token/s: 159.38
Epoch: 0, Step: 3942, Batch(micro): 3942, Batch (considering grad accum): 492,  Loss: 7.0770, Time: 3.22s, Token/s: 159.04
Epoch: 0, Step: 3943, Batch(micro): 3943, Batch (considering grad accum): 492,  Loss: 6.3134, Time: 21.66s, Token/s: 23.64
Epoch: 0, Step: 3944, Batch(micro): 3944, Batch (considering grad accum): 493,  Loss: 6.5295, Time: 7.65s, Token/s: 66.90
Epoch: 0, Step: 3945, Batch(micro): 3945, Batch (considering grad accum): 493,  Loss: 6.8505, Time: 3.74s, Token/s: 136.72
Epoch: 0, Step: 3946, Batch(micro): 3946, Batch (considering grad accum): 493,  Loss: 6.5083, Time: 3.27s, Token/s: 156.65
Epoch: 0, Step: 3947, Batch(micro): 3947, Batch (considering grad accum): 493,  Loss: 6.9934, Time: 3.43s, Token/s: 149.27
Epoch: 0, Step: 3948, Batch(micro): 3948, Batch (considering grad accum): 493,  Loss: 6.3929, Time: 3.53s, Token/s: 145.19
Epoch: 0, Step: 3949, Batch(micro): 3949, Batch (considering grad accum): 493,  Loss: 6.5347, Time: 3.63s, Token/s: 141.15
Epoch: 0, Step: 3950, Batch(micro): 3950, Batch (considering grad accum): 493,  Loss: 6.4859, Time: 3.45s, Token/s: 148.21
Epoch: 0, Step: 3951, Batch(micro): 3951, Batch (considering grad accum): 493,  Loss: 6.2341, Time: 22.23s, Token/s: 23.03
Epoch: 0, Step: 3952, Batch(micro): 3952, Batch (considering grad accum): 494,  Loss: 7.3394, Time: 6.85s, Token/s: 74.73
Epoch: 0, Step: 3953, Batch(micro): 3953, Batch (considering grad accum): 494,  Loss: 7.5076, Time: 3.72s, Token/s: 137.59
Epoch: 0, Step: 3954, Batch(micro): 3954, Batch (considering grad accum): 494,  Loss: 6.5483, Time: 3.41s, Token/s: 150.12
Epoch: 0, Step: 3955, Batch(micro): 3955, Batch (considering grad accum): 494,  Loss: 6.4291, Time: 3.57s, Token/s: 143.56
Epoch: 0, Step: 3956, Batch(micro): 3956, Batch (considering grad accum): 494,  Loss: 6.2506, Time: 3.64s, Token/s: 140.67
Epoch: 0, Step: 3957, Batch(micro): 3957, Batch (considering grad accum): 494,  Loss: 6.6888, Time: 3.68s, Token/s: 139.08
Epoch: 0, Step: 3958, Batch(micro): 3958, Batch (considering grad accum): 494,  Loss: 6.5821, Time: 3.43s, Token/s: 149.11
Epoch: 0, Step: 3959, Batch(micro): 3959, Batch (considering grad accum): 494,  Loss: 6.2535, Time: 20.75s, Token/s: 24.68
Epoch: 0, Step: 3960, Batch(micro): 3960, Batch (considering grad accum): 495,  Loss: 6.7578, Time: 6.48s, Token/s: 79.03
Epoch: 0, Step: 3961, Batch(micro): 3961, Batch (considering grad accum): 495,  Loss: 6.7512, Time: 3.78s, Token/s: 135.29
Epoch: 0, Step: 3962, Batch(micro): 3962, Batch (considering grad accum): 495,  Loss: 6.5238, Time: 3.75s, Token/s: 136.56
Epoch: 0, Step: 3963, Batch(micro): 3963, Batch (considering grad accum): 495,  Loss: 6.6589, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 3964, Batch(micro): 3964, Batch (considering grad accum): 495,  Loss: 7.2818, Time: 3.68s, Token/s: 139.03
Epoch: 0, Step: 3965, Batch(micro): 3965, Batch (considering grad accum): 495,  Loss: 6.2799, Time: 3.51s, Token/s: 145.84
Epoch: 0, Step: 3966, Batch(micro): 3966, Batch (considering grad accum): 495,  Loss: 7.1270, Time: 3.57s, Token/s: 143.56
Epoch: 0, Step: 3967, Batch(micro): 3967, Batch (considering grad accum): 495,  Loss: 6.8444, Time: 19.56s, Token/s: 26.18
Epoch: 0, Step: 3968, Batch(micro): 3968, Batch (considering grad accum): 496,  Loss: 6.6668, Time: 6.93s, Token/s: 73.87
Epoch: 0, Step: 3969, Batch(micro): 3969, Batch (considering grad accum): 496,  Loss: 6.2840, Time: 3.86s, Token/s: 132.65
Epoch: 0, Step: 3970, Batch(micro): 3970, Batch (considering grad accum): 496,  Loss: 7.1635, Time: 3.62s, Token/s: 141.30
Epoch: 0, Step: 3971, Batch(micro): 3971, Batch (considering grad accum): 496,  Loss: 6.4327, Time: 3.51s, Token/s: 145.93
Epoch: 0, Step: 3972, Batch(micro): 3972, Batch (considering grad accum): 496,  Loss: 6.2059, Time: 3.53s, Token/s: 145.10
Epoch: 0, Step: 3973, Batch(micro): 3973, Batch (considering grad accum): 496,  Loss: 6.2365, Time: 4.36s, Token/s: 117.32
Epoch: 0, Step: 3974, Batch(micro): 3974, Batch (considering grad accum): 496,  Loss: 6.4879, Time: 3.46s, Token/s: 148.18
Epoch: 0, Step: 3975, Batch(micro): 3975, Batch (considering grad accum): 496,  Loss: 6.4751, Time: 18.28s, Token/s: 28.00
Epoch: 0, Step: 3976, Batch(micro): 3976, Batch (considering grad accum): 497,  Loss: 6.6563, Time: 6.98s, Token/s: 73.38
Epoch: 0, Step: 3977, Batch(micro): 3977, Batch (considering grad accum): 497,  Loss: 5.7603, Time: 3.81s, Token/s: 134.34
Epoch: 0, Step: 3978, Batch(micro): 3978, Batch (considering grad accum): 497,  Loss: 6.3233, Time: 3.53s, Token/s: 145.14
Epoch: 0, Step: 3979, Batch(micro): 3979, Batch (considering grad accum): 497,  Loss: 6.3360, Time: 3.60s, Token/s: 142.29
Epoch: 0, Step: 3980, Batch(micro): 3980, Batch (considering grad accum): 497,  Loss: 6.8084, Time: 3.55s, Token/s: 144.18
Epoch: 0, Step: 3981, Batch(micro): 3981, Batch (considering grad accum): 497,  Loss: 7.0780, Time: 3.61s, Token/s: 141.99
Epoch: 0, Step: 3982, Batch(micro): 3982, Batch (considering grad accum): 497,  Loss: 7.5537, Time: 3.61s, Token/s: 141.69
Epoch: 0, Step: 3983, Batch(micro): 3983, Batch (considering grad accum): 497,  Loss: 6.2647, Time: 18.01s, Token/s: 28.43
Epoch: 0, Step: 3984, Batch(micro): 3984, Batch (considering grad accum): 498,  Loss: 6.2580, Time: 6.71s, Token/s: 76.33
Epoch: 0, Step: 3985, Batch(micro): 3985, Batch (considering grad accum): 498,  Loss: 6.3620, Time: 3.67s, Token/s: 139.46
Epoch: 0, Step: 3986, Batch(micro): 3986, Batch (considering grad accum): 498,  Loss: 5.8154, Time: 3.17s, Token/s: 161.45
Epoch: 0, Step: 3987, Batch(micro): 3987, Batch (considering grad accum): 498,  Loss: 6.4926, Time: 3.35s, Token/s: 152.68
Epoch: 0, Step: 3988, Batch(micro): 3988, Batch (considering grad accum): 498,  Loss: 5.4219, Time: 3.35s, Token/s: 152.62
Epoch: 0, Step: 3989, Batch(micro): 3989, Batch (considering grad accum): 498,  Loss: 5.8220, Time: 3.37s, Token/s: 151.94
Epoch: 0, Step: 3990, Batch(micro): 3990, Batch (considering grad accum): 498,  Loss: 6.8764, Time: 3.43s, Token/s: 149.39
Epoch: 0, Step: 3991, Batch(micro): 3991, Batch (considering grad accum): 498,  Loss: 6.9232, Time: 18.39s, Token/s: 27.84
Epoch: 0, Step: 3992, Batch(micro): 3992, Batch (considering grad accum): 499,  Loss: 6.4353, Time: 6.61s, Token/s: 77.42
Epoch: 0, Step: 3993, Batch(micro): 3993, Batch (considering grad accum): 499,  Loss: 6.1987, Time: 3.80s, Token/s: 134.64
Epoch: 0, Step: 3994, Batch(micro): 3994, Batch (considering grad accum): 499,  Loss: 6.1816, Time: 3.73s, Token/s: 137.20
Epoch: 0, Step: 3995, Batch(micro): 3995, Batch (considering grad accum): 499,  Loss: 5.9332, Time: 3.16s, Token/s: 161.91
Epoch: 0, Step: 3996, Batch(micro): 3996, Batch (considering grad accum): 499,  Loss: 6.9310, Time: 3.26s, Token/s: 157.16
Epoch: 0, Step: 3997, Batch(micro): 3997, Batch (considering grad accum): 499,  Loss: 6.3877, Time: 3.62s, Token/s: 141.61
Epoch: 0, Step: 3998, Batch(micro): 3998, Batch (considering grad accum): 499,  Loss: 6.4321, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 3999, Batch(micro): 3999, Batch (considering grad accum): 499,  Loss: 6.4727, Time: 19.65s, Token/s: 26.05
Updating MLP bias
Epoch: 0, Step: 4000, Batch(micro): 4000, Batch (considering grad accum): 500,  Loss: 5.5968, Time: 7.07s, Token/s: 72.46
Saved checkpoint at step 4000
What is Gravity? It and other. As an exciting or "At the `X as the human rights and a solid results, and how to develop a significant role of
Epoch: 0, Step: 4001, Batch(micro): 4001, Batch (considering grad accum): 500,  Loss: 6.8489, Time: 16.17s, Token/s: 31.66
Epoch: 0, Step: 4002, Batch(micro): 4002, Batch (considering grad accum): 500,  Loss: 7.5146, Time: 4.04s, Token/s: 126.70
Epoch: 0, Step: 4003, Batch(micro): 4003, Batch (considering grad accum): 500,  Loss: 5.4659, Time: 3.45s, Token/s: 148.49
Epoch: 0, Step: 4004, Batch(micro): 4004, Batch (considering grad accum): 500,  Loss: 6.0129, Time: 3.45s, Token/s: 148.59
Epoch: 0, Step: 4005, Batch(micro): 4005, Batch (considering grad accum): 500,  Loss: 6.1649, Time: 3.58s, Token/s: 143.07
Epoch: 0, Step: 4006, Batch(micro): 4006, Batch (considering grad accum): 500,  Loss: 6.1916, Time: 3.51s, Token/s: 146.05
Epoch: 0, Step: 4007, Batch(micro): 4007, Batch (considering grad accum): 500,  Loss: 6.2235, Time: 24.98s, Token/s: 20.50
Epoch: 0, Step: 4008, Batch(micro): 4008, Batch (considering grad accum): 501,  Loss: 6.0080, Time: 6.55s, Token/s: 78.19
Epoch: 0, Step: 4009, Batch(micro): 4009, Batch (considering grad accum): 501,  Loss: 6.2903, Time: 4.29s, Token/s: 119.21
Epoch: 0, Step: 4010, Batch(micro): 4010, Batch (considering grad accum): 501,  Loss: 6.6308, Time: 3.79s, Token/s: 135.11
Epoch: 0, Step: 4011, Batch(micro): 4011, Batch (considering grad accum): 501,  Loss: 6.5529, Time: 4.34s, Token/s: 117.88
Epoch: 0, Step: 4012, Batch(micro): 4012, Batch (considering grad accum): 501,  Loss: 5.8519, Time: 3.46s, Token/s: 147.92
Epoch: 0, Step: 4013, Batch(micro): 4013, Batch (considering grad accum): 501,  Loss: 6.4379, Time: 3.62s, Token/s: 141.52
Epoch: 0, Step: 4014, Batch(micro): 4014, Batch (considering grad accum): 501,  Loss: 6.1189, Time: 3.65s, Token/s: 140.39
Epoch: 0, Step: 4015, Batch(micro): 4015, Batch (considering grad accum): 501,  Loss: 6.4363, Time: 23.06s, Token/s: 22.20
Epoch: 0, Step: 4016, Batch(micro): 4016, Batch (considering grad accum): 502,  Loss: 6.6454, Time: 9.03s, Token/s: 56.73
Epoch: 0, Step: 4017, Batch(micro): 4017, Batch (considering grad accum): 502,  Loss: 6.8642, Time: 3.46s, Token/s: 147.93
Epoch: 0, Step: 4018, Batch(micro): 4018, Batch (considering grad accum): 502,  Loss: 6.4023, Time: 3.35s, Token/s: 153.02
Epoch: 0, Step: 4019, Batch(micro): 4019, Batch (considering grad accum): 502,  Loss: 6.9275, Time: 3.40s, Token/s: 150.54
Epoch: 0, Step: 4020, Batch(micro): 4020, Batch (considering grad accum): 502,  Loss: 6.1284, Time: 4.18s, Token/s: 122.52
Epoch: 0, Step: 4021, Batch(micro): 4021, Batch (considering grad accum): 502,  Loss: 6.3915, Time: 3.45s, Token/s: 148.59
Epoch: 0, Step: 4022, Batch(micro): 4022, Batch (considering grad accum): 502,  Loss: 6.5337, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 4023, Batch(micro): 4023, Batch (considering grad accum): 502,  Loss: 6.1798, Time: 22.38s, Token/s: 22.88
Epoch: 0, Step: 4024, Batch(micro): 4024, Batch (considering grad accum): 503,  Loss: 6.0502, Time: 9.48s, Token/s: 54.02
Epoch: 0, Step: 4025, Batch(micro): 4025, Batch (considering grad accum): 503,  Loss: 7.0820, Time: 3.37s, Token/s: 151.76
Epoch: 0, Step: 4026, Batch(micro): 4026, Batch (considering grad accum): 503,  Loss: 7.2026, Time: 3.16s, Token/s: 162.17
Epoch: 0, Step: 4027, Batch(micro): 4027, Batch (considering grad accum): 503,  Loss: 6.3347, Time: 3.23s, Token/s: 158.52
Epoch: 0, Step: 4028, Batch(micro): 4028, Batch (considering grad accum): 503,  Loss: 7.2613, Time: 3.99s, Token/s: 128.37
Epoch: 0, Step: 4029, Batch(micro): 4029, Batch (considering grad accum): 503,  Loss: 6.9170, Time: 3.97s, Token/s: 128.89
Epoch: 0, Step: 4030, Batch(micro): 4030, Batch (considering grad accum): 503,  Loss: 7.7828, Time: 3.64s, Token/s: 140.68
Epoch: 0, Step: 4031, Batch(micro): 4031, Batch (considering grad accum): 503,  Loss: 7.4207, Time: 24.31s, Token/s: 21.06
Epoch: 0, Step: 4032, Batch(micro): 4032, Batch (considering grad accum): 504,  Loss: 6.8671, Time: 7.87s, Token/s: 65.06
Epoch: 0, Step: 4033, Batch(micro): 4033, Batch (considering grad accum): 504,  Loss: 6.4289, Time: 4.12s, Token/s: 124.31
Epoch: 0, Step: 4034, Batch(micro): 4034, Batch (considering grad accum): 504,  Loss: 6.6093, Time: 3.52s, Token/s: 145.47
Epoch: 0, Step: 4035, Batch(micro): 4035, Batch (considering grad accum): 504,  Loss: 7.6054, Time: 3.56s, Token/s: 143.79
Epoch: 0, Step: 4036, Batch(micro): 4036, Batch (considering grad accum): 504,  Loss: 6.7722, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 4037, Batch(micro): 4037, Batch (considering grad accum): 504,  Loss: 6.7818, Time: 3.34s, Token/s: 153.18
Epoch: 0, Step: 4038, Batch(micro): 4038, Batch (considering grad accum): 504,  Loss: 6.5427, Time: 3.74s, Token/s: 136.90
Epoch: 0, Step: 4039, Batch(micro): 4039, Batch (considering grad accum): 504,  Loss: 6.4794, Time: 22.73s, Token/s: 22.52
Epoch: 0, Step: 4040, Batch(micro): 4040, Batch (considering grad accum): 505,  Loss: 6.5782, Time: 7.57s, Token/s: 67.62
Epoch: 0, Step: 4041, Batch(micro): 4041, Batch (considering grad accum): 505,  Loss: 7.1780, Time: 3.84s, Token/s: 133.44
Epoch: 0, Step: 4042, Batch(micro): 4042, Batch (considering grad accum): 505,  Loss: 6.7000, Time: 3.37s, Token/s: 152.12
Epoch: 0, Step: 4043, Batch(micro): 4043, Batch (considering grad accum): 505,  Loss: 6.7831, Time: 3.36s, Token/s: 152.37
Epoch: 0, Step: 4044, Batch(micro): 4044, Batch (considering grad accum): 505,  Loss: 5.9114, Time: 3.67s, Token/s: 139.40
Epoch: 0, Step: 4045, Batch(micro): 4045, Batch (considering grad accum): 505,  Loss: 5.9239, Time: 3.90s, Token/s: 131.28
Epoch: 0, Step: 4046, Batch(micro): 4046, Batch (considering grad accum): 505,  Loss: 6.6411, Time: 3.62s, Token/s: 141.28
Epoch: 0, Step: 4047, Batch(micro): 4047, Batch (considering grad accum): 505,  Loss: 6.3853, Time: 24.01s, Token/s: 21.33
Epoch: 0, Step: 4048, Batch(micro): 4048, Batch (considering grad accum): 506,  Loss: 6.9859, Time: 8.39s, Token/s: 61.04
Epoch: 0, Step: 4049, Batch(micro): 4049, Batch (considering grad accum): 506,  Loss: 6.0816, Time: 3.92s, Token/s: 130.45
Epoch: 0, Step: 4050, Batch(micro): 4050, Batch (considering grad accum): 506,  Loss: 6.7470, Time: 3.46s, Token/s: 148.08
Epoch: 0, Step: 4051, Batch(micro): 4051, Batch (considering grad accum): 506,  Loss: 6.3088, Time: 3.73s, Token/s: 137.25
Epoch: 0, Step: 4052, Batch(micro): 4052, Batch (considering grad accum): 506,  Loss: 6.4434, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 4053, Batch(micro): 4053, Batch (considering grad accum): 506,  Loss: 6.2516, Time: 3.59s, Token/s: 142.76
Epoch: 0, Step: 4054, Batch(micro): 4054, Batch (considering grad accum): 506,  Loss: 6.8237, Time: 3.38s, Token/s: 151.46
Epoch: 0, Step: 4055, Batch(micro): 4055, Batch (considering grad accum): 506,  Loss: 6.8825, Time: 25.54s, Token/s: 20.05
Epoch: 0, Step: 4056, Batch(micro): 4056, Batch (considering grad accum): 507,  Loss: 7.0334, Time: 7.00s, Token/s: 73.13
Epoch: 0, Step: 4057, Batch(micro): 4057, Batch (considering grad accum): 507,  Loss: 6.7306, Time: 3.68s, Token/s: 139.02
Epoch: 0, Step: 4058, Batch(micro): 4058, Batch (considering grad accum): 507,  Loss: 6.3268, Time: 3.18s, Token/s: 161.08
Epoch: 0, Step: 4059, Batch(micro): 4059, Batch (considering grad accum): 507,  Loss: 5.9798, Time: 3.21s, Token/s: 159.39
Epoch: 0, Step: 4060, Batch(micro): 4060, Batch (considering grad accum): 507,  Loss: 6.9688, Time: 3.21s, Token/s: 159.28
Epoch: 0, Step: 4061, Batch(micro): 4061, Batch (considering grad accum): 507,  Loss: 6.4828, Time: 3.40s, Token/s: 150.76
Epoch: 0, Step: 4062, Batch(micro): 4062, Batch (considering grad accum): 507,  Loss: 7.0396, Time: 3.56s, Token/s: 143.96
Epoch: 0, Step: 4063, Batch(micro): 4063, Batch (considering grad accum): 507,  Loss: 7.0475, Time: 23.67s, Token/s: 21.63
Epoch: 0, Step: 4064, Batch(micro): 4064, Batch (considering grad accum): 508,  Loss: 7.8644, Time: 8.40s, Token/s: 60.96
Epoch: 0, Step: 4065, Batch(micro): 4065, Batch (considering grad accum): 508,  Loss: 6.8217, Time: 3.23s, Token/s: 158.37
Epoch: 0, Step: 4066, Batch(micro): 4066, Batch (considering grad accum): 508,  Loss: 6.9322, Time: 3.31s, Token/s: 154.64
Epoch: 0, Step: 4067, Batch(micro): 4067, Batch (considering grad accum): 508,  Loss: 6.7620, Time: 3.27s, Token/s: 156.50
Epoch: 0, Step: 4068, Batch(micro): 4068, Batch (considering grad accum): 508,  Loss: 6.2029, Time: 3.40s, Token/s: 150.71
Epoch: 0, Step: 4069, Batch(micro): 4069, Batch (considering grad accum): 508,  Loss: 6.6117, Time: 3.46s, Token/s: 147.78
Epoch: 0, Step: 4070, Batch(micro): 4070, Batch (considering grad accum): 508,  Loss: 6.1643, Time: 3.54s, Token/s: 144.61
Epoch: 0, Step: 4071, Batch(micro): 4071, Batch (considering grad accum): 508,  Loss: 6.9174, Time: 22.34s, Token/s: 22.92
Epoch: 0, Step: 4072, Batch(micro): 4072, Batch (considering grad accum): 509,  Loss: 6.7383, Time: 8.57s, Token/s: 59.71
Epoch: 0, Step: 4073, Batch(micro): 4073, Batch (considering grad accum): 509,  Loss: 6.2884, Time: 3.37s, Token/s: 151.93
Epoch: 0, Step: 4074, Batch(micro): 4074, Batch (considering grad accum): 509,  Loss: 7.5256, Time: 3.29s, Token/s: 155.52
Epoch: 0, Step: 4075, Batch(micro): 4075, Batch (considering grad accum): 509,  Loss: 6.7728, Time: 3.36s, Token/s: 152.46
Epoch: 0, Step: 4076, Batch(micro): 4076, Batch (considering grad accum): 509,  Loss: 7.0885, Time: 3.31s, Token/s: 154.54
Epoch: 0, Step: 4077, Batch(micro): 4077, Batch (considering grad accum): 509,  Loss: 6.4250, Time: 3.46s, Token/s: 147.84
Epoch: 0, Step: 4078, Batch(micro): 4078, Batch (considering grad accum): 509,  Loss: 6.4877, Time: 3.52s, Token/s: 145.64
Epoch: 0, Step: 4079, Batch(micro): 4079, Batch (considering grad accum): 509,  Loss: 6.2153, Time: 22.21s, Token/s: 23.05
Epoch: 0, Step: 4080, Batch(micro): 4080, Batch (considering grad accum): 510,  Loss: 6.2104, Time: 7.80s, Token/s: 65.64
Epoch: 0, Step: 4081, Batch(micro): 4081, Batch (considering grad accum): 510,  Loss: 6.3114, Time: 3.72s, Token/s: 137.79
Epoch: 0, Step: 4082, Batch(micro): 4082, Batch (considering grad accum): 510,  Loss: 6.5256, Time: 4.43s, Token/s: 115.56
Epoch: 0, Step: 4083, Batch(micro): 4083, Batch (considering grad accum): 510,  Loss: 6.5353, Time: 3.61s, Token/s: 141.70
Epoch: 0, Step: 4084, Batch(micro): 4084, Batch (considering grad accum): 510,  Loss: 6.2139, Time: 3.86s, Token/s: 132.75
Epoch: 0, Step: 4085, Batch(micro): 4085, Batch (considering grad accum): 510,  Loss: 7.0404, Time: 3.54s, Token/s: 144.66
Epoch: 0, Step: 4086, Batch(micro): 4086, Batch (considering grad accum): 510,  Loss: 6.3050, Time: 3.59s, Token/s: 142.63
Epoch: 0, Step: 4087, Batch(micro): 4087, Batch (considering grad accum): 510,  Loss: 6.1773, Time: 25.30s, Token/s: 20.24
Epoch: 0, Step: 4088, Batch(micro): 4088, Batch (considering grad accum): 511,  Loss: 7.2174, Time: 9.72s, Token/s: 52.69
Epoch: 0, Step: 4089, Batch(micro): 4089, Batch (considering grad accum): 511,  Loss: 6.3857, Time: 4.16s, Token/s: 123.07
Epoch: 0, Step: 4090, Batch(micro): 4090, Batch (considering grad accum): 511,  Loss: 6.0035, Time: 3.76s, Token/s: 136.35
Epoch: 0, Step: 4091, Batch(micro): 4091, Batch (considering grad accum): 511,  Loss: 6.2938, Time: 3.49s, Token/s: 146.87
Epoch: 0, Step: 4092, Batch(micro): 4092, Batch (considering grad accum): 511,  Loss: 6.7178, Time: 3.71s, Token/s: 137.84
Epoch: 0, Step: 4093, Batch(micro): 4093, Batch (considering grad accum): 511,  Loss: 6.9937, Time: 3.61s, Token/s: 141.86
Epoch: 0, Step: 4094, Batch(micro): 4094, Batch (considering grad accum): 511,  Loss: 6.4466, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 4095, Batch(micro): 4095, Batch (considering grad accum): 511,  Loss: 6.1997, Time: 24.43s, Token/s: 20.96
Epoch: 0, Step: 4096, Batch(micro): 4096, Batch (considering grad accum): 512,  Loss: 6.8672, Time: 8.59s, Token/s: 59.59
Epoch: 0, Step: 4097, Batch(micro): 4097, Batch (considering grad accum): 512,  Loss: 6.6966, Time: 3.82s, Token/s: 133.90
Epoch: 0, Step: 4098, Batch(micro): 4098, Batch (considering grad accum): 512,  Loss: 6.0572, Time: 3.67s, Token/s: 139.48
Epoch: 0, Step: 4099, Batch(micro): 4099, Batch (considering grad accum): 512,  Loss: 7.4825, Time: 3.61s, Token/s: 142.02
Updating MLP bias
Epoch: 0, Step: 4100, Batch(micro): 4100, Batch (considering grad accum): 512,  Loss: 5.9237, Time: 3.89s, Token/s: 131.65
Epoch: 0, Step: 4101, Batch(micro): 4101, Batch (considering grad accum): 512,  Loss: 5.9085, Time: 3.31s, Token/s: 154.50
Epoch: 0, Step: 4102, Batch(micro): 4102, Batch (considering grad accum): 512,  Loss: 6.4527, Time: 3.39s, Token/s: 151.19
Epoch: 0, Step: 4103, Batch(micro): 4103, Batch (considering grad accum): 512,  Loss: 7.5916, Time: 24.14s, Token/s: 21.21
Epoch: 0, Step: 4104, Batch(micro): 4104, Batch (considering grad accum): 513,  Loss: 5.7887, Time: 7.52s, Token/s: 68.11
Epoch: 0, Step: 4105, Batch(micro): 4105, Batch (considering grad accum): 513,  Loss: 5.9642, Time: 3.80s, Token/s: 134.63
Epoch: 0, Step: 4106, Batch(micro): 4106, Batch (considering grad accum): 513,  Loss: 6.2106, Time: 3.40s, Token/s: 150.60
Epoch: 0, Step: 4107, Batch(micro): 4107, Batch (considering grad accum): 513,  Loss: 5.8662, Time: 3.70s, Token/s: 138.35
Epoch: 0, Step: 4108, Batch(micro): 4108, Batch (considering grad accum): 513,  Loss: 5.9863, Time: 3.45s, Token/s: 148.46
Epoch: 0, Step: 4109, Batch(micro): 4109, Batch (considering grad accum): 513,  Loss: 6.6299, Time: 3.47s, Token/s: 147.40
Epoch: 0, Step: 4110, Batch(micro): 4110, Batch (considering grad accum): 513,  Loss: 6.8554, Time: 3.79s, Token/s: 135.25
Epoch: 0, Step: 4111, Batch(micro): 4111, Batch (considering grad accum): 513,  Loss: 6.6022, Time: 23.22s, Token/s: 22.05
Epoch: 0, Step: 4112, Batch(micro): 4112, Batch (considering grad accum): 514,  Loss: 6.3053, Time: 5.61s, Token/s: 91.32
Epoch: 0, Step: 4113, Batch(micro): 4113, Batch (considering grad accum): 514,  Loss: 6.5277, Time: 4.31s, Token/s: 118.84
Epoch: 0, Step: 4114, Batch(micro): 4114, Batch (considering grad accum): 514,  Loss: 6.6468, Time: 3.56s, Token/s: 143.73
Epoch: 0, Step: 4115, Batch(micro): 4115, Batch (considering grad accum): 514,  Loss: 5.8270, Time: 3.32s, Token/s: 154.36
Epoch: 0, Step: 4116, Batch(micro): 4116, Batch (considering grad accum): 514,  Loss: 6.0201, Time: 3.72s, Token/s: 137.63
Epoch: 0, Step: 4117, Batch(micro): 4117, Batch (considering grad accum): 514,  Loss: 5.7020, Time: 3.57s, Token/s: 143.54
Epoch: 0, Step: 4118, Batch(micro): 4118, Batch (considering grad accum): 514,  Loss: 6.1385, Time: 3.47s, Token/s: 147.53
Epoch: 0, Step: 4119, Batch(micro): 4119, Batch (considering grad accum): 514,  Loss: 5.9106, Time: 23.75s, Token/s: 21.56
Epoch: 0, Step: 4120, Batch(micro): 4120, Batch (considering grad accum): 515,  Loss: 6.0682, Time: 7.61s, Token/s: 67.24
Epoch: 0, Step: 4121, Batch(micro): 4121, Batch (considering grad accum): 515,  Loss: 6.9654, Time: 3.29s, Token/s: 155.58
Epoch: 0, Step: 4122, Batch(micro): 4122, Batch (considering grad accum): 515,  Loss: 6.7761, Time: 3.22s, Token/s: 159.21
Epoch: 0, Step: 4123, Batch(micro): 4123, Batch (considering grad accum): 515,  Loss: 6.0050, Time: 3.68s, Token/s: 139.27
Epoch: 0, Step: 4124, Batch(micro): 4124, Batch (considering grad accum): 515,  Loss: 6.8180, Time: 3.54s, Token/s: 144.71
Epoch: 0, Step: 4125, Batch(micro): 4125, Batch (considering grad accum): 515,  Loss: 6.3868, Time: 3.54s, Token/s: 144.83
Epoch: 0, Step: 4126, Batch(micro): 4126, Batch (considering grad accum): 515,  Loss: 6.3672, Time: 3.72s, Token/s: 137.66
Epoch: 0, Step: 4127, Batch(micro): 4127, Batch (considering grad accum): 515,  Loss: 6.3645, Time: 25.55s, Token/s: 20.04
Epoch: 0, Step: 4128, Batch(micro): 4128, Batch (considering grad accum): 516,  Loss: 6.2950, Time: 7.98s, Token/s: 64.16
Epoch: 0, Step: 4129, Batch(micro): 4129, Batch (considering grad accum): 516,  Loss: 6.4797, Time: 3.97s, Token/s: 128.86
Epoch: 0, Step: 4130, Batch(micro): 4130, Batch (considering grad accum): 516,  Loss: 6.5115, Time: 4.21s, Token/s: 121.62
Epoch: 0, Step: 4131, Batch(micro): 4131, Batch (considering grad accum): 516,  Loss: 6.3190, Time: 3.74s, Token/s: 136.97
Epoch: 0, Step: 4132, Batch(micro): 4132, Batch (considering grad accum): 516,  Loss: 6.4667, Time: 3.83s, Token/s: 133.53
Epoch: 0, Step: 4133, Batch(micro): 4133, Batch (considering grad accum): 516,  Loss: 7.0681, Time: 3.65s, Token/s: 140.38
Epoch: 0, Step: 4134, Batch(micro): 4134, Batch (considering grad accum): 516,  Loss: 6.4985, Time: 3.32s, Token/s: 154.21
Epoch: 0, Step: 4135, Batch(micro): 4135, Batch (considering grad accum): 516,  Loss: 6.6078, Time: 22.70s, Token/s: 22.55
Epoch: 0, Step: 4136, Batch(micro): 4136, Batch (considering grad accum): 517,  Loss: 6.2402, Time: 7.84s, Token/s: 65.27
Epoch: 0, Step: 4137, Batch(micro): 4137, Batch (considering grad accum): 517,  Loss: 6.4554, Time: 3.89s, Token/s: 131.78
Epoch: 0, Step: 4138, Batch(micro): 4138, Batch (considering grad accum): 517,  Loss: 6.8347, Time: 3.56s, Token/s: 143.79
Epoch: 0, Step: 4139, Batch(micro): 4139, Batch (considering grad accum): 517,  Loss: 6.5195, Time: 3.31s, Token/s: 154.72
Epoch: 0, Step: 4140, Batch(micro): 4140, Batch (considering grad accum): 517,  Loss: 6.3948, Time: 3.31s, Token/s: 154.72
Epoch: 0, Step: 4141, Batch(micro): 4141, Batch (considering grad accum): 517,  Loss: 6.2172, Time: 3.91s, Token/s: 131.02
Epoch: 0, Step: 4142, Batch(micro): 4142, Batch (considering grad accum): 517,  Loss: 6.7113, Time: 3.54s, Token/s: 144.68
Epoch: 0, Step: 4143, Batch(micro): 4143, Batch (considering grad accum): 517,  Loss: 6.5026, Time: 21.95s, Token/s: 23.32
Epoch: 0, Step: 4144, Batch(micro): 4144, Batch (considering grad accum): 518,  Loss: 5.4458, Time: 6.97s, Token/s: 73.46
Epoch: 0, Step: 4145, Batch(micro): 4145, Batch (considering grad accum): 518,  Loss: 6.1571, Time: 3.80s, Token/s: 134.90
Epoch: 0, Step: 4146, Batch(micro): 4146, Batch (considering grad accum): 518,  Loss: 6.0525, Time: 3.52s, Token/s: 145.58
Epoch: 0, Step: 4147, Batch(micro): 4147, Batch (considering grad accum): 518,  Loss: 7.0713, Time: 3.49s, Token/s: 146.56
Epoch: 0, Step: 4148, Batch(micro): 4148, Batch (considering grad accum): 518,  Loss: 6.9695, Time: 3.67s, Token/s: 139.65
Epoch: 0, Step: 4149, Batch(micro): 4149, Batch (considering grad accum): 518,  Loss: 5.9998, Time: 3.44s, Token/s: 148.77
Epoch: 0, Step: 4150, Batch(micro): 4150, Batch (considering grad accum): 518,  Loss: 6.4499, Time: 3.32s, Token/s: 154.20
Epoch: 0, Step: 4151, Batch(micro): 4151, Batch (considering grad accum): 518,  Loss: 7.1611, Time: 22.90s, Token/s: 22.36
Epoch: 0, Step: 4152, Batch(micro): 4152, Batch (considering grad accum): 519,  Loss: 6.2596, Time: 9.00s, Token/s: 56.92
Epoch: 0, Step: 4153, Batch(micro): 4153, Batch (considering grad accum): 519,  Loss: 7.5266, Time: 3.30s, Token/s: 155.30
Epoch: 0, Step: 4154, Batch(micro): 4154, Batch (considering grad accum): 519,  Loss: 6.6685, Time: 3.20s, Token/s: 159.78
Epoch: 0, Step: 4155, Batch(micro): 4155, Batch (considering grad accum): 519,  Loss: 6.0946, Time: 3.20s, Token/s: 160.06
Epoch: 0, Step: 4156, Batch(micro): 4156, Batch (considering grad accum): 519,  Loss: 6.5388, Time: 3.46s, Token/s: 147.84
Epoch: 0, Step: 4157, Batch(micro): 4157, Batch (considering grad accum): 519,  Loss: 6.9563, Time: 3.30s, Token/s: 155.21
Epoch: 0, Step: 4158, Batch(micro): 4158, Batch (considering grad accum): 519,  Loss: 6.7724, Time: 3.31s, Token/s: 154.89
Epoch: 0, Step: 4159, Batch(micro): 4159, Batch (considering grad accum): 519,  Loss: 7.1295, Time: 27.13s, Token/s: 18.87
Epoch: 0, Step: 4160, Batch(micro): 4160, Batch (considering grad accum): 520,  Loss: 7.1080, Time: 7.61s, Token/s: 67.25
Epoch: 0, Step: 4161, Batch(micro): 4161, Batch (considering grad accum): 520,  Loss: 6.5900, Time: 4.03s, Token/s: 126.90
Epoch: 0, Step: 4162, Batch(micro): 4162, Batch (considering grad accum): 520,  Loss: 6.4340, Time: 3.31s, Token/s: 154.54
Epoch: 0, Step: 4163, Batch(micro): 4163, Batch (considering grad accum): 520,  Loss: 6.5633, Time: 3.36s, Token/s: 152.29
Epoch: 0, Step: 4164, Batch(micro): 4164, Batch (considering grad accum): 520,  Loss: 6.9238, Time: 3.58s, Token/s: 142.82
Epoch: 0, Step: 4165, Batch(micro): 4165, Batch (considering grad accum): 520,  Loss: 6.7743, Time: 3.49s, Token/s: 146.53
Epoch: 0, Step: 4166, Batch(micro): 4166, Batch (considering grad accum): 520,  Loss: 6.4632, Time: 3.54s, Token/s: 144.71
Epoch: 0, Step: 4167, Batch(micro): 4167, Batch (considering grad accum): 520,  Loss: 6.8902, Time: 25.00s, Token/s: 20.48
Epoch: 0, Step: 4168, Batch(micro): 4168, Batch (considering grad accum): 521,  Loss: 6.4315, Time: 6.17s, Token/s: 83.00
Epoch: 0, Step: 4169, Batch(micro): 4169, Batch (considering grad accum): 521,  Loss: 6.8178, Time: 3.65s, Token/s: 140.18
Epoch: 0, Step: 4170, Batch(micro): 4170, Batch (considering grad accum): 521,  Loss: 6.7047, Time: 3.35s, Token/s: 152.63
Epoch: 0, Step: 4171, Batch(micro): 4171, Batch (considering grad accum): 521,  Loss: 6.2821, Time: 3.38s, Token/s: 151.57
Epoch: 0, Step: 4172, Batch(micro): 4172, Batch (considering grad accum): 521,  Loss: 6.1032, Time: 3.53s, Token/s: 144.86
Epoch: 0, Step: 4173, Batch(micro): 4173, Batch (considering grad accum): 521,  Loss: 6.6794, Time: 3.55s, Token/s: 144.03
Epoch: 0, Step: 4174, Batch(micro): 4174, Batch (considering grad accum): 521,  Loss: 7.5430, Time: 3.60s, Token/s: 142.18
Epoch: 0, Step: 4175, Batch(micro): 4175, Batch (considering grad accum): 521,  Loss: 7.9475, Time: 19.88s, Token/s: 25.76
Epoch: 0, Step: 4176, Batch(micro): 4176, Batch (considering grad accum): 522,  Loss: 7.0704, Time: 6.58s, Token/s: 77.80
Epoch: 0, Step: 4177, Batch(micro): 4177, Batch (considering grad accum): 522,  Loss: 7.0497, Time: 3.66s, Token/s: 139.78
Epoch: 0, Step: 4178, Batch(micro): 4178, Batch (considering grad accum): 522,  Loss: 5.6624, Time: 3.40s, Token/s: 150.45
Epoch: 0, Step: 4179, Batch(micro): 4179, Batch (considering grad accum): 522,  Loss: 6.5466, Time: 3.88s, Token/s: 131.85
Epoch: 0, Step: 4180, Batch(micro): 4180, Batch (considering grad accum): 522,  Loss: 6.4039, Time: 3.81s, Token/s: 134.41
Epoch: 0, Step: 4181, Batch(micro): 4181, Batch (considering grad accum): 522,  Loss: 6.4772, Time: 3.74s, Token/s: 136.82
Epoch: 0, Step: 4182, Batch(micro): 4182, Batch (considering grad accum): 522,  Loss: 7.3615, Time: 3.57s, Token/s: 143.28
Epoch: 0, Step: 4183, Batch(micro): 4183, Batch (considering grad accum): 522,  Loss: 6.4156, Time: 18.69s, Token/s: 27.39
Epoch: 0, Step: 4184, Batch(micro): 4184, Batch (considering grad accum): 523,  Loss: 6.0214, Time: 6.57s, Token/s: 77.97
Epoch: 0, Step: 4185, Batch(micro): 4185, Batch (considering grad accum): 523,  Loss: 6.8674, Time: 3.68s, Token/s: 139.00
Epoch: 0, Step: 4186, Batch(micro): 4186, Batch (considering grad accum): 523,  Loss: 7.1178, Time: 3.13s, Token/s: 163.64
Epoch: 0, Step: 4187, Batch(micro): 4187, Batch (considering grad accum): 523,  Loss: 6.6915, Time: 3.22s, Token/s: 159.11
Epoch: 0, Step: 4188, Batch(micro): 4188, Batch (considering grad accum): 523,  Loss: 5.7903, Time: 3.21s, Token/s: 159.39
Epoch: 0, Step: 4189, Batch(micro): 4189, Batch (considering grad accum): 523,  Loss: 6.6811, Time: 3.34s, Token/s: 153.22
Epoch: 0, Step: 4190, Batch(micro): 4190, Batch (considering grad accum): 523,  Loss: 6.6788, Time: 3.41s, Token/s: 150.33
Epoch: 0, Step: 4191, Batch(micro): 4191, Batch (considering grad accum): 523,  Loss: 6.2055, Time: 18.42s, Token/s: 27.79
Epoch: 0, Step: 4192, Batch(micro): 4192, Batch (considering grad accum): 524,  Loss: 6.5946, Time: 8.28s, Token/s: 61.84
Epoch: 0, Step: 4193, Batch(micro): 4193, Batch (considering grad accum): 524,  Loss: 6.4343, Time: 4.21s, Token/s: 121.73
Epoch: 0, Step: 4194, Batch(micro): 4194, Batch (considering grad accum): 524,  Loss: 6.8170, Time: 3.28s, Token/s: 155.90
Epoch: 0, Step: 4195, Batch(micro): 4195, Batch (considering grad accum): 524,  Loss: 6.4013, Time: 3.96s, Token/s: 129.42
Epoch: 0, Step: 4196, Batch(micro): 4196, Batch (considering grad accum): 524,  Loss: 6.0375, Time: 3.55s, Token/s: 144.27
Epoch: 0, Step: 4197, Batch(micro): 4197, Batch (considering grad accum): 524,  Loss: 5.5130, Time: 3.48s, Token/s: 146.99
Epoch: 0, Step: 4198, Batch(micro): 4198, Batch (considering grad accum): 524,  Loss: 6.1991, Time: 3.45s, Token/s: 148.25
Epoch: 0, Step: 4199, Batch(micro): 4199, Batch (considering grad accum): 524,  Loss: 6.6005, Time: 18.89s, Token/s: 27.10
Updating MLP bias
Epoch: 0, Step: 4200, Batch(micro): 4200, Batch (considering grad accum): 525,  Loss: 6.9737, Time: 7.06s, Token/s: 72.47
Epoch: 0, Step: 4201, Batch(micro): 4201, Batch (considering grad accum): 525,  Loss: 6.2217, Time: 3.84s, Token/s: 133.24
Epoch: 0, Step: 4202, Batch(micro): 4202, Batch (considering grad accum): 525,  Loss: 6.5926, Time: 3.48s, Token/s: 147.10
Epoch: 0, Step: 4203, Batch(micro): 4203, Batch (considering grad accum): 525,  Loss: 7.2001, Time: 3.43s, Token/s: 149.29
Epoch: 0, Step: 4204, Batch(micro): 4204, Batch (considering grad accum): 525,  Loss: 6.7478, Time: 3.31s, Token/s: 154.83
Epoch: 0, Step: 4205, Batch(micro): 4205, Batch (considering grad accum): 525,  Loss: 6.0162, Time: 3.47s, Token/s: 147.74
Epoch: 0, Step: 4206, Batch(micro): 4206, Batch (considering grad accum): 525,  Loss: 6.5808, Time: 3.60s, Token/s: 142.33
Epoch: 0, Step: 4207, Batch(micro): 4207, Batch (considering grad accum): 525,  Loss: 6.5588, Time: 19.24s, Token/s: 26.61
Epoch: 0, Step: 4208, Batch(micro): 4208, Batch (considering grad accum): 526,  Loss: 6.1070, Time: 6.87s, Token/s: 74.51
Epoch: 0, Step: 4209, Batch(micro): 4209, Batch (considering grad accum): 526,  Loss: 6.2236, Time: 3.68s, Token/s: 139.08
Epoch: 0, Step: 4210, Batch(micro): 4210, Batch (considering grad accum): 526,  Loss: 6.8591, Time: 3.16s, Token/s: 162.14
Epoch: 0, Step: 4211, Batch(micro): 4211, Batch (considering grad accum): 526,  Loss: 6.5337, Time: 3.21s, Token/s: 159.36
Epoch: 0, Step: 4212, Batch(micro): 4212, Batch (considering grad accum): 526,  Loss: 6.7259, Time: 3.31s, Token/s: 154.63
Epoch: 0, Step: 4213, Batch(micro): 4213, Batch (considering grad accum): 526,  Loss: 5.6712, Time: 3.39s, Token/s: 151.13
Epoch: 0, Step: 4214, Batch(micro): 4214, Batch (considering grad accum): 526,  Loss: 5.8440, Time: 4.11s, Token/s: 124.46
Epoch: 0, Step: 4215, Batch(micro): 4215, Batch (considering grad accum): 526,  Loss: 6.7112, Time: 19.28s, Token/s: 26.55
Epoch: 0, Step: 4216, Batch(micro): 4216, Batch (considering grad accum): 527,  Loss: 6.7426, Time: 6.64s, Token/s: 77.15
Epoch: 0, Step: 4217, Batch(micro): 4217, Batch (considering grad accum): 527,  Loss: 7.4059, Time: 3.77s, Token/s: 135.98
Epoch: 0, Step: 4218, Batch(micro): 4218, Batch (considering grad accum): 527,  Loss: 6.0438, Time: 3.56s, Token/s: 143.71
Epoch: 0, Step: 4219, Batch(micro): 4219, Batch (considering grad accum): 527,  Loss: 5.9752, Time: 3.33s, Token/s: 153.64
Epoch: 0, Step: 4220, Batch(micro): 4220, Batch (considering grad accum): 527,  Loss: 6.5150, Time: 3.37s, Token/s: 152.13
Epoch: 0, Step: 4221, Batch(micro): 4221, Batch (considering grad accum): 527,  Loss: 6.7905, Time: 3.40s, Token/s: 150.69
Epoch: 0, Step: 4222, Batch(micro): 4222, Batch (considering grad accum): 527,  Loss: 6.4629, Time: 3.20s, Token/s: 160.22
Epoch: 0, Step: 4223, Batch(micro): 4223, Batch (considering grad accum): 527,  Loss: 6.8194, Time: 18.02s, Token/s: 28.42
Epoch: 0, Step: 4224, Batch(micro): 4224, Batch (considering grad accum): 528,  Loss: 6.7065, Time: 6.74s, Token/s: 75.95
Epoch: 0, Step: 4225, Batch(micro): 4225, Batch (considering grad accum): 528,  Loss: 5.9940, Time: 3.65s, Token/s: 140.46
Epoch: 0, Step: 4226, Batch(micro): 4226, Batch (considering grad accum): 528,  Loss: 6.0204, Time: 3.39s, Token/s: 151.09
Epoch: 0, Step: 4227, Batch(micro): 4227, Batch (considering grad accum): 528,  Loss: 6.5214, Time: 3.21s, Token/s: 159.60
Epoch: 0, Step: 4228, Batch(micro): 4228, Batch (considering grad accum): 528,  Loss: 5.9888, Time: 3.17s, Token/s: 161.60
Epoch: 0, Step: 4229, Batch(micro): 4229, Batch (considering grad accum): 528,  Loss: 6.1065, Time: 3.18s, Token/s: 161.04
Epoch: 0, Step: 4230, Batch(micro): 4230, Batch (considering grad accum): 528,  Loss: 6.7096, Time: 3.24s, Token/s: 158.21
Epoch: 0, Step: 4231, Batch(micro): 4231, Batch (considering grad accum): 528,  Loss: 6.3761, Time: 19.80s, Token/s: 25.86
Epoch: 0, Step: 4232, Batch(micro): 4232, Batch (considering grad accum): 529,  Loss: 6.6367, Time: 7.30s, Token/s: 70.15
Epoch: 0, Step: 4233, Batch(micro): 4233, Batch (considering grad accum): 529,  Loss: 6.5652, Time: 3.79s, Token/s: 135.10
Epoch: 0, Step: 4234, Batch(micro): 4234, Batch (considering grad accum): 529,  Loss: 7.1002, Time: 3.59s, Token/s: 142.80
Epoch: 0, Step: 4235, Batch(micro): 4235, Batch (considering grad accum): 529,  Loss: 6.8275, Time: 3.56s, Token/s: 143.92
Epoch: 0, Step: 4236, Batch(micro): 4236, Batch (considering grad accum): 529,  Loss: 6.5606, Time: 3.33s, Token/s: 153.73
Epoch: 0, Step: 4237, Batch(micro): 4237, Batch (considering grad accum): 529,  Loss: 6.7245, Time: 3.34s, Token/s: 153.27
Epoch: 0, Step: 4238, Batch(micro): 4238, Batch (considering grad accum): 529,  Loss: 6.1697, Time: 3.34s, Token/s: 153.41
Epoch: 0, Step: 4239, Batch(micro): 4239, Batch (considering grad accum): 529,  Loss: 6.0580, Time: 19.83s, Token/s: 25.82
Epoch: 0, Step: 4240, Batch(micro): 4240, Batch (considering grad accum): 530,  Loss: 6.5126, Time: 7.52s, Token/s: 68.07
Epoch: 0, Step: 4241, Batch(micro): 4241, Batch (considering grad accum): 530,  Loss: 6.3131, Time: 3.90s, Token/s: 131.32
Epoch: 0, Step: 4242, Batch(micro): 4242, Batch (considering grad accum): 530,  Loss: 6.4078, Time: 3.42s, Token/s: 149.63
Epoch: 0, Step: 4243, Batch(micro): 4243, Batch (considering grad accum): 530,  Loss: 6.2496, Time: 3.74s, Token/s: 136.88
Epoch: 0, Step: 4244, Batch(micro): 4244, Batch (considering grad accum): 530,  Loss: 7.4134, Time: 3.91s, Token/s: 130.94
Epoch: 0, Step: 4245, Batch(micro): 4245, Batch (considering grad accum): 530,  Loss: 6.6014, Time: 3.59s, Token/s: 142.59
Epoch: 0, Step: 4246, Batch(micro): 4246, Batch (considering grad accum): 530,  Loss: 6.4019, Time: 3.29s, Token/s: 155.69
Epoch: 0, Step: 4247, Batch(micro): 4247, Batch (considering grad accum): 530,  Loss: 6.5654, Time: 23.22s, Token/s: 22.05
Epoch: 0, Step: 4248, Batch(micro): 4248, Batch (considering grad accum): 531,  Loss: 6.9062, Time: 6.35s, Token/s: 80.66
Epoch: 0, Step: 4249, Batch(micro): 4249, Batch (considering grad accum): 531,  Loss: 7.1560, Time: 3.75s, Token/s: 136.40
Epoch: 0, Step: 4250, Batch(micro): 4250, Batch (considering grad accum): 531,  Loss: 6.7225, Time: 3.31s, Token/s: 154.59
Epoch: 0, Step: 4251, Batch(micro): 4251, Batch (considering grad accum): 531,  Loss: 6.9916, Time: 3.23s, Token/s: 158.76
Epoch: 0, Step: 4252, Batch(micro): 4252, Batch (considering grad accum): 531,  Loss: 6.2154, Time: 3.21s, Token/s: 159.70
Epoch: 0, Step: 4253, Batch(micro): 4253, Batch (considering grad accum): 531,  Loss: 6.7566, Time: 3.64s, Token/s: 140.82
Epoch: 0, Step: 4254, Batch(micro): 4254, Batch (considering grad accum): 531,  Loss: 6.7886, Time: 3.53s, Token/s: 145.02
Epoch: 0, Step: 4255, Batch(micro): 4255, Batch (considering grad accum): 531,  Loss: 6.6833, Time: 24.55s, Token/s: 20.85
Epoch: 0, Step: 4256, Batch(micro): 4256, Batch (considering grad accum): 532,  Loss: 6.3896, Time: 7.41s, Token/s: 69.07
Epoch: 0, Step: 4257, Batch(micro): 4257, Batch (considering grad accum): 532,  Loss: 6.4358, Time: 4.15s, Token/s: 123.43
Epoch: 0, Step: 4258, Batch(micro): 4258, Batch (considering grad accum): 532,  Loss: 6.8118, Time: 3.74s, Token/s: 136.75
Epoch: 0, Step: 4259, Batch(micro): 4259, Batch (considering grad accum): 532,  Loss: 6.6834, Time: 3.69s, Token/s: 138.68
Epoch: 0, Step: 4260, Batch(micro): 4260, Batch (considering grad accum): 532,  Loss: 6.3609, Time: 3.87s, Token/s: 132.13
Epoch: 0, Step: 4261, Batch(micro): 4261, Batch (considering grad accum): 532,  Loss: 6.0207, Time: 3.38s, Token/s: 151.34
Epoch: 0, Step: 4262, Batch(micro): 4262, Batch (considering grad accum): 532,  Loss: 6.4960, Time: 3.62s, Token/s: 141.46
Epoch: 0, Step: 4263, Batch(micro): 4263, Batch (considering grad accum): 532,  Loss: 7.1225, Time: 23.67s, Token/s: 21.63
Epoch: 0, Step: 4264, Batch(micro): 4264, Batch (considering grad accum): 533,  Loss: 6.5578, Time: 7.80s, Token/s: 65.61
Epoch: 0, Step: 4265, Batch(micro): 4265, Batch (considering grad accum): 533,  Loss: 6.4967, Time: 3.67s, Token/s: 139.54
Epoch: 0, Step: 4266, Batch(micro): 4266, Batch (considering grad accum): 533,  Loss: 6.7325, Time: 3.59s, Token/s: 142.61
Epoch: 0, Step: 4267, Batch(micro): 4267, Batch (considering grad accum): 533,  Loss: 6.7650, Time: 3.36s, Token/s: 152.58
Epoch: 0, Step: 4268, Batch(micro): 4268, Batch (considering grad accum): 533,  Loss: 6.5364, Time: 3.44s, Token/s: 148.65
Epoch: 0, Step: 4269, Batch(micro): 4269, Batch (considering grad accum): 533,  Loss: 6.5145, Time: 3.36s, Token/s: 152.28
Epoch: 0, Step: 4270, Batch(micro): 4270, Batch (considering grad accum): 533,  Loss: 6.4038, Time: 3.30s, Token/s: 155.30
Epoch: 0, Step: 4271, Batch(micro): 4271, Batch (considering grad accum): 533,  Loss: 6.3849, Time: 23.76s, Token/s: 21.55
Epoch: 0, Step: 4272, Batch(micro): 4272, Batch (considering grad accum): 534,  Loss: 7.3005, Time: 8.10s, Token/s: 63.21
Epoch: 0, Step: 4273, Batch(micro): 4273, Batch (considering grad accum): 534,  Loss: 6.6112, Time: 3.79s, Token/s: 135.07
Epoch: 0, Step: 4274, Batch(micro): 4274, Batch (considering grad accum): 534,  Loss: 6.1739, Time: 3.36s, Token/s: 152.59
Epoch: 0, Step: 4275, Batch(micro): 4275, Batch (considering grad accum): 534,  Loss: 6.1408, Time: 3.39s, Token/s: 150.87
Epoch: 0, Step: 4276, Batch(micro): 4276, Batch (considering grad accum): 534,  Loss: 6.2151, Time: 3.46s, Token/s: 147.99
Epoch: 0, Step: 4277, Batch(micro): 4277, Batch (considering grad accum): 534,  Loss: 6.7677, Time: 3.38s, Token/s: 151.27
Epoch: 0, Step: 4278, Batch(micro): 4278, Batch (considering grad accum): 534,  Loss: 7.1124, Time: 3.33s, Token/s: 153.57
Epoch: 0, Step: 4279, Batch(micro): 4279, Batch (considering grad accum): 534,  Loss: 6.1841, Time: 26.09s, Token/s: 19.62
Epoch: 0, Step: 4280, Batch(micro): 4280, Batch (considering grad accum): 535,  Loss: 6.5529, Time: 8.47s, Token/s: 60.46
Epoch: 0, Step: 4281, Batch(micro): 4281, Batch (considering grad accum): 535,  Loss: 7.3941, Time: 3.87s, Token/s: 132.43
Epoch: 0, Step: 4282, Batch(micro): 4282, Batch (considering grad accum): 535,  Loss: 6.9248, Time: 3.30s, Token/s: 155.02
Epoch: 0, Step: 4283, Batch(micro): 4283, Batch (considering grad accum): 535,  Loss: 5.8521, Time: 3.39s, Token/s: 150.94
Epoch: 0, Step: 4284, Batch(micro): 4284, Batch (considering grad accum): 535,  Loss: 6.1341, Time: 3.49s, Token/s: 146.75
Epoch: 0, Step: 4285, Batch(micro): 4285, Batch (considering grad accum): 535,  Loss: 6.4167, Time: 3.61s, Token/s: 141.98
Epoch: 0, Step: 4286, Batch(micro): 4286, Batch (considering grad accum): 535,  Loss: 6.1220, Time: 3.34s, Token/s: 153.51
Epoch: 0, Step: 4287, Batch(micro): 4287, Batch (considering grad accum): 535,  Loss: 6.1865, Time: 25.91s, Token/s: 19.76
Epoch: 0, Step: 4288, Batch(micro): 4288, Batch (considering grad accum): 536,  Loss: 6.3987, Time: 8.69s, Token/s: 58.91
Epoch: 0, Step: 4289, Batch(micro): 4289, Batch (considering grad accum): 536,  Loss: 6.6575, Time: 3.57s, Token/s: 143.58
Epoch: 0, Step: 4290, Batch(micro): 4290, Batch (considering grad accum): 536,  Loss: 6.8357, Time: 3.55s, Token/s: 144.35
Epoch: 0, Step: 4291, Batch(micro): 4291, Batch (considering grad accum): 536,  Loss: 6.1387, Time: 3.50s, Token/s: 146.29
Epoch: 0, Step: 4292, Batch(micro): 4292, Batch (considering grad accum): 536,  Loss: 6.2688, Time: 3.51s, Token/s: 145.74
Epoch: 0, Step: 4293, Batch(micro): 4293, Batch (considering grad accum): 536,  Loss: 7.2547, Time: 3.63s, Token/s: 140.88
Epoch: 0, Step: 4294, Batch(micro): 4294, Batch (considering grad accum): 536,  Loss: 6.6941, Time: 3.34s, Token/s: 153.31
Epoch: 0, Step: 4295, Batch(micro): 4295, Batch (considering grad accum): 536,  Loss: 6.3580, Time: 23.29s, Token/s: 21.99
Epoch: 0, Step: 4296, Batch(micro): 4296, Batch (considering grad accum): 537,  Loss: 6.5780, Time: 7.14s, Token/s: 71.66
Epoch: 0, Step: 4297, Batch(micro): 4297, Batch (considering grad accum): 537,  Loss: 6.8757, Time: 3.91s, Token/s: 131.09
Epoch: 0, Step: 4298, Batch(micro): 4298, Batch (considering grad accum): 537,  Loss: 7.0525, Time: 3.88s, Token/s: 132.02
Epoch: 0, Step: 4299, Batch(micro): 4299, Batch (considering grad accum): 537,  Loss: 7.0851, Time: 3.54s, Token/s: 144.68
Updating MLP bias
Epoch: 0, Step: 4300, Batch(micro): 4300, Batch (considering grad accum): 537,  Loss: 6.5267, Time: 3.52s, Token/s: 145.59
Epoch: 0, Step: 4301, Batch(micro): 4301, Batch (considering grad accum): 537,  Loss: 6.5172, Time: 3.74s, Token/s: 136.91
Epoch: 0, Step: 4302, Batch(micro): 4302, Batch (considering grad accum): 537,  Loss: 6.4100, Time: 3.48s, Token/s: 147.13
Epoch: 0, Step: 4303, Batch(micro): 4303, Batch (considering grad accum): 537,  Loss: 6.3009, Time: 23.08s, Token/s: 22.19
Epoch: 0, Step: 4304, Batch(micro): 4304, Batch (considering grad accum): 538,  Loss: 6.2273, Time: 7.38s, Token/s: 69.34
Epoch: 0, Step: 4305, Batch(micro): 4305, Batch (considering grad accum): 538,  Loss: 6.5525, Time: 3.71s, Token/s: 138.07
Epoch: 0, Step: 4306, Batch(micro): 4306, Batch (considering grad accum): 538,  Loss: 5.9022, Time: 3.21s, Token/s: 159.53
Epoch: 0, Step: 4307, Batch(micro): 4307, Batch (considering grad accum): 538,  Loss: 6.3523, Time: 3.16s, Token/s: 162.22
Epoch: 0, Step: 4308, Batch(micro): 4308, Batch (considering grad accum): 538,  Loss: 6.6735, Time: 3.37s, Token/s: 152.13
Epoch: 0, Step: 4309, Batch(micro): 4309, Batch (considering grad accum): 538,  Loss: 6.7207, Time: 3.56s, Token/s: 143.71
Epoch: 0, Step: 4310, Batch(micro): 4310, Batch (considering grad accum): 538,  Loss: 6.7803, Time: 3.52s, Token/s: 145.53
Epoch: 0, Step: 4311, Batch(micro): 4311, Batch (considering grad accum): 538,  Loss: 5.8952, Time: 22.43s, Token/s: 22.83
Epoch: 0, Step: 4312, Batch(micro): 4312, Batch (considering grad accum): 539,  Loss: 6.5974, Time: 7.48s, Token/s: 68.49
Epoch: 0, Step: 4313, Batch(micro): 4313, Batch (considering grad accum): 539,  Loss: 5.9615, Time: 3.66s, Token/s: 139.74
Epoch: 0, Step: 4314, Batch(micro): 4314, Batch (considering grad accum): 539,  Loss: 6.5760, Time: 3.22s, Token/s: 159.14
Epoch: 0, Step: 4315, Batch(micro): 4315, Batch (considering grad accum): 539,  Loss: 6.6213, Time: 3.26s, Token/s: 157.24
Epoch: 0, Step: 4316, Batch(micro): 4316, Batch (considering grad accum): 539,  Loss: 7.2807, Time: 3.25s, Token/s: 157.62
Epoch: 0, Step: 4317, Batch(micro): 4317, Batch (considering grad accum): 539,  Loss: 6.6619, Time: 3.38s, Token/s: 151.58
Epoch: 0, Step: 4318, Batch(micro): 4318, Batch (considering grad accum): 539,  Loss: 6.2662, Time: 3.57s, Token/s: 143.47
Epoch: 0, Step: 4319, Batch(micro): 4319, Batch (considering grad accum): 539,  Loss: 6.3364, Time: 24.40s, Token/s: 20.98
Epoch: 0, Step: 4320, Batch(micro): 4320, Batch (considering grad accum): 540,  Loss: 6.6327, Time: 7.56s, Token/s: 67.69
Epoch: 0, Step: 4321, Batch(micro): 4321, Batch (considering grad accum): 540,  Loss: 5.9854, Time: 3.94s, Token/s: 129.85
Epoch: 0, Step: 4322, Batch(micro): 4322, Batch (considering grad accum): 540,  Loss: 6.1442, Time: 3.60s, Token/s: 142.37
Epoch: 0, Step: 4323, Batch(micro): 4323, Batch (considering grad accum): 540,  Loss: 6.7952, Time: 3.35s, Token/s: 152.94
Epoch: 0, Step: 4324, Batch(micro): 4324, Batch (considering grad accum): 540,  Loss: 6.5040, Time: 3.43s, Token/s: 149.11
Epoch: 0, Step: 4325, Batch(micro): 4325, Batch (considering grad accum): 540,  Loss: 6.5651, Time: 3.57s, Token/s: 143.33
Epoch: 0, Step: 4326, Batch(micro): 4326, Batch (considering grad accum): 540,  Loss: 6.2984, Time: 3.52s, Token/s: 145.35
Epoch: 0, Step: 4327, Batch(micro): 4327, Batch (considering grad accum): 540,  Loss: 6.5960, Time: 25.54s, Token/s: 20.04
Epoch: 0, Step: 4328, Batch(micro): 4328, Batch (considering grad accum): 541,  Loss: 6.6924, Time: 7.39s, Token/s: 69.32
Epoch: 0, Step: 4329, Batch(micro): 4329, Batch (considering grad accum): 541,  Loss: 6.6581, Time: 3.66s, Token/s: 139.77
Epoch: 0, Step: 4330, Batch(micro): 4330, Batch (considering grad accum): 541,  Loss: 6.5698, Time: 3.25s, Token/s: 157.53
Epoch: 0, Step: 4331, Batch(micro): 4331, Batch (considering grad accum): 541,  Loss: 6.0895, Time: 3.62s, Token/s: 141.34
Epoch: 0, Step: 4332, Batch(micro): 4332, Batch (considering grad accum): 541,  Loss: 6.1468, Time: 3.66s, Token/s: 139.82
Epoch: 0, Step: 4333, Batch(micro): 4333, Batch (considering grad accum): 541,  Loss: 6.5105, Time: 3.50s, Token/s: 146.41
Epoch: 0, Step: 4334, Batch(micro): 4334, Batch (considering grad accum): 541,  Loss: 6.1645, Time: 3.49s, Token/s: 146.76
Epoch: 0, Step: 4335, Batch(micro): 4335, Batch (considering grad accum): 541,  Loss: 6.0066, Time: 25.29s, Token/s: 20.25
Epoch: 0, Step: 4336, Batch(micro): 4336, Batch (considering grad accum): 542,  Loss: 6.5878, Time: 6.36s, Token/s: 80.49
Epoch: 0, Step: 4337, Batch(micro): 4337, Batch (considering grad accum): 542,  Loss: 7.0616, Time: 4.51s, Token/s: 113.62
Epoch: 0, Step: 4338, Batch(micro): 4338, Batch (considering grad accum): 542,  Loss: 6.7018, Time: 3.58s, Token/s: 143.06
Epoch: 0, Step: 4339, Batch(micro): 4339, Batch (considering grad accum): 542,  Loss: 5.5901, Time: 4.00s, Token/s: 127.92
Epoch: 0, Step: 4340, Batch(micro): 4340, Batch (considering grad accum): 542,  Loss: 7.6324, Time: 3.53s, Token/s: 144.91
Epoch: 0, Step: 4341, Batch(micro): 4341, Batch (considering grad accum): 542,  Loss: 7.8184, Time: 3.23s, Token/s: 158.61
Epoch: 0, Step: 4342, Batch(micro): 4342, Batch (considering grad accum): 542,  Loss: 6.6363, Time: 3.26s, Token/s: 156.91
Epoch: 0, Step: 4343, Batch(micro): 4343, Batch (considering grad accum): 542,  Loss: 6.9129, Time: 24.04s, Token/s: 21.30
Epoch: 0, Step: 4344, Batch(micro): 4344, Batch (considering grad accum): 543,  Loss: 6.4206, Time: 7.02s, Token/s: 72.90
Epoch: 0, Step: 4345, Batch(micro): 4345, Batch (considering grad accum): 543,  Loss: 6.5722, Time: 3.68s, Token/s: 139.04
Epoch: 0, Step: 4346, Batch(micro): 4346, Batch (considering grad accum): 543,  Loss: 6.3628, Time: 3.53s, Token/s: 145.08
Epoch: 0, Step: 4347, Batch(micro): 4347, Batch (considering grad accum): 543,  Loss: 6.3478, Time: 3.53s, Token/s: 145.18
Epoch: 0, Step: 4348, Batch(micro): 4348, Batch (considering grad accum): 543,  Loss: 6.3544, Time: 3.42s, Token/s: 149.59
Epoch: 0, Step: 4349, Batch(micro): 4349, Batch (considering grad accum): 543,  Loss: 6.7064, Time: 3.84s, Token/s: 133.16
Epoch: 0, Step: 4350, Batch(micro): 4350, Batch (considering grad accum): 543,  Loss: 6.4837, Time: 3.64s, Token/s: 140.49
Epoch: 0, Step: 4351, Batch(micro): 4351, Batch (considering grad accum): 543,  Loss: 6.0679, Time: 23.69s, Token/s: 21.61
Epoch: 0, Step: 4352, Batch(micro): 4352, Batch (considering grad accum): 544,  Loss: 6.3944, Time: 7.16s, Token/s: 71.51
Epoch: 0, Step: 4353, Batch(micro): 4353, Batch (considering grad accum): 544,  Loss: 6.3545, Time: 3.84s, Token/s: 133.35
Epoch: 0, Step: 4354, Batch(micro): 4354, Batch (considering grad accum): 544,  Loss: 6.9521, Time: 3.37s, Token/s: 151.84
Epoch: 0, Step: 4355, Batch(micro): 4355, Batch (considering grad accum): 544,  Loss: 7.6737, Time: 3.43s, Token/s: 149.44
Epoch: 0, Step: 4356, Batch(micro): 4356, Batch (considering grad accum): 544,  Loss: 7.3299, Time: 3.53s, Token/s: 144.96
Epoch: 0, Step: 4357, Batch(micro): 4357, Batch (considering grad accum): 544,  Loss: 6.6239, Time: 3.74s, Token/s: 136.82
Epoch: 0, Step: 4358, Batch(micro): 4358, Batch (considering grad accum): 544,  Loss: 6.5126, Time: 3.46s, Token/s: 147.77
Epoch: 0, Step: 4359, Batch(micro): 4359, Batch (considering grad accum): 544,  Loss: 6.3239, Time: 22.45s, Token/s: 22.81
Epoch: 0, Step: 4360, Batch(micro): 4360, Batch (considering grad accum): 545,  Loss: 6.3331, Time: 8.69s, Token/s: 58.91
Epoch: 0, Step: 4361, Batch(micro): 4361, Batch (considering grad accum): 545,  Loss: 6.3055, Time: 3.85s, Token/s: 132.85
Epoch: 0, Step: 4362, Batch(micro): 4362, Batch (considering grad accum): 545,  Loss: 6.5715, Time: 3.53s, Token/s: 145.05
Epoch: 0, Step: 4363, Batch(micro): 4363, Batch (considering grad accum): 545,  Loss: 6.3703, Time: 3.53s, Token/s: 145.11
Epoch: 0, Step: 4364, Batch(micro): 4364, Batch (considering grad accum): 545,  Loss: 5.8996, Time: 3.45s, Token/s: 148.53
Epoch: 0, Step: 4365, Batch(micro): 4365, Batch (considering grad accum): 545,  Loss: 6.3067, Time: 3.78s, Token/s: 135.36
Epoch: 0, Step: 4366, Batch(micro): 4366, Batch (considering grad accum): 545,  Loss: 6.3468, Time: 3.42s, Token/s: 149.77
Epoch: 0, Step: 4367, Batch(micro): 4367, Batch (considering grad accum): 545,  Loss: 7.1011, Time: 22.33s, Token/s: 22.93
Epoch: 0, Step: 4368, Batch(micro): 4368, Batch (considering grad accum): 546,  Loss: 5.6592, Time: 7.15s, Token/s: 71.62
Epoch: 0, Step: 4369, Batch(micro): 4369, Batch (considering grad accum): 546,  Loss: 5.5480, Time: 3.93s, Token/s: 130.36
Epoch: 0, Step: 4370, Batch(micro): 4370, Batch (considering grad accum): 546,  Loss: 5.9494, Time: 3.57s, Token/s: 143.49
Epoch: 0, Step: 4371, Batch(micro): 4371, Batch (considering grad accum): 546,  Loss: 6.1996, Time: 3.43s, Token/s: 149.15
Epoch: 0, Step: 4372, Batch(micro): 4372, Batch (considering grad accum): 546,  Loss: 6.8687, Time: 3.63s, Token/s: 140.99
Epoch: 0, Step: 4373, Batch(micro): 4373, Batch (considering grad accum): 546,  Loss: 6.9852, Time: 3.78s, Token/s: 135.44
Epoch: 0, Step: 4374, Batch(micro): 4374, Batch (considering grad accum): 546,  Loss: 6.1560, Time: 3.43s, Token/s: 149.37
Epoch: 0, Step: 4375, Batch(micro): 4375, Batch (considering grad accum): 546,  Loss: 6.0591, Time: 25.62s, Token/s: 19.99
Epoch: 0, Step: 4376, Batch(micro): 4376, Batch (considering grad accum): 547,  Loss: 6.6176, Time: 6.83s, Token/s: 74.94
Epoch: 0, Step: 4377, Batch(micro): 4377, Batch (considering grad accum): 547,  Loss: 6.6839, Time: 3.94s, Token/s: 130.09
Epoch: 0, Step: 4378, Batch(micro): 4378, Batch (considering grad accum): 547,  Loss: 6.7127, Time: 3.64s, Token/s: 140.49
Epoch: 0, Step: 4379, Batch(micro): 4379, Batch (considering grad accum): 547,  Loss: 6.1220, Time: 3.42s, Token/s: 149.85
Epoch: 0, Step: 4380, Batch(micro): 4380, Batch (considering grad accum): 547,  Loss: 6.4514, Time: 3.47s, Token/s: 147.39
Epoch: 0, Step: 4381, Batch(micro): 4381, Batch (considering grad accum): 547,  Loss: 6.2977, Time: 3.26s, Token/s: 156.90
Epoch: 0, Step: 4382, Batch(micro): 4382, Batch (considering grad accum): 547,  Loss: 5.8120, Time: 3.49s, Token/s: 146.54
Epoch: 0, Step: 4383, Batch(micro): 4383, Batch (considering grad accum): 547,  Loss: 6.4564, Time: 24.69s, Token/s: 20.73
Epoch: 0, Step: 4384, Batch(micro): 4384, Batch (considering grad accum): 548,  Loss: 6.3286, Time: 6.65s, Token/s: 77.02
Epoch: 0, Step: 4385, Batch(micro): 4385, Batch (considering grad accum): 548,  Loss: 6.4934, Time: 4.11s, Token/s: 124.53
Epoch: 0, Step: 4386, Batch(micro): 4386, Batch (considering grad accum): 548,  Loss: 6.9776, Time: 3.67s, Token/s: 139.69
Epoch: 0, Step: 4387, Batch(micro): 4387, Batch (considering grad accum): 548,  Loss: 6.0128, Time: 3.60s, Token/s: 142.26
Epoch: 0, Step: 4388, Batch(micro): 4388, Batch (considering grad accum): 548,  Loss: 6.8856, Time: 3.78s, Token/s: 135.28
Epoch: 0, Step: 4389, Batch(micro): 4389, Batch (considering grad accum): 548,  Loss: 6.6364, Time: 3.54s, Token/s: 144.54
Epoch: 0, Step: 4390, Batch(micro): 4390, Batch (considering grad accum): 548,  Loss: 6.3786, Time: 3.39s, Token/s: 151.16
Epoch: 0, Step: 4391, Batch(micro): 4391, Batch (considering grad accum): 548,  Loss: 6.3964, Time: 21.29s, Token/s: 24.05
Epoch: 0, Step: 4392, Batch(micro): 4392, Batch (considering grad accum): 549,  Loss: 6.3792, Time: 6.95s, Token/s: 73.72
Epoch: 0, Step: 4393, Batch(micro): 4393, Batch (considering grad accum): 549,  Loss: 6.4357, Time: 3.96s, Token/s: 129.39
Epoch: 0, Step: 4394, Batch(micro): 4394, Batch (considering grad accum): 549,  Loss: 6.2924, Time: 3.54s, Token/s: 144.48
Epoch: 0, Step: 4395, Batch(micro): 4395, Batch (considering grad accum): 549,  Loss: 6.4313, Time: 3.65s, Token/s: 140.16
Epoch: 0, Step: 4396, Batch(micro): 4396, Batch (considering grad accum): 549,  Loss: 5.7662, Time: 3.61s, Token/s: 141.88
Epoch: 0, Step: 4397, Batch(micro): 4397, Batch (considering grad accum): 549,  Loss: 6.1261, Time: 3.56s, Token/s: 143.68
Epoch: 0, Step: 4398, Batch(micro): 4398, Batch (considering grad accum): 549,  Loss: 6.0993, Time: 3.95s, Token/s: 129.61
Epoch: 0, Step: 4399, Batch(micro): 4399, Batch (considering grad accum): 549,  Loss: 6.9212, Time: 19.07s, Token/s: 26.85
Updating MLP bias
Epoch: 0, Step: 4400, Batch(micro): 4400, Batch (considering grad accum): 550,  Loss: 6.7174, Time: 6.88s, Token/s: 74.38
Epoch: 0, Step: 4401, Batch(micro): 4401, Batch (considering grad accum): 550,  Loss: 6.7735, Time: 3.67s, Token/s: 139.47
Epoch: 0, Step: 4402, Batch(micro): 4402, Batch (considering grad accum): 550,  Loss: 6.6747, Time: 3.47s, Token/s: 147.51
Epoch: 0, Step: 4403, Batch(micro): 4403, Batch (considering grad accum): 550,  Loss: 6.7267, Time: 3.52s, Token/s: 145.56
Epoch: 0, Step: 4404, Batch(micro): 4404, Batch (considering grad accum): 550,  Loss: 6.5608, Time: 3.58s, Token/s: 142.94
Epoch: 0, Step: 4405, Batch(micro): 4405, Batch (considering grad accum): 550,  Loss: 7.1894, Time: 3.70s, Token/s: 138.26
Epoch: 0, Step: 4406, Batch(micro): 4406, Batch (considering grad accum): 550,  Loss: 7.6093, Time: 3.52s, Token/s: 145.44
Epoch: 0, Step: 4407, Batch(micro): 4407, Batch (considering grad accum): 550,  Loss: 6.1550, Time: 18.09s, Token/s: 28.30
Epoch: 0, Step: 4408, Batch(micro): 4408, Batch (considering grad accum): 551,  Loss: 6.5215, Time: 6.49s, Token/s: 78.95
Epoch: 0, Step: 4409, Batch(micro): 4409, Batch (considering grad accum): 551,  Loss: 6.1802, Time: 3.77s, Token/s: 135.76
Epoch: 0, Step: 4410, Batch(micro): 4410, Batch (considering grad accum): 551,  Loss: 6.3269, Time: 3.34s, Token/s: 153.19
Epoch: 0, Step: 4411, Batch(micro): 4411, Batch (considering grad accum): 551,  Loss: 6.2078, Time: 3.36s, Token/s: 152.49
Epoch: 0, Step: 4412, Batch(micro): 4412, Batch (considering grad accum): 551,  Loss: 6.6343, Time: 3.57s, Token/s: 143.56
Epoch: 0, Step: 4413, Batch(micro): 4413, Batch (considering grad accum): 551,  Loss: 6.5165, Time: 3.37s, Token/s: 151.93
Epoch: 0, Step: 4414, Batch(micro): 4414, Batch (considering grad accum): 551,  Loss: 6.3627, Time: 3.46s, Token/s: 147.81
Epoch: 0, Step: 4415, Batch(micro): 4415, Batch (considering grad accum): 551,  Loss: 6.3298, Time: 18.56s, Token/s: 27.59
Epoch: 0, Step: 4416, Batch(micro): 4416, Batch (considering grad accum): 552,  Loss: 5.8555, Time: 6.59s, Token/s: 77.71
Epoch: 0, Step: 4417, Batch(micro): 4417, Batch (considering grad accum): 552,  Loss: 6.2675, Time: 3.70s, Token/s: 138.34
Epoch: 0, Step: 4418, Batch(micro): 4418, Batch (considering grad accum): 552,  Loss: 7.1525, Time: 3.35s, Token/s: 152.70
Epoch: 0, Step: 4419, Batch(micro): 4419, Batch (considering grad accum): 552,  Loss: 6.3510, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 4420, Batch(micro): 4420, Batch (considering grad accum): 552,  Loss: 6.9188, Time: 3.51s, Token/s: 145.74
Epoch: 0, Step: 4421, Batch(micro): 4421, Batch (considering grad accum): 552,  Loss: 6.4461, Time: 3.74s, Token/s: 136.73
Epoch: 0, Step: 4422, Batch(micro): 4422, Batch (considering grad accum): 552,  Loss: 6.6611, Time: 3.61s, Token/s: 141.76
Epoch: 0, Step: 4423, Batch(micro): 4423, Batch (considering grad accum): 552,  Loss: 7.3193, Time: 19.39s, Token/s: 26.40
Epoch: 0, Step: 4424, Batch(micro): 4424, Batch (considering grad accum): 553,  Loss: 6.2372, Time: 6.46s, Token/s: 79.30
Epoch: 0, Step: 4425, Batch(micro): 4425, Batch (considering grad accum): 553,  Loss: 6.2266, Time: 3.90s, Token/s: 131.29
Epoch: 0, Step: 4426, Batch(micro): 4426, Batch (considering grad accum): 553,  Loss: 6.6409, Time: 3.63s, Token/s: 141.21
Epoch: 0, Step: 4427, Batch(micro): 4427, Batch (considering grad accum): 553,  Loss: 6.1155, Time: 3.60s, Token/s: 142.38
Epoch: 0, Step: 4428, Batch(micro): 4428, Batch (considering grad accum): 553,  Loss: 6.4115, Time: 3.67s, Token/s: 139.69
Epoch: 0, Step: 4429, Batch(micro): 4429, Batch (considering grad accum): 553,  Loss: 7.1178, Time: 3.55s, Token/s: 144.30
Epoch: 0, Step: 4430, Batch(micro): 4430, Batch (considering grad accum): 553,  Loss: 6.5864, Time: 3.74s, Token/s: 137.00
Epoch: 0, Step: 4431, Batch(micro): 4431, Batch (considering grad accum): 553,  Loss: 6.4637, Time: 19.32s, Token/s: 26.50
Epoch: 0, Step: 4432, Batch(micro): 4432, Batch (considering grad accum): 554,  Loss: 6.2881, Time: 7.12s, Token/s: 71.93
Epoch: 0, Step: 4433, Batch(micro): 4433, Batch (considering grad accum): 554,  Loss: 7.1930, Time: 3.74s, Token/s: 136.73
Epoch: 0, Step: 4434, Batch(micro): 4434, Batch (considering grad accum): 554,  Loss: 6.9269, Time: 3.51s, Token/s: 145.75
Epoch: 0, Step: 4435, Batch(micro): 4435, Batch (considering grad accum): 554,  Loss: 6.2725, Time: 3.50s, Token/s: 146.36
Epoch: 0, Step: 4436, Batch(micro): 4436, Batch (considering grad accum): 554,  Loss: 6.6850, Time: 3.50s, Token/s: 146.18
Epoch: 0, Step: 4437, Batch(micro): 4437, Batch (considering grad accum): 554,  Loss: 7.3411, Time: 3.68s, Token/s: 139.00
Epoch: 0, Step: 4438, Batch(micro): 4438, Batch (considering grad accum): 554,  Loss: 6.8586, Time: 3.47s, Token/s: 147.61
Epoch: 0, Step: 4439, Batch(micro): 4439, Batch (considering grad accum): 554,  Loss: 6.3856, Time: 18.49s, Token/s: 27.68
Epoch: 0, Step: 4440, Batch(micro): 4440, Batch (considering grad accum): 555,  Loss: 6.0296, Time: 6.38s, Token/s: 80.20
Epoch: 0, Step: 4441, Batch(micro): 4441, Batch (considering grad accum): 555,  Loss: 5.8830, Time: 3.76s, Token/s: 136.26
Epoch: 0, Step: 4442, Batch(micro): 4442, Batch (considering grad accum): 555,  Loss: 6.7324, Time: 3.55s, Token/s: 144.31
Epoch: 0, Step: 4443, Batch(micro): 4443, Batch (considering grad accum): 555,  Loss: 6.7931, Time: 3.77s, Token/s: 135.78
Epoch: 0, Step: 4444, Batch(micro): 4444, Batch (considering grad accum): 555,  Loss: 6.6848, Time: 3.45s, Token/s: 148.19
Epoch: 0, Step: 4445, Batch(micro): 4445, Batch (considering grad accum): 555,  Loss: 6.6410, Time: 3.57s, Token/s: 143.55
Epoch: 0, Step: 4446, Batch(micro): 4446, Batch (considering grad accum): 555,  Loss: 5.8755, Time: 3.56s, Token/s: 143.68
Epoch: 0, Step: 4447, Batch(micro): 4447, Batch (considering grad accum): 555,  Loss: 6.4687, Time: 18.24s, Token/s: 28.07
Epoch: 0, Step: 4448, Batch(micro): 4448, Batch (considering grad accum): 556,  Loss: 6.4564, Time: 6.79s, Token/s: 75.45
Epoch: 0, Step: 4449, Batch(micro): 4449, Batch (considering grad accum): 556,  Loss: 5.9934, Time: 3.92s, Token/s: 130.70
Epoch: 0, Step: 4450, Batch(micro): 4450, Batch (considering grad accum): 556,  Loss: 6.0016, Time: 3.49s, Token/s: 146.67
Epoch: 0, Step: 4451, Batch(micro): 4451, Batch (considering grad accum): 556,  Loss: 6.0723, Time: 3.66s, Token/s: 140.07
Epoch: 0, Step: 4452, Batch(micro): 4452, Batch (considering grad accum): 556,  Loss: 5.7216, Time: 3.42s, Token/s: 149.54
Epoch: 0, Step: 4453, Batch(micro): 4453, Batch (considering grad accum): 556,  Loss: 6.5070, Time: 3.41s, Token/s: 150.05
Epoch: 0, Step: 4454, Batch(micro): 4454, Batch (considering grad accum): 556,  Loss: 6.5065, Time: 3.19s, Token/s: 160.54
Epoch: 0, Step: 4455, Batch(micro): 4455, Batch (considering grad accum): 556,  Loss: 6.7531, Time: 18.28s, Token/s: 28.00
Epoch: 0, Step: 4456, Batch(micro): 4456, Batch (considering grad accum): 557,  Loss: 6.8687, Time: 7.33s, Token/s: 69.88
Epoch: 0, Step: 4457, Batch(micro): 4457, Batch (considering grad accum): 557,  Loss: 6.7347, Time: 3.81s, Token/s: 134.38
Epoch: 0, Step: 4458, Batch(micro): 4458, Batch (considering grad accum): 557,  Loss: 6.4522, Time: 3.35s, Token/s: 152.98
Epoch: 0, Step: 4459, Batch(micro): 4459, Batch (considering grad accum): 557,  Loss: 6.2774, Time: 3.28s, Token/s: 156.03
Epoch: 0, Step: 4460, Batch(micro): 4460, Batch (considering grad accum): 557,  Loss: 6.6075, Time: 3.35s, Token/s: 152.66
Epoch: 0, Step: 4461, Batch(micro): 4461, Batch (considering grad accum): 557,  Loss: 6.4271, Time: 3.33s, Token/s: 153.60
Epoch: 0, Step: 4462, Batch(micro): 4462, Batch (considering grad accum): 557,  Loss: 6.2697, Time: 3.42s, Token/s: 149.75
Epoch: 0, Step: 4463, Batch(micro): 4463, Batch (considering grad accum): 557,  Loss: 6.5991, Time: 19.63s, Token/s: 26.09
Epoch: 0, Step: 4464, Batch(micro): 4464, Batch (considering grad accum): 558,  Loss: 6.4858, Time: 6.36s, Token/s: 80.49
Epoch: 0, Step: 4465, Batch(micro): 4465, Batch (considering grad accum): 558,  Loss: 6.0908, Time: 3.87s, Token/s: 132.21
Epoch: 0, Step: 4466, Batch(micro): 4466, Batch (considering grad accum): 558,  Loss: 6.8118, Time: 3.63s, Token/s: 141.24
Epoch: 0, Step: 4467, Batch(micro): 4467, Batch (considering grad accum): 558,  Loss: 6.3134, Time: 3.83s, Token/s: 133.78
Epoch: 0, Step: 4468, Batch(micro): 4468, Batch (considering grad accum): 558,  Loss: 5.6813, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 4469, Batch(micro): 4469, Batch (considering grad accum): 558,  Loss: 5.6192, Time: 3.44s, Token/s: 148.69
Epoch: 0, Step: 4470, Batch(micro): 4470, Batch (considering grad accum): 558,  Loss: 7.3579, Time: 3.43s, Token/s: 149.29
Epoch: 0, Step: 4471, Batch(micro): 4471, Batch (considering grad accum): 558,  Loss: 7.2980, Time: 20.70s, Token/s: 24.74
Epoch: 0, Step: 4472, Batch(micro): 4472, Batch (considering grad accum): 559,  Loss: 6.3878, Time: 7.61s, Token/s: 67.27
Epoch: 0, Step: 4473, Batch(micro): 4473, Batch (considering grad accum): 559,  Loss: 6.1325, Time: 3.75s, Token/s: 136.64
Epoch: 0, Step: 4474, Batch(micro): 4474, Batch (considering grad accum): 559,  Loss: 7.1772, Time: 3.20s, Token/s: 160.21
Epoch: 0, Step: 4475, Batch(micro): 4475, Batch (considering grad accum): 559,  Loss: 6.3681, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 4476, Batch(micro): 4476, Batch (considering grad accum): 559,  Loss: 6.5806, Time: 3.49s, Token/s: 146.75
Epoch: 0, Step: 4477, Batch(micro): 4477, Batch (considering grad accum): 559,  Loss: 6.8190, Time: 3.52s, Token/s: 145.51
Epoch: 0, Step: 4478, Batch(micro): 4478, Batch (considering grad accum): 559,  Loss: 6.8951, Time: 3.32s, Token/s: 154.14
Epoch: 0, Step: 4479, Batch(micro): 4479, Batch (considering grad accum): 559,  Loss: 6.2471, Time: 19.15s, Token/s: 26.74
Epoch: 0, Step: 4480, Batch(micro): 4480, Batch (considering grad accum): 560,  Loss: 6.1659, Time: 6.79s, Token/s: 75.44
Epoch: 0, Step: 4481, Batch(micro): 4481, Batch (considering grad accum): 560,  Loss: 6.1242, Time: 3.54s, Token/s: 144.51
Epoch: 0, Step: 4482, Batch(micro): 4482, Batch (considering grad accum): 560,  Loss: 5.8772, Time: 3.58s, Token/s: 143.05
Epoch: 0, Step: 4483, Batch(micro): 4483, Batch (considering grad accum): 560,  Loss: 5.6968, Time: 3.35s, Token/s: 153.01
Epoch: 0, Step: 4484, Batch(micro): 4484, Batch (considering grad accum): 560,  Loss: 6.0317, Time: 3.39s, Token/s: 150.95
Epoch: 0, Step: 4485, Batch(micro): 4485, Batch (considering grad accum): 560,  Loss: 6.1861, Time: 3.67s, Token/s: 139.45
Epoch: 0, Step: 4486, Batch(micro): 4486, Batch (considering grad accum): 560,  Loss: 6.9526, Time: 3.54s, Token/s: 144.46
Epoch: 0, Step: 4487, Batch(micro): 4487, Batch (considering grad accum): 560,  Loss: 6.6954, Time: 18.64s, Token/s: 27.47
Epoch: 0, Step: 4488, Batch(micro): 4488, Batch (considering grad accum): 561,  Loss: 6.5071, Time: 7.75s, Token/s: 66.08
Epoch: 0, Step: 4489, Batch(micro): 4489, Batch (considering grad accum): 561,  Loss: 6.1865, Time: 4.11s, Token/s: 124.43
Epoch: 0, Step: 4490, Batch(micro): 4490, Batch (considering grad accum): 561,  Loss: 6.1647, Time: 3.55s, Token/s: 144.31
Epoch: 0, Step: 4491, Batch(micro): 4491, Batch (considering grad accum): 561,  Loss: 6.4066, Time: 3.81s, Token/s: 134.49
Epoch: 0, Step: 4492, Batch(micro): 4492, Batch (considering grad accum): 561,  Loss: 6.5404, Time: 3.17s, Token/s: 161.41
Epoch: 0, Step: 4493, Batch(micro): 4493, Batch (considering grad accum): 561,  Loss: 6.3926, Time: 3.14s, Token/s: 163.12
Epoch: 0, Step: 4494, Batch(micro): 4494, Batch (considering grad accum): 561,  Loss: 6.1846, Time: 3.15s, Token/s: 162.70
Epoch: 0, Step: 4495, Batch(micro): 4495, Batch (considering grad accum): 561,  Loss: 6.3644, Time: 20.80s, Token/s: 24.61
Epoch: 0, Step: 4496, Batch(micro): 4496, Batch (considering grad accum): 562,  Loss: 6.1215, Time: 6.51s, Token/s: 78.63
Epoch: 0, Step: 4497, Batch(micro): 4497, Batch (considering grad accum): 562,  Loss: 7.4670, Time: 3.72s, Token/s: 137.59
Epoch: 0, Step: 4498, Batch(micro): 4498, Batch (considering grad accum): 562,  Loss: 6.3811, Time: 3.19s, Token/s: 160.28
Epoch: 0, Step: 4499, Batch(micro): 4499, Batch (considering grad accum): 562,  Loss: 5.9762, Time: 3.25s, Token/s: 157.56
Updating MLP bias
Epoch: 0, Step: 4500, Batch(micro): 4500, Batch (considering grad accum): 562,  Loss: 6.3767, Time: 3.48s, Token/s: 147.21
Epoch: 0, Step: 4501, Batch(micro): 4501, Batch (considering grad accum): 562,  Loss: 6.8181, Time: 3.28s, Token/s: 156.06
Epoch: 0, Step: 4502, Batch(micro): 4502, Batch (considering grad accum): 562,  Loss: 6.5489, Time: 3.26s, Token/s: 157.20
Epoch: 0, Step: 4503, Batch(micro): 4503, Batch (considering grad accum): 562,  Loss: 6.9683, Time: 23.87s, Token/s: 21.45
Epoch: 0, Step: 4504, Batch(micro): 4504, Batch (considering grad accum): 563,  Loss: 6.2932, Time: 6.76s, Token/s: 75.73
Epoch: 0, Step: 4505, Batch(micro): 4505, Batch (considering grad accum): 563,  Loss: 5.9223, Time: 4.20s, Token/s: 122.01
Epoch: 0, Step: 4506, Batch(micro): 4506, Batch (considering grad accum): 563,  Loss: 6.2681, Time: 3.53s, Token/s: 144.89
Epoch: 0, Step: 4507, Batch(micro): 4507, Batch (considering grad accum): 563,  Loss: 6.5425, Time: 3.17s, Token/s: 161.28
Epoch: 0, Step: 4508, Batch(micro): 4508, Batch (considering grad accum): 563,  Loss: 6.1168, Time: 3.13s, Token/s: 163.71
Epoch: 0, Step: 4509, Batch(micro): 4509, Batch (considering grad accum): 563,  Loss: 6.1441, Time: 3.22s, Token/s: 159.12
Epoch: 0, Step: 4510, Batch(micro): 4510, Batch (considering grad accum): 563,  Loss: 6.8918, Time: 3.17s, Token/s: 161.71
Epoch: 0, Step: 4511, Batch(micro): 4511, Batch (considering grad accum): 563,  Loss: 7.1600, Time: 23.35s, Token/s: 21.93
Epoch: 0, Step: 4512, Batch(micro): 4512, Batch (considering grad accum): 564,  Loss: 6.5290, Time: 7.71s, Token/s: 66.38
Epoch: 0, Step: 4513, Batch(micro): 4513, Batch (considering grad accum): 564,  Loss: 7.0206, Time: 3.73s, Token/s: 137.30
Epoch: 0, Step: 4514, Batch(micro): 4514, Batch (considering grad accum): 564,  Loss: 6.0870, Time: 3.59s, Token/s: 142.72
Epoch: 0, Step: 4515, Batch(micro): 4515, Batch (considering grad accum): 564,  Loss: 5.7832, Time: 3.48s, Token/s: 147.18
Epoch: 0, Step: 4516, Batch(micro): 4516, Batch (considering grad accum): 564,  Loss: 7.5155, Time: 3.54s, Token/s: 144.67
Epoch: 0, Step: 4517, Batch(micro): 4517, Batch (considering grad accum): 564,  Loss: 8.4424, Time: 3.85s, Token/s: 132.88
Epoch: 0, Step: 4518, Batch(micro): 4518, Batch (considering grad accum): 564,  Loss: 9.1135, Time: 3.57s, Token/s: 143.39
Epoch: 0, Step: 4519, Batch(micro): 4519, Batch (considering grad accum): 564,  Loss: 7.5360, Time: 22.16s, Token/s: 23.10
Epoch: 0, Step: 4520, Batch(micro): 4520, Batch (considering grad accum): 565,  Loss: 6.5792, Time: 7.88s, Token/s: 64.96
Epoch: 0, Step: 4521, Batch(micro): 4521, Batch (considering grad accum): 565,  Loss: 5.9762, Time: 3.37s, Token/s: 151.77
Epoch: 0, Step: 4522, Batch(micro): 4522, Batch (considering grad accum): 565,  Loss: 6.1217, Time: 3.24s, Token/s: 158.18
Epoch: 0, Step: 4523, Batch(micro): 4523, Batch (considering grad accum): 565,  Loss: 6.5591, Time: 3.26s, Token/s: 157.24
Epoch: 0, Step: 4524, Batch(micro): 4524, Batch (considering grad accum): 565,  Loss: 6.8070, Time: 3.32s, Token/s: 154.20
Epoch: 0, Step: 4525, Batch(micro): 4525, Batch (considering grad accum): 565,  Loss: 6.5636, Time: 3.42s, Token/s: 149.76
Epoch: 0, Step: 4526, Batch(micro): 4526, Batch (considering grad accum): 565,  Loss: 6.5473, Time: 3.81s, Token/s: 134.55
Epoch: 0, Step: 4527, Batch(micro): 4527, Batch (considering grad accum): 565,  Loss: 6.9580, Time: 24.11s, Token/s: 21.23
Epoch: 0, Step: 4528, Batch(micro): 4528, Batch (considering grad accum): 566,  Loss: 6.3727, Time: 8.13s, Token/s: 63.01
Epoch: 0, Step: 4529, Batch(micro): 4529, Batch (considering grad accum): 566,  Loss: 6.0554, Time: 3.89s, Token/s: 131.46
Epoch: 0, Step: 4530, Batch(micro): 4530, Batch (considering grad accum): 566,  Loss: 6.6162, Time: 3.46s, Token/s: 148.15
Epoch: 0, Step: 4531, Batch(micro): 4531, Batch (considering grad accum): 566,  Loss: 6.5107, Time: 3.60s, Token/s: 142.10
Epoch: 0, Step: 4532, Batch(micro): 4532, Batch (considering grad accum): 566,  Loss: 6.0664, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 4533, Batch(micro): 4533, Batch (considering grad accum): 566,  Loss: 5.9403, Time: 3.69s, Token/s: 138.68
Epoch: 0, Step: 4534, Batch(micro): 4534, Batch (considering grad accum): 566,  Loss: 6.1926, Time: 3.33s, Token/s: 153.54
Epoch: 0, Step: 4535, Batch(micro): 4535, Batch (considering grad accum): 566,  Loss: 6.2699, Time: 23.66s, Token/s: 21.64
Epoch: 0, Step: 4536, Batch(micro): 4536, Batch (considering grad accum): 567,  Loss: 6.7531, Time: 6.95s, Token/s: 73.65
Epoch: 0, Step: 4537, Batch(micro): 4537, Batch (considering grad accum): 567,  Loss: 6.3538, Time: 3.97s, Token/s: 128.92
Epoch: 0, Step: 4538, Batch(micro): 4538, Batch (considering grad accum): 567,  Loss: 6.4689, Time: 3.90s, Token/s: 131.44
Epoch: 0, Step: 4539, Batch(micro): 4539, Batch (considering grad accum): 567,  Loss: 6.1599, Time: 3.59s, Token/s: 142.77
Epoch: 0, Step: 4540, Batch(micro): 4540, Batch (considering grad accum): 567,  Loss: 6.9344, Time: 3.65s, Token/s: 140.45
Epoch: 0, Step: 4541, Batch(micro): 4541, Batch (considering grad accum): 567,  Loss: 6.3775, Time: 3.17s, Token/s: 161.54
Epoch: 0, Step: 4542, Batch(micro): 4542, Batch (considering grad accum): 567,  Loss: 5.9960, Time: 3.36s, Token/s: 152.38
Epoch: 0, Step: 4543, Batch(micro): 4543, Batch (considering grad accum): 567,  Loss: 5.5054, Time: 25.73s, Token/s: 19.90
Epoch: 0, Step: 4544, Batch(micro): 4544, Batch (considering grad accum): 568,  Loss: 6.2714, Time: 7.70s, Token/s: 66.52
Epoch: 0, Step: 4545, Batch(micro): 4545, Batch (considering grad accum): 568,  Loss: 5.8363, Time: 3.77s, Token/s: 135.87
Epoch: 0, Step: 4546, Batch(micro): 4546, Batch (considering grad accum): 568,  Loss: 6.6080, Time: 3.08s, Token/s: 166.46
Epoch: 0, Step: 4547, Batch(micro): 4547, Batch (considering grad accum): 568,  Loss: 7.2485, Time: 3.13s, Token/s: 163.71
Epoch: 0, Step: 4548, Batch(micro): 4548, Batch (considering grad accum): 568,  Loss: 5.2061, Time: 3.19s, Token/s: 160.59
Epoch: 0, Step: 4549, Batch(micro): 4549, Batch (considering grad accum): 568,  Loss: 5.4460, Time: 3.07s, Token/s: 166.56
Epoch: 0, Step: 4550, Batch(micro): 4550, Batch (considering grad accum): 568,  Loss: 6.3641, Time: 2.80s, Token/s: 182.90
Epoch: 0, Step: 4551, Batch(micro): 4551, Batch (considering grad accum): 568,  Loss: 5.5769, Time: 22.23s, Token/s: 23.03
Epoch: 0, Step: 4552, Batch(micro): 4552, Batch (considering grad accum): 569,  Loss: 5.8048, Time: 8.22s, Token/s: 62.26
Epoch: 0, Step: 4553, Batch(micro): 4553, Batch (considering grad accum): 569,  Loss: 6.6134, Time: 3.86s, Token/s: 132.60
Epoch: 0, Step: 4554, Batch(micro): 4554, Batch (considering grad accum): 569,  Loss: 6.7070, Time: 3.49s, Token/s: 146.80
Epoch: 0, Step: 4555, Batch(micro): 4555, Batch (considering grad accum): 569,  Loss: 7.0065, Time: 3.60s, Token/s: 142.29
Epoch: 0, Step: 4556, Batch(micro): 4556, Batch (considering grad accum): 569,  Loss: 6.9645, Time: 3.67s, Token/s: 139.69
Epoch: 0, Step: 4557, Batch(micro): 4557, Batch (considering grad accum): 569,  Loss: 5.9314, Time: 3.52s, Token/s: 145.49
Epoch: 0, Step: 4558, Batch(micro): 4558, Batch (considering grad accum): 569,  Loss: 5.7970, Time: 3.29s, Token/s: 155.52
Epoch: 0, Step: 4559, Batch(micro): 4559, Batch (considering grad accum): 569,  Loss: 6.1606, Time: 21.77s, Token/s: 23.51
Epoch: 0, Step: 4560, Batch(micro): 4560, Batch (considering grad accum): 570,  Loss: 6.2629, Time: 8.10s, Token/s: 63.25
Epoch: 0, Step: 4561, Batch(micro): 4561, Batch (considering grad accum): 570,  Loss: 6.1993, Time: 4.08s, Token/s: 125.44
Epoch: 0, Step: 4562, Batch(micro): 4562, Batch (considering grad accum): 570,  Loss: 6.3373, Time: 3.57s, Token/s: 143.23
Epoch: 0, Step: 4563, Batch(micro): 4563, Batch (considering grad accum): 570,  Loss: 6.7696, Time: 3.25s, Token/s: 157.37
Epoch: 0, Step: 4564, Batch(micro): 4564, Batch (considering grad accum): 570,  Loss: 7.2889, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 4565, Batch(micro): 4565, Batch (considering grad accum): 570,  Loss: 6.6566, Time: 3.22s, Token/s: 158.90
Epoch: 0, Step: 4566, Batch(micro): 4566, Batch (considering grad accum): 570,  Loss: 6.7434, Time: 3.61s, Token/s: 141.83
Epoch: 0, Step: 4567, Batch(micro): 4567, Batch (considering grad accum): 570,  Loss: 6.6963, Time: 21.23s, Token/s: 24.12
Epoch: 0, Step: 4568, Batch(micro): 4568, Batch (considering grad accum): 571,  Loss: 5.7226, Time: 6.63s, Token/s: 77.25
Epoch: 0, Step: 4569, Batch(micro): 4569, Batch (considering grad accum): 571,  Loss: 5.6528, Time: 3.92s, Token/s: 130.70
Epoch: 0, Step: 4570, Batch(micro): 4570, Batch (considering grad accum): 571,  Loss: 6.0501, Time: 3.75s, Token/s: 136.46
Epoch: 0, Step: 4571, Batch(micro): 4571, Batch (considering grad accum): 571,  Loss: 6.1144, Time: 3.53s, Token/s: 144.94
Epoch: 0, Step: 4572, Batch(micro): 4572, Batch (considering grad accum): 571,  Loss: 6.8299, Time: 3.25s, Token/s: 157.44
Epoch: 0, Step: 4573, Batch(micro): 4573, Batch (considering grad accum): 571,  Loss: 6.0640, Time: 3.38s, Token/s: 151.36
Epoch: 0, Step: 4574, Batch(micro): 4574, Batch (considering grad accum): 571,  Loss: 6.1599, Time: 3.52s, Token/s: 145.51
Epoch: 0, Step: 4575, Batch(micro): 4575, Batch (considering grad accum): 571,  Loss: 6.7275, Time: 23.48s, Token/s: 21.81
Epoch: 0, Step: 4576, Batch(micro): 4576, Batch (considering grad accum): 572,  Loss: 6.3041, Time: 9.00s, Token/s: 56.86
Epoch: 0, Step: 4577, Batch(micro): 4577, Batch (considering grad accum): 572,  Loss: 6.4064, Time: 3.88s, Token/s: 132.10
Epoch: 0, Step: 4578, Batch(micro): 4578, Batch (considering grad accum): 572,  Loss: 6.9833, Time: 3.38s, Token/s: 151.43
Epoch: 0, Step: 4579, Batch(micro): 4579, Batch (considering grad accum): 572,  Loss: 7.2566, Time: 3.48s, Token/s: 147.25
Epoch: 0, Step: 4580, Batch(micro): 4580, Batch (considering grad accum): 572,  Loss: 6.4987, Time: 3.63s, Token/s: 141.00
Epoch: 0, Step: 4581, Batch(micro): 4581, Batch (considering grad accum): 572,  Loss: 6.3553, Time: 3.58s, Token/s: 143.01
Epoch: 0, Step: 4582, Batch(micro): 4582, Batch (considering grad accum): 572,  Loss: 6.7805, Time: 3.52s, Token/s: 145.46
Epoch: 0, Step: 4583, Batch(micro): 4583, Batch (considering grad accum): 572,  Loss: 6.4643, Time: 21.50s, Token/s: 23.81
Epoch: 0, Step: 4584, Batch(micro): 4584, Batch (considering grad accum): 573,  Loss: 6.7837, Time: 7.58s, Token/s: 67.51
Epoch: 0, Step: 4585, Batch(micro): 4585, Batch (considering grad accum): 573,  Loss: 6.5149, Time: 3.85s, Token/s: 133.00
Epoch: 0, Step: 4586, Batch(micro): 4586, Batch (considering grad accum): 573,  Loss: 6.9410, Time: 3.60s, Token/s: 142.10
Epoch: 0, Step: 4587, Batch(micro): 4587, Batch (considering grad accum): 573,  Loss: 6.1913, Time: 3.16s, Token/s: 162.26
Epoch: 0, Step: 4588, Batch(micro): 4588, Batch (considering grad accum): 573,  Loss: 6.1974, Time: 3.94s, Token/s: 129.83
Epoch: 0, Step: 4589, Batch(micro): 4589, Batch (considering grad accum): 573,  Loss: 5.8614, Time: 3.88s, Token/s: 131.91
Epoch: 0, Step: 4590, Batch(micro): 4590, Batch (considering grad accum): 573,  Loss: 6.2696, Time: 3.50s, Token/s: 146.12
Epoch: 0, Step: 4591, Batch(micro): 4591, Batch (considering grad accum): 573,  Loss: 6.0914, Time: 21.65s, Token/s: 23.65
Epoch: 0, Step: 4592, Batch(micro): 4592, Batch (considering grad accum): 574,  Loss: 6.6335, Time: 6.52s, Token/s: 78.47
Epoch: 0, Step: 4593, Batch(micro): 4593, Batch (considering grad accum): 574,  Loss: 6.2834, Time: 3.79s, Token/s: 135.17
Epoch: 0, Step: 4594, Batch(micro): 4594, Batch (considering grad accum): 574,  Loss: 6.4260, Time: 3.16s, Token/s: 162.20
Epoch: 0, Step: 4595, Batch(micro): 4595, Batch (considering grad accum): 574,  Loss: 6.6582, Time: 3.21s, Token/s: 159.71
Epoch: 0, Step: 4596, Batch(micro): 4596, Batch (considering grad accum): 574,  Loss: 6.6981, Time: 3.18s, Token/s: 160.82
Epoch: 0, Step: 4597, Batch(micro): 4597, Batch (considering grad accum): 574,  Loss: 6.9339, Time: 3.21s, Token/s: 159.27
Epoch: 0, Step: 4598, Batch(micro): 4598, Batch (considering grad accum): 574,  Loss: 6.0354, Time: 3.23s, Token/s: 158.53
Epoch: 0, Step: 4599, Batch(micro): 4599, Batch (considering grad accum): 574,  Loss: 5.8891, Time: 22.41s, Token/s: 22.85
Updating MLP bias
Epoch: 0, Step: 4600, Batch(micro): 4600, Batch (considering grad accum): 575,  Loss: 5.7108, Time: 6.97s, Token/s: 73.42
Epoch: 0, Step: 4601, Batch(micro): 4601, Batch (considering grad accum): 575,  Loss: 5.9850, Time: 4.10s, Token/s: 124.82
Epoch: 0, Step: 4602, Batch(micro): 4602, Batch (considering grad accum): 575,  Loss: 6.3691, Time: 3.61s, Token/s: 141.92
Epoch: 0, Step: 4603, Batch(micro): 4603, Batch (considering grad accum): 575,  Loss: 7.5056, Time: 3.48s, Token/s: 147.17
Epoch: 0, Step: 4604, Batch(micro): 4604, Batch (considering grad accum): 575,  Loss: 7.5941, Time: 3.39s, Token/s: 151.06
Epoch: 0, Step: 4605, Batch(micro): 4605, Batch (considering grad accum): 575,  Loss: 6.6947, Time: 3.66s, Token/s: 139.84
Epoch: 0, Step: 4606, Batch(micro): 4606, Batch (considering grad accum): 575,  Loss: 5.9945, Time: 3.20s, Token/s: 160.00
Epoch: 0, Step: 4607, Batch(micro): 4607, Batch (considering grad accum): 575,  Loss: 6.4990, Time: 22.11s, Token/s: 23.16
Epoch: 0, Step: 4608, Batch(micro): 4608, Batch (considering grad accum): 576,  Loss: 6.0442, Time: 7.83s, Token/s: 65.40
Epoch: 0, Step: 4609, Batch(micro): 4609, Batch (considering grad accum): 576,  Loss: 6.1959, Time: 3.98s, Token/s: 128.53
Epoch: 0, Step: 4610, Batch(micro): 4610, Batch (considering grad accum): 576,  Loss: 6.1413, Time: 3.54s, Token/s: 144.65
Epoch: 0, Step: 4611, Batch(micro): 4611, Batch (considering grad accum): 576,  Loss: 6.2845, Time: 3.64s, Token/s: 140.84
Epoch: 0, Step: 4612, Batch(micro): 4612, Batch (considering grad accum): 576,  Loss: 6.3024, Time: 3.48s, Token/s: 147.19
Epoch: 0, Step: 4613, Batch(micro): 4613, Batch (considering grad accum): 576,  Loss: 6.2765, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 4614, Batch(micro): 4614, Batch (considering grad accum): 576,  Loss: 7.2483, Time: 3.34s, Token/s: 153.35
Epoch: 0, Step: 4615, Batch(micro): 4615, Batch (considering grad accum): 576,  Loss: 6.6761, Time: 22.33s, Token/s: 22.93
Epoch: 0, Step: 4616, Batch(micro): 4616, Batch (considering grad accum): 577,  Loss: 6.4173, Time: 6.59s, Token/s: 77.69
Epoch: 0, Step: 4617, Batch(micro): 4617, Batch (considering grad accum): 577,  Loss: 6.1723, Time: 4.03s, Token/s: 127.18
Epoch: 0, Step: 4618, Batch(micro): 4618, Batch (considering grad accum): 577,  Loss: 6.8081, Time: 3.58s, Token/s: 143.01
Epoch: 0, Step: 4619, Batch(micro): 4619, Batch (considering grad accum): 577,  Loss: 6.7077, Time: 3.64s, Token/s: 140.58
Epoch: 0, Step: 4620, Batch(micro): 4620, Batch (considering grad accum): 577,  Loss: 6.8309, Time: 3.69s, Token/s: 138.80
Epoch: 0, Step: 4621, Batch(micro): 4621, Batch (considering grad accum): 577,  Loss: 5.9177, Time: 3.40s, Token/s: 150.55
Epoch: 0, Step: 4622, Batch(micro): 4622, Batch (considering grad accum): 577,  Loss: 6.2352, Time: 3.21s, Token/s: 159.39
Epoch: 0, Step: 4623, Batch(micro): 4623, Batch (considering grad accum): 577,  Loss: 5.8186, Time: 21.21s, Token/s: 24.14
Epoch: 0, Step: 4624, Batch(micro): 4624, Batch (considering grad accum): 578,  Loss: 6.8667, Time: 7.59s, Token/s: 67.44
Epoch: 0, Step: 4625, Batch(micro): 4625, Batch (considering grad accum): 578,  Loss: 5.6552, Time: 4.23s, Token/s: 120.98
Epoch: 0, Step: 4626, Batch(micro): 4626, Batch (considering grad accum): 578,  Loss: 5.9950, Time: 3.57s, Token/s: 143.27
Epoch: 0, Step: 4627, Batch(micro): 4627, Batch (considering grad accum): 578,  Loss: 6.9187, Time: 3.38s, Token/s: 151.67
Epoch: 0, Step: 4628, Batch(micro): 4628, Batch (considering grad accum): 578,  Loss: 7.0477, Time: 3.47s, Token/s: 147.67
Epoch: 0, Step: 4629, Batch(micro): 4629, Batch (considering grad accum): 578,  Loss: 7.0057, Time: 3.47s, Token/s: 147.41
Epoch: 0, Step: 4630, Batch(micro): 4630, Batch (considering grad accum): 578,  Loss: 5.9866, Time: 3.74s, Token/s: 136.99
Epoch: 0, Step: 4631, Batch(micro): 4631, Batch (considering grad accum): 578,  Loss: 5.8994, Time: 23.38s, Token/s: 21.90
Epoch: 0, Step: 4632, Batch(micro): 4632, Batch (considering grad accum): 579,  Loss: 6.4039, Time: 7.84s, Token/s: 65.29
Epoch: 0, Step: 4633, Batch(micro): 4633, Batch (considering grad accum): 579,  Loss: 6.2192, Time: 4.21s, Token/s: 121.64
Epoch: 0, Step: 4634, Batch(micro): 4634, Batch (considering grad accum): 579,  Loss: 7.1790, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 4635, Batch(micro): 4635, Batch (considering grad accum): 579,  Loss: 6.0257, Time: 3.37s, Token/s: 151.71
Epoch: 0, Step: 4636, Batch(micro): 4636, Batch (considering grad accum): 579,  Loss: 6.1987, Time: 3.43s, Token/s: 149.19
Epoch: 0, Step: 4637, Batch(micro): 4637, Batch (considering grad accum): 579,  Loss: 5.6543, Time: 3.16s, Token/s: 161.94
Epoch: 0, Step: 4638, Batch(micro): 4638, Batch (considering grad accum): 579,  Loss: 6.7087, Time: 3.21s, Token/s: 159.63
Epoch: 0, Step: 4639, Batch(micro): 4639, Batch (considering grad accum): 579,  Loss: 6.5188, Time: 23.87s, Token/s: 21.45
Epoch: 0, Step: 4640, Batch(micro): 4640, Batch (considering grad accum): 580,  Loss: 6.9726, Time: 6.91s, Token/s: 74.05
Epoch: 0, Step: 4641, Batch(micro): 4641, Batch (considering grad accum): 580,  Loss: 5.6079, Time: 3.70s, Token/s: 138.55
Epoch: 0, Step: 4642, Batch(micro): 4642, Batch (considering grad accum): 580,  Loss: 7.3861, Time: 3.76s, Token/s: 136.30
Epoch: 0, Step: 4643, Batch(micro): 4643, Batch (considering grad accum): 580,  Loss: 6.4468, Time: 3.55s, Token/s: 144.16
Epoch: 0, Step: 4644, Batch(micro): 4644, Batch (considering grad accum): 580,  Loss: 6.4183, Time: 3.31s, Token/s: 154.88
Epoch: 0, Step: 4645, Batch(micro): 4645, Batch (considering grad accum): 580,  Loss: 6.3468, Time: 3.55s, Token/s: 144.38
Epoch: 0, Step: 4646, Batch(micro): 4646, Batch (considering grad accum): 580,  Loss: 6.3750, Time: 3.41s, Token/s: 150.18
Epoch: 0, Step: 4647, Batch(micro): 4647, Batch (considering grad accum): 580,  Loss: 6.3442, Time: 22.10s, Token/s: 23.17
Epoch: 0, Step: 4648, Batch(micro): 4648, Batch (considering grad accum): 581,  Loss: 6.0037, Time: 7.48s, Token/s: 68.47
Epoch: 0, Step: 4649, Batch(micro): 4649, Batch (considering grad accum): 581,  Loss: 6.6947, Time: 3.67s, Token/s: 139.42
Epoch: 0, Step: 4650, Batch(micro): 4650, Batch (considering grad accum): 581,  Loss: 6.7512, Time: 3.30s, Token/s: 155.11
Epoch: 0, Step: 4651, Batch(micro): 4651, Batch (considering grad accum): 581,  Loss: 6.4666, Time: 3.32s, Token/s: 154.35
Epoch: 0, Step: 4652, Batch(micro): 4652, Batch (considering grad accum): 581,  Loss: 6.9868, Time: 3.41s, Token/s: 150.01
Epoch: 0, Step: 4653, Batch(micro): 4653, Batch (considering grad accum): 581,  Loss: 6.6656, Time: 3.64s, Token/s: 140.72
Epoch: 0, Step: 4654, Batch(micro): 4654, Batch (considering grad accum): 581,  Loss: 6.0082, Time: 3.59s, Token/s: 142.51
Epoch: 0, Step: 4655, Batch(micro): 4655, Batch (considering grad accum): 581,  Loss: 6.1733, Time: 27.31s, Token/s: 18.75
Epoch: 0, Step: 4656, Batch(micro): 4656, Batch (considering grad accum): 582,  Loss: 6.4969, Time: 9.13s, Token/s: 56.06
Epoch: 0, Step: 4657, Batch(micro): 4657, Batch (considering grad accum): 582,  Loss: 5.9898, Time: 4.05s, Token/s: 126.33
Epoch: 0, Step: 4658, Batch(micro): 4658, Batch (considering grad accum): 582,  Loss: 6.2161, Time: 3.68s, Token/s: 139.13
Epoch: 0, Step: 4659, Batch(micro): 4659, Batch (considering grad accum): 582,  Loss: 6.1611, Time: 3.49s, Token/s: 146.56
Epoch: 0, Step: 4660, Batch(micro): 4660, Batch (considering grad accum): 582,  Loss: 6.3248, Time: 3.44s, Token/s: 148.93
Epoch: 0, Step: 4661, Batch(micro): 4661, Batch (considering grad accum): 582,  Loss: 6.1548, Time: 3.23s, Token/s: 158.53
Epoch: 0, Step: 4662, Batch(micro): 4662, Batch (considering grad accum): 582,  Loss: 5.5905, Time: 3.15s, Token/s: 162.47
Epoch: 0, Step: 4663, Batch(micro): 4663, Batch (considering grad accum): 582,  Loss: 7.4468, Time: 23.78s, Token/s: 21.53
Epoch: 0, Step: 4664, Batch(micro): 4664, Batch (considering grad accum): 583,  Loss: 5.9265, Time: 8.55s, Token/s: 59.87
Epoch: 0, Step: 4665, Batch(micro): 4665, Batch (considering grad accum): 583,  Loss: 6.0301, Time: 3.81s, Token/s: 134.50
Epoch: 0, Step: 4666, Batch(micro): 4666, Batch (considering grad accum): 583,  Loss: 6.4614, Time: 3.49s, Token/s: 146.85
Epoch: 0, Step: 4667, Batch(micro): 4667, Batch (considering grad accum): 583,  Loss: 6.4751, Time: 3.59s, Token/s: 142.75
Epoch: 0, Step: 4668, Batch(micro): 4668, Batch (considering grad accum): 583,  Loss: 6.7947, Time: 3.49s, Token/s: 146.87
Epoch: 0, Step: 4669, Batch(micro): 4669, Batch (considering grad accum): 583,  Loss: 6.1515, Time: 3.60s, Token/s: 142.38
Epoch: 0, Step: 4670, Batch(micro): 4670, Batch (considering grad accum): 583,  Loss: 6.3719, Time: 3.47s, Token/s: 147.39
Epoch: 0, Step: 4671, Batch(micro): 4671, Batch (considering grad accum): 583,  Loss: 6.4589, Time: 24.30s, Token/s: 21.07
Epoch: 0, Step: 4672, Batch(micro): 4672, Batch (considering grad accum): 584,  Loss: 6.3698, Time: 9.34s, Token/s: 54.84
Epoch: 0, Step: 4673, Batch(micro): 4673, Batch (considering grad accum): 584,  Loss: 5.9282, Time: 3.68s, Token/s: 139.19
Epoch: 0, Step: 4674, Batch(micro): 4674, Batch (considering grad accum): 584,  Loss: 6.4537, Time: 3.19s, Token/s: 160.67
Epoch: 0, Step: 4675, Batch(micro): 4675, Batch (considering grad accum): 584,  Loss: 6.5368, Time: 3.31s, Token/s: 154.76
Epoch: 0, Step: 4676, Batch(micro): 4676, Batch (considering grad accum): 584,  Loss: 6.0559, Time: 3.20s, Token/s: 159.93
Epoch: 0, Step: 4677, Batch(micro): 4677, Batch (considering grad accum): 584,  Loss: 5.8831, Time: 3.36s, Token/s: 152.27
Epoch: 0, Step: 4678, Batch(micro): 4678, Batch (considering grad accum): 584,  Loss: 7.3193, Time: 3.24s, Token/s: 157.87
Epoch: 0, Step: 4679, Batch(micro): 4679, Batch (considering grad accum): 584,  Loss: 6.4259, Time: 25.65s, Token/s: 19.96
Epoch: 0, Step: 4680, Batch(micro): 4680, Batch (considering grad accum): 585,  Loss: 6.5719, Time: 8.00s, Token/s: 64.04
Epoch: 0, Step: 4681, Batch(micro): 4681, Batch (considering grad accum): 585,  Loss: 6.7012, Time: 3.88s, Token/s: 131.96
Epoch: 0, Step: 4682, Batch(micro): 4682, Batch (considering grad accum): 585,  Loss: 6.7716, Time: 3.31s, Token/s: 154.48
Epoch: 0, Step: 4683, Batch(micro): 4683, Batch (considering grad accum): 585,  Loss: 6.2078, Time: 3.54s, Token/s: 144.50
Epoch: 0, Step: 4684, Batch(micro): 4684, Batch (considering grad accum): 585,  Loss: 5.8812, Time: 3.61s, Token/s: 141.94
Epoch: 0, Step: 4685, Batch(micro): 4685, Batch (considering grad accum): 585,  Loss: 5.9427, Time: 3.39s, Token/s: 150.97
Epoch: 0, Step: 4686, Batch(micro): 4686, Batch (considering grad accum): 585,  Loss: 5.7805, Time: 3.43s, Token/s: 149.33
Epoch: 0, Step: 4687, Batch(micro): 4687, Batch (considering grad accum): 585,  Loss: 6.2803, Time: 24.68s, Token/s: 20.75
Epoch: 0, Step: 4688, Batch(micro): 4688, Batch (considering grad accum): 586,  Loss: 6.4829, Time: 7.66s, Token/s: 66.84
Epoch: 0, Step: 4689, Batch(micro): 4689, Batch (considering grad accum): 586,  Loss: 6.5683, Time: 4.03s, Token/s: 127.05
Epoch: 0, Step: 4690, Batch(micro): 4690, Batch (considering grad accum): 586,  Loss: 6.2660, Time: 3.83s, Token/s: 133.62
Epoch: 0, Step: 4691, Batch(micro): 4691, Batch (considering grad accum): 586,  Loss: 6.7356, Time: 3.96s, Token/s: 129.24
Epoch: 0, Step: 4692, Batch(micro): 4692, Batch (considering grad accum): 586,  Loss: 6.8113, Time: 3.31s, Token/s: 154.76
Epoch: 0, Step: 4693, Batch(micro): 4693, Batch (considering grad accum): 586,  Loss: 6.4701, Time: 3.53s, Token/s: 145.21
Epoch: 0, Step: 4694, Batch(micro): 4694, Batch (considering grad accum): 586,  Loss: 6.6605, Time: 3.40s, Token/s: 150.38
Epoch: 0, Step: 4695, Batch(micro): 4695, Batch (considering grad accum): 586,  Loss: 6.7806, Time: 19.81s, Token/s: 25.85
Epoch: 0, Step: 4696, Batch(micro): 4696, Batch (considering grad accum): 587,  Loss: 5.9798, Time: 7.03s, Token/s: 72.86
Epoch: 0, Step: 4697, Batch(micro): 4697, Batch (considering grad accum): 587,  Loss: 5.6652, Time: 3.77s, Token/s: 135.89
Epoch: 0, Step: 4698, Batch(micro): 4698, Batch (considering grad accum): 587,  Loss: 6.3540, Time: 3.36s, Token/s: 152.33
Epoch: 0, Step: 4699, Batch(micro): 4699, Batch (considering grad accum): 587,  Loss: 6.2827, Time: 3.35s, Token/s: 152.65
Updating MLP bias
Epoch: 0, Step: 4700, Batch(micro): 4700, Batch (considering grad accum): 587,  Loss: 6.3824, Time: 3.26s, Token/s: 156.92
Epoch: 0, Step: 4701, Batch(micro): 4701, Batch (considering grad accum): 587,  Loss: 6.3791, Time: 3.24s, Token/s: 158.02
Epoch: 0, Step: 4702, Batch(micro): 4702, Batch (considering grad accum): 587,  Loss: 7.0396, Time: 3.22s, Token/s: 159.25
Epoch: 0, Step: 4703, Batch(micro): 4703, Batch (considering grad accum): 587,  Loss: 6.2220, Time: 17.10s, Token/s: 29.95
Epoch: 0, Step: 4704, Batch(micro): 4704, Batch (considering grad accum): 588,  Loss: 6.2804, Time: 6.94s, Token/s: 73.79
Epoch: 0, Step: 4705, Batch(micro): 4705, Batch (considering grad accum): 588,  Loss: 6.7436, Time: 3.65s, Token/s: 140.22
Epoch: 0, Step: 4706, Batch(micro): 4706, Batch (considering grad accum): 588,  Loss: 6.4339, Time: 3.22s, Token/s: 158.90
Epoch: 0, Step: 4707, Batch(micro): 4707, Batch (considering grad accum): 588,  Loss: 6.4145, Time: 3.46s, Token/s: 148.04
Epoch: 0, Step: 4708, Batch(micro): 4708, Batch (considering grad accum): 588,  Loss: 6.1920, Time: 4.18s, Token/s: 122.54
Epoch: 0, Step: 4709, Batch(micro): 4709, Batch (considering grad accum): 588,  Loss: 6.2147, Time: 3.81s, Token/s: 134.56
Epoch: 0, Step: 4710, Batch(micro): 4710, Batch (considering grad accum): 588,  Loss: 6.7074, Time: 3.86s, Token/s: 132.60
Epoch: 0, Step: 4711, Batch(micro): 4711, Batch (considering grad accum): 588,  Loss: 6.1576, Time: 19.09s, Token/s: 26.82
Epoch: 0, Step: 4712, Batch(micro): 4712, Batch (considering grad accum): 589,  Loss: 6.4872, Time: 6.40s, Token/s: 80.05
Epoch: 0, Step: 4713, Batch(micro): 4713, Batch (considering grad accum): 589,  Loss: 6.5453, Time: 3.96s, Token/s: 129.20
Epoch: 0, Step: 4714, Batch(micro): 4714, Batch (considering grad accum): 589,  Loss: 7.0127, Time: 3.75s, Token/s: 136.38
Epoch: 0, Step: 4715, Batch(micro): 4715, Batch (considering grad accum): 589,  Loss: 7.2300, Time: 3.66s, Token/s: 140.04
Epoch: 0, Step: 4716, Batch(micro): 4716, Batch (considering grad accum): 589,  Loss: 6.5216, Time: 3.43s, Token/s: 149.31
Epoch: 0, Step: 4717, Batch(micro): 4717, Batch (considering grad accum): 589,  Loss: 6.6787, Time: 3.50s, Token/s: 146.23
Epoch: 0, Step: 4718, Batch(micro): 4718, Batch (considering grad accum): 589,  Loss: 6.9943, Time: 3.54s, Token/s: 144.54
Epoch: 0, Step: 4719, Batch(micro): 4719, Batch (considering grad accum): 589,  Loss: 6.5285, Time: 19.02s, Token/s: 26.92
Epoch: 0, Step: 4720, Batch(micro): 4720, Batch (considering grad accum): 590,  Loss: 6.6309, Time: 6.39s, Token/s: 80.14
Epoch: 0, Step: 4721, Batch(micro): 4721, Batch (considering grad accum): 590,  Loss: 6.7020, Time: 4.11s, Token/s: 124.63
Epoch: 0, Step: 4722, Batch(micro): 4722, Batch (considering grad accum): 590,  Loss: 6.4253, Time: 3.57s, Token/s: 143.31
Epoch: 0, Step: 4723, Batch(micro): 4723, Batch (considering grad accum): 590,  Loss: 7.1632, Time: 3.45s, Token/s: 148.44
Epoch: 0, Step: 4724, Batch(micro): 4724, Batch (considering grad accum): 590,  Loss: 7.0597, Time: 3.70s, Token/s: 138.49
Epoch: 0, Step: 4725, Batch(micro): 4725, Batch (considering grad accum): 590,  Loss: 6.7229, Time: 3.37s, Token/s: 151.84
Epoch: 0, Step: 4726, Batch(micro): 4726, Batch (considering grad accum): 590,  Loss: 6.4005, Time: 3.55s, Token/s: 144.18
Epoch: 0, Step: 4727, Batch(micro): 4727, Batch (considering grad accum): 590,  Loss: 6.1735, Time: 19.05s, Token/s: 26.87
Epoch: 0, Step: 4728, Batch(micro): 4728, Batch (considering grad accum): 591,  Loss: 6.7479, Time: 6.04s, Token/s: 84.80
Epoch: 0, Step: 4729, Batch(micro): 4729, Batch (considering grad accum): 591,  Loss: 6.3368, Time: 3.76s, Token/s: 136.25
Epoch: 0, Step: 4730, Batch(micro): 4730, Batch (considering grad accum): 591,  Loss: 6.5425, Time: 3.28s, Token/s: 156.08
Epoch: 0, Step: 4731, Batch(micro): 4731, Batch (considering grad accum): 591,  Loss: 7.0518, Time: 3.33s, Token/s: 153.68
Epoch: 0, Step: 4732, Batch(micro): 4732, Batch (considering grad accum): 591,  Loss: 5.7760, Time: 3.39s, Token/s: 151.05
Epoch: 0, Step: 4733, Batch(micro): 4733, Batch (considering grad accum): 591,  Loss: 6.3326, Time: 3.25s, Token/s: 157.45
Epoch: 0, Step: 4734, Batch(micro): 4734, Batch (considering grad accum): 591,  Loss: 6.6849, Time: 3.28s, Token/s: 156.03
Epoch: 0, Step: 4735, Batch(micro): 4735, Batch (considering grad accum): 591,  Loss: 6.1237, Time: 19.53s, Token/s: 26.22
Epoch: 0, Step: 4736, Batch(micro): 4736, Batch (considering grad accum): 592,  Loss: 6.3460, Time: 6.52s, Token/s: 78.54
Epoch: 0, Step: 4737, Batch(micro): 4737, Batch (considering grad accum): 592,  Loss: 6.3757, Time: 3.97s, Token/s: 128.98
Epoch: 0, Step: 4738, Batch(micro): 4738, Batch (considering grad accum): 592,  Loss: 6.2245, Time: 3.81s, Token/s: 134.55
Epoch: 0, Step: 4739, Batch(micro): 4739, Batch (considering grad accum): 592,  Loss: 6.9427, Time: 3.68s, Token/s: 139.32
Epoch: 0, Step: 4740, Batch(micro): 4740, Batch (considering grad accum): 592,  Loss: 6.2697, Time: 3.47s, Token/s: 147.62
Epoch: 0, Step: 4741, Batch(micro): 4741, Batch (considering grad accum): 592,  Loss: 5.8843, Time: 3.62s, Token/s: 141.29
Epoch: 0, Step: 4742, Batch(micro): 4742, Batch (considering grad accum): 592,  Loss: 5.7326, Time: 3.49s, Token/s: 146.70
Epoch: 0, Step: 4743, Batch(micro): 4743, Batch (considering grad accum): 592,  Loss: 5.9050, Time: 18.86s, Token/s: 27.15
Epoch: 0, Step: 4744, Batch(micro): 4744, Batch (considering grad accum): 593,  Loss: 6.2925, Time: 6.53s, Token/s: 78.36
Epoch: 0, Step: 4745, Batch(micro): 4745, Batch (considering grad accum): 593,  Loss: 6.0317, Time: 3.76s, Token/s: 136.33
Epoch: 0, Step: 4746, Batch(micro): 4746, Batch (considering grad accum): 593,  Loss: 6.7760, Time: 3.37s, Token/s: 151.75
Epoch: 0, Step: 4747, Batch(micro): 4747, Batch (considering grad accum): 593,  Loss: 5.6965, Time: 3.46s, Token/s: 147.79
Epoch: 0, Step: 4748, Batch(micro): 4748, Batch (considering grad accum): 593,  Loss: 5.8660, Time: 3.62s, Token/s: 141.42
Epoch: 0, Step: 4749, Batch(micro): 4749, Batch (considering grad accum): 593,  Loss: 6.1616, Time: 3.32s, Token/s: 154.31
Epoch: 0, Step: 4750, Batch(micro): 4750, Batch (considering grad accum): 593,  Loss: 6.4545, Time: 3.58s, Token/s: 143.10
Epoch: 0, Step: 4751, Batch(micro): 4751, Batch (considering grad accum): 593,  Loss: 6.6165, Time: 22.41s, Token/s: 22.85
Epoch: 0, Step: 4752, Batch(micro): 4752, Batch (considering grad accum): 594,  Loss: 7.2179, Time: 6.74s, Token/s: 75.98
Epoch: 0, Step: 4753, Batch(micro): 4753, Batch (considering grad accum): 594,  Loss: 6.4023, Time: 3.61s, Token/s: 141.95
Epoch: 0, Step: 4754, Batch(micro): 4754, Batch (considering grad accum): 594,  Loss: 6.2432, Time: 3.19s, Token/s: 160.25
Epoch: 0, Step: 4755, Batch(micro): 4755, Batch (considering grad accum): 594,  Loss: 6.4640, Time: 3.22s, Token/s: 159.21
Epoch: 0, Step: 4756, Batch(micro): 4756, Batch (considering grad accum): 594,  Loss: 6.1203, Time: 3.24s, Token/s: 158.05
Epoch: 0, Step: 4757, Batch(micro): 4757, Batch (considering grad accum): 594,  Loss: 5.9083, Time: 3.25s, Token/s: 157.35
Epoch: 0, Step: 4758, Batch(micro): 4758, Batch (considering grad accum): 594,  Loss: 6.1680, Time: 3.30s, Token/s: 155.05
Epoch: 0, Step: 4759, Batch(micro): 4759, Batch (considering grad accum): 594,  Loss: 6.5516, Time: 22.22s, Token/s: 23.04
Epoch: 0, Step: 4760, Batch(micro): 4760, Batch (considering grad accum): 595,  Loss: 5.6499, Time: 8.22s, Token/s: 62.27
Epoch: 0, Step: 4761, Batch(micro): 4761, Batch (considering grad accum): 595,  Loss: 5.6497, Time: 3.77s, Token/s: 135.76
Epoch: 0, Step: 4762, Batch(micro): 4762, Batch (considering grad accum): 595,  Loss: 5.9655, Time: 3.51s, Token/s: 145.88
Epoch: 0, Step: 4763, Batch(micro): 4763, Batch (considering grad accum): 595,  Loss: 6.1425, Time: 3.47s, Token/s: 147.71
Epoch: 0, Step: 4764, Batch(micro): 4764, Batch (considering grad accum): 595,  Loss: 5.9667, Time: 3.63s, Token/s: 141.08
Epoch: 0, Step: 4765, Batch(micro): 4765, Batch (considering grad accum): 595,  Loss: 6.1918, Time: 3.52s, Token/s: 145.27
Epoch: 0, Step: 4766, Batch(micro): 4766, Batch (considering grad accum): 595,  Loss: 5.6854, Time: 3.47s, Token/s: 147.42
Epoch: 0, Step: 4767, Batch(micro): 4767, Batch (considering grad accum): 595,  Loss: 5.5411, Time: 23.41s, Token/s: 21.87
Epoch: 0, Step: 4768, Batch(micro): 4768, Batch (considering grad accum): 596,  Loss: 6.1579, Time: 6.96s, Token/s: 73.53
Epoch: 0, Step: 4769, Batch(micro): 4769, Batch (considering grad accum): 596,  Loss: 6.0996, Time: 3.64s, Token/s: 140.68
Epoch: 0, Step: 4770, Batch(micro): 4770, Batch (considering grad accum): 596,  Loss: 6.3919, Time: 3.19s, Token/s: 160.74
Epoch: 0, Step: 4771, Batch(micro): 4771, Batch (considering grad accum): 596,  Loss: 5.8017, Time: 3.27s, Token/s: 156.57
Epoch: 0, Step: 4772, Batch(micro): 4772, Batch (considering grad accum): 596,  Loss: 6.5280, Time: 3.35s, Token/s: 152.74
Epoch: 0, Step: 4773, Batch(micro): 4773, Batch (considering grad accum): 596,  Loss: 6.5671, Time: 3.56s, Token/s: 143.67
Epoch: 0, Step: 4774, Batch(micro): 4774, Batch (considering grad accum): 596,  Loss: 6.7403, Time: 3.50s, Token/s: 146.43
Epoch: 0, Step: 4775, Batch(micro): 4775, Batch (considering grad accum): 596,  Loss: 7.0627, Time: 21.36s, Token/s: 23.97
Epoch: 0, Step: 4776, Batch(micro): 4776, Batch (considering grad accum): 597,  Loss: 6.3600, Time: 8.05s, Token/s: 63.63
Epoch: 0, Step: 4777, Batch(micro): 4777, Batch (considering grad accum): 597,  Loss: 6.9531, Time: 3.27s, Token/s: 156.68
Epoch: 0, Step: 4778, Batch(micro): 4778, Batch (considering grad accum): 597,  Loss: 7.4392, Time: 3.20s, Token/s: 160.14
Epoch: 0, Step: 4779, Batch(micro): 4779, Batch (considering grad accum): 597,  Loss: 6.3648, Time: 3.16s, Token/s: 162.20
Epoch: 0, Step: 4780, Batch(micro): 4780, Batch (considering grad accum): 597,  Loss: 6.0259, Time: 3.40s, Token/s: 150.44
Epoch: 0, Step: 4781, Batch(micro): 4781, Batch (considering grad accum): 597,  Loss: 6.4279, Time: 3.74s, Token/s: 136.91
Epoch: 0, Step: 4782, Batch(micro): 4782, Batch (considering grad accum): 597,  Loss: 6.7462, Time: 3.55s, Token/s: 144.19
Epoch: 0, Step: 4783, Batch(micro): 4783, Batch (considering grad accum): 597,  Loss: 6.3512, Time: 23.87s, Token/s: 21.45
Epoch: 0, Step: 4784, Batch(micro): 4784, Batch (considering grad accum): 598,  Loss: 6.4737, Time: 6.82s, Token/s: 75.08
Epoch: 0, Step: 4785, Batch(micro): 4785, Batch (considering grad accum): 598,  Loss: 6.5523, Time: 4.07s, Token/s: 125.66
Epoch: 0, Step: 4786, Batch(micro): 4786, Batch (considering grad accum): 598,  Loss: 7.3016, Time: 3.42s, Token/s: 149.53
Epoch: 0, Step: 4787, Batch(micro): 4787, Batch (considering grad accum): 598,  Loss: 6.8339, Time: 3.62s, Token/s: 141.55
Epoch: 0, Step: 4788, Batch(micro): 4788, Batch (considering grad accum): 598,  Loss: 6.4590, Time: 3.50s, Token/s: 146.24
Epoch: 0, Step: 4789, Batch(micro): 4789, Batch (considering grad accum): 598,  Loss: 6.2409, Time: 3.76s, Token/s: 136.01
Epoch: 0, Step: 4790, Batch(micro): 4790, Batch (considering grad accum): 598,  Loss: 6.0242, Time: 3.19s, Token/s: 160.42
Epoch: 0, Step: 4791, Batch(micro): 4791, Batch (considering grad accum): 598,  Loss: 6.1462, Time: 22.01s, Token/s: 23.26
Epoch: 0, Step: 4792, Batch(micro): 4792, Batch (considering grad accum): 599,  Loss: 6.1588, Time: 6.39s, Token/s: 80.19
Epoch: 0, Step: 4793, Batch(micro): 4793, Batch (considering grad accum): 599,  Loss: 7.3587, Time: 4.01s, Token/s: 127.63
Epoch: 0, Step: 4794, Batch(micro): 4794, Batch (considering grad accum): 599,  Loss: 6.1119, Time: 3.65s, Token/s: 140.40
Epoch: 0, Step: 4795, Batch(micro): 4795, Batch (considering grad accum): 599,  Loss: 6.4068, Time: 3.38s, Token/s: 151.30
Epoch: 0, Step: 4796, Batch(micro): 4796, Batch (considering grad accum): 599,  Loss: 5.6255, Time: 3.61s, Token/s: 141.75
Epoch: 0, Step: 4797, Batch(micro): 4797, Batch (considering grad accum): 599,  Loss: 6.0521, Time: 3.62s, Token/s: 141.62
Epoch: 0, Step: 4798, Batch(micro): 4798, Batch (considering grad accum): 599,  Loss: 6.8270, Time: 3.55s, Token/s: 144.35
Epoch: 0, Step: 4799, Batch(micro): 4799, Batch (considering grad accum): 599,  Loss: 6.3818, Time: 23.77s, Token/s: 21.54
Updating MLP bias
Epoch: 0, Step: 4800, Batch(micro): 4800, Batch (considering grad accum): 600,  Loss: 6.3078, Time: 8.17s, Token/s: 62.66
Epoch: 0, Step: 4801, Batch(micro): 4801, Batch (considering grad accum): 600,  Loss: 6.9492, Time: 3.92s, Token/s: 130.53
Epoch: 0, Step: 4802, Batch(micro): 4802, Batch (considering grad accum): 600,  Loss: 7.7980, Time: 3.79s, Token/s: 135.20
Epoch: 0, Step: 4803, Batch(micro): 4803, Batch (considering grad accum): 600,  Loss: 6.3160, Time: 3.65s, Token/s: 140.37
Epoch: 0, Step: 4804, Batch(micro): 4804, Batch (considering grad accum): 600,  Loss: 7.3137, Time: 3.54s, Token/s: 144.75
Epoch: 0, Step: 4805, Batch(micro): 4805, Batch (considering grad accum): 600,  Loss: 6.0004, Time: 3.48s, Token/s: 146.92
Epoch: 0, Step: 4806, Batch(micro): 4806, Batch (considering grad accum): 600,  Loss: 5.7786, Time: 3.41s, Token/s: 149.97
Epoch: 0, Step: 4807, Batch(micro): 4807, Batch (considering grad accum): 600,  Loss: 6.1591, Time: 24.93s, Token/s: 20.54
Epoch: 0, Step: 4808, Batch(micro): 4808, Batch (considering grad accum): 601,  Loss: 6.6002, Time: 6.53s, Token/s: 78.42
Epoch: 0, Step: 4809, Batch(micro): 4809, Batch (considering grad accum): 601,  Loss: 7.0236, Time: 3.84s, Token/s: 133.27
Epoch: 0, Step: 4810, Batch(micro): 4810, Batch (considering grad accum): 601,  Loss: 6.5995, Time: 3.97s, Token/s: 129.10
Epoch: 0, Step: 4811, Batch(micro): 4811, Batch (considering grad accum): 601,  Loss: 5.8529, Time: 3.32s, Token/s: 154.38
Epoch: 0, Step: 4812, Batch(micro): 4812, Batch (considering grad accum): 601,  Loss: 6.3905, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 4813, Batch(micro): 4813, Batch (considering grad accum): 601,  Loss: 6.5648, Time: 3.47s, Token/s: 147.38
Epoch: 0, Step: 4814, Batch(micro): 4814, Batch (considering grad accum): 601,  Loss: 6.0776, Time: 3.60s, Token/s: 142.41
Epoch: 0, Step: 4815, Batch(micro): 4815, Batch (considering grad accum): 601,  Loss: 6.2742, Time: 26.48s, Token/s: 19.34
Epoch: 0, Step: 4816, Batch(micro): 4816, Batch (considering grad accum): 602,  Loss: 5.5060, Time: 7.37s, Token/s: 69.51
Epoch: 0, Step: 4817, Batch(micro): 4817, Batch (considering grad accum): 602,  Loss: 6.0424, Time: 3.92s, Token/s: 130.65
Epoch: 0, Step: 4818, Batch(micro): 4818, Batch (considering grad accum): 602,  Loss: 6.1317, Time: 3.58s, Token/s: 142.83
Epoch: 0, Step: 4819, Batch(micro): 4819, Batch (considering grad accum): 602,  Loss: 5.9638, Time: 3.59s, Token/s: 142.72
Epoch: 0, Step: 4820, Batch(micro): 4820, Batch (considering grad accum): 602,  Loss: 6.6260, Time: 3.38s, Token/s: 151.49
Epoch: 0, Step: 4821, Batch(micro): 4821, Batch (considering grad accum): 602,  Loss: 6.7551, Time: 3.30s, Token/s: 155.27
Epoch: 0, Step: 4822, Batch(micro): 4822, Batch (considering grad accum): 602,  Loss: 6.4986, Time: 3.26s, Token/s: 156.87
Epoch: 0, Step: 4823, Batch(micro): 4823, Batch (considering grad accum): 602,  Loss: 6.9901, Time: 22.60s, Token/s: 22.65
Epoch: 0, Step: 4824, Batch(micro): 4824, Batch (considering grad accum): 603,  Loss: 5.9849, Time: 6.71s, Token/s: 76.35
Epoch: 0, Step: 4825, Batch(micro): 4825, Batch (considering grad accum): 603,  Loss: 6.6997, Time: 3.70s, Token/s: 138.34
Epoch: 0, Step: 4826, Batch(micro): 4826, Batch (considering grad accum): 603,  Loss: 6.2784, Time: 3.24s, Token/s: 157.81
Epoch: 0, Step: 4827, Batch(micro): 4827, Batch (considering grad accum): 603,  Loss: 6.3139, Time: 3.47s, Token/s: 147.47
Epoch: 0, Step: 4828, Batch(micro): 4828, Batch (considering grad accum): 603,  Loss: 6.6805, Time: 3.69s, Token/s: 138.79
Epoch: 0, Step: 4829, Batch(micro): 4829, Batch (considering grad accum): 603,  Loss: 6.6339, Time: 3.60s, Token/s: 142.23
Epoch: 0, Step: 4830, Batch(micro): 4830, Batch (considering grad accum): 603,  Loss: 6.6668, Time: 3.20s, Token/s: 160.11
Epoch: 0, Step: 4831, Batch(micro): 4831, Batch (considering grad accum): 603,  Loss: 6.4724, Time: 21.99s, Token/s: 23.29
Epoch: 0, Step: 4832, Batch(micro): 4832, Batch (considering grad accum): 604,  Loss: 6.0227, Time: 7.47s, Token/s: 68.52
Epoch: 0, Step: 4833, Batch(micro): 4833, Batch (considering grad accum): 604,  Loss: 6.0223, Time: 3.78s, Token/s: 135.51
Epoch: 0, Step: 4834, Batch(micro): 4834, Batch (considering grad accum): 604,  Loss: 6.4073, Time: 3.23s, Token/s: 158.62
Epoch: 0, Step: 4835, Batch(micro): 4835, Batch (considering grad accum): 604,  Loss: 6.4035, Time: 3.29s, Token/s: 155.85
Epoch: 0, Step: 4836, Batch(micro): 4836, Batch (considering grad accum): 604,  Loss: 5.9383, Time: 3.24s, Token/s: 157.95
Epoch: 0, Step: 4837, Batch(micro): 4837, Batch (considering grad accum): 604,  Loss: 6.0737, Time: 3.49s, Token/s: 146.58
Epoch: 0, Step: 4838, Batch(micro): 4838, Batch (considering grad accum): 604,  Loss: 7.1886, Time: 3.35s, Token/s: 152.75
Epoch: 0, Step: 4839, Batch(micro): 4839, Batch (considering grad accum): 604,  Loss: 7.0714, Time: 23.16s, Token/s: 22.10
Epoch: 0, Step: 4840, Batch(micro): 4840, Batch (considering grad accum): 605,  Loss: 6.5135, Time: 7.10s, Token/s: 72.09
Epoch: 0, Step: 4841, Batch(micro): 4841, Batch (considering grad accum): 605,  Loss: 6.0184, Time: 4.21s, Token/s: 121.70
Epoch: 0, Step: 4842, Batch(micro): 4842, Batch (considering grad accum): 605,  Loss: 6.2440, Time: 3.74s, Token/s: 136.73
Epoch: 0, Step: 4843, Batch(micro): 4843, Batch (considering grad accum): 605,  Loss: 5.6306, Time: 3.48s, Token/s: 147.32
Epoch: 0, Step: 4844, Batch(micro): 4844, Batch (considering grad accum): 605,  Loss: 6.0831, Time: 3.57s, Token/s: 143.59
Epoch: 0, Step: 4845, Batch(micro): 4845, Batch (considering grad accum): 605,  Loss: 5.8109, Time: 3.21s, Token/s: 159.61
Epoch: 0, Step: 4846, Batch(micro): 4846, Batch (considering grad accum): 605,  Loss: 6.0090, Time: 3.20s, Token/s: 160.06
Epoch: 0, Step: 4847, Batch(micro): 4847, Batch (considering grad accum): 605,  Loss: 6.4072, Time: 26.00s, Token/s: 19.69
Epoch: 0, Step: 4848, Batch(micro): 4848, Batch (considering grad accum): 606,  Loss: 6.6428, Time: 7.80s, Token/s: 65.66
Epoch: 0, Step: 4849, Batch(micro): 4849, Batch (considering grad accum): 606,  Loss: 6.2527, Time: 4.24s, Token/s: 120.63
Epoch: 0, Step: 4850, Batch(micro): 4850, Batch (considering grad accum): 606,  Loss: 5.7648, Time: 3.78s, Token/s: 135.37
Epoch: 0, Step: 4851, Batch(micro): 4851, Batch (considering grad accum): 606,  Loss: 6.7215, Time: 3.66s, Token/s: 139.88
Epoch: 0, Step: 4852, Batch(micro): 4852, Batch (considering grad accum): 606,  Loss: 6.2991, Time: 3.63s, Token/s: 141.04
Epoch: 0, Step: 4853, Batch(micro): 4853, Batch (considering grad accum): 606,  Loss: 6.4409, Time: 3.60s, Token/s: 142.05
Epoch: 0, Step: 4854, Batch(micro): 4854, Batch (considering grad accum): 606,  Loss: 6.6503, Time: 3.34s, Token/s: 153.27
Epoch: 0, Step: 4855, Batch(micro): 4855, Batch (considering grad accum): 606,  Loss: 6.6007, Time: 23.07s, Token/s: 22.20
Epoch: 0, Step: 4856, Batch(micro): 4856, Batch (considering grad accum): 607,  Loss: 6.1849, Time: 6.64s, Token/s: 77.09
Epoch: 0, Step: 4857, Batch(micro): 4857, Batch (considering grad accum): 607,  Loss: 7.2232, Time: 3.80s, Token/s: 134.76
Epoch: 0, Step: 4858, Batch(micro): 4858, Batch (considering grad accum): 607,  Loss: 7.1151, Time: 3.48s, Token/s: 147.06
Epoch: 0, Step: 4859, Batch(micro): 4859, Batch (considering grad accum): 607,  Loss: 6.4076, Time: 3.41s, Token/s: 150.06
Epoch: 0, Step: 4860, Batch(micro): 4860, Batch (considering grad accum): 607,  Loss: 6.2948, Time: 3.91s, Token/s: 130.87
Epoch: 0, Step: 4861, Batch(micro): 4861, Batch (considering grad accum): 607,  Loss: 6.0236, Time: 3.40s, Token/s: 150.50
Epoch: 0, Step: 4862, Batch(micro): 4862, Batch (considering grad accum): 607,  Loss: 5.8215, Time: 3.49s, Token/s: 146.60
Epoch: 0, Step: 4863, Batch(micro): 4863, Batch (considering grad accum): 607,  Loss: 6.0669, Time: 22.68s, Token/s: 22.58
Epoch: 0, Step: 4864, Batch(micro): 4864, Batch (considering grad accum): 608,  Loss: 6.9170, Time: 6.71s, Token/s: 76.31
Epoch: 0, Step: 4865, Batch(micro): 4865, Batch (considering grad accum): 608,  Loss: 6.3345, Time: 4.24s, Token/s: 120.85
Epoch: 0, Step: 4866, Batch(micro): 4866, Batch (considering grad accum): 608,  Loss: 5.6574, Time: 3.35s, Token/s: 152.65
Epoch: 0, Step: 4867, Batch(micro): 4867, Batch (considering grad accum): 608,  Loss: 6.6457, Time: 3.19s, Token/s: 160.62
Epoch: 0, Step: 4868, Batch(micro): 4868, Batch (considering grad accum): 608,  Loss: 6.0374, Time: 3.81s, Token/s: 134.39
Epoch: 0, Step: 4869, Batch(micro): 4869, Batch (considering grad accum): 608,  Loss: 6.4683, Time: 3.70s, Token/s: 138.25
Epoch: 0, Step: 4870, Batch(micro): 4870, Batch (considering grad accum): 608,  Loss: 6.5536, Time: 3.58s, Token/s: 142.84
Epoch: 0, Step: 4871, Batch(micro): 4871, Batch (considering grad accum): 608,  Loss: 6.2530, Time: 22.65s, Token/s: 22.61
Epoch: 0, Step: 4872, Batch(micro): 4872, Batch (considering grad accum): 609,  Loss: 5.8734, Time: 7.26s, Token/s: 70.50
Epoch: 0, Step: 4873, Batch(micro): 4873, Batch (considering grad accum): 609,  Loss: 6.6653, Time: 3.38s, Token/s: 151.54
Epoch: 0, Step: 4874, Batch(micro): 4874, Batch (considering grad accum): 609,  Loss: 6.3296, Time: 3.34s, Token/s: 153.27
Epoch: 0, Step: 4875, Batch(micro): 4875, Batch (considering grad accum): 609,  Loss: 6.2822, Time: 3.26s, Token/s: 157.24
Epoch: 0, Step: 4876, Batch(micro): 4876, Batch (considering grad accum): 609,  Loss: 6.3755, Time: 3.46s, Token/s: 147.92
Epoch: 0, Step: 4877, Batch(micro): 4877, Batch (considering grad accum): 609,  Loss: 6.2933, Time: 3.51s, Token/s: 145.86
Epoch: 0, Step: 4878, Batch(micro): 4878, Batch (considering grad accum): 609,  Loss: 5.2581, Time: 3.53s, Token/s: 145.22
Epoch: 0, Step: 4879, Batch(micro): 4879, Batch (considering grad accum): 609,  Loss: 5.8128, Time: 22.18s, Token/s: 23.08
Epoch: 0, Step: 4880, Batch(micro): 4880, Batch (considering grad accum): 610,  Loss: 6.3483, Time: 6.80s, Token/s: 75.26
Epoch: 0, Step: 4881, Batch(micro): 4881, Batch (considering grad accum): 610,  Loss: 6.3300, Time: 3.73s, Token/s: 137.43
Epoch: 0, Step: 4882, Batch(micro): 4882, Batch (considering grad accum): 610,  Loss: 5.7761, Time: 3.78s, Token/s: 135.44
Epoch: 0, Step: 4883, Batch(micro): 4883, Batch (considering grad accum): 610,  Loss: 6.1419, Time: 3.76s, Token/s: 136.25
Epoch: 0, Step: 4884, Batch(micro): 4884, Batch (considering grad accum): 610,  Loss: 6.7675, Time: 3.82s, Token/s: 134.02
Epoch: 0, Step: 4885, Batch(micro): 4885, Batch (considering grad accum): 610,  Loss: 6.0097, Time: 3.21s, Token/s: 159.61
Epoch: 0, Step: 4886, Batch(micro): 4886, Batch (considering grad accum): 610,  Loss: 6.6560, Time: 3.16s, Token/s: 161.82
Epoch: 0, Step: 4887, Batch(micro): 4887, Batch (considering grad accum): 610,  Loss: 6.0722, Time: 24.26s, Token/s: 21.11
Epoch: 0, Step: 4888, Batch(micro): 4888, Batch (considering grad accum): 611,  Loss: 6.1178, Time: 8.23s, Token/s: 62.24
Epoch: 0, Step: 4889, Batch(micro): 4889, Batch (considering grad accum): 611,  Loss: 6.4314, Time: 3.79s, Token/s: 135.08
Epoch: 0, Step: 4890, Batch(micro): 4890, Batch (considering grad accum): 611,  Loss: 6.3114, Time: 3.46s, Token/s: 148.06
Epoch: 0, Step: 4891, Batch(micro): 4891, Batch (considering grad accum): 611,  Loss: 5.6273, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 4892, Batch(micro): 4892, Batch (considering grad accum): 611,  Loss: 5.7557, Time: 3.27s, Token/s: 156.63
Epoch: 0, Step: 4893, Batch(micro): 4893, Batch (considering grad accum): 611,  Loss: 6.1357, Time: 3.19s, Token/s: 160.40
Epoch: 0, Step: 4894, Batch(micro): 4894, Batch (considering grad accum): 611,  Loss: 6.6291, Time: 3.29s, Token/s: 155.48
Epoch: 0, Step: 4895, Batch(micro): 4895, Batch (considering grad accum): 611,  Loss: 7.1325, Time: 25.44s, Token/s: 20.12
Epoch: 0, Step: 4896, Batch(micro): 4896, Batch (considering grad accum): 612,  Loss: 6.5860, Time: 6.73s, Token/s: 76.12
Epoch: 0, Step: 4897, Batch(micro): 4897, Batch (considering grad accum): 612,  Loss: 7.5388, Time: 4.24s, Token/s: 120.86
Epoch: 0, Step: 4898, Batch(micro): 4898, Batch (considering grad accum): 612,  Loss: 6.2732, Time: 3.56s, Token/s: 143.63
Epoch: 0, Step: 4899, Batch(micro): 4899, Batch (considering grad accum): 612,  Loss: 7.1513, Time: 3.33s, Token/s: 153.73
Updating MLP bias
Epoch: 0, Step: 4900, Batch(micro): 4900, Batch (considering grad accum): 612,  Loss: 6.4141, Time: 3.35s, Token/s: 152.94
Epoch: 0, Step: 4901, Batch(micro): 4901, Batch (considering grad accum): 612,  Loss: 5.9235, Time: 3.26s, Token/s: 157.20
Epoch: 0, Step: 4902, Batch(micro): 4902, Batch (considering grad accum): 612,  Loss: 6.7405, Time: 3.27s, Token/s: 156.56
Epoch: 0, Step: 4903, Batch(micro): 4903, Batch (considering grad accum): 612,  Loss: 6.8501, Time: 21.67s, Token/s: 23.63
Epoch: 0, Step: 4904, Batch(micro): 4904, Batch (considering grad accum): 613,  Loss: 6.5991, Time: 7.22s, Token/s: 70.95
Epoch: 0, Step: 4905, Batch(micro): 4905, Batch (considering grad accum): 613,  Loss: 6.4218, Time: 3.76s, Token/s: 136.03
Epoch: 0, Step: 4906, Batch(micro): 4906, Batch (considering grad accum): 613,  Loss: 6.8216, Time: 3.22s, Token/s: 159.10
Epoch: 0, Step: 4907, Batch(micro): 4907, Batch (considering grad accum): 613,  Loss: 6.7118, Time: 3.41s, Token/s: 149.98
Epoch: 0, Step: 4908, Batch(micro): 4908, Batch (considering grad accum): 613,  Loss: 7.0759, Time: 3.47s, Token/s: 147.39
Epoch: 0, Step: 4909, Batch(micro): 4909, Batch (considering grad accum): 613,  Loss: 6.5940, Time: 3.35s, Token/s: 152.82
Epoch: 0, Step: 4910, Batch(micro): 4910, Batch (considering grad accum): 613,  Loss: 6.8773, Time: 3.39s, Token/s: 150.98
Epoch: 0, Step: 4911, Batch(micro): 4911, Batch (considering grad accum): 613,  Loss: 6.1241, Time: 18.94s, Token/s: 27.04
Epoch: 0, Step: 4912, Batch(micro): 4912, Batch (considering grad accum): 614,  Loss: 5.7078, Time: 6.45s, Token/s: 79.42
Epoch: 0, Step: 4913, Batch(micro): 4913, Batch (considering grad accum): 614,  Loss: 5.8955, Time: 4.15s, Token/s: 123.29
Epoch: 0, Step: 4914, Batch(micro): 4914, Batch (considering grad accum): 614,  Loss: 6.4726, Time: 3.72s, Token/s: 137.54
Epoch: 0, Step: 4915, Batch(micro): 4915, Batch (considering grad accum): 614,  Loss: 5.6851, Time: 3.51s, Token/s: 145.83
Epoch: 0, Step: 4916, Batch(micro): 4916, Batch (considering grad accum): 614,  Loss: 6.1583, Time: 3.29s, Token/s: 155.57
Epoch: 0, Step: 4917, Batch(micro): 4917, Batch (considering grad accum): 614,  Loss: 6.0438, Time: 3.37s, Token/s: 151.83
Epoch: 0, Step: 4918, Batch(micro): 4918, Batch (considering grad accum): 614,  Loss: 6.3257, Time: 3.78s, Token/s: 135.51
Epoch: 0, Step: 4919, Batch(micro): 4919, Batch (considering grad accum): 614,  Loss: 6.7046, Time: 18.42s, Token/s: 27.80
Epoch: 0, Step: 4920, Batch(micro): 4920, Batch (considering grad accum): 615,  Loss: 8.1030, Time: 5.85s, Token/s: 87.55
Epoch: 0, Step: 4921, Batch(micro): 4921, Batch (considering grad accum): 615,  Loss: 6.6671, Time: 3.80s, Token/s: 134.61
Epoch: 0, Step: 4922, Batch(micro): 4922, Batch (considering grad accum): 615,  Loss: 7.2883, Time: 3.38s, Token/s: 151.26
Epoch: 0, Step: 4923, Batch(micro): 4923, Batch (considering grad accum): 615,  Loss: 6.1641, Time: 3.39s, Token/s: 150.86
Epoch: 0, Step: 4924, Batch(micro): 4924, Batch (considering grad accum): 615,  Loss: 6.5103, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 4925, Batch(micro): 4925, Batch (considering grad accum): 615,  Loss: 6.9023, Time: 3.55s, Token/s: 144.39
Epoch: 0, Step: 4926, Batch(micro): 4926, Batch (considering grad accum): 615,  Loss: 7.0131, Time: 3.40s, Token/s: 150.63
Epoch: 0, Step: 4927, Batch(micro): 4927, Batch (considering grad accum): 615,  Loss: 7.0810, Time: 17.86s, Token/s: 28.68
Epoch: 0, Step: 4928, Batch(micro): 4928, Batch (considering grad accum): 616,  Loss: 5.9264, Time: 6.43s, Token/s: 79.65
Epoch: 0, Step: 4929, Batch(micro): 4929, Batch (considering grad accum): 616,  Loss: 5.7440, Time: 3.68s, Token/s: 139.06
Epoch: 0, Step: 4930, Batch(micro): 4930, Batch (considering grad accum): 616,  Loss: 5.8068, Time: 3.40s, Token/s: 150.40
Epoch: 0, Step: 4931, Batch(micro): 4931, Batch (considering grad accum): 616,  Loss: 6.2731, Time: 3.73s, Token/s: 137.44
Epoch: 0, Step: 4932, Batch(micro): 4932, Batch (considering grad accum): 616,  Loss: 7.0998, Time: 4.38s, Token/s: 116.89
Epoch: 0, Step: 4933, Batch(micro): 4933, Batch (considering grad accum): 616,  Loss: 7.2297, Time: 3.49s, Token/s: 146.53
Epoch: 0, Step: 4934, Batch(micro): 4934, Batch (considering grad accum): 616,  Loss: 6.5499, Time: 3.18s, Token/s: 160.89
Epoch: 0, Step: 4935, Batch(micro): 4935, Batch (considering grad accum): 616,  Loss: 5.7525, Time: 18.43s, Token/s: 27.78
Epoch: 0, Step: 4936, Batch(micro): 4936, Batch (considering grad accum): 617,  Loss: 6.0549, Time: 6.09s, Token/s: 84.11
Epoch: 0, Step: 4937, Batch(micro): 4937, Batch (considering grad accum): 617,  Loss: 6.0422, Time: 3.71s, Token/s: 137.90
Epoch: 0, Step: 4938, Batch(micro): 4938, Batch (considering grad accum): 617,  Loss: 6.5055, Time: 3.19s, Token/s: 160.33
Epoch: 0, Step: 4939, Batch(micro): 4939, Batch (considering grad accum): 617,  Loss: 5.9758, Time: 3.15s, Token/s: 162.70
Epoch: 0, Step: 4940, Batch(micro): 4940, Batch (considering grad accum): 617,  Loss: 7.1176, Time: 3.37s, Token/s: 152.00
Epoch: 0, Step: 4941, Batch(micro): 4941, Batch (considering grad accum): 617,  Loss: 7.2693, Time: 3.56s, Token/s: 143.96
Epoch: 0, Step: 4942, Batch(micro): 4942, Batch (considering grad accum): 617,  Loss: 6.7631, Time: 3.43s, Token/s: 149.44
Epoch: 0, Step: 4943, Batch(micro): 4943, Batch (considering grad accum): 617,  Loss: 6.7486, Time: 20.22s, Token/s: 25.33
Epoch: 0, Step: 4944, Batch(micro): 4944, Batch (considering grad accum): 618,  Loss: 6.3218, Time: 6.88s, Token/s: 74.40
Epoch: 0, Step: 4945, Batch(micro): 4945, Batch (considering grad accum): 618,  Loss: 6.2086, Time: 4.05s, Token/s: 126.55
Epoch: 0, Step: 4946, Batch(micro): 4946, Batch (considering grad accum): 618,  Loss: 6.3034, Time: 3.62s, Token/s: 141.41
Epoch: 0, Step: 4947, Batch(micro): 4947, Batch (considering grad accum): 618,  Loss: 5.9440, Time: 3.93s, Token/s: 130.44
Epoch: 0, Step: 4948, Batch(micro): 4948, Batch (considering grad accum): 618,  Loss: 6.0658, Time: 3.60s, Token/s: 142.20
Epoch: 0, Step: 4949, Batch(micro): 4949, Batch (considering grad accum): 618,  Loss: 6.3256, Time: 3.28s, Token/s: 156.09
Epoch: 0, Step: 4950, Batch(micro): 4950, Batch (considering grad accum): 618,  Loss: 6.9769, Time: 3.41s, Token/s: 150.30
Epoch: 0, Step: 4951, Batch(micro): 4951, Batch (considering grad accum): 618,  Loss: 7.4826, Time: 18.97s, Token/s: 27.00
Epoch: 0, Step: 4952, Batch(micro): 4952, Batch (considering grad accum): 619,  Loss: 7.6359, Time: 6.97s, Token/s: 73.49
Epoch: 0, Step: 4953, Batch(micro): 4953, Batch (considering grad accum): 619,  Loss: 6.4773, Time: 4.25s, Token/s: 120.36
Epoch: 0, Step: 4954, Batch(micro): 4954, Batch (considering grad accum): 619,  Loss: 6.0746, Time: 3.81s, Token/s: 134.30
Epoch: 0, Step: 4955, Batch(micro): 4955, Batch (considering grad accum): 619,  Loss: 6.5665, Time: 4.46s, Token/s: 114.77
Epoch: 0, Step: 4956, Batch(micro): 4956, Batch (considering grad accum): 619,  Loss: 5.6779, Time: 3.21s, Token/s: 159.74
Epoch: 0, Step: 4957, Batch(micro): 4957, Batch (considering grad accum): 619,  Loss: 5.8160, Time: 3.18s, Token/s: 160.98
Epoch: 0, Step: 4958, Batch(micro): 4958, Batch (considering grad accum): 619,  Loss: 6.2320, Time: 3.22s, Token/s: 158.81
Epoch: 0, Step: 4959, Batch(micro): 4959, Batch (considering grad accum): 619,  Loss: 6.4020, Time: 18.47s, Token/s: 27.72
Epoch: 0, Step: 4960, Batch(micro): 4960, Batch (considering grad accum): 620,  Loss: 5.5384, Time: 6.62s, Token/s: 77.39
Epoch: 0, Step: 4961, Batch(micro): 4961, Batch (considering grad accum): 620,  Loss: 6.1299, Time: 3.64s, Token/s: 140.80
Epoch: 0, Step: 4962, Batch(micro): 4962, Batch (considering grad accum): 620,  Loss: 6.8602, Time: 3.21s, Token/s: 159.35
Epoch: 0, Step: 4963, Batch(micro): 4963, Batch (considering grad accum): 620,  Loss: 6.5542, Time: 3.24s, Token/s: 158.06
Epoch: 0, Step: 4964, Batch(micro): 4964, Batch (considering grad accum): 620,  Loss: 6.4008, Time: 3.92s, Token/s: 130.60
Epoch: 0, Step: 4965, Batch(micro): 4965, Batch (considering grad accum): 620,  Loss: 6.9661, Time: 3.46s, Token/s: 148.01
Epoch: 0, Step: 4966, Batch(micro): 4966, Batch (considering grad accum): 620,  Loss: 7.1250, Time: 3.27s, Token/s: 156.36
Epoch: 0, Step: 4967, Batch(micro): 4967, Batch (considering grad accum): 620,  Loss: 6.1130, Time: 18.58s, Token/s: 27.55
Epoch: 0, Step: 4968, Batch(micro): 4968, Batch (considering grad accum): 621,  Loss: 6.2535, Time: 6.10s, Token/s: 83.96
Epoch: 0, Step: 4969, Batch(micro): 4969, Batch (considering grad accum): 621,  Loss: 6.6214, Time: 3.72s, Token/s: 137.64
Epoch: 0, Step: 4970, Batch(micro): 4970, Batch (considering grad accum): 621,  Loss: 6.0250, Time: 3.27s, Token/s: 156.35
Epoch: 0, Step: 4971, Batch(micro): 4971, Batch (considering grad accum): 621,  Loss: 5.9165, Time: 3.35s, Token/s: 152.70
Epoch: 0, Step: 4972, Batch(micro): 4972, Batch (considering grad accum): 621,  Loss: 6.5781, Time: 3.42s, Token/s: 149.87
Epoch: 0, Step: 4973, Batch(micro): 4973, Batch (considering grad accum): 621,  Loss: 6.8782, Time: 3.52s, Token/s: 145.51
Epoch: 0, Step: 4974, Batch(micro): 4974, Batch (considering grad accum): 621,  Loss: 6.8556, Time: 3.42s, Token/s: 149.72
Epoch: 0, Step: 4975, Batch(micro): 4975, Batch (considering grad accum): 621,  Loss: 6.8571, Time: 18.25s, Token/s: 28.05
Epoch: 0, Step: 4976, Batch(micro): 4976, Batch (considering grad accum): 622,  Loss: 6.5143, Time: 6.77s, Token/s: 75.62
Epoch: 0, Step: 4977, Batch(micro): 4977, Batch (considering grad accum): 622,  Loss: 6.6702, Time: 3.72s, Token/s: 137.47
Epoch: 0, Step: 4978, Batch(micro): 4978, Batch (considering grad accum): 622,  Loss: 6.4159, Time: 3.21s, Token/s: 159.61
Epoch: 0, Step: 4979, Batch(micro): 4979, Batch (considering grad accum): 622,  Loss: 6.1466, Time: 3.26s, Token/s: 157.07
Epoch: 0, Step: 4980, Batch(micro): 4980, Batch (considering grad accum): 622,  Loss: 6.2524, Time: 3.25s, Token/s: 157.58
Epoch: 0, Step: 4981, Batch(micro): 4981, Batch (considering grad accum): 622,  Loss: 6.7374, Time: 3.26s, Token/s: 156.83
Epoch: 0, Step: 4982, Batch(micro): 4982, Batch (considering grad accum): 622,  Loss: 6.4131, Time: 3.26s, Token/s: 156.88
Epoch: 0, Step: 4983, Batch(micro): 4983, Batch (considering grad accum): 622,  Loss: 6.5893, Time: 18.26s, Token/s: 28.04
Epoch: 0, Step: 4984, Batch(micro): 4984, Batch (considering grad accum): 623,  Loss: 5.8383, Time: 6.87s, Token/s: 74.50
Epoch: 0, Step: 4985, Batch(micro): 4985, Batch (considering grad accum): 623,  Loss: 5.8137, Time: 3.76s, Token/s: 136.28
Epoch: 0, Step: 4986, Batch(micro): 4986, Batch (considering grad accum): 623,  Loss: 6.4306, Time: 3.23s, Token/s: 158.45
Epoch: 0, Step: 4987, Batch(micro): 4987, Batch (considering grad accum): 623,  Loss: 6.0928, Time: 3.20s, Token/s: 159.91
Epoch: 0, Step: 4988, Batch(micro): 4988, Batch (considering grad accum): 623,  Loss: 7.1704, Time: 3.17s, Token/s: 161.36
Epoch: 0, Step: 4989, Batch(micro): 4989, Batch (considering grad accum): 623,  Loss: 6.2128, Time: 3.64s, Token/s: 140.59
Epoch: 0, Step: 4990, Batch(micro): 4990, Batch (considering grad accum): 623,  Loss: 6.0584, Time: 3.66s, Token/s: 140.02
Epoch: 0, Step: 4991, Batch(micro): 4991, Batch (considering grad accum): 623,  Loss: 6.1613, Time: 18.77s, Token/s: 27.28
Epoch: 0, Step: 4992, Batch(micro): 4992, Batch (considering grad accum): 624,  Loss: 6.7370, Time: 6.40s, Token/s: 80.01
Epoch: 0, Step: 4993, Batch(micro): 4993, Batch (considering grad accum): 624,  Loss: 6.3968, Time: 4.01s, Token/s: 127.73
Epoch: 0, Step: 4994, Batch(micro): 4994, Batch (considering grad accum): 624,  Loss: 7.7279, Time: 3.74s, Token/s: 137.06
Epoch: 0, Step: 4995, Batch(micro): 4995, Batch (considering grad accum): 624,  Loss: 7.5952, Time: 3.50s, Token/s: 146.36
Epoch: 0, Step: 4996, Batch(micro): 4996, Batch (considering grad accum): 624,  Loss: 6.4869, Time: 3.42s, Token/s: 149.89
Epoch: 0, Step: 4997, Batch(micro): 4997, Batch (considering grad accum): 624,  Loss: 5.7519, Time: 3.40s, Token/s: 150.57
Epoch: 0, Step: 4998, Batch(micro): 4998, Batch (considering grad accum): 624,  Loss: 6.1885, Time: 3.52s, Token/s: 145.46
Epoch: 0, Step: 4999, Batch(micro): 4999, Batch (considering grad accum): 624,  Loss: 6.0194, Time: 18.57s, Token/s: 27.58
Updating MLP bias
Epoch: 0, Step: 5000, Batch(micro): 5000, Batch (considering grad accum): 625,  Loss: 6.5473, Time: 6.59s, Token/s: 77.65
Saved checkpoint at step 5000
What is Gravity? It was crucial to do this is to the way of his own unique. This method to a small town are you might think of this amazing of the
Epoch: 0, Step: 5001, Batch(micro): 5001, Batch (considering grad accum): 625,  Loss: 5.8871, Time: 13.91s, Token/s: 36.81
Epoch: 0, Step: 5002, Batch(micro): 5002, Batch (considering grad accum): 625,  Loss: 6.7426, Time: 3.69s, Token/s: 138.78
Epoch: 0, Step: 5003, Batch(micro): 5003, Batch (considering grad accum): 625,  Loss: 6.7222, Time: 3.38s, Token/s: 151.29
Epoch: 0, Step: 5004, Batch(micro): 5004, Batch (considering grad accum): 625,  Loss: 6.3883, Time: 3.43s, Token/s: 149.40
Epoch: 0, Step: 5005, Batch(micro): 5005, Batch (considering grad accum): 625,  Loss: 6.3461, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 5006, Batch(micro): 5006, Batch (considering grad accum): 625,  Loss: 7.0579, Time: 3.53s, Token/s: 144.95
Epoch: 0, Step: 5007, Batch(micro): 5007, Batch (considering grad accum): 625,  Loss: 7.1256, Time: 23.85s, Token/s: 21.47
Epoch: 0, Step: 5008, Batch(micro): 5008, Batch (considering grad accum): 626,  Loss: 6.9619, Time: 7.28s, Token/s: 70.37
Epoch: 0, Step: 5009, Batch(micro): 5009, Batch (considering grad accum): 626,  Loss: 6.4139, Time: 3.91s, Token/s: 130.86
Epoch: 0, Step: 5010, Batch(micro): 5010, Batch (considering grad accum): 626,  Loss: 6.4079, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 5011, Batch(micro): 5011, Batch (considering grad accum): 626,  Loss: 6.1088, Time: 3.37s, Token/s: 152.14
Epoch: 0, Step: 5012, Batch(micro): 5012, Batch (considering grad accum): 626,  Loss: 6.2162, Time: 3.30s, Token/s: 155.32
Epoch: 0, Step: 5013, Batch(micro): 5013, Batch (considering grad accum): 626,  Loss: 5.8664, Time: 3.35s, Token/s: 152.72
Epoch: 0, Step: 5014, Batch(micro): 5014, Batch (considering grad accum): 626,  Loss: 6.5787, Time: 3.31s, Token/s: 154.56
Epoch: 0, Step: 5015, Batch(micro): 5015, Batch (considering grad accum): 626,  Loss: 5.9674, Time: 24.26s, Token/s: 21.11
Epoch: 0, Step: 5016, Batch(micro): 5016, Batch (considering grad accum): 627,  Loss: 6.3853, Time: 7.36s, Token/s: 69.59
Epoch: 0, Step: 5017, Batch(micro): 5017, Batch (considering grad accum): 627,  Loss: 6.0887, Time: 3.78s, Token/s: 135.56
Epoch: 0, Step: 5018, Batch(micro): 5018, Batch (considering grad accum): 627,  Loss: 6.0779, Time: 3.31s, Token/s: 154.74
Epoch: 0, Step: 5019, Batch(micro): 5019, Batch (considering grad accum): 627,  Loss: 5.9008, Time: 3.32s, Token/s: 154.21
Epoch: 0, Step: 5020, Batch(micro): 5020, Batch (considering grad accum): 627,  Loss: 6.7732, Time: 3.36s, Token/s: 152.58
Epoch: 0, Step: 5021, Batch(micro): 5021, Batch (considering grad accum): 627,  Loss: 6.3823, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 5022, Batch(micro): 5022, Batch (considering grad accum): 627,  Loss: 6.5298, Time: 3.27s, Token/s: 156.39
Epoch: 0, Step: 5023, Batch(micro): 5023, Batch (considering grad accum): 627,  Loss: 5.8961, Time: 23.12s, Token/s: 22.14
Epoch: 0, Step: 5024, Batch(micro): 5024, Batch (considering grad accum): 628,  Loss: 6.4731, Time: 6.66s, Token/s: 76.88
Epoch: 0, Step: 5025, Batch(micro): 5025, Batch (considering grad accum): 628,  Loss: 6.1504, Time: 3.69s, Token/s: 138.58
Epoch: 0, Step: 5026, Batch(micro): 5026, Batch (considering grad accum): 628,  Loss: 6.4710, Time: 3.54s, Token/s: 144.64
Epoch: 0, Step: 5027, Batch(micro): 5027, Batch (considering grad accum): 628,  Loss: 5.8978, Time: 3.62s, Token/s: 141.53
Epoch: 0, Step: 5028, Batch(micro): 5028, Batch (considering grad accum): 628,  Loss: 6.0706, Time: 3.56s, Token/s: 143.85
Epoch: 0, Step: 5029, Batch(micro): 5029, Batch (considering grad accum): 628,  Loss: 6.0829, Time: 3.48s, Token/s: 147.14
Epoch: 0, Step: 5030, Batch(micro): 5030, Batch (considering grad accum): 628,  Loss: 6.1337, Time: 3.23s, Token/s: 158.52
Epoch: 0, Step: 5031, Batch(micro): 5031, Batch (considering grad accum): 628,  Loss: 5.4255, Time: 22.67s, Token/s: 22.59
Epoch: 0, Step: 5032, Batch(micro): 5032, Batch (considering grad accum): 629,  Loss: 6.4227, Time: 8.34s, Token/s: 61.41
Epoch: 0, Step: 5033, Batch(micro): 5033, Batch (considering grad accum): 629,  Loss: 6.3239, Time: 3.95s, Token/s: 129.68
Epoch: 0, Step: 5034, Batch(micro): 5034, Batch (considering grad accum): 629,  Loss: 6.4876, Time: 3.59s, Token/s: 142.45
Epoch: 0, Step: 5035, Batch(micro): 5035, Batch (considering grad accum): 629,  Loss: 5.5637, Time: 3.77s, Token/s: 135.70
Epoch: 0, Step: 5036, Batch(micro): 5036, Batch (considering grad accum): 629,  Loss: 6.3005, Time: 3.69s, Token/s: 138.79
Epoch: 0, Step: 5037, Batch(micro): 5037, Batch (considering grad accum): 629,  Loss: 5.8384, Time: 3.39s, Token/s: 151.19
Epoch: 0, Step: 5038, Batch(micro): 5038, Batch (considering grad accum): 629,  Loss: 6.2636, Time: 3.48s, Token/s: 146.92
Epoch: 0, Step: 5039, Batch(micro): 5039, Batch (considering grad accum): 629,  Loss: 6.5389, Time: 23.71s, Token/s: 21.60
Epoch: 0, Step: 5040, Batch(micro): 5040, Batch (considering grad accum): 630,  Loss: 6.5038, Time: 7.40s, Token/s: 69.21
Epoch: 0, Step: 5041, Batch(micro): 5041, Batch (considering grad accum): 630,  Loss: 6.3859, Time: 4.21s, Token/s: 121.60
Epoch: 0, Step: 5042, Batch(micro): 5042, Batch (considering grad accum): 630,  Loss: 5.9224, Time: 3.98s, Token/s: 128.59
Epoch: 0, Step: 5043, Batch(micro): 5043, Batch (considering grad accum): 630,  Loss: 5.7976, Time: 3.55s, Token/s: 144.40
Epoch: 0, Step: 5044, Batch(micro): 5044, Batch (considering grad accum): 630,  Loss: 6.5608, Time: 3.78s, Token/s: 135.27
Epoch: 0, Step: 5045, Batch(micro): 5045, Batch (considering grad accum): 630,  Loss: 6.3257, Time: 3.47s, Token/s: 147.55
Epoch: 0, Step: 5046, Batch(micro): 5046, Batch (considering grad accum): 630,  Loss: 6.6333, Time: 3.66s, Token/s: 139.95
Epoch: 0, Step: 5047, Batch(micro): 5047, Batch (considering grad accum): 630,  Loss: 5.9629, Time: 23.88s, Token/s: 21.44
Epoch: 0, Step: 5048, Batch(micro): 5048, Batch (considering grad accum): 631,  Loss: 5.9861, Time: 6.75s, Token/s: 75.82
Epoch: 0, Step: 5049, Batch(micro): 5049, Batch (considering grad accum): 631,  Loss: 5.8415, Time: 4.14s, Token/s: 123.61
Epoch: 0, Step: 5050, Batch(micro): 5050, Batch (considering grad accum): 631,  Loss: 6.6240, Time: 4.15s, Token/s: 123.41
Epoch: 0, Step: 5051, Batch(micro): 5051, Batch (considering grad accum): 631,  Loss: 6.1960, Time: 3.22s, Token/s: 159.02
Epoch: 0, Step: 5052, Batch(micro): 5052, Batch (considering grad accum): 631,  Loss: 5.8411, Time: 3.09s, Token/s: 165.63
Epoch: 0, Step: 5053, Batch(micro): 5053, Batch (considering grad accum): 631,  Loss: 5.8670, Time: 3.87s, Token/s: 132.40
Epoch: 0, Step: 5054, Batch(micro): 5054, Batch (considering grad accum): 631,  Loss: 5.7971, Time: 3.59s, Token/s: 142.44
Epoch: 0, Step: 5055, Batch(micro): 5055, Batch (considering grad accum): 631,  Loss: 5.8886, Time: 23.41s, Token/s: 21.87
Epoch: 0, Step: 5056, Batch(micro): 5056, Batch (considering grad accum): 632,  Loss: 6.3247, Time: 6.19s, Token/s: 82.72
Epoch: 0, Step: 5057, Batch(micro): 5057, Batch (considering grad accum): 632,  Loss: 6.5900, Time: 3.56s, Token/s: 143.79
Epoch: 0, Step: 5058, Batch(micro): 5058, Batch (considering grad accum): 632,  Loss: 6.4643, Time: 3.49s, Token/s: 146.60
Epoch: 0, Step: 5059, Batch(micro): 5059, Batch (considering grad accum): 632,  Loss: 6.3224, Time: 3.40s, Token/s: 150.39
Epoch: 0, Step: 5060, Batch(micro): 5060, Batch (considering grad accum): 632,  Loss: 5.9970, Time: 3.02s, Token/s: 169.48
Epoch: 0, Step: 5061, Batch(micro): 5061, Batch (considering grad accum): 632,  Loss: 6.0196, Time: 3.20s, Token/s: 160.09
Epoch: 0, Step: 5062, Batch(micro): 5062, Batch (considering grad accum): 632,  Loss: 5.9841, Time: 2.84s, Token/s: 180.59
Epoch: 0, Step: 5063, Batch(micro): 5063, Batch (considering grad accum): 632,  Loss: 5.9645, Time: 25.32s, Token/s: 20.22
Epoch: 0, Step: 5064, Batch(micro): 5064, Batch (considering grad accum): 633,  Loss: 6.2661, Time: 6.74s, Token/s: 76.01
Epoch: 0, Step: 5065, Batch(micro): 5065, Batch (considering grad accum): 633,  Loss: 6.3023, Time: 5.15s, Token/s: 99.44
Epoch: 0, Step: 5066, Batch(micro): 5066, Batch (considering grad accum): 633,  Loss: 5.8569, Time: 3.19s, Token/s: 160.27
Epoch: 0, Step: 5067, Batch(micro): 5067, Batch (considering grad accum): 633,  Loss: 6.1379, Time: 3.12s, Token/s: 163.98
Epoch: 0, Step: 5068, Batch(micro): 5068, Batch (considering grad accum): 633,  Loss: 6.1460, Time: 3.19s, Token/s: 160.68
Epoch: 0, Step: 5069, Batch(micro): 5069, Batch (considering grad accum): 633,  Loss: 6.1765, Time: 3.25s, Token/s: 157.38
Epoch: 0, Step: 5070, Batch(micro): 5070, Batch (considering grad accum): 633,  Loss: 5.9969, Time: 2.75s, Token/s: 185.88
Epoch: 0, Step: 5071, Batch(micro): 5071, Batch (considering grad accum): 633,  Loss: 6.3784, Time: 23.24s, Token/s: 22.03
Epoch: 0, Step: 5072, Batch(micro): 5072, Batch (considering grad accum): 634,  Loss: 6.2223, Time: 6.91s, Token/s: 74.09
Epoch: 0, Step: 5073, Batch(micro): 5073, Batch (considering grad accum): 634,  Loss: 6.4828, Time: 4.02s, Token/s: 127.47
Epoch: 0, Step: 5074, Batch(micro): 5074, Batch (considering grad accum): 634,  Loss: 6.0256, Time: 3.64s, Token/s: 140.74
Epoch: 0, Step: 5075, Batch(micro): 5075, Batch (considering grad accum): 634,  Loss: 7.3432, Time: 3.89s, Token/s: 131.56
Epoch: 0, Step: 5076, Batch(micro): 5076, Batch (considering grad accum): 634,  Loss: 6.6891, Time: 3.44s, Token/s: 149.04
Epoch: 0, Step: 5077, Batch(micro): 5077, Batch (considering grad accum): 634,  Loss: 6.2190, Time: 3.38s, Token/s: 151.58
Epoch: 0, Step: 5078, Batch(micro): 5078, Batch (considering grad accum): 634,  Loss: 6.6712, Time: 3.15s, Token/s: 162.56
Epoch: 0, Step: 5079, Batch(micro): 5079, Batch (considering grad accum): 634,  Loss: 6.2341, Time: 25.19s, Token/s: 20.32
Epoch: 0, Step: 5080, Batch(micro): 5080, Batch (considering grad accum): 635,  Loss: 6.8825, Time: 7.32s, Token/s: 69.92
Epoch: 0, Step: 5081, Batch(micro): 5081, Batch (considering grad accum): 635,  Loss: 5.7613, Time: 3.77s, Token/s: 135.64
Epoch: 0, Step: 5082, Batch(micro): 5082, Batch (considering grad accum): 635,  Loss: 5.4806, Time: 3.32s, Token/s: 154.01
Epoch: 0, Step: 5083, Batch(micro): 5083, Batch (considering grad accum): 635,  Loss: 5.7257, Time: 3.39s, Token/s: 150.99
Epoch: 0, Step: 5084, Batch(micro): 5084, Batch (considering grad accum): 635,  Loss: 6.9635, Time: 3.28s, Token/s: 156.00
Epoch: 0, Step: 5085, Batch(micro): 5085, Batch (considering grad accum): 635,  Loss: 7.0366, Time: 3.28s, Token/s: 155.86
Epoch: 0, Step: 5086, Batch(micro): 5086, Batch (considering grad accum): 635,  Loss: 6.0655, Time: 3.16s, Token/s: 162.00
Epoch: 0, Step: 5087, Batch(micro): 5087, Batch (considering grad accum): 635,  Loss: 5.8355, Time: 25.63s, Token/s: 19.97
Epoch: 0, Step: 5088, Batch(micro): 5088, Batch (considering grad accum): 636,  Loss: 6.0880, Time: 6.73s, Token/s: 76.05
Epoch: 0, Step: 5089, Batch(micro): 5089, Batch (considering grad accum): 636,  Loss: 6.4700, Time: 3.77s, Token/s: 135.96
Epoch: 0, Step: 5090, Batch(micro): 5090, Batch (considering grad accum): 636,  Loss: 5.9173, Time: 3.22s, Token/s: 159.22
Epoch: 0, Step: 5091, Batch(micro): 5091, Batch (considering grad accum): 636,  Loss: 6.1800, Time: 3.23s, Token/s: 158.53
Epoch: 0, Step: 5092, Batch(micro): 5092, Batch (considering grad accum): 636,  Loss: 5.9910, Time: 3.18s, Token/s: 161.25
Epoch: 0, Step: 5093, Batch(micro): 5093, Batch (considering grad accum): 636,  Loss: 6.1949, Time: 3.16s, Token/s: 161.79
Epoch: 0, Step: 5094, Batch(micro): 5094, Batch (considering grad accum): 636,  Loss: 6.2148, Time: 3.23s, Token/s: 158.33
Epoch: 0, Step: 5095, Batch(micro): 5095, Batch (considering grad accum): 636,  Loss: 6.2518, Time: 22.70s, Token/s: 22.56
Epoch: 0, Step: 5096, Batch(micro): 5096, Batch (considering grad accum): 637,  Loss: 7.1803, Time: 5.55s, Token/s: 92.30
Epoch: 0, Step: 5097, Batch(micro): 5097, Batch (considering grad accum): 637,  Loss: 6.3337, Time: 3.77s, Token/s: 135.98
Epoch: 0, Step: 5098, Batch(micro): 5098, Batch (considering grad accum): 637,  Loss: 6.6077, Time: 3.46s, Token/s: 147.84
Epoch: 0, Step: 5099, Batch(micro): 5099, Batch (considering grad accum): 637,  Loss: 6.2816, Time: 3.34s, Token/s: 153.34
Updating MLP bias
Epoch: 0, Step: 5100, Batch(micro): 5100, Batch (considering grad accum): 637,  Loss: 6.2908, Time: 3.23s, Token/s: 158.28
Epoch: 0, Step: 5101, Batch(micro): 5101, Batch (considering grad accum): 637,  Loss: 6.2993, Time: 3.15s, Token/s: 162.72
Epoch: 0, Step: 5102, Batch(micro): 5102, Batch (considering grad accum): 637,  Loss: 6.6112, Time: 3.20s, Token/s: 159.80
Epoch: 0, Step: 5103, Batch(micro): 5103, Batch (considering grad accum): 637,  Loss: 6.2215, Time: 21.94s, Token/s: 23.33
Epoch: 0, Step: 5104, Batch(micro): 5104, Batch (considering grad accum): 638,  Loss: 6.3275, Time: 5.93s, Token/s: 86.39
Epoch: 0, Step: 5105, Batch(micro): 5105, Batch (considering grad accum): 638,  Loss: 6.5721, Time: 4.20s, Token/s: 121.88
Epoch: 0, Step: 5106, Batch(micro): 5106, Batch (considering grad accum): 638,  Loss: 6.0795, Time: 4.08s, Token/s: 125.54
Epoch: 0, Step: 5107, Batch(micro): 5107, Batch (considering grad accum): 638,  Loss: 5.5217, Time: 3.56s, Token/s: 143.73
Epoch: 0, Step: 5108, Batch(micro): 5108, Batch (considering grad accum): 638,  Loss: 6.2167, Time: 3.19s, Token/s: 160.57
Epoch: 0, Step: 5109, Batch(micro): 5109, Batch (considering grad accum): 638,  Loss: 5.8935, Time: 3.22s, Token/s: 159.18
Epoch: 0, Step: 5110, Batch(micro): 5110, Batch (considering grad accum): 638,  Loss: 6.1697, Time: 3.25s, Token/s: 157.32
Epoch: 0, Step: 5111, Batch(micro): 5111, Batch (considering grad accum): 638,  Loss: 6.2671, Time: 22.95s, Token/s: 22.31
Epoch: 0, Step: 5112, Batch(micro): 5112, Batch (considering grad accum): 639,  Loss: 6.0221, Time: 6.91s, Token/s: 74.07
Epoch: 0, Step: 5113, Batch(micro): 5113, Batch (considering grad accum): 639,  Loss: 6.6022, Time: 4.07s, Token/s: 125.83
Epoch: 0, Step: 5114, Batch(micro): 5114, Batch (considering grad accum): 639,  Loss: 5.5804, Time: 3.20s, Token/s: 160.14
Epoch: 0, Step: 5115, Batch(micro): 5115, Batch (considering grad accum): 639,  Loss: 6.0570, Time: 3.37s, Token/s: 152.01
Epoch: 0, Step: 5116, Batch(micro): 5116, Batch (considering grad accum): 639,  Loss: 5.9132, Time: 3.36s, Token/s: 152.32
Epoch: 0, Step: 5117, Batch(micro): 5117, Batch (considering grad accum): 639,  Loss: 6.0254, Time: 3.72s, Token/s: 137.72
Epoch: 0, Step: 5118, Batch(micro): 5118, Batch (considering grad accum): 639,  Loss: 6.2849, Time: 3.29s, Token/s: 155.45
Epoch: 0, Step: 5119, Batch(micro): 5119, Batch (considering grad accum): 639,  Loss: 6.4567, Time: 23.13s, Token/s: 22.13
Epoch: 0, Step: 5120, Batch(micro): 5120, Batch (considering grad accum): 640,  Loss: 6.2535, Time: 8.10s, Token/s: 63.17
Epoch: 0, Step: 5121, Batch(micro): 5121, Batch (considering grad accum): 640,  Loss: 7.1358, Time: 3.66s, Token/s: 139.98
Epoch: 0, Step: 5122, Batch(micro): 5122, Batch (considering grad accum): 640,  Loss: 6.1617, Time: 3.66s, Token/s: 139.96
Epoch: 0, Step: 5123, Batch(micro): 5123, Batch (considering grad accum): 640,  Loss: 6.2073, Time: 3.48s, Token/s: 147.15
Epoch: 0, Step: 5124, Batch(micro): 5124, Batch (considering grad accum): 640,  Loss: 5.9264, Time: 3.20s, Token/s: 159.95
Epoch: 0, Step: 5125, Batch(micro): 5125, Batch (considering grad accum): 640,  Loss: 6.0861, Time: 3.32s, Token/s: 154.13
Epoch: 0, Step: 5126, Batch(micro): 5126, Batch (considering grad accum): 640,  Loss: 5.9585, Time: 3.74s, Token/s: 136.99
Epoch: 0, Step: 5127, Batch(micro): 5127, Batch (considering grad accum): 640,  Loss: 6.1324, Time: 22.32s, Token/s: 22.94
Epoch: 0, Step: 5128, Batch(micro): 5128, Batch (considering grad accum): 641,  Loss: 6.3286, Time: 8.58s, Token/s: 59.66
Epoch: 0, Step: 5129, Batch(micro): 5129, Batch (considering grad accum): 641,  Loss: 6.0180, Time: 3.34s, Token/s: 153.12
Epoch: 0, Step: 5130, Batch(micro): 5130, Batch (considering grad accum): 641,  Loss: 5.7736, Time: 3.20s, Token/s: 160.01
Epoch: 0, Step: 5131, Batch(micro): 5131, Batch (considering grad accum): 641,  Loss: 6.3177, Time: 3.24s, Token/s: 157.81
Epoch: 0, Step: 5132, Batch(micro): 5132, Batch (considering grad accum): 641,  Loss: 6.4216, Time: 3.28s, Token/s: 156.23
Epoch: 0, Step: 5133, Batch(micro): 5133, Batch (considering grad accum): 641,  Loss: 5.9623, Time: 3.36s, Token/s: 152.52
Epoch: 0, Step: 5134, Batch(micro): 5134, Batch (considering grad accum): 641,  Loss: 5.7505, Time: 3.40s, Token/s: 150.53
Epoch: 0, Step: 5135, Batch(micro): 5135, Batch (considering grad accum): 641,  Loss: 6.6803, Time: 21.26s, Token/s: 24.08
Epoch: 0, Step: 5136, Batch(micro): 5136, Batch (considering grad accum): 642,  Loss: 6.8712, Time: 6.67s, Token/s: 76.81
Epoch: 0, Step: 5137, Batch(micro): 5137, Batch (considering grad accum): 642,  Loss: 5.9040, Time: 3.85s, Token/s: 133.03
Epoch: 0, Step: 5138, Batch(micro): 5138, Batch (considering grad accum): 642,  Loss: 5.7776, Time: 3.28s, Token/s: 156.10
Epoch: 0, Step: 5139, Batch(micro): 5139, Batch (considering grad accum): 642,  Loss: 5.8555, Time: 3.23s, Token/s: 158.36
Epoch: 0, Step: 5140, Batch(micro): 5140, Batch (considering grad accum): 642,  Loss: 5.8003, Time: 3.22s, Token/s: 158.84
Epoch: 0, Step: 5141, Batch(micro): 5141, Batch (considering grad accum): 642,  Loss: 6.2547, Time: 3.30s, Token/s: 155.02
Epoch: 0, Step: 5142, Batch(micro): 5142, Batch (considering grad accum): 642,  Loss: 6.7446, Time: 3.19s, Token/s: 160.65
Epoch: 0, Step: 5143, Batch(micro): 5143, Batch (considering grad accum): 642,  Loss: 6.4650, Time: 22.76s, Token/s: 22.50
Epoch: 0, Step: 5144, Batch(micro): 5144, Batch (considering grad accum): 643,  Loss: 6.1469, Time: 6.93s, Token/s: 73.89
Epoch: 0, Step: 5145, Batch(micro): 5145, Batch (considering grad accum): 643,  Loss: 6.2124, Time: 3.78s, Token/s: 135.37
Epoch: 0, Step: 5146, Batch(micro): 5146, Batch (considering grad accum): 643,  Loss: 6.8363, Time: 3.52s, Token/s: 145.47
Epoch: 0, Step: 5147, Batch(micro): 5147, Batch (considering grad accum): 643,  Loss: 6.5091, Time: 3.38s, Token/s: 151.31
Epoch: 0, Step: 5148, Batch(micro): 5148, Batch (considering grad accum): 643,  Loss: 6.1939, Time: 3.47s, Token/s: 147.47
Epoch: 0, Step: 5149, Batch(micro): 5149, Batch (considering grad accum): 643,  Loss: 5.7742, Time: 3.43s, Token/s: 149.27
Epoch: 0, Step: 5150, Batch(micro): 5150, Batch (considering grad accum): 643,  Loss: 5.5468, Time: 3.41s, Token/s: 150.21
Epoch: 0, Step: 5151, Batch(micro): 5151, Batch (considering grad accum): 643,  Loss: 6.2889, Time: 21.86s, Token/s: 23.42
Epoch: 0, Step: 5152, Batch(micro): 5152, Batch (considering grad accum): 644,  Loss: 6.2121, Time: 6.38s, Token/s: 80.24
Epoch: 0, Step: 5153, Batch(micro): 5153, Batch (considering grad accum): 644,  Loss: 6.4743, Time: 3.71s, Token/s: 138.11
Epoch: 0, Step: 5154, Batch(micro): 5154, Batch (considering grad accum): 644,  Loss: 5.9965, Time: 3.59s, Token/s: 142.54
Epoch: 0, Step: 5155, Batch(micro): 5155, Batch (considering grad accum): 644,  Loss: 5.9931, Time: 3.54s, Token/s: 144.62
Epoch: 0, Step: 5156, Batch(micro): 5156, Batch (considering grad accum): 644,  Loss: 6.5908, Time: 3.21s, Token/s: 159.72
Epoch: 0, Step: 5157, Batch(micro): 5157, Batch (considering grad accum): 644,  Loss: 6.5306, Time: 3.25s, Token/s: 157.58
Epoch: 0, Step: 5158, Batch(micro): 5158, Batch (considering grad accum): 644,  Loss: 5.8184, Time: 3.18s, Token/s: 160.75
Epoch: 0, Step: 5159, Batch(micro): 5159, Batch (considering grad accum): 644,  Loss: 6.5305, Time: 22.97s, Token/s: 22.29
Epoch: 0, Step: 5160, Batch(micro): 5160, Batch (considering grad accum): 645,  Loss: 6.1376, Time: 6.65s, Token/s: 76.95
Epoch: 0, Step: 5161, Batch(micro): 5161, Batch (considering grad accum): 645,  Loss: 7.0795, Time: 3.62s, Token/s: 141.27
Epoch: 0, Step: 5162, Batch(micro): 5162, Batch (considering grad accum): 645,  Loss: 6.1351, Time: 3.29s, Token/s: 155.48
Epoch: 0, Step: 5163, Batch(micro): 5163, Batch (considering grad accum): 645,  Loss: 6.4357, Time: 3.29s, Token/s: 155.62
Epoch: 0, Step: 5164, Batch(micro): 5164, Batch (considering grad accum): 645,  Loss: 5.9410, Time: 3.33s, Token/s: 153.60
Epoch: 0, Step: 5165, Batch(micro): 5165, Batch (considering grad accum): 645,  Loss: 6.4975, Time: 3.33s, Token/s: 153.63
Epoch: 0, Step: 5166, Batch(micro): 5166, Batch (considering grad accum): 645,  Loss: 6.1817, Time: 3.27s, Token/s: 156.72
Epoch: 0, Step: 5167, Batch(micro): 5167, Batch (considering grad accum): 645,  Loss: 6.4943, Time: 22.15s, Token/s: 23.12
Epoch: 0, Step: 5168, Batch(micro): 5168, Batch (considering grad accum): 646,  Loss: 5.5329, Time: 6.67s, Token/s: 76.74
Epoch: 0, Step: 5169, Batch(micro): 5169, Batch (considering grad accum): 646,  Loss: 6.1058, Time: 3.81s, Token/s: 134.41
Epoch: 0, Step: 5170, Batch(micro): 5170, Batch (considering grad accum): 646,  Loss: 6.8490, Time: 3.59s, Token/s: 142.46
Epoch: 0, Step: 5171, Batch(micro): 5171, Batch (considering grad accum): 646,  Loss: 6.2294, Time: 3.81s, Token/s: 134.33
Epoch: 0, Step: 5172, Batch(micro): 5172, Batch (considering grad accum): 646,  Loss: 6.1890, Time: 3.86s, Token/s: 132.80
Epoch: 0, Step: 5173, Batch(micro): 5173, Batch (considering grad accum): 646,  Loss: 5.8331, Time: 3.36s, Token/s: 152.18
Epoch: 0, Step: 5174, Batch(micro): 5174, Batch (considering grad accum): 646,  Loss: 5.6659, Time: 3.19s, Token/s: 160.39
Epoch: 0, Step: 5175, Batch(micro): 5175, Batch (considering grad accum): 646,  Loss: 6.0963, Time: 20.54s, Token/s: 24.93
Epoch: 0, Step: 5176, Batch(micro): 5176, Batch (considering grad accum): 647,  Loss: 6.7333, Time: 7.49s, Token/s: 68.39
Epoch: 0, Step: 5177, Batch(micro): 5177, Batch (considering grad accum): 647,  Loss: 6.8771, Time: 3.72s, Token/s: 137.75
Epoch: 0, Step: 5178, Batch(micro): 5178, Batch (considering grad accum): 647,  Loss: 6.1083, Time: 3.58s, Token/s: 143.20
Epoch: 0, Step: 5179, Batch(micro): 5179, Batch (considering grad accum): 647,  Loss: 5.6025, Time: 3.59s, Token/s: 142.67
Epoch: 0, Step: 5180, Batch(micro): 5180, Batch (considering grad accum): 647,  Loss: 5.8850, Time: 3.50s, Token/s: 146.43
Epoch: 0, Step: 5181, Batch(micro): 5181, Batch (considering grad accum): 647,  Loss: 6.5416, Time: 3.58s, Token/s: 143.05
Epoch: 0, Step: 5182, Batch(micro): 5182, Batch (considering grad accum): 647,  Loss: 5.8314, Time: 3.33s, Token/s: 153.73
Epoch: 0, Step: 5183, Batch(micro): 5183, Batch (considering grad accum): 647,  Loss: 6.6573, Time: 18.20s, Token/s: 28.13
Epoch: 0, Step: 5184, Batch(micro): 5184, Batch (considering grad accum): 648,  Loss: 7.0164, Time: 6.49s, Token/s: 78.89
Epoch: 0, Step: 5185, Batch(micro): 5185, Batch (considering grad accum): 648,  Loss: 5.9744, Time: 3.81s, Token/s: 134.29
Epoch: 0, Step: 5186, Batch(micro): 5186, Batch (considering grad accum): 648,  Loss: 6.0418, Time: 3.69s, Token/s: 138.86
Epoch: 0, Step: 5187, Batch(micro): 5187, Batch (considering grad accum): 648,  Loss: 5.8174, Time: 3.49s, Token/s: 146.79
Epoch: 0, Step: 5188, Batch(micro): 5188, Batch (considering grad accum): 648,  Loss: 5.9551, Time: 3.27s, Token/s: 156.43
Epoch: 0, Step: 5189, Batch(micro): 5189, Batch (considering grad accum): 648,  Loss: 6.5102, Time: 3.17s, Token/s: 161.36
Epoch: 0, Step: 5190, Batch(micro): 5190, Batch (considering grad accum): 648,  Loss: 7.0614, Time: 3.20s, Token/s: 160.09
Epoch: 0, Step: 5191, Batch(micro): 5191, Batch (considering grad accum): 648,  Loss: 7.1431, Time: 18.18s, Token/s: 28.16
Epoch: 0, Step: 5192, Batch(micro): 5192, Batch (considering grad accum): 649,  Loss: 6.1683, Time: 5.91s, Token/s: 86.60
Epoch: 0, Step: 5193, Batch(micro): 5193, Batch (considering grad accum): 649,  Loss: 6.2211, Time: 3.79s, Token/s: 135.06
Epoch: 0, Step: 5194, Batch(micro): 5194, Batch (considering grad accum): 649,  Loss: 6.2562, Time: 3.32s, Token/s: 154.26
Epoch: 0, Step: 5195, Batch(micro): 5195, Batch (considering grad accum): 649,  Loss: 6.2586, Time: 3.37s, Token/s: 151.92
Epoch: 0, Step: 5196, Batch(micro): 5196, Batch (considering grad accum): 649,  Loss: 7.3257, Time: 3.33s, Token/s: 153.88
Epoch: 0, Step: 5197, Batch(micro): 5197, Batch (considering grad accum): 649,  Loss: 6.2050, Time: 3.50s, Token/s: 146.25
Epoch: 0, Step: 5198, Batch(micro): 5198, Batch (considering grad accum): 649,  Loss: 6.1616, Time: 3.71s, Token/s: 137.84
Epoch: 0, Step: 5199, Batch(micro): 5199, Batch (considering grad accum): 649,  Loss: 6.5717, Time: 18.03s, Token/s: 28.40
Updating MLP bias
Epoch: 0, Step: 5200, Batch(micro): 5200, Batch (considering grad accum): 650,  Loss: 6.0675, Time: 6.45s, Token/s: 79.39
Epoch: 0, Step: 5201, Batch(micro): 5201, Batch (considering grad accum): 650,  Loss: 5.6806, Time: 4.16s, Token/s: 123.12
Epoch: 0, Step: 5202, Batch(micro): 5202, Batch (considering grad accum): 650,  Loss: 6.1200, Time: 3.86s, Token/s: 132.51
Epoch: 0, Step: 5203, Batch(micro): 5203, Batch (considering grad accum): 650,  Loss: 6.5267, Time: 3.65s, Token/s: 140.16
Epoch: 0, Step: 5204, Batch(micro): 5204, Batch (considering grad accum): 650,  Loss: 6.5137, Time: 3.52s, Token/s: 145.28
Epoch: 0, Step: 5205, Batch(micro): 5205, Batch (considering grad accum): 650,  Loss: 5.7683, Time: 3.56s, Token/s: 143.89
Epoch: 0, Step: 5206, Batch(micro): 5206, Batch (considering grad accum): 650,  Loss: 6.8058, Time: 3.42s, Token/s: 149.92
Epoch: 0, Step: 5207, Batch(micro): 5207, Batch (considering grad accum): 650,  Loss: 7.5001, Time: 19.18s, Token/s: 26.69
Epoch: 0, Step: 5208, Batch(micro): 5208, Batch (considering grad accum): 651,  Loss: 6.3026, Time: 6.13s, Token/s: 83.50
Epoch: 0, Step: 5209, Batch(micro): 5209, Batch (considering grad accum): 651,  Loss: 5.7435, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 5210, Batch(micro): 5210, Batch (considering grad accum): 651,  Loss: 5.8700, Time: 3.01s, Token/s: 170.02
Epoch: 0, Step: 5211, Batch(micro): 5211, Batch (considering grad accum): 651,  Loss: 5.6641, Time: 3.40s, Token/s: 150.54
Epoch: 0, Step: 5212, Batch(micro): 5212, Batch (considering grad accum): 651,  Loss: 5.5240, Time: 3.51s, Token/s: 145.77
Epoch: 0, Step: 5213, Batch(micro): 5213, Batch (considering grad accum): 651,  Loss: 5.9544, Time: 3.17s, Token/s: 161.70
Epoch: 0, Step: 5214, Batch(micro): 5214, Batch (considering grad accum): 651,  Loss: 6.3701, Time: 3.18s, Token/s: 160.88
Epoch: 0, Step: 5215, Batch(micro): 5215, Batch (considering grad accum): 651,  Loss: 6.6661, Time: 17.87s, Token/s: 28.65
Epoch: 0, Step: 5216, Batch(micro): 5216, Batch (considering grad accum): 652,  Loss: 6.2420, Time: 6.63s, Token/s: 77.19
Epoch: 0, Step: 5217, Batch(micro): 5217, Batch (considering grad accum): 652,  Loss: 7.0534, Time: 3.81s, Token/s: 134.50
Epoch: 0, Step: 5218, Batch(micro): 5218, Batch (considering grad accum): 652,  Loss: 6.5168, Time: 3.25s, Token/s: 157.72
Epoch: 0, Step: 5219, Batch(micro): 5219, Batch (considering grad accum): 652,  Loss: 5.9208, Time: 3.31s, Token/s: 154.61
Epoch: 0, Step: 5220, Batch(micro): 5220, Batch (considering grad accum): 652,  Loss: 6.3251, Time: 3.61s, Token/s: 141.90
Epoch: 0, Step: 5221, Batch(micro): 5221, Batch (considering grad accum): 652,  Loss: 6.2438, Time: 3.55s, Token/s: 144.03
Epoch: 0, Step: 5222, Batch(micro): 5222, Batch (considering grad accum): 652,  Loss: 6.7193, Time: 3.59s, Token/s: 142.67
Epoch: 0, Step: 5223, Batch(micro): 5223, Batch (considering grad accum): 652,  Loss: 6.4837, Time: 18.43s, Token/s: 27.78
Epoch: 0, Step: 5224, Batch(micro): 5224, Batch (considering grad accum): 653,  Loss: 8.0560, Time: 6.81s, Token/s: 75.17
Epoch: 0, Step: 5225, Batch(micro): 5225, Batch (considering grad accum): 653,  Loss: 6.6430, Time: 3.46s, Token/s: 148.16
Epoch: 0, Step: 5226, Batch(micro): 5226, Batch (considering grad accum): 653,  Loss: 6.1374, Time: 3.34s, Token/s: 153.32
Epoch: 0, Step: 5227, Batch(micro): 5227, Batch (considering grad accum): 653,  Loss: 6.3811, Time: 3.31s, Token/s: 154.76
Epoch: 0, Step: 5228, Batch(micro): 5228, Batch (considering grad accum): 653,  Loss: 6.2100, Time: 3.30s, Token/s: 155.29
Epoch: 0, Step: 5229, Batch(micro): 5229, Batch (considering grad accum): 653,  Loss: 6.4496, Time: 3.35s, Token/s: 152.65
Epoch: 0, Step: 5230, Batch(micro): 5230, Batch (considering grad accum): 653,  Loss: 6.3203, Time: 3.46s, Token/s: 147.82
Epoch: 0, Step: 5231, Batch(micro): 5231, Batch (considering grad accum): 653,  Loss: 6.0674, Time: 18.46s, Token/s: 27.74
Epoch: 0, Step: 5232, Batch(micro): 5232, Batch (considering grad accum): 654,  Loss: 5.9389, Time: 5.81s, Token/s: 88.08
Epoch: 0, Step: 5233, Batch(micro): 5233, Batch (considering grad accum): 654,  Loss: 6.1375, Time: 3.65s, Token/s: 140.19
Epoch: 0, Step: 5234, Batch(micro): 5234, Batch (considering grad accum): 654,  Loss: 6.5159, Time: 3.21s, Token/s: 159.72
Epoch: 0, Step: 5235, Batch(micro): 5235, Batch (considering grad accum): 654,  Loss: 6.1500, Time: 3.17s, Token/s: 161.46
Epoch: 0, Step: 5236, Batch(micro): 5236, Batch (considering grad accum): 654,  Loss: 6.5356, Time: 3.21s, Token/s: 159.54
Epoch: 0, Step: 5237, Batch(micro): 5237, Batch (considering grad accum): 654,  Loss: 5.6038, Time: 3.27s, Token/s: 156.46
Epoch: 0, Step: 5238, Batch(micro): 5238, Batch (considering grad accum): 654,  Loss: 6.7003, Time: 3.19s, Token/s: 160.31
Epoch: 0, Step: 5239, Batch(micro): 5239, Batch (considering grad accum): 654,  Loss: 6.8397, Time: 19.49s, Token/s: 26.26
Epoch: 0, Step: 5240, Batch(micro): 5240, Batch (considering grad accum): 655,  Loss: 8.0129, Time: 9.24s, Token/s: 55.40
Epoch: 0, Step: 5241, Batch(micro): 5241, Batch (considering grad accum): 655,  Loss: 7.6333, Time: 3.81s, Token/s: 134.27
Epoch: 0, Step: 5242, Batch(micro): 5242, Batch (considering grad accum): 655,  Loss: 7.0974, Time: 3.24s, Token/s: 157.88
Epoch: 0, Step: 5243, Batch(micro): 5243, Batch (considering grad accum): 655,  Loss: 5.8651, Time: 3.22s, Token/s: 158.76
Epoch: 0, Step: 5244, Batch(micro): 5244, Batch (considering grad accum): 655,  Loss: 5.8115, Time: 3.24s, Token/s: 158.24
Epoch: 0, Step: 5245, Batch(micro): 5245, Batch (considering grad accum): 655,  Loss: 6.7781, Time: 3.45s, Token/s: 148.58
Epoch: 0, Step: 5246, Batch(micro): 5246, Batch (considering grad accum): 655,  Loss: 6.2167, Time: 3.34s, Token/s: 153.43
Epoch: 0, Step: 5247, Batch(micro): 5247, Batch (considering grad accum): 655,  Loss: 6.6210, Time: 23.05s, Token/s: 22.21
Epoch: 0, Step: 5248, Batch(micro): 5248, Batch (considering grad accum): 656,  Loss: 6.6927, Time: 6.48s, Token/s: 79.05
Epoch: 0, Step: 5249, Batch(micro): 5249, Batch (considering grad accum): 656,  Loss: 7.2400, Time: 3.80s, Token/s: 134.65
Epoch: 0, Step: 5250, Batch(micro): 5250, Batch (considering grad accum): 656,  Loss: 6.2822, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 5251, Batch(micro): 5251, Batch (considering grad accum): 656,  Loss: 6.7174, Time: 3.53s, Token/s: 144.99
Epoch: 0, Step: 5252, Batch(micro): 5252, Batch (considering grad accum): 656,  Loss: 6.5768, Time: 3.40s, Token/s: 150.64
Epoch: 0, Step: 5253, Batch(micro): 5253, Batch (considering grad accum): 656,  Loss: 6.1536, Time: 3.40s, Token/s: 150.78
Epoch: 0, Step: 5254, Batch(micro): 5254, Batch (considering grad accum): 656,  Loss: 6.2985, Time: 3.52s, Token/s: 145.61
Epoch: 0, Step: 5255, Batch(micro): 5255, Batch (considering grad accum): 656,  Loss: 5.9173, Time: 22.88s, Token/s: 22.37
Epoch: 0, Step: 5256, Batch(micro): 5256, Batch (considering grad accum): 657,  Loss: 5.9800, Time: 6.84s, Token/s: 74.81
Epoch: 0, Step: 5257, Batch(micro): 5257, Batch (considering grad accum): 657,  Loss: 6.3522, Time: 3.99s, Token/s: 128.20
Epoch: 0, Step: 5258, Batch(micro): 5258, Batch (considering grad accum): 657,  Loss: 6.7264, Time: 3.60s, Token/s: 142.17
Epoch: 0, Step: 5259, Batch(micro): 5259, Batch (considering grad accum): 657,  Loss: 6.6146, Time: 3.53s, Token/s: 145.23
Epoch: 0, Step: 5260, Batch(micro): 5260, Batch (considering grad accum): 657,  Loss: 6.3615, Time: 3.77s, Token/s: 135.82
Epoch: 0, Step: 5261, Batch(micro): 5261, Batch (considering grad accum): 657,  Loss: 6.0902, Time: 3.32s, Token/s: 154.13
Epoch: 0, Step: 5262, Batch(micro): 5262, Batch (considering grad accum): 657,  Loss: 6.5347, Time: 3.24s, Token/s: 157.96
Epoch: 0, Step: 5263, Batch(micro): 5263, Batch (considering grad accum): 657,  Loss: 6.2993, Time: 22.87s, Token/s: 22.39
Epoch: 0, Step: 5264, Batch(micro): 5264, Batch (considering grad accum): 658,  Loss: 6.7282, Time: 8.29s, Token/s: 61.76
Epoch: 0, Step: 5265, Batch(micro): 5265, Batch (considering grad accum): 658,  Loss: 6.5798, Time: 3.57s, Token/s: 143.34
Epoch: 0, Step: 5266, Batch(micro): 5266, Batch (considering grad accum): 658,  Loss: 7.0492, Time: 3.72s, Token/s: 137.55
Epoch: 0, Step: 5267, Batch(micro): 5267, Batch (considering grad accum): 658,  Loss: 6.0462, Time: 3.42s, Token/s: 149.61
Epoch: 0, Step: 5268, Batch(micro): 5268, Batch (considering grad accum): 658,  Loss: 6.6129, Time: 3.69s, Token/s: 138.81
Epoch: 0, Step: 5269, Batch(micro): 5269, Batch (considering grad accum): 658,  Loss: 7.3395, Time: 3.24s, Token/s: 158.26
Epoch: 0, Step: 5270, Batch(micro): 5270, Batch (considering grad accum): 658,  Loss: 6.9663, Time: 3.54s, Token/s: 144.53
Epoch: 0, Step: 5271, Batch(micro): 5271, Batch (considering grad accum): 658,  Loss: 6.7552, Time: 22.74s, Token/s: 22.51
Epoch: 0, Step: 5272, Batch(micro): 5272, Batch (considering grad accum): 659,  Loss: 6.4195, Time: 6.56s, Token/s: 78.06
Epoch: 0, Step: 5273, Batch(micro): 5273, Batch (considering grad accum): 659,  Loss: 6.5840, Time: 4.03s, Token/s: 127.04
Epoch: 0, Step: 5274, Batch(micro): 5274, Batch (considering grad accum): 659,  Loss: 5.9594, Time: 3.49s, Token/s: 146.82
Epoch: 0, Step: 5275, Batch(micro): 5275, Batch (considering grad accum): 659,  Loss: 5.6342, Time: 3.67s, Token/s: 139.61
Epoch: 0, Step: 5276, Batch(micro): 5276, Batch (considering grad accum): 659,  Loss: 6.0263, Time: 3.51s, Token/s: 145.66
Epoch: 0, Step: 5277, Batch(micro): 5277, Batch (considering grad accum): 659,  Loss: 7.3915, Time: 3.69s, Token/s: 138.82
Epoch: 0, Step: 5278, Batch(micro): 5278, Batch (considering grad accum): 659,  Loss: 6.5089, Time: 3.41s, Token/s: 149.95
Epoch: 0, Step: 5279, Batch(micro): 5279, Batch (considering grad accum): 659,  Loss: 6.6707, Time: 22.69s, Token/s: 22.56
Epoch: 0, Step: 5280, Batch(micro): 5280, Batch (considering grad accum): 660,  Loss: 6.9349, Time: 6.99s, Token/s: 73.26
Epoch: 0, Step: 5281, Batch(micro): 5281, Batch (considering grad accum): 660,  Loss: 6.4984, Time: 3.60s, Token/s: 142.06
Epoch: 0, Step: 5282, Batch(micro): 5282, Batch (considering grad accum): 660,  Loss: 6.9503, Time: 3.21s, Token/s: 159.36
Epoch: 0, Step: 5283, Batch(micro): 5283, Batch (considering grad accum): 660,  Loss: 6.4566, Time: 3.33s, Token/s: 153.83
Epoch: 0, Step: 5284, Batch(micro): 5284, Batch (considering grad accum): 660,  Loss: 6.1665, Time: 3.28s, Token/s: 155.97
Epoch: 0, Step: 5285, Batch(micro): 5285, Batch (considering grad accum): 660,  Loss: 5.7018, Time: 3.21s, Token/s: 159.34
Epoch: 0, Step: 5286, Batch(micro): 5286, Batch (considering grad accum): 660,  Loss: 6.6794, Time: 3.17s, Token/s: 161.47
Epoch: 0, Step: 5287, Batch(micro): 5287, Batch (considering grad accum): 660,  Loss: 5.9045, Time: 23.81s, Token/s: 21.50
Epoch: 0, Step: 5288, Batch(micro): 5288, Batch (considering grad accum): 661,  Loss: 5.6730, Time: 6.46s, Token/s: 79.22
Epoch: 0, Step: 5289, Batch(micro): 5289, Batch (considering grad accum): 661,  Loss: 5.6747, Time: 3.73s, Token/s: 137.41
Epoch: 0, Step: 5290, Batch(micro): 5290, Batch (considering grad accum): 661,  Loss: 5.9791, Time: 3.26s, Token/s: 156.95
Epoch: 0, Step: 5291, Batch(micro): 5291, Batch (considering grad accum): 661,  Loss: 6.7611, Time: 3.24s, Token/s: 157.92
Epoch: 0, Step: 5292, Batch(micro): 5292, Batch (considering grad accum): 661,  Loss: 6.3303, Time: 3.21s, Token/s: 159.33
Epoch: 0, Step: 5293, Batch(micro): 5293, Batch (considering grad accum): 661,  Loss: 5.6696, Time: 3.52s, Token/s: 145.40
Epoch: 0, Step: 5294, Batch(micro): 5294, Batch (considering grad accum): 661,  Loss: 6.1002, Time: 3.71s, Token/s: 137.95
Epoch: 0, Step: 5295, Batch(micro): 5295, Batch (considering grad accum): 661,  Loss: 6.5522, Time: 23.49s, Token/s: 21.79
Epoch: 0, Step: 5296, Batch(micro): 5296, Batch (considering grad accum): 662,  Loss: 7.0071, Time: 6.29s, Token/s: 81.34
Epoch: 0, Step: 5297, Batch(micro): 5297, Batch (considering grad accum): 662,  Loss: 6.0884, Time: 3.87s, Token/s: 132.32
Epoch: 0, Step: 5298, Batch(micro): 5298, Batch (considering grad accum): 662,  Loss: 6.0802, Time: 3.77s, Token/s: 135.83
Epoch: 0, Step: 5299, Batch(micro): 5299, Batch (considering grad accum): 662,  Loss: 6.2701, Time: 3.71s, Token/s: 138.06
Updating MLP bias
Epoch: 0, Step: 5300, Batch(micro): 5300, Batch (considering grad accum): 662,  Loss: 5.5448, Time: 3.54s, Token/s: 144.74
Epoch: 0, Step: 5301, Batch(micro): 5301, Batch (considering grad accum): 662,  Loss: 6.2436, Time: 3.33s, Token/s: 153.70
Epoch: 0, Step: 5302, Batch(micro): 5302, Batch (considering grad accum): 662,  Loss: 5.9203, Time: 3.20s, Token/s: 159.93
Epoch: 0, Step: 5303, Batch(micro): 5303, Batch (considering grad accum): 662,  Loss: 5.9156, Time: 24.25s, Token/s: 21.12
Epoch: 0, Step: 5304, Batch(micro): 5304, Batch (considering grad accum): 663,  Loss: 6.8186, Time: 6.83s, Token/s: 74.96
Epoch: 0, Step: 5305, Batch(micro): 5305, Batch (considering grad accum): 663,  Loss: 5.6074, Time: 3.95s, Token/s: 129.60
Epoch: 0, Step: 5306, Batch(micro): 5306, Batch (considering grad accum): 663,  Loss: 5.6762, Time: 3.32s, Token/s: 154.33
Epoch: 0, Step: 5307, Batch(micro): 5307, Batch (considering grad accum): 663,  Loss: 6.3675, Time: 3.35s, Token/s: 152.99
Epoch: 0, Step: 5308, Batch(micro): 5308, Batch (considering grad accum): 663,  Loss: 5.8186, Time: 3.51s, Token/s: 145.93
Epoch: 0, Step: 5309, Batch(micro): 5309, Batch (considering grad accum): 663,  Loss: 5.8832, Time: 3.60s, Token/s: 142.33
Epoch: 0, Step: 5310, Batch(micro): 5310, Batch (considering grad accum): 663,  Loss: 6.8185, Time: 4.18s, Token/s: 122.55
Epoch: 0, Step: 5311, Batch(micro): 5311, Batch (considering grad accum): 663,  Loss: 6.3594, Time: 23.33s, Token/s: 21.94
Epoch: 0, Step: 5312, Batch(micro): 5312, Batch (considering grad accum): 664,  Loss: 6.8509, Time: 7.97s, Token/s: 64.24
Epoch: 0, Step: 5313, Batch(micro): 5313, Batch (considering grad accum): 664,  Loss: 6.1855, Time: 3.78s, Token/s: 135.58
Epoch: 0, Step: 5314, Batch(micro): 5314, Batch (considering grad accum): 664,  Loss: 6.2051, Time: 3.25s, Token/s: 157.33
Epoch: 0, Step: 5315, Batch(micro): 5315, Batch (considering grad accum): 664,  Loss: 5.9627, Time: 3.21s, Token/s: 159.54
Epoch: 0, Step: 5316, Batch(micro): 5316, Batch (considering grad accum): 664,  Loss: 5.4449, Time: 3.23s, Token/s: 158.70
Epoch: 0, Step: 5317, Batch(micro): 5317, Batch (considering grad accum): 664,  Loss: 6.2682, Time: 3.36s, Token/s: 152.20
Epoch: 0, Step: 5318, Batch(micro): 5318, Batch (considering grad accum): 664,  Loss: 6.4182, Time: 3.16s, Token/s: 161.95
Epoch: 0, Step: 5319, Batch(micro): 5319, Batch (considering grad accum): 664,  Loss: 6.4341, Time: 25.52s, Token/s: 20.07
Epoch: 0, Step: 5320, Batch(micro): 5320, Batch (considering grad accum): 665,  Loss: 7.0068, Time: 6.72s, Token/s: 76.19
Epoch: 0, Step: 5321, Batch(micro): 5321, Batch (considering grad accum): 665,  Loss: 5.7324, Time: 4.11s, Token/s: 124.61
Epoch: 0, Step: 5322, Batch(micro): 5322, Batch (considering grad accum): 665,  Loss: 5.8781, Time: 3.50s, Token/s: 146.49
Epoch: 0, Step: 5323, Batch(micro): 5323, Batch (considering grad accum): 665,  Loss: 7.3196, Time: 3.65s, Token/s: 140.40
Epoch: 0, Step: 5324, Batch(micro): 5324, Batch (considering grad accum): 665,  Loss: 6.0787, Time: 3.63s, Token/s: 141.12
Epoch: 0, Step: 5325, Batch(micro): 5325, Batch (considering grad accum): 665,  Loss: 6.1373, Time: 3.39s, Token/s: 151.19
Epoch: 0, Step: 5326, Batch(micro): 5326, Batch (considering grad accum): 665,  Loss: 5.8818, Time: 3.56s, Token/s: 143.66
Epoch: 0, Step: 5327, Batch(micro): 5327, Batch (considering grad accum): 665,  Loss: 6.6863, Time: 23.09s, Token/s: 22.17
Epoch: 0, Step: 5328, Batch(micro): 5328, Batch (considering grad accum): 666,  Loss: 5.7305, Time: 8.64s, Token/s: 59.26
Epoch: 0, Step: 5329, Batch(micro): 5329, Batch (considering grad accum): 666,  Loss: 5.9615, Time: 3.42s, Token/s: 149.70
Epoch: 0, Step: 5330, Batch(micro): 5330, Batch (considering grad accum): 666,  Loss: 5.9554, Time: 3.19s, Token/s: 160.41
Epoch: 0, Step: 5331, Batch(micro): 5331, Batch (considering grad accum): 666,  Loss: 7.0732, Time: 3.23s, Token/s: 158.63
Epoch: 0, Step: 5332, Batch(micro): 5332, Batch (considering grad accum): 666,  Loss: 6.6168, Time: 3.68s, Token/s: 139.22
Epoch: 0, Step: 5333, Batch(micro): 5333, Batch (considering grad accum): 666,  Loss: 6.1259, Time: 3.52s, Token/s: 145.32
Epoch: 0, Step: 5334, Batch(micro): 5334, Batch (considering grad accum): 666,  Loss: 6.0879, Time: 3.47s, Token/s: 147.72
Epoch: 0, Step: 5335, Batch(micro): 5335, Batch (considering grad accum): 666,  Loss: 6.3440, Time: 24.29s, Token/s: 21.08
Epoch: 0, Step: 5336, Batch(micro): 5336, Batch (considering grad accum): 667,  Loss: 6.5352, Time: 7.64s, Token/s: 66.99
Epoch: 0, Step: 5337, Batch(micro): 5337, Batch (considering grad accum): 667,  Loss: 6.9092, Time: 4.01s, Token/s: 127.54
Epoch: 0, Step: 5338, Batch(micro): 5338, Batch (considering grad accum): 667,  Loss: 6.4069, Time: 3.91s, Token/s: 130.85
Epoch: 0, Step: 5339, Batch(micro): 5339, Batch (considering grad accum): 667,  Loss: 6.0118, Time: 3.82s, Token/s: 133.98
Epoch: 0, Step: 5340, Batch(micro): 5340, Batch (considering grad accum): 667,  Loss: 5.8327, Time: 3.66s, Token/s: 140.04
Epoch: 0, Step: 5341, Batch(micro): 5341, Batch (considering grad accum): 667,  Loss: 5.8910, Time: 3.53s, Token/s: 145.12
Epoch: 0, Step: 5342, Batch(micro): 5342, Batch (considering grad accum): 667,  Loss: 6.2301, Time: 3.37s, Token/s: 151.81
Epoch: 0, Step: 5343, Batch(micro): 5343, Batch (considering grad accum): 667,  Loss: 6.8174, Time: 24.00s, Token/s: 21.33
Epoch: 0, Step: 5344, Batch(micro): 5344, Batch (considering grad accum): 668,  Loss: 6.2804, Time: 8.36s, Token/s: 61.27
Epoch: 0, Step: 5345, Batch(micro): 5345, Batch (considering grad accum): 668,  Loss: 6.2729, Time: 3.17s, Token/s: 161.60
Epoch: 0, Step: 5346, Batch(micro): 5346, Batch (considering grad accum): 668,  Loss: 6.1031, Time: 3.27s, Token/s: 156.74
Epoch: 0, Step: 5347, Batch(micro): 5347, Batch (considering grad accum): 668,  Loss: 6.5947, Time: 3.22s, Token/s: 159.03
Epoch: 0, Step: 5348, Batch(micro): 5348, Batch (considering grad accum): 668,  Loss: 6.0533, Time: 3.30s, Token/s: 154.96
Epoch: 0, Step: 5349, Batch(micro): 5349, Batch (considering grad accum): 668,  Loss: 6.2953, Time: 3.56s, Token/s: 143.78
Epoch: 0, Step: 5350, Batch(micro): 5350, Batch (considering grad accum): 668,  Loss: 6.0947, Time: 3.27s, Token/s: 156.59
Epoch: 0, Step: 5351, Batch(micro): 5351, Batch (considering grad accum): 668,  Loss: 6.3844, Time: 23.67s, Token/s: 21.63
Epoch: 0, Step: 5352, Batch(micro): 5352, Batch (considering grad accum): 669,  Loss: 6.9806, Time: 8.20s, Token/s: 62.42
Epoch: 0, Step: 5353, Batch(micro): 5353, Batch (considering grad accum): 669,  Loss: 6.7123, Time: 3.86s, Token/s: 132.64
Epoch: 0, Step: 5354, Batch(micro): 5354, Batch (considering grad accum): 669,  Loss: 6.9416, Time: 3.69s, Token/s: 138.78
Epoch: 0, Step: 5355, Batch(micro): 5355, Batch (considering grad accum): 669,  Loss: 6.1633, Time: 3.50s, Token/s: 146.12
Epoch: 0, Step: 5356, Batch(micro): 5356, Batch (considering grad accum): 669,  Loss: 5.8780, Time: 3.59s, Token/s: 142.55
Epoch: 0, Step: 5357, Batch(micro): 5357, Batch (considering grad accum): 669,  Loss: 6.3726, Time: 3.48s, Token/s: 146.95
Epoch: 0, Step: 5358, Batch(micro): 5358, Batch (considering grad accum): 669,  Loss: 5.9406, Time: 3.96s, Token/s: 129.36
Epoch: 0, Step: 5359, Batch(micro): 5359, Batch (considering grad accum): 669,  Loss: 6.4681, Time: 26.65s, Token/s: 19.21
Epoch: 0, Step: 5360, Batch(micro): 5360, Batch (considering grad accum): 670,  Loss: 7.0467, Time: 7.98s, Token/s: 64.19
Epoch: 0, Step: 5361, Batch(micro): 5361, Batch (considering grad accum): 670,  Loss: 6.1377, Time: 3.99s, Token/s: 128.34
Epoch: 0, Step: 5362, Batch(micro): 5362, Batch (considering grad accum): 670,  Loss: 6.3290, Time: 3.47s, Token/s: 147.67
Epoch: 0, Step: 5363, Batch(micro): 5363, Batch (considering grad accum): 670,  Loss: 6.1223, Time: 3.61s, Token/s: 141.83
Epoch: 0, Step: 5364, Batch(micro): 5364, Batch (considering grad accum): 670,  Loss: 6.0228, Time: 3.66s, Token/s: 139.75
Epoch: 0, Step: 5365, Batch(micro): 5365, Batch (considering grad accum): 670,  Loss: 6.6063, Time: 3.20s, Token/s: 160.07
Epoch: 0, Step: 5366, Batch(micro): 5366, Batch (considering grad accum): 670,  Loss: 6.8852, Time: 3.18s, Token/s: 161.19
Epoch: 0, Step: 5367, Batch(micro): 5367, Batch (considering grad accum): 670,  Loss: 6.1136, Time: 24.28s, Token/s: 21.08
Epoch: 0, Step: 5368, Batch(micro): 5368, Batch (considering grad accum): 671,  Loss: 5.7384, Time: 7.24s, Token/s: 70.76
Epoch: 0, Step: 5369, Batch(micro): 5369, Batch (considering grad accum): 671,  Loss: 6.8432, Time: 3.91s, Token/s: 131.00
Epoch: 0, Step: 5370, Batch(micro): 5370, Batch (considering grad accum): 671,  Loss: 5.8592, Time: 3.49s, Token/s: 146.80
Epoch: 0, Step: 5371, Batch(micro): 5371, Batch (considering grad accum): 671,  Loss: 7.0033, Time: 3.49s, Token/s: 146.54
Epoch: 0, Step: 5372, Batch(micro): 5372, Batch (considering grad accum): 671,  Loss: 6.5769, Time: 3.45s, Token/s: 148.57
Epoch: 0, Step: 5373, Batch(micro): 5373, Batch (considering grad accum): 671,  Loss: 6.9777, Time: 3.49s, Token/s: 146.52
Epoch: 0, Step: 5374, Batch(micro): 5374, Batch (considering grad accum): 671,  Loss: 6.3826, Time: 3.39s, Token/s: 151.02
Epoch: 0, Step: 5375, Batch(micro): 5375, Batch (considering grad accum): 671,  Loss: 6.5163, Time: 21.72s, Token/s: 23.58
Epoch: 0, Step: 5376, Batch(micro): 5376, Batch (considering grad accum): 672,  Loss: 6.8059, Time: 6.41s, Token/s: 79.89
Epoch: 0, Step: 5377, Batch(micro): 5377, Batch (considering grad accum): 672,  Loss: 6.9249, Time: 3.92s, Token/s: 130.73
Epoch: 0, Step: 5378, Batch(micro): 5378, Batch (considering grad accum): 672,  Loss: 6.5522, Time: 3.61s, Token/s: 141.66
Epoch: 0, Step: 5379, Batch(micro): 5379, Batch (considering grad accum): 672,  Loss: 6.8123, Time: 3.19s, Token/s: 160.37
Epoch: 0, Step: 5380, Batch(micro): 5380, Batch (considering grad accum): 672,  Loss: 6.1167, Time: 3.73s, Token/s: 137.15
Epoch: 0, Step: 5381, Batch(micro): 5381, Batch (considering grad accum): 672,  Loss: 6.0032, Time: 3.45s, Token/s: 148.59
Epoch: 0, Step: 5382, Batch(micro): 5382, Batch (considering grad accum): 672,  Loss: 5.7662, Time: 3.19s, Token/s: 160.67
Epoch: 0, Step: 5383, Batch(micro): 5383, Batch (considering grad accum): 672,  Loss: 6.5252, Time: 22.65s, Token/s: 22.61
Epoch: 0, Step: 5384, Batch(micro): 5384, Batch (considering grad accum): 673,  Loss: 6.3623, Time: 6.79s, Token/s: 75.36
Epoch: 0, Step: 5385, Batch(micro): 5385, Batch (considering grad accum): 673,  Loss: 6.0202, Time: 3.90s, Token/s: 131.22
Epoch: 0, Step: 5386, Batch(micro): 5386, Batch (considering grad accum): 673,  Loss: 6.1701, Time: 3.52s, Token/s: 145.55
Epoch: 0, Step: 5387, Batch(micro): 5387, Batch (considering grad accum): 673,  Loss: 6.7983, Time: 3.36s, Token/s: 152.35
Epoch: 0, Step: 5388, Batch(micro): 5388, Batch (considering grad accum): 673,  Loss: 6.2724, Time: 3.47s, Token/s: 147.34
Epoch: 0, Step: 5389, Batch(micro): 5389, Batch (considering grad accum): 673,  Loss: 6.0140, Time: 3.44s, Token/s: 148.71
Epoch: 0, Step: 5390, Batch(micro): 5390, Batch (considering grad accum): 673,  Loss: 6.0668, Time: 3.56s, Token/s: 143.70
Epoch: 0, Step: 5391, Batch(micro): 5391, Batch (considering grad accum): 673,  Loss: 6.4755, Time: 22.61s, Token/s: 22.64
Epoch: 0, Step: 5392, Batch(micro): 5392, Batch (considering grad accum): 674,  Loss: 5.7693, Time: 7.41s, Token/s: 69.10
Epoch: 0, Step: 5393, Batch(micro): 5393, Batch (considering grad accum): 674,  Loss: 6.2105, Time: 4.18s, Token/s: 122.49
Epoch: 0, Step: 5394, Batch(micro): 5394, Batch (considering grad accum): 674,  Loss: 6.2506, Time: 3.53s, Token/s: 145.23
Epoch: 0, Step: 5395, Batch(micro): 5395, Batch (considering grad accum): 674,  Loss: 6.6512, Time: 3.46s, Token/s: 148.19
Epoch: 0, Step: 5396, Batch(micro): 5396, Batch (considering grad accum): 674,  Loss: 6.3605, Time: 3.54s, Token/s: 144.43
Epoch: 0, Step: 5397, Batch(micro): 5397, Batch (considering grad accum): 674,  Loss: 6.3301, Time: 3.59s, Token/s: 142.62
Epoch: 0, Step: 5398, Batch(micro): 5398, Batch (considering grad accum): 674,  Loss: 6.3175, Time: 3.66s, Token/s: 140.07
Epoch: 0, Step: 5399, Batch(micro): 5399, Batch (considering grad accum): 674,  Loss: 6.2109, Time: 17.89s, Token/s: 28.61
Updating MLP bias
Epoch: 0, Step: 5400, Batch(micro): 5400, Batch (considering grad accum): 675,  Loss: 5.9571, Time: 6.40s, Token/s: 79.99
Epoch: 0, Step: 5401, Batch(micro): 5401, Batch (considering grad accum): 675,  Loss: 6.2362, Time: 3.92s, Token/s: 130.47
Epoch: 0, Step: 5402, Batch(micro): 5402, Batch (considering grad accum): 675,  Loss: 5.9681, Time: 3.42s, Token/s: 149.61
Epoch: 0, Step: 5403, Batch(micro): 5403, Batch (considering grad accum): 675,  Loss: 5.4446, Time: 3.52s, Token/s: 145.25
Epoch: 0, Step: 5404, Batch(micro): 5404, Batch (considering grad accum): 675,  Loss: 5.6075, Time: 3.52s, Token/s: 145.57
Epoch: 0, Step: 5405, Batch(micro): 5405, Batch (considering grad accum): 675,  Loss: 6.2431, Time: 3.30s, Token/s: 155.09
Epoch: 0, Step: 5406, Batch(micro): 5406, Batch (considering grad accum): 675,  Loss: 6.2621, Time: 3.29s, Token/s: 155.65
Epoch: 0, Step: 5407, Batch(micro): 5407, Batch (considering grad accum): 675,  Loss: 6.8133, Time: 17.97s, Token/s: 28.50
Epoch: 0, Step: 5408, Batch(micro): 5408, Batch (considering grad accum): 676,  Loss: 6.1373, Time: 6.22s, Token/s: 82.36
Epoch: 0, Step: 5409, Batch(micro): 5409, Batch (considering grad accum): 676,  Loss: 5.7175, Time: 3.73s, Token/s: 137.42
Epoch: 0, Step: 5410, Batch(micro): 5410, Batch (considering grad accum): 676,  Loss: 6.0147, Time: 3.21s, Token/s: 159.36
Epoch: 0, Step: 5411, Batch(micro): 5411, Batch (considering grad accum): 676,  Loss: 6.1592, Time: 3.18s, Token/s: 160.87
Epoch: 0, Step: 5412, Batch(micro): 5412, Batch (considering grad accum): 676,  Loss: 6.4324, Time: 3.44s, Token/s: 149.01
Epoch: 0, Step: 5413, Batch(micro): 5413, Batch (considering grad accum): 676,  Loss: 6.5699, Time: 3.40s, Token/s: 150.78
Epoch: 0, Step: 5414, Batch(micro): 5414, Batch (considering grad accum): 676,  Loss: 6.0250, Time: 3.39s, Token/s: 151.02
Epoch: 0, Step: 5415, Batch(micro): 5415, Batch (considering grad accum): 676,  Loss: 6.5413, Time: 18.78s, Token/s: 27.26
Epoch: 0, Step: 5416, Batch(micro): 5416, Batch (considering grad accum): 677,  Loss: 5.7801, Time: 6.78s, Token/s: 75.50
Epoch: 0, Step: 5417, Batch(micro): 5417, Batch (considering grad accum): 677,  Loss: 6.7175, Time: 3.90s, Token/s: 131.20
Epoch: 0, Step: 5418, Batch(micro): 5418, Batch (considering grad accum): 677,  Loss: 6.0077, Time: 3.28s, Token/s: 156.02
Epoch: 0, Step: 5419, Batch(micro): 5419, Batch (considering grad accum): 677,  Loss: 5.9748, Time: 3.24s, Token/s: 158.04
Epoch: 0, Step: 5420, Batch(micro): 5420, Batch (considering grad accum): 677,  Loss: 5.5630, Time: 3.19s, Token/s: 160.45
Epoch: 0, Step: 5421, Batch(micro): 5421, Batch (considering grad accum): 677,  Loss: 5.3227, Time: 3.29s, Token/s: 155.45
Epoch: 0, Step: 5422, Batch(micro): 5422, Batch (considering grad accum): 677,  Loss: 5.6373, Time: 3.26s, Token/s: 157.24
Epoch: 0, Step: 5423, Batch(micro): 5423, Batch (considering grad accum): 677,  Loss: 5.3321, Time: 17.71s, Token/s: 28.91
Epoch: 0, Step: 5424, Batch(micro): 5424, Batch (considering grad accum): 678,  Loss: 7.5599, Time: 6.13s, Token/s: 83.55
Epoch: 0, Step: 5425, Batch(micro): 5425, Batch (considering grad accum): 678,  Loss: 5.9200, Time: 3.66s, Token/s: 139.78
Epoch: 0, Step: 5426, Batch(micro): 5426, Batch (considering grad accum): 678,  Loss: 6.5891, Time: 3.43s, Token/s: 149.37
Epoch: 0, Step: 5427, Batch(micro): 5427, Batch (considering grad accum): 678,  Loss: 7.2044, Time: 3.59s, Token/s: 142.68
Epoch: 0, Step: 5428, Batch(micro): 5428, Batch (considering grad accum): 678,  Loss: 6.2031, Time: 3.34s, Token/s: 153.16
Epoch: 0, Step: 5429, Batch(micro): 5429, Batch (considering grad accum): 678,  Loss: 6.3821, Time: 3.28s, Token/s: 156.28
Epoch: 0, Step: 5430, Batch(micro): 5430, Batch (considering grad accum): 678,  Loss: 6.6198, Time: 3.38s, Token/s: 151.67
Epoch: 0, Step: 5431, Batch(micro): 5431, Batch (considering grad accum): 678,  Loss: 6.4533, Time: 18.03s, Token/s: 28.39
Epoch: 0, Step: 5432, Batch(micro): 5432, Batch (considering grad accum): 679,  Loss: 6.4766, Time: 6.31s, Token/s: 81.11
Epoch: 0, Step: 5433, Batch(micro): 5433, Batch (considering grad accum): 679,  Loss: 6.4582, Time: 3.65s, Token/s: 140.38
Epoch: 0, Step: 5434, Batch(micro): 5434, Batch (considering grad accum): 679,  Loss: 5.9535, Time: 3.19s, Token/s: 160.73
Epoch: 0, Step: 5435, Batch(micro): 5435, Batch (considering grad accum): 679,  Loss: 6.4001, Time: 3.20s, Token/s: 160.10
Epoch: 0, Step: 5436, Batch(micro): 5436, Batch (considering grad accum): 679,  Loss: 6.7866, Time: 3.15s, Token/s: 162.49
Epoch: 0, Step: 5437, Batch(micro): 5437, Batch (considering grad accum): 679,  Loss: 6.4399, Time: 3.86s, Token/s: 132.67
Epoch: 0, Step: 5438, Batch(micro): 5438, Batch (considering grad accum): 679,  Loss: 6.9040, Time: 3.41s, Token/s: 150.24
Epoch: 0, Step: 5439, Batch(micro): 5439, Batch (considering grad accum): 679,  Loss: 6.8672, Time: 18.32s, Token/s: 27.95
Epoch: 0, Step: 5440, Batch(micro): 5440, Batch (considering grad accum): 680,  Loss: 7.1383, Time: 7.15s, Token/s: 71.58
Epoch: 0, Step: 5441, Batch(micro): 5441, Batch (considering grad accum): 680,  Loss: 6.6312, Time: 3.94s, Token/s: 129.90
Epoch: 0, Step: 5442, Batch(micro): 5442, Batch (considering grad accum): 680,  Loss: 6.5198, Time: 3.86s, Token/s: 132.69
Epoch: 0, Step: 5443, Batch(micro): 5443, Batch (considering grad accum): 680,  Loss: 6.5040, Time: 3.51s, Token/s: 145.84
Epoch: 0, Step: 5444, Batch(micro): 5444, Batch (considering grad accum): 680,  Loss: 5.1091, Time: 3.23s, Token/s: 158.73
Epoch: 0, Step: 5445, Batch(micro): 5445, Batch (considering grad accum): 680,  Loss: 6.4346, Time: 3.62s, Token/s: 141.37
Epoch: 0, Step: 5446, Batch(micro): 5446, Batch (considering grad accum): 680,  Loss: 6.2306, Time: 3.45s, Token/s: 148.30
Epoch: 0, Step: 5447, Batch(micro): 5447, Batch (considering grad accum): 680,  Loss: 6.4581, Time: 20.53s, Token/s: 24.94
Epoch: 0, Step: 5448, Batch(micro): 5448, Batch (considering grad accum): 681,  Loss: 6.4498, Time: 6.76s, Token/s: 75.75
Epoch: 0, Step: 5449, Batch(micro): 5449, Batch (considering grad accum): 681,  Loss: 6.0876, Time: 3.81s, Token/s: 134.27
Epoch: 0, Step: 5450, Batch(micro): 5450, Batch (considering grad accum): 681,  Loss: 6.5765, Time: 3.38s, Token/s: 151.31
Epoch: 0, Step: 5451, Batch(micro): 5451, Batch (considering grad accum): 681,  Loss: 6.4103, Time: 3.39s, Token/s: 150.94
Epoch: 0, Step: 5452, Batch(micro): 5452, Batch (considering grad accum): 681,  Loss: 6.6439, Time: 3.17s, Token/s: 161.41
Epoch: 0, Step: 5453, Batch(micro): 5453, Batch (considering grad accum): 681,  Loss: 7.3253, Time: 3.55s, Token/s: 144.41
Epoch: 0, Step: 5454, Batch(micro): 5454, Batch (considering grad accum): 681,  Loss: 6.5149, Time: 3.34s, Token/s: 153.10
Epoch: 0, Step: 5455, Batch(micro): 5455, Batch (considering grad accum): 681,  Loss: 6.3020, Time: 18.92s, Token/s: 27.06
Epoch: 0, Step: 5456, Batch(micro): 5456, Batch (considering grad accum): 682,  Loss: 6.5919, Time: 6.69s, Token/s: 76.55
Epoch: 0, Step: 5457, Batch(micro): 5457, Batch (considering grad accum): 682,  Loss: 6.3783, Time: 3.72s, Token/s: 137.82
Epoch: 0, Step: 5458, Batch(micro): 5458, Batch (considering grad accum): 682,  Loss: 6.1092, Time: 3.51s, Token/s: 146.03
Epoch: 0, Step: 5459, Batch(micro): 5459, Batch (considering grad accum): 682,  Loss: 6.4612, Time: 3.54s, Token/s: 144.70
Epoch: 0, Step: 5460, Batch(micro): 5460, Batch (considering grad accum): 682,  Loss: 6.1791, Time: 3.55s, Token/s: 144.27
Epoch: 0, Step: 5461, Batch(micro): 5461, Batch (considering grad accum): 682,  Loss: 6.4939, Time: 3.18s, Token/s: 160.77
Epoch: 0, Step: 5462, Batch(micro): 5462, Batch (considering grad accum): 682,  Loss: 5.4500, Time: 3.23s, Token/s: 158.33
Epoch: 0, Step: 5463, Batch(micro): 5463, Batch (considering grad accum): 682,  Loss: 6.5337, Time: 19.36s, Token/s: 26.45
Epoch: 0, Step: 5464, Batch(micro): 5464, Batch (considering grad accum): 683,  Loss: 6.9750, Time: 6.43s, Token/s: 79.68
Epoch: 0, Step: 5465, Batch(micro): 5465, Batch (considering grad accum): 683,  Loss: 6.6299, Time: 3.87s, Token/s: 132.17
Epoch: 0, Step: 5466, Batch(micro): 5466, Batch (considering grad accum): 683,  Loss: 5.9707, Time: 3.65s, Token/s: 140.28
Epoch: 0, Step: 5467, Batch(micro): 5467, Batch (considering grad accum): 683,  Loss: 6.0648, Time: 3.51s, Token/s: 145.94
Epoch: 0, Step: 5468, Batch(micro): 5468, Batch (considering grad accum): 683,  Loss: 6.6134, Time: 3.43s, Token/s: 149.48
Epoch: 0, Step: 5469, Batch(micro): 5469, Batch (considering grad accum): 683,  Loss: 6.3328, Time: 3.82s, Token/s: 133.88
Epoch: 0, Step: 5470, Batch(micro): 5470, Batch (considering grad accum): 683,  Loss: 6.6166, Time: 3.54s, Token/s: 144.49
Epoch: 0, Step: 5471, Batch(micro): 5471, Batch (considering grad accum): 683,  Loss: 6.1869, Time: 19.17s, Token/s: 26.70
Epoch: 0, Step: 5472, Batch(micro): 5472, Batch (considering grad accum): 684,  Loss: 5.9265, Time: 6.32s, Token/s: 81.05
Epoch: 0, Step: 5473, Batch(micro): 5473, Batch (considering grad accum): 684,  Loss: 6.0896, Time: 3.78s, Token/s: 135.57
Epoch: 0, Step: 5474, Batch(micro): 5474, Batch (considering grad accum): 684,  Loss: 6.4791, Time: 3.28s, Token/s: 156.21
Epoch: 0, Step: 5475, Batch(micro): 5475, Batch (considering grad accum): 684,  Loss: 5.9838, Time: 3.29s, Token/s: 155.44
Epoch: 0, Step: 5476, Batch(micro): 5476, Batch (considering grad accum): 684,  Loss: 6.3395, Time: 3.43s, Token/s: 149.16
Epoch: 0, Step: 5477, Batch(micro): 5477, Batch (considering grad accum): 684,  Loss: 6.4400, Time: 3.65s, Token/s: 140.16
Epoch: 0, Step: 5478, Batch(micro): 5478, Batch (considering grad accum): 684,  Loss: 6.5358, Time: 3.55s, Token/s: 144.03
Epoch: 0, Step: 5479, Batch(micro): 5479, Batch (considering grad accum): 684,  Loss: 6.7548, Time: 18.66s, Token/s: 27.45
Epoch: 0, Step: 5480, Batch(micro): 5480, Batch (considering grad accum): 685,  Loss: 7.2256, Time: 6.14s, Token/s: 83.43
Epoch: 0, Step: 5481, Batch(micro): 5481, Batch (considering grad accum): 685,  Loss: 6.6053, Time: 4.25s, Token/s: 120.49
Epoch: 0, Step: 5482, Batch(micro): 5482, Batch (considering grad accum): 685,  Loss: 6.0808, Time: 3.72s, Token/s: 137.51
Epoch: 0, Step: 5483, Batch(micro): 5483, Batch (considering grad accum): 685,  Loss: 7.0757, Time: 3.44s, Token/s: 148.96
Epoch: 0, Step: 5484, Batch(micro): 5484, Batch (considering grad accum): 685,  Loss: 5.7115, Time: 3.36s, Token/s: 152.33
Epoch: 0, Step: 5485, Batch(micro): 5485, Batch (considering grad accum): 685,  Loss: 5.5430, Time: 3.46s, Token/s: 148.14
Epoch: 0, Step: 5486, Batch(micro): 5486, Batch (considering grad accum): 685,  Loss: 6.2467, Time: 3.58s, Token/s: 142.97
Epoch: 0, Step: 5487, Batch(micro): 5487, Batch (considering grad accum): 685,  Loss: 6.2894, Time: 18.77s, Token/s: 27.28
Epoch: 0, Step: 5488, Batch(micro): 5488, Batch (considering grad accum): 686,  Loss: 6.2765, Time: 6.67s, Token/s: 76.79
Epoch: 0, Step: 5489, Batch(micro): 5489, Batch (considering grad accum): 686,  Loss: 6.2496, Time: 3.34s, Token/s: 153.09
Epoch: 0, Step: 5490, Batch(micro): 5490, Batch (considering grad accum): 686,  Loss: 5.6821, Time: 3.33s, Token/s: 153.80
Epoch: 0, Step: 5491, Batch(micro): 5491, Batch (considering grad accum): 686,  Loss: 6.2716, Time: 3.63s, Token/s: 141.03
Epoch: 0, Step: 5492, Batch(micro): 5492, Batch (considering grad accum): 686,  Loss: 6.1869, Time: 3.64s, Token/s: 140.81
Epoch: 0, Step: 5493, Batch(micro): 5493, Batch (considering grad accum): 686,  Loss: 6.0643, Time: 3.56s, Token/s: 143.75
Epoch: 0, Step: 5494, Batch(micro): 5494, Batch (considering grad accum): 686,  Loss: 6.0189, Time: 3.38s, Token/s: 151.33
Epoch: 0, Step: 5495, Batch(micro): 5495, Batch (considering grad accum): 686,  Loss: 6.0276, Time: 20.68s, Token/s: 24.75
Epoch: 0, Step: 5496, Batch(micro): 5496, Batch (considering grad accum): 687,  Loss: 6.7425, Time: 10.90s, Token/s: 46.99
Epoch: 0, Step: 5497, Batch(micro): 5497, Batch (considering grad accum): 687,  Loss: 6.0165, Time: 4.37s, Token/s: 117.03
Epoch: 0, Step: 5498, Batch(micro): 5498, Batch (considering grad accum): 687,  Loss: 6.0832, Time: 3.56s, Token/s: 143.85
Epoch: 0, Step: 5499, Batch(micro): 5499, Batch (considering grad accum): 687,  Loss: 5.7101, Time: 3.47s, Token/s: 147.51
Updating MLP bias
Epoch: 0, Step: 5500, Batch(micro): 5500, Batch (considering grad accum): 687,  Loss: 5.9673, Time: 3.40s, Token/s: 150.62
Epoch: 0, Step: 5501, Batch(micro): 5501, Batch (considering grad accum): 687,  Loss: 5.8470, Time: 3.50s, Token/s: 146.20
Epoch: 0, Step: 5502, Batch(micro): 5502, Batch (considering grad accum): 687,  Loss: 6.6906, Time: 3.57s, Token/s: 143.27
Epoch: 0, Step: 5503, Batch(micro): 5503, Batch (considering grad accum): 687,  Loss: 6.3851, Time: 23.67s, Token/s: 21.63
Epoch: 0, Step: 5504, Batch(micro): 5504, Batch (considering grad accum): 688,  Loss: 6.8620, Time: 6.72s, Token/s: 76.17
Epoch: 0, Step: 5505, Batch(micro): 5505, Batch (considering grad accum): 688,  Loss: 6.6113, Time: 4.02s, Token/s: 127.21
Epoch: 0, Step: 5506, Batch(micro): 5506, Batch (considering grad accum): 688,  Loss: 6.6097, Time: 3.61s, Token/s: 141.69
Epoch: 0, Step: 5507, Batch(micro): 5507, Batch (considering grad accum): 688,  Loss: 6.6139, Time: 3.53s, Token/s: 145.09
Epoch: 0, Step: 5508, Batch(micro): 5508, Batch (considering grad accum): 688,  Loss: 6.0529, Time: 3.54s, Token/s: 144.77
Epoch: 0, Step: 5509, Batch(micro): 5509, Batch (considering grad accum): 688,  Loss: 6.2674, Time: 3.60s, Token/s: 142.19
Epoch: 0, Step: 5510, Batch(micro): 5510, Batch (considering grad accum): 688,  Loss: 6.1367, Time: 3.47s, Token/s: 147.55
Epoch: 0, Step: 5511, Batch(micro): 5511, Batch (considering grad accum): 688,  Loss: 5.8646, Time: 22.41s, Token/s: 22.85
Epoch: 0, Step: 5512, Batch(micro): 5512, Batch (considering grad accum): 689,  Loss: 6.2441, Time: 6.15s, Token/s: 83.25
Epoch: 0, Step: 5513, Batch(micro): 5513, Batch (considering grad accum): 689,  Loss: 6.3861, Time: 3.88s, Token/s: 132.02
Epoch: 0, Step: 5514, Batch(micro): 5514, Batch (considering grad accum): 689,  Loss: 7.3072, Time: 3.77s, Token/s: 135.89
Epoch: 0, Step: 5515, Batch(micro): 5515, Batch (considering grad accum): 689,  Loss: 6.9083, Time: 3.68s, Token/s: 139.13
Epoch: 0, Step: 5516, Batch(micro): 5516, Batch (considering grad accum): 689,  Loss: 6.0708, Time: 3.57s, Token/s: 143.36
Epoch: 0, Step: 5517, Batch(micro): 5517, Batch (considering grad accum): 689,  Loss: 6.5065, Time: 3.47s, Token/s: 147.49
Epoch: 0, Step: 5518, Batch(micro): 5518, Batch (considering grad accum): 689,  Loss: 6.7025, Time: 3.59s, Token/s: 142.48
Epoch: 0, Step: 5519, Batch(micro): 5519, Batch (considering grad accum): 689,  Loss: 6.8016, Time: 21.73s, Token/s: 23.56
Epoch: 0, Step: 5520, Batch(micro): 5520, Batch (considering grad accum): 690,  Loss: 6.3488, Time: 7.36s, Token/s: 69.52
Epoch: 0, Step: 5521, Batch(micro): 5521, Batch (considering grad accum): 690,  Loss: 7.1121, Time: 3.82s, Token/s: 134.07
Epoch: 0, Step: 5522, Batch(micro): 5522, Batch (considering grad accum): 690,  Loss: 7.2332, Time: 3.30s, Token/s: 155.13
Epoch: 0, Step: 5523, Batch(micro): 5523, Batch (considering grad accum): 690,  Loss: 6.8879, Time: 3.27s, Token/s: 156.49
Epoch: 0, Step: 5524, Batch(micro): 5524, Batch (considering grad accum): 690,  Loss: 5.8813, Time: 3.25s, Token/s: 157.63
Epoch: 0, Step: 5525, Batch(micro): 5525, Batch (considering grad accum): 690,  Loss: 5.9656, Time: 3.34s, Token/s: 153.47
Epoch: 0, Step: 5526, Batch(micro): 5526, Batch (considering grad accum): 690,  Loss: 6.7666, Time: 3.52s, Token/s: 145.31
Epoch: 0, Step: 5527, Batch(micro): 5527, Batch (considering grad accum): 690,  Loss: 6.9485, Time: 22.20s, Token/s: 23.06
Epoch: 0, Step: 5528, Batch(micro): 5528, Batch (considering grad accum): 691,  Loss: 6.1422, Time: 8.05s, Token/s: 63.60
Epoch: 0, Step: 5529, Batch(micro): 5529, Batch (considering grad accum): 691,  Loss: 6.6342, Time: 3.26s, Token/s: 157.05
Epoch: 0, Step: 5530, Batch(micro): 5530, Batch (considering grad accum): 691,  Loss: 6.3744, Time: 3.19s, Token/s: 160.28
Epoch: 0, Step: 5531, Batch(micro): 5531, Batch (considering grad accum): 691,  Loss: 6.2852, Time: 3.36s, Token/s: 152.33
Epoch: 0, Step: 5532, Batch(micro): 5532, Batch (considering grad accum): 691,  Loss: 6.6560, Time: 3.32s, Token/s: 154.30
Epoch: 0, Step: 5533, Batch(micro): 5533, Batch (considering grad accum): 691,  Loss: 6.6255, Time: 3.44s, Token/s: 148.72
Epoch: 0, Step: 5534, Batch(micro): 5534, Batch (considering grad accum): 691,  Loss: 7.0540, Time: 3.68s, Token/s: 139.17
Epoch: 0, Step: 5535, Batch(micro): 5535, Batch (considering grad accum): 691,  Loss: 6.3851, Time: 26.27s, Token/s: 19.49
Epoch: 0, Step: 5536, Batch(micro): 5536, Batch (considering grad accum): 692,  Loss: 6.5011, Time: 7.25s, Token/s: 70.60
Epoch: 0, Step: 5537, Batch(micro): 5537, Batch (considering grad accum): 692,  Loss: 7.1719, Time: 3.89s, Token/s: 131.53
Epoch: 0, Step: 5538, Batch(micro): 5538, Batch (considering grad accum): 692,  Loss: 6.7404, Time: 3.20s, Token/s: 159.87
Epoch: 0, Step: 5539, Batch(micro): 5539, Batch (considering grad accum): 692,  Loss: 5.8637, Time: 3.18s, Token/s: 161.03
Epoch: 0, Step: 5540, Batch(micro): 5540, Batch (considering grad accum): 692,  Loss: 5.7962, Time: 3.30s, Token/s: 155.06
Epoch: 0, Step: 5541, Batch(micro): 5541, Batch (considering grad accum): 692,  Loss: 6.7053, Time: 3.33s, Token/s: 153.87
Epoch: 0, Step: 5542, Batch(micro): 5542, Batch (considering grad accum): 692,  Loss: 6.4056, Time: 3.34s, Token/s: 153.22
Epoch: 0, Step: 5543, Batch(micro): 5543, Batch (considering grad accum): 692,  Loss: 6.0539, Time: 24.05s, Token/s: 21.29
Epoch: 0, Step: 5544, Batch(micro): 5544, Batch (considering grad accum): 693,  Loss: 5.9296, Time: 8.49s, Token/s: 60.32
Epoch: 0, Step: 5545, Batch(micro): 5545, Batch (considering grad accum): 693,  Loss: 6.5338, Time: 4.11s, Token/s: 124.61
Epoch: 0, Step: 5546, Batch(micro): 5546, Batch (considering grad accum): 693,  Loss: 7.2747, Time: 3.53s, Token/s: 145.23
Epoch: 0, Step: 5547, Batch(micro): 5547, Batch (considering grad accum): 693,  Loss: 6.1982, Time: 3.67s, Token/s: 139.49
Epoch: 0, Step: 5548, Batch(micro): 5548, Batch (considering grad accum): 693,  Loss: 6.0479, Time: 3.46s, Token/s: 148.19
Epoch: 0, Step: 5549, Batch(micro): 5549, Batch (considering grad accum): 693,  Loss: 6.8666, Time: 3.28s, Token/s: 156.32
Epoch: 0, Step: 5550, Batch(micro): 5550, Batch (considering grad accum): 693,  Loss: 5.6157, Time: 3.45s, Token/s: 148.53
Epoch: 0, Step: 5551, Batch(micro): 5551, Batch (considering grad accum): 693,  Loss: 6.0400, Time: 24.18s, Token/s: 21.18
Epoch: 0, Step: 5552, Batch(micro): 5552, Batch (considering grad accum): 694,  Loss: 5.9324, Time: 7.11s, Token/s: 72.02
Epoch: 0, Step: 5553, Batch(micro): 5553, Batch (considering grad accum): 694,  Loss: 6.1844, Time: 3.93s, Token/s: 130.29
Epoch: 0, Step: 5554, Batch(micro): 5554, Batch (considering grad accum): 694,  Loss: 6.9295, Time: 3.68s, Token/s: 139.22
Epoch: 0, Step: 5555, Batch(micro): 5555, Batch (considering grad accum): 694,  Loss: 6.2747, Time: 3.57s, Token/s: 143.34
Epoch: 0, Step: 5556, Batch(micro): 5556, Batch (considering grad accum): 694,  Loss: 6.3526, Time: 3.44s, Token/s: 148.92
Epoch: 0, Step: 5557, Batch(micro): 5557, Batch (considering grad accum): 694,  Loss: 6.2189, Time: 4.34s, Token/s: 117.93
Epoch: 0, Step: 5558, Batch(micro): 5558, Batch (considering grad accum): 694,  Loss: 6.1209, Time: 3.22s, Token/s: 159.23
Epoch: 0, Step: 5559, Batch(micro): 5559, Batch (considering grad accum): 694,  Loss: 6.2968, Time: 22.15s, Token/s: 23.11
Epoch: 0, Step: 5560, Batch(micro): 5560, Batch (considering grad accum): 695,  Loss: 6.0393, Time: 8.30s, Token/s: 61.71
Epoch: 0, Step: 5561, Batch(micro): 5561, Batch (considering grad accum): 695,  Loss: 5.8370, Time: 3.34s, Token/s: 153.52
Epoch: 0, Step: 5562, Batch(micro): 5562, Batch (considering grad accum): 695,  Loss: 6.3341, Time: 3.32s, Token/s: 154.41
Epoch: 0, Step: 5563, Batch(micro): 5563, Batch (considering grad accum): 695,  Loss: 6.9427, Time: 3.53s, Token/s: 144.90
Epoch: 0, Step: 5564, Batch(micro): 5564, Batch (considering grad accum): 695,  Loss: 6.1644, Time: 3.50s, Token/s: 146.26
Epoch: 0, Step: 5565, Batch(micro): 5565, Batch (considering grad accum): 695,  Loss: 6.6844, Time: 3.87s, Token/s: 132.18
Epoch: 0, Step: 5566, Batch(micro): 5566, Batch (considering grad accum): 695,  Loss: 7.0491, Time: 3.36s, Token/s: 152.42
Epoch: 0, Step: 5567, Batch(micro): 5567, Batch (considering grad accum): 695,  Loss: 6.7619, Time: 23.30s, Token/s: 21.97
Epoch: 0, Step: 5568, Batch(micro): 5568, Batch (considering grad accum): 696,  Loss: 6.1224, Time: 5.77s, Token/s: 88.75
Epoch: 0, Step: 5569, Batch(micro): 5569, Batch (considering grad accum): 696,  Loss: 6.2189, Time: 3.76s, Token/s: 136.27
Epoch: 0, Step: 5570, Batch(micro): 5570, Batch (considering grad accum): 696,  Loss: 5.8653, Time: 3.44s, Token/s: 149.04
Epoch: 0, Step: 5571, Batch(micro): 5571, Batch (considering grad accum): 696,  Loss: 6.4458, Time: 3.33s, Token/s: 153.88
Epoch: 0, Step: 5572, Batch(micro): 5572, Batch (considering grad accum): 696,  Loss: 5.9191, Time: 3.48s, Token/s: 147.12
Epoch: 0, Step: 5573, Batch(micro): 5573, Batch (considering grad accum): 696,  Loss: 6.4561, Time: 3.55s, Token/s: 144.28
Epoch: 0, Step: 5574, Batch(micro): 5574, Batch (considering grad accum): 696,  Loss: 6.2235, Time: 3.90s, Token/s: 131.34
Epoch: 0, Step: 5575, Batch(micro): 5575, Batch (considering grad accum): 696,  Loss: 6.1602, Time: 22.93s, Token/s: 22.33
Epoch: 0, Step: 5576, Batch(micro): 5576, Batch (considering grad accum): 697,  Loss: 6.6594, Time: 7.27s, Token/s: 70.46
Epoch: 0, Step: 5577, Batch(micro): 5577, Batch (considering grad accum): 697,  Loss: 6.3111, Time: 4.06s, Token/s: 126.12
Epoch: 0, Step: 5578, Batch(micro): 5578, Batch (considering grad accum): 697,  Loss: 5.7442, Time: 3.62s, Token/s: 141.39
Epoch: 0, Step: 5579, Batch(micro): 5579, Batch (considering grad accum): 697,  Loss: 6.1297, Time: 3.31s, Token/s: 154.68
Epoch: 0, Step: 5580, Batch(micro): 5580, Batch (considering grad accum): 697,  Loss: 5.9674, Time: 3.49s, Token/s: 146.75
Epoch: 0, Step: 5581, Batch(micro): 5581, Batch (considering grad accum): 697,  Loss: 6.4641, Time: 3.87s, Token/s: 132.28
Epoch: 0, Step: 5582, Batch(micro): 5582, Batch (considering grad accum): 697,  Loss: 6.3565, Time: 3.57s, Token/s: 143.37
Epoch: 0, Step: 5583, Batch(micro): 5583, Batch (considering grad accum): 697,  Loss: 6.1980, Time: 24.49s, Token/s: 20.91
Epoch: 0, Step: 5584, Batch(micro): 5584, Batch (considering grad accum): 698,  Loss: 5.9995, Time: 7.62s, Token/s: 67.17
Epoch: 0, Step: 5585, Batch(micro): 5585, Batch (considering grad accum): 698,  Loss: 7.2553, Time: 4.23s, Token/s: 120.97
Epoch: 0, Step: 5586, Batch(micro): 5586, Batch (considering grad accum): 698,  Loss: 6.2370, Time: 3.61s, Token/s: 141.87
Epoch: 0, Step: 5587, Batch(micro): 5587, Batch (considering grad accum): 698,  Loss: 6.4606, Time: 3.51s, Token/s: 146.00
Epoch: 0, Step: 5588, Batch(micro): 5588, Batch (considering grad accum): 698,  Loss: 6.0516, Time: 3.75s, Token/s: 136.68
Epoch: 0, Step: 5589, Batch(micro): 5589, Batch (considering grad accum): 698,  Loss: 6.3980, Time: 3.50s, Token/s: 146.35
Epoch: 0, Step: 5590, Batch(micro): 5590, Batch (considering grad accum): 698,  Loss: 6.2730, Time: 3.59s, Token/s: 142.58
Epoch: 0, Step: 5591, Batch(micro): 5591, Batch (considering grad accum): 698,  Loss: 5.9310, Time: 23.78s, Token/s: 21.53
Epoch: 0, Step: 5592, Batch(micro): 5592, Batch (considering grad accum): 699,  Loss: 7.0101, Time: 7.47s, Token/s: 68.56
Epoch: 0, Step: 5593, Batch(micro): 5593, Batch (considering grad accum): 699,  Loss: 6.5267, Time: 3.95s, Token/s: 129.47
Epoch: 0, Step: 5594, Batch(micro): 5594, Batch (considering grad accum): 699,  Loss: 6.1258, Time: 3.81s, Token/s: 134.42
Epoch: 0, Step: 5595, Batch(micro): 5595, Batch (considering grad accum): 699,  Loss: 6.0495, Time: 3.64s, Token/s: 140.62
Epoch: 0, Step: 5596, Batch(micro): 5596, Batch (considering grad accum): 699,  Loss: 5.9864, Time: 3.52s, Token/s: 145.45
Epoch: 0, Step: 5597, Batch(micro): 5597, Batch (considering grad accum): 699,  Loss: 5.6176, Time: 3.58s, Token/s: 143.07
Epoch: 0, Step: 5598, Batch(micro): 5598, Batch (considering grad accum): 699,  Loss: 5.8695, Time: 3.46s, Token/s: 147.77
Epoch: 0, Step: 5599, Batch(micro): 5599, Batch (considering grad accum): 699,  Loss: 6.1074, Time: 24.53s, Token/s: 20.88
Updating MLP bias
Epoch: 0, Step: 5600, Batch(micro): 5600, Batch (considering grad accum): 700,  Loss: 6.3689, Time: 7.73s, Token/s: 66.21
Epoch: 0, Step: 5601, Batch(micro): 5601, Batch (considering grad accum): 700,  Loss: 5.9095, Time: 3.76s, Token/s: 136.21
Epoch: 0, Step: 5602, Batch(micro): 5602, Batch (considering grad accum): 700,  Loss: 6.6750, Time: 3.15s, Token/s: 162.62
Epoch: 0, Step: 5603, Batch(micro): 5603, Batch (considering grad accum): 700,  Loss: 6.3678, Time: 3.18s, Token/s: 161.20
Epoch: 0, Step: 5604, Batch(micro): 5604, Batch (considering grad accum): 700,  Loss: 6.2628, Time: 3.25s, Token/s: 157.30
Epoch: 0, Step: 5605, Batch(micro): 5605, Batch (considering grad accum): 700,  Loss: 6.4559, Time: 3.46s, Token/s: 148.07
Epoch: 0, Step: 5606, Batch(micro): 5606, Batch (considering grad accum): 700,  Loss: 5.8511, Time: 3.19s, Token/s: 160.70
Epoch: 0, Step: 5607, Batch(micro): 5607, Batch (considering grad accum): 700,  Loss: 6.1904, Time: 23.47s, Token/s: 21.82
Epoch: 0, Step: 5608, Batch(micro): 5608, Batch (considering grad accum): 701,  Loss: 7.0000, Time: 8.33s, Token/s: 61.44
Epoch: 0, Step: 5609, Batch(micro): 5609, Batch (considering grad accum): 701,  Loss: 5.9811, Time: 3.96s, Token/s: 129.41
Epoch: 0, Step: 5610, Batch(micro): 5610, Batch (considering grad accum): 701,  Loss: 5.6673, Time: 3.66s, Token/s: 139.95
Epoch: 0, Step: 5611, Batch(micro): 5611, Batch (considering grad accum): 701,  Loss: 6.2543, Time: 3.13s, Token/s: 163.52
Epoch: 0, Step: 5612, Batch(micro): 5612, Batch (considering grad accum): 701,  Loss: 6.5003, Time: 3.38s, Token/s: 151.52
Epoch: 0, Step: 5613, Batch(micro): 5613, Batch (considering grad accum): 701,  Loss: 6.7840, Time: 3.13s, Token/s: 163.39
Epoch: 0, Step: 5614, Batch(micro): 5614, Batch (considering grad accum): 701,  Loss: 5.9608, Time: 3.39s, Token/s: 151.15
Epoch: 0, Step: 5615, Batch(micro): 5615, Batch (considering grad accum): 701,  Loss: 6.6728, Time: 23.70s, Token/s: 21.60
Epoch: 0, Step: 5616, Batch(micro): 5616, Batch (considering grad accum): 702,  Loss: 6.3225, Time: 6.68s, Token/s: 76.59
Epoch: 0, Step: 5617, Batch(micro): 5617, Batch (considering grad accum): 702,  Loss: 6.4714, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 5618, Batch(micro): 5618, Batch (considering grad accum): 702,  Loss: 6.9905, Time: 2.93s, Token/s: 174.74
Epoch: 0, Step: 5619, Batch(micro): 5619, Batch (considering grad accum): 702,  Loss: 6.4015, Time: 3.13s, Token/s: 163.74
Epoch: 0, Step: 5620, Batch(micro): 5620, Batch (considering grad accum): 702,  Loss: 5.9730, Time: 3.16s, Token/s: 162.24
Epoch: 0, Step: 5621, Batch(micro): 5621, Batch (considering grad accum): 702,  Loss: 6.0238, Time: 3.03s, Token/s: 169.01
Epoch: 0, Step: 5622, Batch(micro): 5622, Batch (considering grad accum): 702,  Loss: 5.9833, Time: 3.00s, Token/s: 170.87
Epoch: 0, Step: 5623, Batch(micro): 5623, Batch (considering grad accum): 702,  Loss: 6.2609, Time: 23.38s, Token/s: 21.90
Epoch: 0, Step: 5624, Batch(micro): 5624, Batch (considering grad accum): 703,  Loss: 6.0046, Time: 7.21s, Token/s: 71.03
Epoch: 0, Step: 5625, Batch(micro): 5625, Batch (considering grad accum): 703,  Loss: 6.2393, Time: 3.72s, Token/s: 137.75
Epoch: 0, Step: 5626, Batch(micro): 5626, Batch (considering grad accum): 703,  Loss: 6.2081, Time: 3.15s, Token/s: 162.41
Epoch: 0, Step: 5627, Batch(micro): 5627, Batch (considering grad accum): 703,  Loss: 5.9321, Time: 3.04s, Token/s: 168.68
Epoch: 0, Step: 5628, Batch(micro): 5628, Batch (considering grad accum): 703,  Loss: 6.1216, Time: 3.08s, Token/s: 166.34
Epoch: 0, Step: 5629, Batch(micro): 5629, Batch (considering grad accum): 703,  Loss: 6.1421, Time: 3.61s, Token/s: 141.80
Epoch: 0, Step: 5630, Batch(micro): 5630, Batch (considering grad accum): 703,  Loss: 6.2152, Time: 3.49s, Token/s: 146.82
Epoch: 0, Step: 5631, Batch(micro): 5631, Batch (considering grad accum): 703,  Loss: 6.1417, Time: 24.14s, Token/s: 21.21
Epoch: 0, Step: 5632, Batch(micro): 5632, Batch (considering grad accum): 704,  Loss: 5.8593, Time: 8.22s, Token/s: 62.31
Epoch: 0, Step: 5633, Batch(micro): 5633, Batch (considering grad accum): 704,  Loss: 6.3586, Time: 3.99s, Token/s: 128.35
Epoch: 0, Step: 5634, Batch(micro): 5634, Batch (considering grad accum): 704,  Loss: 6.6282, Time: 3.40s, Token/s: 150.38
Epoch: 0, Step: 5635, Batch(micro): 5635, Batch (considering grad accum): 704,  Loss: 6.1044, Time: 3.15s, Token/s: 162.73
Epoch: 0, Step: 5636, Batch(micro): 5636, Batch (considering grad accum): 704,  Loss: 5.9198, Time: 3.13s, Token/s: 163.33
Epoch: 0, Step: 5637, Batch(micro): 5637, Batch (considering grad accum): 704,  Loss: 6.1318, Time: 3.11s, Token/s: 164.39
Epoch: 0, Step: 5638, Batch(micro): 5638, Batch (considering grad accum): 704,  Loss: 6.3213, Time: 2.99s, Token/s: 171.27
Epoch: 0, Step: 5639, Batch(micro): 5639, Batch (considering grad accum): 704,  Loss: 5.8547, Time: 22.91s, Token/s: 22.35
Epoch: 0, Step: 5640, Batch(micro): 5640, Batch (considering grad accum): 705,  Loss: 6.4710, Time: 6.54s, Token/s: 78.24
Epoch: 0, Step: 5641, Batch(micro): 5641, Batch (considering grad accum): 705,  Loss: 5.9687, Time: 3.71s, Token/s: 138.10
Epoch: 0, Step: 5642, Batch(micro): 5642, Batch (considering grad accum): 705,  Loss: 6.0802, Time: 3.61s, Token/s: 141.81
Epoch: 0, Step: 5643, Batch(micro): 5643, Batch (considering grad accum): 705,  Loss: 6.3646, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 5644, Batch(micro): 5644, Batch (considering grad accum): 705,  Loss: 6.5957, Time: 3.50s, Token/s: 146.14
Epoch: 0, Step: 5645, Batch(micro): 5645, Batch (considering grad accum): 705,  Loss: 5.4090, Time: 3.46s, Token/s: 147.85
Epoch: 0, Step: 5646, Batch(micro): 5646, Batch (considering grad accum): 705,  Loss: 6.5432, Time: 3.15s, Token/s: 162.76
Epoch: 0, Step: 5647, Batch(micro): 5647, Batch (considering grad accum): 705,  Loss: 5.9659, Time: 23.29s, Token/s: 21.99
Epoch: 0, Step: 5648, Batch(micro): 5648, Batch (considering grad accum): 706,  Loss: 5.5907, Time: 9.19s, Token/s: 55.69
Epoch: 0, Step: 5649, Batch(micro): 5649, Batch (considering grad accum): 706,  Loss: 5.6026, Time: 3.09s, Token/s: 165.92
Epoch: 0, Step: 5650, Batch(micro): 5650, Batch (considering grad accum): 706,  Loss: 5.7498, Time: 2.95s, Token/s: 173.50
Epoch: 0, Step: 5651, Batch(micro): 5651, Batch (considering grad accum): 706,  Loss: 6.2568, Time: 3.32s, Token/s: 154.20
Epoch: 0, Step: 5652, Batch(micro): 5652, Batch (considering grad accum): 706,  Loss: 6.8568, Time: 3.19s, Token/s: 160.49
Epoch: 0, Step: 5653, Batch(micro): 5653, Batch (considering grad accum): 706,  Loss: 6.8101, Time: 3.30s, Token/s: 155.01
Epoch: 0, Step: 5654, Batch(micro): 5654, Batch (considering grad accum): 706,  Loss: 6.4591, Time: 3.04s, Token/s: 168.46
Epoch: 0, Step: 5655, Batch(micro): 5655, Batch (considering grad accum): 706,  Loss: 5.7656, Time: 22.35s, Token/s: 22.91
Epoch: 0, Step: 5656, Batch(micro): 5656, Batch (considering grad accum): 707,  Loss: 5.8600, Time: 7.80s, Token/s: 65.64
Epoch: 0, Step: 5657, Batch(micro): 5657, Batch (considering grad accum): 707,  Loss: 5.1773, Time: 3.34s, Token/s: 153.26
Epoch: 0, Step: 5658, Batch(micro): 5658, Batch (considering grad accum): 707,  Loss: 6.5363, Time: 3.10s, Token/s: 165.40
Epoch: 0, Step: 5659, Batch(micro): 5659, Batch (considering grad accum): 707,  Loss: 6.3459, Time: 2.98s, Token/s: 172.04
Epoch: 0, Step: 5660, Batch(micro): 5660, Batch (considering grad accum): 707,  Loss: 6.3769, Time: 2.92s, Token/s: 175.37
Epoch: 0, Step: 5661, Batch(micro): 5661, Batch (considering grad accum): 707,  Loss: 7.0305, Time: 3.17s, Token/s: 161.63
Epoch: 0, Step: 5662, Batch(micro): 5662, Batch (considering grad accum): 707,  Loss: 6.8950, Time: 3.27s, Token/s: 156.45
Epoch: 0, Step: 5663, Batch(micro): 5663, Batch (considering grad accum): 707,  Loss: 6.6750, Time: 20.85s, Token/s: 24.56
Epoch: 0, Step: 5664, Batch(micro): 5664, Batch (considering grad accum): 708,  Loss: 6.1545, Time: 9.05s, Token/s: 56.58
Epoch: 0, Step: 5665, Batch(micro): 5665, Batch (considering grad accum): 708,  Loss: 5.9617, Time: 3.13s, Token/s: 163.64
Epoch: 0, Step: 5666, Batch(micro): 5666, Batch (considering grad accum): 708,  Loss: 6.9898, Time: 3.22s, Token/s: 159.12
Epoch: 0, Step: 5667, Batch(micro): 5667, Batch (considering grad accum): 708,  Loss: 6.6125, Time: 3.35s, Token/s: 152.66
Epoch: 0, Step: 5668, Batch(micro): 5668, Batch (considering grad accum): 708,  Loss: 7.0439, Time: 3.46s, Token/s: 148.10
Epoch: 0, Step: 5669, Batch(micro): 5669, Batch (considering grad accum): 708,  Loss: 6.5112, Time: 3.13s, Token/s: 163.60
Epoch: 0, Step: 5670, Batch(micro): 5670, Batch (considering grad accum): 708,  Loss: 6.7227, Time: 3.22s, Token/s: 159.01
Epoch: 0, Step: 5671, Batch(micro): 5671, Batch (considering grad accum): 708,  Loss: 7.1392, Time: 23.46s, Token/s: 21.82
Epoch: 0, Step: 5672, Batch(micro): 5672, Batch (considering grad accum): 709,  Loss: 6.3388, Time: 6.98s, Token/s: 73.36
Epoch: 0, Step: 5673, Batch(micro): 5673, Batch (considering grad accum): 709,  Loss: 6.5987, Time: 4.05s, Token/s: 126.37
Epoch: 0, Step: 5674, Batch(micro): 5674, Batch (considering grad accum): 709,  Loss: 7.4235, Time: 3.23s, Token/s: 158.33
Epoch: 0, Step: 5675, Batch(micro): 5675, Batch (considering grad accum): 709,  Loss: 6.9163, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 5676, Batch(micro): 5676, Batch (considering grad accum): 709,  Loss: 5.4997, Time: 3.25s, Token/s: 157.70
Epoch: 0, Step: 5677, Batch(micro): 5677, Batch (considering grad accum): 709,  Loss: 6.0270, Time: 3.54s, Token/s: 144.47
Epoch: 0, Step: 5678, Batch(micro): 5678, Batch (considering grad accum): 709,  Loss: 6.3207, Time: 2.97s, Token/s: 172.58
Epoch: 0, Step: 5679, Batch(micro): 5679, Batch (considering grad accum): 709,  Loss: 6.0887, Time: 24.40s, Token/s: 20.98
Epoch: 0, Step: 5680, Batch(micro): 5680, Batch (considering grad accum): 710,  Loss: 6.1798, Time: 6.89s, Token/s: 74.34
Epoch: 0, Step: 5681, Batch(micro): 5681, Batch (considering grad accum): 710,  Loss: 5.6705, Time: 3.76s, Token/s: 136.00
Epoch: 0, Step: 5682, Batch(micro): 5682, Batch (considering grad accum): 710,  Loss: 5.9687, Time: 3.19s, Token/s: 160.75
Epoch: 0, Step: 5683, Batch(micro): 5683, Batch (considering grad accum): 710,  Loss: 6.4704, Time: 3.02s, Token/s: 169.37
Epoch: 0, Step: 5684, Batch(micro): 5684, Batch (considering grad accum): 710,  Loss: 6.5343, Time: 3.05s, Token/s: 167.66
Epoch: 0, Step: 5685, Batch(micro): 5685, Batch (considering grad accum): 710,  Loss: 5.9538, Time: 3.07s, Token/s: 167.00
Epoch: 0, Step: 5686, Batch(micro): 5686, Batch (considering grad accum): 710,  Loss: 6.1375, Time: 2.89s, Token/s: 177.07
Epoch: 0, Step: 5687, Batch(micro): 5687, Batch (considering grad accum): 710,  Loss: 6.1536, Time: 24.64s, Token/s: 20.78
Epoch: 0, Step: 5688, Batch(micro): 5688, Batch (considering grad accum): 711,  Loss: 6.1459, Time: 8.12s, Token/s: 63.03
Epoch: 0, Step: 5689, Batch(micro): 5689, Batch (considering grad accum): 711,  Loss: 6.3580, Time: 3.42s, Token/s: 149.62
Epoch: 0, Step: 5690, Batch(micro): 5690, Batch (considering grad accum): 711,  Loss: 6.3046, Time: 3.49s, Token/s: 146.64
Epoch: 0, Step: 5691, Batch(micro): 5691, Batch (considering grad accum): 711,  Loss: 6.2108, Time: 3.38s, Token/s: 151.32
Epoch: 0, Step: 5692, Batch(micro): 5692, Batch (considering grad accum): 711,  Loss: 6.3053, Time: 3.29s, Token/s: 155.43
Epoch: 0, Step: 5693, Batch(micro): 5693, Batch (considering grad accum): 711,  Loss: 6.1627, Time: 3.39s, Token/s: 150.86
Epoch: 0, Step: 5694, Batch(micro): 5694, Batch (considering grad accum): 711,  Loss: 6.6236, Time: 3.12s, Token/s: 164.30
Epoch: 0, Step: 5695, Batch(micro): 5695, Batch (considering grad accum): 711,  Loss: 6.5850, Time: 19.73s, Token/s: 25.94
Epoch: 0, Step: 5696, Batch(micro): 5696, Batch (considering grad accum): 712,  Loss: 6.5227, Time: 6.37s, Token/s: 80.40
Epoch: 0, Step: 5697, Batch(micro): 5697, Batch (considering grad accum): 712,  Loss: 6.1040, Time: 3.52s, Token/s: 145.39
Epoch: 0, Step: 5698, Batch(micro): 5698, Batch (considering grad accum): 712,  Loss: 6.4236, Time: 3.03s, Token/s: 168.91
Epoch: 0, Step: 5699, Batch(micro): 5699, Batch (considering grad accum): 712,  Loss: 6.1133, Time: 3.01s, Token/s: 169.94
Updating MLP bias
Epoch: 0, Step: 5700, Batch(micro): 5700, Batch (considering grad accum): 712,  Loss: 6.0066, Time: 3.09s, Token/s: 165.95
Epoch: 0, Step: 5701, Batch(micro): 5701, Batch (considering grad accum): 712,  Loss: 5.9085, Time: 2.91s, Token/s: 175.74
Epoch: 0, Step: 5702, Batch(micro): 5702, Batch (considering grad accum): 712,  Loss: 6.6495, Time: 2.99s, Token/s: 171.42
Epoch: 0, Step: 5703, Batch(micro): 5703, Batch (considering grad accum): 712,  Loss: 6.3153, Time: 17.62s, Token/s: 29.06
Epoch: 0, Step: 5704, Batch(micro): 5704, Batch (considering grad accum): 713,  Loss: 8.3131, Time: 6.51s, Token/s: 78.60
Epoch: 0, Step: 5705, Batch(micro): 5705, Batch (considering grad accum): 713,  Loss: 7.3389, Time: 3.95s, Token/s: 129.63
Epoch: 0, Step: 5706, Batch(micro): 5706, Batch (considering grad accum): 713,  Loss: 5.9244, Time: 3.54s, Token/s: 144.61
Epoch: 0, Step: 5707, Batch(micro): 5707, Batch (considering grad accum): 713,  Loss: 6.4489, Time: 3.65s, Token/s: 140.14
Epoch: 0, Step: 5708, Batch(micro): 5708, Batch (considering grad accum): 713,  Loss: 6.8977, Time: 3.32s, Token/s: 154.42
Epoch: 0, Step: 5709, Batch(micro): 5709, Batch (considering grad accum): 713,  Loss: 6.1599, Time: 2.93s, Token/s: 174.97
Epoch: 0, Step: 5710, Batch(micro): 5710, Batch (considering grad accum): 713,  Loss: 6.1491, Time: 2.95s, Token/s: 173.40
Epoch: 0, Step: 5711, Batch(micro): 5711, Batch (considering grad accum): 713,  Loss: 6.6790, Time: 17.81s, Token/s: 28.75
Epoch: 0, Step: 5712, Batch(micro): 5712, Batch (considering grad accum): 714,  Loss: 6.1145, Time: 5.92s, Token/s: 86.41
Epoch: 0, Step: 5713, Batch(micro): 5713, Batch (considering grad accum): 714,  Loss: 5.7171, Time: 3.62s, Token/s: 141.38
Epoch: 0, Step: 5714, Batch(micro): 5714, Batch (considering grad accum): 714,  Loss: 5.5002, Time: 3.38s, Token/s: 151.51
Epoch: 0, Step: 5715, Batch(micro): 5715, Batch (considering grad accum): 714,  Loss: 6.5414, Time: 3.22s, Token/s: 159.24
Epoch: 0, Step: 5716, Batch(micro): 5716, Batch (considering grad accum): 714,  Loss: 6.9541, Time: 3.27s, Token/s: 156.52
Epoch: 0, Step: 5717, Batch(micro): 5717, Batch (considering grad accum): 714,  Loss: 6.5365, Time: 3.33s, Token/s: 153.94
Epoch: 0, Step: 5718, Batch(micro): 5718, Batch (considering grad accum): 714,  Loss: 6.2835, Time: 3.07s, Token/s: 166.61
Epoch: 0, Step: 5719, Batch(micro): 5719, Batch (considering grad accum): 714,  Loss: 6.3534, Time: 20.07s, Token/s: 25.51
Epoch: 0, Step: 5720, Batch(micro): 5720, Batch (considering grad accum): 715,  Loss: 5.0628, Time: 7.07s, Token/s: 72.40
Epoch: 0, Step: 5721, Batch(micro): 5721, Batch (considering grad accum): 715,  Loss: 6.1190, Time: 4.00s, Token/s: 127.94
Epoch: 0, Step: 5722, Batch(micro): 5722, Batch (considering grad accum): 715,  Loss: 6.1891, Time: 3.47s, Token/s: 147.67
Epoch: 0, Step: 5723, Batch(micro): 5723, Batch (considering grad accum): 715,  Loss: 6.1057, Time: 3.36s, Token/s: 152.31
Epoch: 0, Step: 5724, Batch(micro): 5724, Batch (considering grad accum): 715,  Loss: 6.4000, Time: 3.36s, Token/s: 152.39
Epoch: 0, Step: 5725, Batch(micro): 5725, Batch (considering grad accum): 715,  Loss: 6.0921, Time: 3.39s, Token/s: 150.91
Epoch: 0, Step: 5726, Batch(micro): 5726, Batch (considering grad accum): 715,  Loss: 6.2916, Time: 3.36s, Token/s: 152.19
Epoch: 0, Step: 5727, Batch(micro): 5727, Batch (considering grad accum): 715,  Loss: 6.0542, Time: 19.84s, Token/s: 25.81
Epoch: 0, Step: 5728, Batch(micro): 5728, Batch (considering grad accum): 716,  Loss: 5.5843, Time: 6.88s, Token/s: 74.46
Epoch: 0, Step: 5729, Batch(micro): 5729, Batch (considering grad accum): 716,  Loss: 6.2517, Time: 3.65s, Token/s: 140.30
Epoch: 0, Step: 5730, Batch(micro): 5730, Batch (considering grad accum): 716,  Loss: 6.3861, Time: 3.01s, Token/s: 170.04
Epoch: 0, Step: 5731, Batch(micro): 5731, Batch (considering grad accum): 716,  Loss: 6.6551, Time: 3.16s, Token/s: 161.99
Epoch: 0, Step: 5732, Batch(micro): 5732, Batch (considering grad accum): 716,  Loss: 6.7056, Time: 3.14s, Token/s: 162.93
Epoch: 0, Step: 5733, Batch(micro): 5733, Batch (considering grad accum): 716,  Loss: 6.6525, Time: 3.06s, Token/s: 167.33
Epoch: 0, Step: 5734, Batch(micro): 5734, Batch (considering grad accum): 716,  Loss: 6.6045, Time: 3.07s, Token/s: 166.89
Epoch: 0, Step: 5735, Batch(micro): 5735, Batch (considering grad accum): 716,  Loss: 5.8294, Time: 18.52s, Token/s: 27.64
Epoch: 0, Step: 5736, Batch(micro): 5736, Batch (considering grad accum): 717,  Loss: 5.9833, Time: 6.11s, Token/s: 83.74
Epoch: 0, Step: 5737, Batch(micro): 5737, Batch (considering grad accum): 717,  Loss: 6.6226, Time: 4.05s, Token/s: 126.36
Epoch: 0, Step: 5738, Batch(micro): 5738, Batch (considering grad accum): 717,  Loss: 6.6008, Time: 3.37s, Token/s: 151.74
Epoch: 0, Step: 5739, Batch(micro): 5739, Batch (considering grad accum): 717,  Loss: 5.8969, Time: 2.97s, Token/s: 172.23
Epoch: 0, Step: 5740, Batch(micro): 5740, Batch (considering grad accum): 717,  Loss: 6.3207, Time: 3.00s, Token/s: 170.71
Epoch: 0, Step: 5741, Batch(micro): 5741, Batch (considering grad accum): 717,  Loss: 6.4775, Time: 2.88s, Token/s: 177.58
Epoch: 0, Step: 5742, Batch(micro): 5742, Batch (considering grad accum): 717,  Loss: 6.3213, Time: 2.92s, Token/s: 175.10
Epoch: 0, Step: 5743, Batch(micro): 5743, Batch (considering grad accum): 717,  Loss: 6.9858, Time: 18.28s, Token/s: 28.01
Epoch: 0, Step: 5744, Batch(micro): 5744, Batch (considering grad accum): 718,  Loss: 6.4589, Time: 7.37s, Token/s: 69.47
Epoch: 0, Step: 5745, Batch(micro): 5745, Batch (considering grad accum): 718,  Loss: 5.8316, Time: 3.33s, Token/s: 153.76
Epoch: 0, Step: 5746, Batch(micro): 5746, Batch (considering grad accum): 718,  Loss: 6.3804, Time: 2.98s, Token/s: 171.98
Epoch: 0, Step: 5747, Batch(micro): 5747, Batch (considering grad accum): 718,  Loss: 5.6609, Time: 3.01s, Token/s: 170.05
Epoch: 0, Step: 5748, Batch(micro): 5748, Batch (considering grad accum): 718,  Loss: 6.5800, Time: 2.99s, Token/s: 171.37
Epoch: 0, Step: 5749, Batch(micro): 5749, Batch (considering grad accum): 718,  Loss: 5.5178, Time: 3.18s, Token/s: 161.23
Epoch: 0, Step: 5750, Batch(micro): 5750, Batch (considering grad accum): 718,  Loss: 5.6845, Time: 3.28s, Token/s: 156.04
Epoch: 0, Step: 5751, Batch(micro): 5751, Batch (considering grad accum): 718,  Loss: 6.2597, Time: 21.48s, Token/s: 23.84
Epoch: 0, Step: 5752, Batch(micro): 5752, Batch (considering grad accum): 719,  Loss: 6.1060, Time: 6.82s, Token/s: 75.03
Epoch: 0, Step: 5753, Batch(micro): 5753, Batch (considering grad accum): 719,  Loss: 6.2280, Time: 3.70s, Token/s: 138.49
Epoch: 0, Step: 5754, Batch(micro): 5754, Batch (considering grad accum): 719,  Loss: 5.7762, Time: 3.41s, Token/s: 150.09
Epoch: 0, Step: 5755, Batch(micro): 5755, Batch (considering grad accum): 719,  Loss: 5.5452, Time: 3.42s, Token/s: 149.73
Epoch: 0, Step: 5756, Batch(micro): 5756, Batch (considering grad accum): 719,  Loss: 5.5450, Time: 3.27s, Token/s: 156.38
Epoch: 0, Step: 5757, Batch(micro): 5757, Batch (considering grad accum): 719,  Loss: 6.0029, Time: 3.26s, Token/s: 157.03
Epoch: 0, Step: 5758, Batch(micro): 5758, Batch (considering grad accum): 719,  Loss: 6.8056, Time: 3.29s, Token/s: 155.71
Epoch: 0, Step: 5759, Batch(micro): 5759, Batch (considering grad accum): 719,  Loss: 5.7088, Time: 21.99s, Token/s: 23.28
Epoch: 0, Step: 5760, Batch(micro): 5760, Batch (considering grad accum): 720,  Loss: 5.7865, Time: 6.83s, Token/s: 74.91
Epoch: 0, Step: 5761, Batch(micro): 5761, Batch (considering grad accum): 720,  Loss: 6.4740, Time: 3.49s, Token/s: 146.89
Epoch: 0, Step: 5762, Batch(micro): 5762, Batch (considering grad accum): 720,  Loss: 6.4561, Time: 3.04s, Token/s: 168.24
Epoch: 0, Step: 5763, Batch(micro): 5763, Batch (considering grad accum): 720,  Loss: 6.5452, Time: 2.96s, Token/s: 173.24
Epoch: 0, Step: 5764, Batch(micro): 5764, Batch (considering grad accum): 720,  Loss: 7.1436, Time: 2.92s, Token/s: 175.46
Epoch: 0, Step: 5765, Batch(micro): 5765, Batch (considering grad accum): 720,  Loss: 5.9647, Time: 2.91s, Token/s: 175.93
Epoch: 0, Step: 5766, Batch(micro): 5766, Batch (considering grad accum): 720,  Loss: 6.3644, Time: 2.88s, Token/s: 177.88
Epoch: 0, Step: 5767, Batch(micro): 5767, Batch (considering grad accum): 720,  Loss: 6.4111, Time: 22.70s, Token/s: 22.56
Epoch: 0, Step: 5768, Batch(micro): 5768, Batch (considering grad accum): 721,  Loss: 6.2487, Time: 7.09s, Token/s: 72.19
Epoch: 0, Step: 5769, Batch(micro): 5769, Batch (considering grad accum): 721,  Loss: 5.8844, Time: 3.48s, Token/s: 147.22
Epoch: 0, Step: 5770, Batch(micro): 5770, Batch (considering grad accum): 721,  Loss: 5.9767, Time: 2.91s, Token/s: 176.08
Epoch: 0, Step: 5771, Batch(micro): 5771, Batch (considering grad accum): 721,  Loss: 5.9414, Time: 3.19s, Token/s: 160.45
Epoch: 0, Step: 5772, Batch(micro): 5772, Batch (considering grad accum): 721,  Loss: 6.1445, Time: 3.14s, Token/s: 163.01
Epoch: 0, Step: 5773, Batch(micro): 5773, Batch (considering grad accum): 721,  Loss: 6.0192, Time: 3.14s, Token/s: 163.26
Epoch: 0, Step: 5774, Batch(micro): 5774, Batch (considering grad accum): 721,  Loss: 6.8448, Time: 3.30s, Token/s: 154.92
Epoch: 0, Step: 5775, Batch(micro): 5775, Batch (considering grad accum): 721,  Loss: 5.1728, Time: 23.33s, Token/s: 21.94
Epoch: 0, Step: 5776, Batch(micro): 5776, Batch (considering grad accum): 722,  Loss: 5.5330, Time: 7.22s, Token/s: 70.90
Epoch: 0, Step: 5777, Batch(micro): 5777, Batch (considering grad accum): 722,  Loss: 6.0471, Time: 3.72s, Token/s: 137.59
Epoch: 0, Step: 5778, Batch(micro): 5778, Batch (considering grad accum): 722,  Loss: 6.5758, Time: 3.37s, Token/s: 152.06
Epoch: 0, Step: 5779, Batch(micro): 5779, Batch (considering grad accum): 722,  Loss: 6.1237, Time: 3.10s, Token/s: 165.32
Epoch: 0, Step: 5780, Batch(micro): 5780, Batch (considering grad accum): 722,  Loss: 6.0272, Time: 3.00s, Token/s: 170.85
Epoch: 0, Step: 5781, Batch(micro): 5781, Batch (considering grad accum): 722,  Loss: 6.3630, Time: 3.04s, Token/s: 168.53
Epoch: 0, Step: 5782, Batch(micro): 5782, Batch (considering grad accum): 722,  Loss: 6.6613, Time: 3.00s, Token/s: 170.95
Epoch: 0, Step: 5783, Batch(micro): 5783, Batch (considering grad accum): 722,  Loss: 5.7642, Time: 24.13s, Token/s: 21.22
Epoch: 0, Step: 5784, Batch(micro): 5784, Batch (considering grad accum): 723,  Loss: 6.7553, Time: 6.80s, Token/s: 75.34
Epoch: 0, Step: 5785, Batch(micro): 5785, Batch (considering grad accum): 723,  Loss: 6.3126, Time: 3.80s, Token/s: 134.57
Epoch: 0, Step: 5786, Batch(micro): 5786, Batch (considering grad accum): 723,  Loss: 6.1338, Time: 3.26s, Token/s: 157.19
Epoch: 0, Step: 5787, Batch(micro): 5787, Batch (considering grad accum): 723,  Loss: 6.1948, Time: 3.24s, Token/s: 157.96
Epoch: 0, Step: 5788, Batch(micro): 5788, Batch (considering grad accum): 723,  Loss: 6.1650, Time: 3.03s, Token/s: 168.91
Epoch: 0, Step: 5789, Batch(micro): 5789, Batch (considering grad accum): 723,  Loss: 6.2447, Time: 3.11s, Token/s: 164.67
Epoch: 0, Step: 5790, Batch(micro): 5790, Batch (considering grad accum): 723,  Loss: 7.2727, Time: 2.94s, Token/s: 174.02
Epoch: 0, Step: 5791, Batch(micro): 5791, Batch (considering grad accum): 723,  Loss: 6.3787, Time: 23.18s, Token/s: 22.09
Epoch: 0, Step: 5792, Batch(micro): 5792, Batch (considering grad accum): 724,  Loss: 5.8617, Time: 8.44s, Token/s: 60.63
Epoch: 0, Step: 5793, Batch(micro): 5793, Batch (considering grad accum): 724,  Loss: 5.4876, Time: 3.20s, Token/s: 160.15
Epoch: 0, Step: 5794, Batch(micro): 5794, Batch (considering grad accum): 724,  Loss: 5.7938, Time: 3.42s, Token/s: 149.71
Epoch: 0, Step: 5795, Batch(micro): 5795, Batch (considering grad accum): 724,  Loss: 6.2763, Time: 3.62s, Token/s: 141.41
Epoch: 0, Step: 5796, Batch(micro): 5796, Batch (considering grad accum): 724,  Loss: 6.4272, Time: 3.10s, Token/s: 165.01
Epoch: 0, Step: 5797, Batch(micro): 5797, Batch (considering grad accum): 724,  Loss: 6.2679, Time: 3.10s, Token/s: 165.10
Epoch: 0, Step: 5798, Batch(micro): 5798, Batch (considering grad accum): 724,  Loss: 6.3387, Time: 2.90s, Token/s: 176.34
Epoch: 0, Step: 5799, Batch(micro): 5799, Batch (considering grad accum): 724,  Loss: 6.6438, Time: 22.06s, Token/s: 23.21
Updating MLP bias
Epoch: 0, Step: 5800, Batch(micro): 5800, Batch (considering grad accum): 725,  Loss: 6.2551, Time: 8.80s, Token/s: 58.21
Epoch: 0, Step: 5801, Batch(micro): 5801, Batch (considering grad accum): 725,  Loss: 6.4840, Time: 2.93s, Token/s: 174.79
Epoch: 0, Step: 5802, Batch(micro): 5802, Batch (considering grad accum): 725,  Loss: 6.5164, Time: 3.78s, Token/s: 135.30
Epoch: 0, Step: 5803, Batch(micro): 5803, Batch (considering grad accum): 725,  Loss: 6.0724, Time: 3.20s, Token/s: 159.95
Epoch: 0, Step: 5804, Batch(micro): 5804, Batch (considering grad accum): 725,  Loss: 6.2887, Time: 3.36s, Token/s: 152.57
Epoch: 0, Step: 5805, Batch(micro): 5805, Batch (considering grad accum): 725,  Loss: 6.8234, Time: 3.25s, Token/s: 157.64
Epoch: 0, Step: 5806, Batch(micro): 5806, Batch (considering grad accum): 725,  Loss: 7.1111, Time: 3.29s, Token/s: 155.63
Epoch: 0, Step: 5807, Batch(micro): 5807, Batch (considering grad accum): 725,  Loss: 6.2068, Time: 22.38s, Token/s: 22.88
Epoch: 0, Step: 5808, Batch(micro): 5808, Batch (considering grad accum): 726,  Loss: 6.2378, Time: 7.12s, Token/s: 71.87
Epoch: 0, Step: 5809, Batch(micro): 5809, Batch (considering grad accum): 726,  Loss: 6.1832, Time: 3.41s, Token/s: 150.01
Epoch: 0, Step: 5810, Batch(micro): 5810, Batch (considering grad accum): 726,  Loss: 6.2640, Time: 3.54s, Token/s: 144.75
Epoch: 0, Step: 5811, Batch(micro): 5811, Batch (considering grad accum): 726,  Loss: 6.1131, Time: 3.72s, Token/s: 137.61
Epoch: 0, Step: 5812, Batch(micro): 5812, Batch (considering grad accum): 726,  Loss: 7.5161, Time: 3.42s, Token/s: 149.57
Epoch: 0, Step: 5813, Batch(micro): 5813, Batch (considering grad accum): 726,  Loss: 7.4697, Time: 3.32s, Token/s: 153.99
Epoch: 0, Step: 5814, Batch(micro): 5814, Batch (considering grad accum): 726,  Loss: 5.9474, Time: 3.23s, Token/s: 158.48
Epoch: 0, Step: 5815, Batch(micro): 5815, Batch (considering grad accum): 726,  Loss: 6.5960, Time: 25.35s, Token/s: 20.20
Epoch: 0, Step: 5816, Batch(micro): 5816, Batch (considering grad accum): 727,  Loss: 6.2060, Time: 8.01s, Token/s: 63.92
Epoch: 0, Step: 5817, Batch(micro): 5817, Batch (considering grad accum): 727,  Loss: 6.5637, Time: 3.53s, Token/s: 145.10
Epoch: 0, Step: 5818, Batch(micro): 5818, Batch (considering grad accum): 727,  Loss: 6.8641, Time: 3.15s, Token/s: 162.74
Epoch: 0, Step: 5819, Batch(micro): 5819, Batch (considering grad accum): 727,  Loss: 6.0202, Time: 3.21s, Token/s: 159.39
Epoch: 0, Step: 5820, Batch(micro): 5820, Batch (considering grad accum): 727,  Loss: 5.9073, Time: 3.26s, Token/s: 157.11
Epoch: 0, Step: 5821, Batch(micro): 5821, Batch (considering grad accum): 727,  Loss: 5.9008, Time: 3.44s, Token/s: 148.86
Epoch: 0, Step: 5822, Batch(micro): 5822, Batch (considering grad accum): 727,  Loss: 6.4162, Time: 3.12s, Token/s: 163.90
Epoch: 0, Step: 5823, Batch(micro): 5823, Batch (considering grad accum): 727,  Loss: 6.8241, Time: 23.96s, Token/s: 21.37
Epoch: 0, Step: 5824, Batch(micro): 5824, Batch (considering grad accum): 728,  Loss: 6.5783, Time: 7.45s, Token/s: 68.72
Epoch: 0, Step: 5825, Batch(micro): 5825, Batch (considering grad accum): 728,  Loss: 5.3558, Time: 3.57s, Token/s: 143.22
Epoch: 0, Step: 5826, Batch(micro): 5826, Batch (considering grad accum): 728,  Loss: 6.7071, Time: 3.18s, Token/s: 160.86
Epoch: 0, Step: 5827, Batch(micro): 5827, Batch (considering grad accum): 728,  Loss: 6.3511, Time: 3.11s, Token/s: 164.43
Epoch: 0, Step: 5828, Batch(micro): 5828, Batch (considering grad accum): 728,  Loss: 6.3391, Time: 3.42s, Token/s: 149.74
Epoch: 0, Step: 5829, Batch(micro): 5829, Batch (considering grad accum): 728,  Loss: 5.9220, Time: 3.47s, Token/s: 147.34
Epoch: 0, Step: 5830, Batch(micro): 5830, Batch (considering grad accum): 728,  Loss: 6.8194, Time: 2.95s, Token/s: 173.27
Epoch: 0, Step: 5831, Batch(micro): 5831, Batch (considering grad accum): 728,  Loss: 6.1730, Time: 22.47s, Token/s: 22.78
Epoch: 0, Step: 5832, Batch(micro): 5832, Batch (considering grad accum): 729,  Loss: 5.8282, Time: 6.89s, Token/s: 74.30
Epoch: 0, Step: 5833, Batch(micro): 5833, Batch (considering grad accum): 729,  Loss: 6.7356, Time: 3.63s, Token/s: 140.96
Epoch: 0, Step: 5834, Batch(micro): 5834, Batch (considering grad accum): 729,  Loss: 6.4104, Time: 3.60s, Token/s: 142.12
Epoch: 0, Step: 5835, Batch(micro): 5835, Batch (considering grad accum): 729,  Loss: 6.2621, Time: 3.21s, Token/s: 159.42
Epoch: 0, Step: 5836, Batch(micro): 5836, Batch (considering grad accum): 729,  Loss: 5.7340, Time: 3.04s, Token/s: 168.37
Epoch: 0, Step: 5837, Batch(micro): 5837, Batch (considering grad accum): 729,  Loss: 6.8423, Time: 3.17s, Token/s: 161.75
Epoch: 0, Step: 5838, Batch(micro): 5838, Batch (considering grad accum): 729,  Loss: 6.9212, Time: 3.05s, Token/s: 167.66
Epoch: 0, Step: 5839, Batch(micro): 5839, Batch (considering grad accum): 729,  Loss: 6.6957, Time: 23.33s, Token/s: 21.94
Epoch: 0, Step: 5840, Batch(micro): 5840, Batch (considering grad accum): 730,  Loss: 6.0195, Time: 6.72s, Token/s: 76.20
Epoch: 0, Step: 5841, Batch(micro): 5841, Batch (considering grad accum): 730,  Loss: 6.5844, Time: 3.74s, Token/s: 136.99
Epoch: 0, Step: 5842, Batch(micro): 5842, Batch (considering grad accum): 730,  Loss: 6.3766, Time: 3.27s, Token/s: 156.49
Epoch: 0, Step: 5843, Batch(micro): 5843, Batch (considering grad accum): 730,  Loss: 6.0240, Time: 3.29s, Token/s: 155.74
Epoch: 0, Step: 5844, Batch(micro): 5844, Batch (considering grad accum): 730,  Loss: 6.2537, Time: 3.23s, Token/s: 158.33
Epoch: 0, Step: 5845, Batch(micro): 5845, Batch (considering grad accum): 730,  Loss: 6.0956, Time: 3.17s, Token/s: 161.32
Epoch: 0, Step: 5846, Batch(micro): 5846, Batch (considering grad accum): 730,  Loss: 7.4539, Time: 3.15s, Token/s: 162.36
Epoch: 0, Step: 5847, Batch(micro): 5847, Batch (considering grad accum): 730,  Loss: 7.3851, Time: 24.03s, Token/s: 21.31
Epoch: 0, Step: 5848, Batch(micro): 5848, Batch (considering grad accum): 731,  Loss: 6.3463, Time: 6.67s, Token/s: 76.80
Epoch: 0, Step: 5849, Batch(micro): 5849, Batch (considering grad accum): 731,  Loss: 6.3157, Time: 3.67s, Token/s: 139.35
Epoch: 0, Step: 5850, Batch(micro): 5850, Batch (considering grad accum): 731,  Loss: 6.8347, Time: 3.21s, Token/s: 159.75
Epoch: 0, Step: 5851, Batch(micro): 5851, Batch (considering grad accum): 731,  Loss: 6.4750, Time: 3.25s, Token/s: 157.35
Epoch: 0, Step: 5852, Batch(micro): 5852, Batch (considering grad accum): 731,  Loss: 5.8507, Time: 2.98s, Token/s: 171.60
Epoch: 0, Step: 5853, Batch(micro): 5853, Batch (considering grad accum): 731,  Loss: 6.4554, Time: 3.34s, Token/s: 153.21
Epoch: 0, Step: 5854, Batch(micro): 5854, Batch (considering grad accum): 731,  Loss: 6.6996, Time: 3.29s, Token/s: 155.62
Epoch: 0, Step: 5855, Batch(micro): 5855, Batch (considering grad accum): 731,  Loss: 6.2870, Time: 23.26s, Token/s: 22.01
Epoch: 0, Step: 5856, Batch(micro): 5856, Batch (considering grad accum): 732,  Loss: 6.8169, Time: 7.46s, Token/s: 68.61
Epoch: 0, Step: 5857, Batch(micro): 5857, Batch (considering grad accum): 732,  Loss: 6.3758, Time: 4.06s, Token/s: 126.01
Epoch: 0, Step: 5858, Batch(micro): 5858, Batch (considering grad accum): 732,  Loss: 6.4615, Time: 3.46s, Token/s: 148.12
Epoch: 0, Step: 5859, Batch(micro): 5859, Batch (considering grad accum): 732,  Loss: 6.1687, Time: 3.19s, Token/s: 160.35
Epoch: 0, Step: 5860, Batch(micro): 5860, Batch (considering grad accum): 732,  Loss: 5.7570, Time: 3.22s, Token/s: 158.87
Epoch: 0, Step: 5861, Batch(micro): 5861, Batch (considering grad accum): 732,  Loss: 6.2689, Time: 3.17s, Token/s: 161.64
Epoch: 0, Step: 5862, Batch(micro): 5862, Batch (considering grad accum): 732,  Loss: 6.5668, Time: 3.22s, Token/s: 159.09
Epoch: 0, Step: 5863, Batch(micro): 5863, Batch (considering grad accum): 732,  Loss: 6.0767, Time: 23.47s, Token/s: 21.81
Epoch: 0, Step: 5864, Batch(micro): 5864, Batch (considering grad accum): 733,  Loss: 6.0177, Time: 6.66s, Token/s: 76.92
Epoch: 0, Step: 5865, Batch(micro): 5865, Batch (considering grad accum): 733,  Loss: 6.0038, Time: 3.84s, Token/s: 133.19
Epoch: 0, Step: 5866, Batch(micro): 5866, Batch (considering grad accum): 733,  Loss: 5.5081, Time: 3.61s, Token/s: 141.85
Epoch: 0, Step: 5867, Batch(micro): 5867, Batch (considering grad accum): 733,  Loss: 5.2330, Time: 3.54s, Token/s: 144.62
Epoch: 0, Step: 5868, Batch(micro): 5868, Batch (considering grad accum): 733,  Loss: 6.7543, Time: 3.25s, Token/s: 157.53
Epoch: 0, Step: 5869, Batch(micro): 5869, Batch (considering grad accum): 733,  Loss: 6.5241, Time: 2.99s, Token/s: 171.01
Epoch: 0, Step: 5870, Batch(micro): 5870, Batch (considering grad accum): 733,  Loss: 5.9690, Time: 3.00s, Token/s: 170.76
Epoch: 0, Step: 5871, Batch(micro): 5871, Batch (considering grad accum): 733,  Loss: 6.1467, Time: 23.91s, Token/s: 21.42
Epoch: 0, Step: 5872, Batch(micro): 5872, Batch (considering grad accum): 734,  Loss: 6.2384, Time: 7.57s, Token/s: 67.65
Epoch: 0, Step: 5873, Batch(micro): 5873, Batch (considering grad accum): 734,  Loss: 6.4187, Time: 3.60s, Token/s: 142.04
Epoch: 0, Step: 5874, Batch(micro): 5874, Batch (considering grad accum): 734,  Loss: 7.2363, Time: 3.25s, Token/s: 157.70
Epoch: 0, Step: 5875, Batch(micro): 5875, Batch (considering grad accum): 734,  Loss: 6.1681, Time: 3.42s, Token/s: 149.80
Epoch: 0, Step: 5876, Batch(micro): 5876, Batch (considering grad accum): 734,  Loss: 6.1528, Time: 3.12s, Token/s: 164.27
Epoch: 0, Step: 5877, Batch(micro): 5877, Batch (considering grad accum): 734,  Loss: 6.5818, Time: 3.21s, Token/s: 159.62
Epoch: 0, Step: 5878, Batch(micro): 5878, Batch (considering grad accum): 734,  Loss: 6.2524, Time: 3.36s, Token/s: 152.39
Epoch: 0, Step: 5879, Batch(micro): 5879, Batch (considering grad accum): 734,  Loss: 6.6176, Time: 22.05s, Token/s: 23.22
Epoch: 0, Step: 5880, Batch(micro): 5880, Batch (considering grad accum): 735,  Loss: 6.3956, Time: 6.96s, Token/s: 73.55
Epoch: 0, Step: 5881, Batch(micro): 5881, Batch (considering grad accum): 735,  Loss: 5.5884, Time: 3.49s, Token/s: 146.73
Epoch: 0, Step: 5882, Batch(micro): 5882, Batch (considering grad accum): 735,  Loss: 5.9572, Time: 3.13s, Token/s: 163.55
Epoch: 0, Step: 5883, Batch(micro): 5883, Batch (considering grad accum): 735,  Loss: 5.8651, Time: 3.21s, Token/s: 159.55
Epoch: 0, Step: 5884, Batch(micro): 5884, Batch (considering grad accum): 735,  Loss: 5.7713, Time: 3.20s, Token/s: 159.82
Epoch: 0, Step: 5885, Batch(micro): 5885, Batch (considering grad accum): 735,  Loss: 6.2383, Time: 3.09s, Token/s: 165.49
Epoch: 0, Step: 5886, Batch(micro): 5886, Batch (considering grad accum): 735,  Loss: 6.4055, Time: 3.14s, Token/s: 163.07
Epoch: 0, Step: 5887, Batch(micro): 5887, Batch (considering grad accum): 735,  Loss: 6.4352, Time: 22.95s, Token/s: 22.31
Epoch: 0, Step: 5888, Batch(micro): 5888, Batch (considering grad accum): 736,  Loss: 6.7015, Time: 6.00s, Token/s: 85.29
Epoch: 0, Step: 5889, Batch(micro): 5889, Batch (considering grad accum): 736,  Loss: 6.3916, Time: 3.52s, Token/s: 145.58
Epoch: 0, Step: 5890, Batch(micro): 5890, Batch (considering grad accum): 736,  Loss: 7.4277, Time: 3.15s, Token/s: 162.66
Epoch: 0, Step: 5891, Batch(micro): 5891, Batch (considering grad accum): 736,  Loss: 7.4851, Time: 3.25s, Token/s: 157.45
Epoch: 0, Step: 5892, Batch(micro): 5892, Batch (considering grad accum): 736,  Loss: 6.8360, Time: 3.16s, Token/s: 161.88
Epoch: 0, Step: 5893, Batch(micro): 5893, Batch (considering grad accum): 736,  Loss: 6.4956, Time: 3.17s, Token/s: 161.63
Epoch: 0, Step: 5894, Batch(micro): 5894, Batch (considering grad accum): 736,  Loss: 7.2254, Time: 3.44s, Token/s: 148.90
Epoch: 0, Step: 5895, Batch(micro): 5895, Batch (considering grad accum): 736,  Loss: 6.8012, Time: 22.92s, Token/s: 22.34
Epoch: 0, Step: 5896, Batch(micro): 5896, Batch (considering grad accum): 737,  Loss: 6.2278, Time: 7.60s, Token/s: 67.36
Epoch: 0, Step: 5897, Batch(micro): 5897, Batch (considering grad accum): 737,  Loss: 6.0625, Time: 3.55s, Token/s: 144.40
Epoch: 0, Step: 5898, Batch(micro): 5898, Batch (considering grad accum): 737,  Loss: 6.1294, Time: 2.94s, Token/s: 174.42
Epoch: 0, Step: 5899, Batch(micro): 5899, Batch (considering grad accum): 737,  Loss: 5.9762, Time: 3.35s, Token/s: 152.66
Updating MLP bias
Epoch: 0, Step: 5900, Batch(micro): 5900, Batch (considering grad accum): 737,  Loss: 6.0190, Time: 3.34s, Token/s: 153.34
Epoch: 0, Step: 5901, Batch(micro): 5901, Batch (considering grad accum): 737,  Loss: 6.4278, Time: 3.24s, Token/s: 158.25
Epoch: 0, Step: 5902, Batch(micro): 5902, Batch (considering grad accum): 737,  Loss: 6.0182, Time: 3.29s, Token/s: 155.72
Epoch: 0, Step: 5903, Batch(micro): 5903, Batch (considering grad accum): 737,  Loss: 6.0682, Time: 23.96s, Token/s: 21.37
Epoch: 0, Step: 5904, Batch(micro): 5904, Batch (considering grad accum): 738,  Loss: 5.9826, Time: 7.30s, Token/s: 70.15
Epoch: 0, Step: 5905, Batch(micro): 5905, Batch (considering grad accum): 738,  Loss: 6.8915, Time: 3.66s, Token/s: 139.98
Epoch: 0, Step: 5906, Batch(micro): 5906, Batch (considering grad accum): 738,  Loss: 6.9992, Time: 4.05s, Token/s: 126.52
Epoch: 0, Step: 5907, Batch(micro): 5907, Batch (considering grad accum): 738,  Loss: 5.9786, Time: 3.50s, Token/s: 146.10
Epoch: 0, Step: 5908, Batch(micro): 5908, Batch (considering grad accum): 738,  Loss: 6.5622, Time: 3.19s, Token/s: 160.68
Epoch: 0, Step: 5909, Batch(micro): 5909, Batch (considering grad accum): 738,  Loss: 5.8594, Time: 3.73s, Token/s: 137.14
Epoch: 0, Step: 5910, Batch(micro): 5910, Batch (considering grad accum): 738,  Loss: 6.3083, Time: 3.14s, Token/s: 163.10
Epoch: 0, Step: 5911, Batch(micro): 5911, Batch (considering grad accum): 738,  Loss: 6.6049, Time: 25.40s, Token/s: 20.16
Epoch: 0, Step: 5912, Batch(micro): 5912, Batch (considering grad accum): 739,  Loss: 6.7650, Time: 7.78s, Token/s: 65.83
Epoch: 0, Step: 5913, Batch(micro): 5913, Batch (considering grad accum): 739,  Loss: 6.3417, Time: 3.95s, Token/s: 129.47
Epoch: 0, Step: 5914, Batch(micro): 5914, Batch (considering grad accum): 739,  Loss: 6.0056, Time: 3.38s, Token/s: 151.69
Epoch: 0, Step: 5915, Batch(micro): 5915, Batch (considering grad accum): 739,  Loss: 6.0261, Time: 3.31s, Token/s: 154.62
Epoch: 0, Step: 5916, Batch(micro): 5916, Batch (considering grad accum): 739,  Loss: 6.2064, Time: 3.92s, Token/s: 130.53
Epoch: 0, Step: 5917, Batch(micro): 5917, Batch (considering grad accum): 739,  Loss: 5.7854, Time: 2.96s, Token/s: 173.21
Epoch: 0, Step: 5918, Batch(micro): 5918, Batch (considering grad accum): 739,  Loss: 6.2581, Time: 3.10s, Token/s: 165.36
Epoch: 0, Step: 5919, Batch(micro): 5919, Batch (considering grad accum): 739,  Loss: 5.8466, Time: 24.11s, Token/s: 21.23
Epoch: 0, Step: 5920, Batch(micro): 5920, Batch (considering grad accum): 740,  Loss: 6.1702, Time: 7.52s, Token/s: 68.05
Epoch: 0, Step: 5921, Batch(micro): 5921, Batch (considering grad accum): 740,  Loss: 6.2946, Time: 3.92s, Token/s: 130.73
Epoch: 0, Step: 5922, Batch(micro): 5922, Batch (considering grad accum): 740,  Loss: 7.2234, Time: 3.80s, Token/s: 134.75
Epoch: 0, Step: 5923, Batch(micro): 5923, Batch (considering grad accum): 740,  Loss: 6.0969, Time: 3.38s, Token/s: 151.30
Epoch: 0, Step: 5924, Batch(micro): 5924, Batch (considering grad accum): 740,  Loss: 6.1469, Time: 3.14s, Token/s: 163.14
Epoch: 0, Step: 5925, Batch(micro): 5925, Batch (considering grad accum): 740,  Loss: 5.9885, Time: 3.33s, Token/s: 153.73
Epoch: 0, Step: 5926, Batch(micro): 5926, Batch (considering grad accum): 740,  Loss: 6.1268, Time: 3.41s, Token/s: 150.31
Epoch: 0, Step: 5927, Batch(micro): 5927, Batch (considering grad accum): 740,  Loss: 5.7705, Time: 23.46s, Token/s: 21.82
Epoch: 0, Step: 5928, Batch(micro): 5928, Batch (considering grad accum): 741,  Loss: 6.3706, Time: 7.76s, Token/s: 66.02
Epoch: 0, Step: 5929, Batch(micro): 5929, Batch (considering grad accum): 741,  Loss: 6.4148, Time: 3.97s, Token/s: 128.89
Epoch: 0, Step: 5930, Batch(micro): 5930, Batch (considering grad accum): 741,  Loss: 6.4690, Time: 3.42s, Token/s: 149.66
Epoch: 0, Step: 5931, Batch(micro): 5931, Batch (considering grad accum): 741,  Loss: 6.1399, Time: 3.02s, Token/s: 169.40
Epoch: 0, Step: 5932, Batch(micro): 5932, Batch (considering grad accum): 741,  Loss: 5.9235, Time: 3.28s, Token/s: 156.20
Epoch: 0, Step: 5933, Batch(micro): 5933, Batch (considering grad accum): 741,  Loss: 5.6708, Time: 3.27s, Token/s: 156.65
Epoch: 0, Step: 5934, Batch(micro): 5934, Batch (considering grad accum): 741,  Loss: 6.2800, Time: 4.35s, Token/s: 117.83
Epoch: 0, Step: 5935, Batch(micro): 5935, Batch (considering grad accum): 741,  Loss: 6.9395, Time: 23.29s, Token/s: 21.99
Epoch: 0, Step: 5936, Batch(micro): 5936, Batch (considering grad accum): 742,  Loss: 6.5716, Time: 6.94s, Token/s: 73.73
Epoch: 0, Step: 5937, Batch(micro): 5937, Batch (considering grad accum): 742,  Loss: 6.1681, Time: 4.44s, Token/s: 115.20
Epoch: 0, Step: 5938, Batch(micro): 5938, Batch (considering grad accum): 742,  Loss: 6.1580, Time: 3.18s, Token/s: 161.16
Epoch: 0, Step: 5939, Batch(micro): 5939, Batch (considering grad accum): 742,  Loss: 6.0077, Time: 3.62s, Token/s: 141.63
Epoch: 0, Step: 5940, Batch(micro): 5940, Batch (considering grad accum): 742,  Loss: 6.2143, Time: 3.24s, Token/s: 158.08
Epoch: 0, Step: 5941, Batch(micro): 5941, Batch (considering grad accum): 742,  Loss: 6.5802, Time: 3.64s, Token/s: 140.73
Epoch: 0, Step: 5942, Batch(micro): 5942, Batch (considering grad accum): 742,  Loss: 6.3791, Time: 3.36s, Token/s: 152.57
Epoch: 0, Step: 5943, Batch(micro): 5943, Batch (considering grad accum): 742,  Loss: 6.9256, Time: 19.36s, Token/s: 26.45
Epoch: 0, Step: 5944, Batch(micro): 5944, Batch (considering grad accum): 743,  Loss: 6.5958, Time: 6.65s, Token/s: 77.02
Epoch: 0, Step: 5945, Batch(micro): 5945, Batch (considering grad accum): 743,  Loss: 6.1217, Time: 4.11s, Token/s: 124.64
Epoch: 0, Step: 5946, Batch(micro): 5946, Batch (considering grad accum): 743,  Loss: 6.2734, Time: 3.43s, Token/s: 149.16
Epoch: 0, Step: 5947, Batch(micro): 5947, Batch (considering grad accum): 743,  Loss: 5.7225, Time: 3.30s, Token/s: 155.30
Epoch: 0, Step: 5948, Batch(micro): 5948, Batch (considering grad accum): 743,  Loss: 6.1484, Time: 3.45s, Token/s: 148.41
Epoch: 0, Step: 5949, Batch(micro): 5949, Batch (considering grad accum): 743,  Loss: 5.9880, Time: 3.69s, Token/s: 138.76
Epoch: 0, Step: 5950, Batch(micro): 5950, Batch (considering grad accum): 743,  Loss: 6.0696, Time: 3.53s, Token/s: 144.98
Epoch: 0, Step: 5951, Batch(micro): 5951, Batch (considering grad accum): 743,  Loss: 6.7619, Time: 19.95s, Token/s: 25.66
Epoch: 0, Step: 5952, Batch(micro): 5952, Batch (considering grad accum): 744,  Loss: 6.2137, Time: 6.84s, Token/s: 74.90
Epoch: 0, Step: 5953, Batch(micro): 5953, Batch (considering grad accum): 744,  Loss: 5.8488, Time: 3.98s, Token/s: 128.57
Epoch: 0, Step: 5954, Batch(micro): 5954, Batch (considering grad accum): 744,  Loss: 6.3883, Time: 3.89s, Token/s: 131.64
Epoch: 0, Step: 5955, Batch(micro): 5955, Batch (considering grad accum): 744,  Loss: 6.3984, Time: 3.22s, Token/s: 159.12
Epoch: 0, Step: 5956, Batch(micro): 5956, Batch (considering grad accum): 744,  Loss: 6.5271, Time: 3.38s, Token/s: 151.46
Epoch: 0, Step: 5957, Batch(micro): 5957, Batch (considering grad accum): 744,  Loss: 6.4336, Time: 3.20s, Token/s: 160.12
Epoch: 0, Step: 5958, Batch(micro): 5958, Batch (considering grad accum): 744,  Loss: 6.3492, Time: 3.69s, Token/s: 138.59
Epoch: 0, Step: 5959, Batch(micro): 5959, Batch (considering grad accum): 744,  Loss: 5.7398, Time: 20.21s, Token/s: 25.34
Epoch: 0, Step: 5960, Batch(micro): 5960, Batch (considering grad accum): 745,  Loss: 6.0860, Time: 7.16s, Token/s: 71.54
Epoch: 0, Step: 5961, Batch(micro): 5961, Batch (considering grad accum): 745,  Loss: 5.9739, Time: 4.26s, Token/s: 120.13
Epoch: 0, Step: 5962, Batch(micro): 5962, Batch (considering grad accum): 745,  Loss: 6.4636, Time: 3.27s, Token/s: 156.64
Epoch: 0, Step: 5963, Batch(micro): 5963, Batch (considering grad accum): 745,  Loss: 5.8596, Time: 3.54s, Token/s: 144.46
Epoch: 0, Step: 5964, Batch(micro): 5964, Batch (considering grad accum): 745,  Loss: 6.2288, Time: 3.70s, Token/s: 138.34
Epoch: 0, Step: 5965, Batch(micro): 5965, Batch (considering grad accum): 745,  Loss: 7.1631, Time: 3.50s, Token/s: 146.45
Epoch: 0, Step: 5966, Batch(micro): 5966, Batch (considering grad accum): 745,  Loss: 6.1564, Time: 3.67s, Token/s: 139.55
Epoch: 0, Step: 5967, Batch(micro): 5967, Batch (considering grad accum): 745,  Loss: 6.0832, Time: 20.05s, Token/s: 25.54
Epoch: 0, Step: 5968, Batch(micro): 5968, Batch (considering grad accum): 746,  Loss: 6.3405, Time: 7.23s, Token/s: 70.78
Epoch: 0, Step: 5969, Batch(micro): 5969, Batch (considering grad accum): 746,  Loss: 6.1445, Time: 3.80s, Token/s: 134.89
Epoch: 0, Step: 5970, Batch(micro): 5970, Batch (considering grad accum): 746,  Loss: 6.5861, Time: 3.43s, Token/s: 149.38
Epoch: 0, Step: 5971, Batch(micro): 5971, Batch (considering grad accum): 746,  Loss: 6.4782, Time: 3.51s, Token/s: 145.79
Epoch: 0, Step: 5972, Batch(micro): 5972, Batch (considering grad accum): 746,  Loss: 5.8228, Time: 3.54s, Token/s: 144.81
Epoch: 0, Step: 5973, Batch(micro): 5973, Batch (considering grad accum): 746,  Loss: 5.8276, Time: 3.45s, Token/s: 148.23
Epoch: 0, Step: 5974, Batch(micro): 5974, Batch (considering grad accum): 746,  Loss: 6.0187, Time: 3.46s, Token/s: 147.89
Epoch: 0, Step: 5975, Batch(micro): 5975, Batch (considering grad accum): 746,  Loss: 5.8933, Time: 19.28s, Token/s: 26.55
Epoch: 0, Step: 5976, Batch(micro): 5976, Batch (considering grad accum): 747,  Loss: 5.8140, Time: 6.53s, Token/s: 78.35
Epoch: 0, Step: 5977, Batch(micro): 5977, Batch (considering grad accum): 747,  Loss: 7.0168, Time: 4.24s, Token/s: 120.89
Epoch: 0, Step: 5978, Batch(micro): 5978, Batch (considering grad accum): 747,  Loss: 7.0865, Time: 3.63s, Token/s: 141.16
Epoch: 0, Step: 5979, Batch(micro): 5979, Batch (considering grad accum): 747,  Loss: 6.3968, Time: 3.66s, Token/s: 139.88
Epoch: 0, Step: 5980, Batch(micro): 5980, Batch (considering grad accum): 747,  Loss: 6.9173, Time: 3.57s, Token/s: 143.50
Epoch: 0, Step: 5981, Batch(micro): 5981, Batch (considering grad accum): 747,  Loss: 6.2716, Time: 3.48s, Token/s: 147.25
Epoch: 0, Step: 5982, Batch(micro): 5982, Batch (considering grad accum): 747,  Loss: 5.7347, Time: 3.26s, Token/s: 156.95
Epoch: 0, Step: 5983, Batch(micro): 5983, Batch (considering grad accum): 747,  Loss: 6.2225, Time: 18.53s, Token/s: 27.62
Epoch: 0, Step: 5984, Batch(micro): 5984, Batch (considering grad accum): 748,  Loss: 5.9207, Time: 6.44s, Token/s: 79.54
Epoch: 0, Step: 5985, Batch(micro): 5985, Batch (considering grad accum): 748,  Loss: 6.2700, Time: 3.61s, Token/s: 141.76
Epoch: 0, Step: 5986, Batch(micro): 5986, Batch (considering grad accum): 748,  Loss: 6.5338, Time: 3.18s, Token/s: 160.75
Epoch: 0, Step: 5987, Batch(micro): 5987, Batch (considering grad accum): 748,  Loss: 6.0540, Time: 3.92s, Token/s: 130.63
Epoch: 0, Step: 5988, Batch(micro): 5988, Batch (considering grad accum): 748,  Loss: 5.6826, Time: 3.83s, Token/s: 133.54
Epoch: 0, Step: 5989, Batch(micro): 5989, Batch (considering grad accum): 748,  Loss: 5.5208, Time: 3.68s, Token/s: 139.10
Epoch: 0, Step: 5990, Batch(micro): 5990, Batch (considering grad accum): 748,  Loss: 6.0082, Time: 3.62s, Token/s: 141.35
Epoch: 0, Step: 5991, Batch(micro): 5991, Batch (considering grad accum): 748,  Loss: 6.0780, Time: 19.42s, Token/s: 26.37
Epoch: 0, Step: 5992, Batch(micro): 5992, Batch (considering grad accum): 749,  Loss: 6.0021, Time: 9.01s, Token/s: 56.83
Epoch: 0, Step: 5993, Batch(micro): 5993, Batch (considering grad accum): 749,  Loss: 5.7623, Time: 3.34s, Token/s: 153.41
Epoch: 0, Step: 5994, Batch(micro): 5994, Batch (considering grad accum): 749,  Loss: 5.5994, Time: 3.21s, Token/s: 159.42
Epoch: 0, Step: 5995, Batch(micro): 5995, Batch (considering grad accum): 749,  Loss: 6.1686, Time: 3.23s, Token/s: 158.74
Epoch: 0, Step: 5996, Batch(micro): 5996, Batch (considering grad accum): 749,  Loss: 5.5900, Time: 4.01s, Token/s: 127.67
Epoch: 0, Step: 5997, Batch(micro): 5997, Batch (considering grad accum): 749,  Loss: 5.8368, Time: 3.53s, Token/s: 144.97
Epoch: 0, Step: 5998, Batch(micro): 5998, Batch (considering grad accum): 749,  Loss: 5.8416, Time: 3.87s, Token/s: 132.44
Epoch: 0, Step: 5999, Batch(micro): 5999, Batch (considering grad accum): 749,  Loss: 5.7513, Time: 23.23s, Token/s: 22.04
Updating MLP bias
Epoch: 0, Step: 6000, Batch(micro): 6000, Batch (considering grad accum): 750,  Loss: 5.9685, Time: 6.61s, Token/s: 77.49
Saved checkpoint at step 6000
What is Gravity?

In a part. It is a well in 10), though, it might seem for the concept? That's dive deep in the
Epoch: 0, Step: 6001, Batch(micro): 6001, Batch (considering grad accum): 750,  Loss: 6.0974, Time: 15.55s, Token/s: 32.92
Epoch: 0, Step: 6002, Batch(micro): 6002, Batch (considering grad accum): 750,  Loss: 6.4265, Time: 3.82s, Token/s: 134.17
Epoch: 0, Step: 6003, Batch(micro): 6003, Batch (considering grad accum): 750,  Loss: 5.7875, Time: 3.29s, Token/s: 155.44
Epoch: 0, Step: 6004, Batch(micro): 6004, Batch (considering grad accum): 750,  Loss: 5.8554, Time: 3.20s, Token/s: 160.11
Epoch: 0, Step: 6005, Batch(micro): 6005, Batch (considering grad accum): 750,  Loss: 5.8159, Time: 3.36s, Token/s: 152.52
Epoch: 0, Step: 6006, Batch(micro): 6006, Batch (considering grad accum): 750,  Loss: 5.9701, Time: 3.42s, Token/s: 149.65
Epoch: 0, Step: 6007, Batch(micro): 6007, Batch (considering grad accum): 750,  Loss: 6.1903, Time: 24.69s, Token/s: 20.74
Epoch: 0, Step: 6008, Batch(micro): 6008, Batch (considering grad accum): 751,  Loss: 6.2143, Time: 7.61s, Token/s: 67.30
Epoch: 0, Step: 6009, Batch(micro): 6009, Batch (considering grad accum): 751,  Loss: 6.4391, Time: 3.96s, Token/s: 129.43
Epoch: 0, Step: 6010, Batch(micro): 6010, Batch (considering grad accum): 751,  Loss: 6.1288, Time: 3.70s, Token/s: 138.37
Epoch: 0, Step: 6011, Batch(micro): 6011, Batch (considering grad accum): 751,  Loss: 6.3523, Time: 3.55s, Token/s: 144.33
Epoch: 0, Step: 6012, Batch(micro): 6012, Batch (considering grad accum): 751,  Loss: 7.2669, Time: 3.41s, Token/s: 150.29
Epoch: 0, Step: 6013, Batch(micro): 6013, Batch (considering grad accum): 751,  Loss: 6.2544, Time: 3.55s, Token/s: 144.18
Epoch: 0, Step: 6014, Batch(micro): 6014, Batch (considering grad accum): 751,  Loss: 6.0880, Time: 3.55s, Token/s: 144.06
Epoch: 0, Step: 6015, Batch(micro): 6015, Batch (considering grad accum): 751,  Loss: 5.8427, Time: 24.72s, Token/s: 20.71
Epoch: 0, Step: 6016, Batch(micro): 6016, Batch (considering grad accum): 752,  Loss: 6.5337, Time: 6.89s, Token/s: 74.34
Epoch: 0, Step: 6017, Batch(micro): 6017, Batch (considering grad accum): 752,  Loss: 5.7383, Time: 3.71s, Token/s: 137.94
Epoch: 0, Step: 6018, Batch(micro): 6018, Batch (considering grad accum): 752,  Loss: 5.9424, Time: 3.49s, Token/s: 146.58
Epoch: 0, Step: 6019, Batch(micro): 6019, Batch (considering grad accum): 752,  Loss: 5.5214, Time: 3.34s, Token/s: 153.24
Epoch: 0, Step: 6020, Batch(micro): 6020, Batch (considering grad accum): 752,  Loss: 5.9727, Time: 3.38s, Token/s: 151.37
Epoch: 0, Step: 6021, Batch(micro): 6021, Batch (considering grad accum): 752,  Loss: 5.8493, Time: 3.41s, Token/s: 150.31
Epoch: 0, Step: 6022, Batch(micro): 6022, Batch (considering grad accum): 752,  Loss: 5.2642, Time: 3.43s, Token/s: 149.38
Epoch: 0, Step: 6023, Batch(micro): 6023, Batch (considering grad accum): 752,  Loss: 6.1137, Time: 23.38s, Token/s: 21.90
Epoch: 0, Step: 6024, Batch(micro): 6024, Batch (considering grad accum): 753,  Loss: 6.2800, Time: 7.16s, Token/s: 71.52
Epoch: 0, Step: 6025, Batch(micro): 6025, Batch (considering grad accum): 753,  Loss: 6.6338, Time: 4.09s, Token/s: 125.15
Epoch: 0, Step: 6026, Batch(micro): 6026, Batch (considering grad accum): 753,  Loss: 6.3300, Time: 3.86s, Token/s: 132.77
Epoch: 0, Step: 6027, Batch(micro): 6027, Batch (considering grad accum): 753,  Loss: 6.2019, Time: 3.62s, Token/s: 141.33
Epoch: 0, Step: 6028, Batch(micro): 6028, Batch (considering grad accum): 753,  Loss: 5.8966, Time: 3.47s, Token/s: 147.36
Epoch: 0, Step: 6029, Batch(micro): 6029, Batch (considering grad accum): 753,  Loss: 6.4349, Time: 3.86s, Token/s: 132.52
Epoch: 0, Step: 6030, Batch(micro): 6030, Batch (considering grad accum): 753,  Loss: 6.2539, Time: 3.69s, Token/s: 138.88
Epoch: 0, Step: 6031, Batch(micro): 6031, Batch (considering grad accum): 753,  Loss: 6.2136, Time: 22.96s, Token/s: 22.30
Epoch: 0, Step: 6032, Batch(micro): 6032, Batch (considering grad accum): 754,  Loss: 6.3722, Time: 7.79s, Token/s: 65.72
Epoch: 0, Step: 6033, Batch(micro): 6033, Batch (considering grad accum): 754,  Loss: 5.9333, Time: 3.82s, Token/s: 133.90
Epoch: 0, Step: 6034, Batch(micro): 6034, Batch (considering grad accum): 754,  Loss: 6.7176, Time: 3.18s, Token/s: 161.01
Epoch: 0, Step: 6035, Batch(micro): 6035, Batch (considering grad accum): 754,  Loss: 6.4339, Time: 4.03s, Token/s: 127.14
Epoch: 0, Step: 6036, Batch(micro): 6036, Batch (considering grad accum): 754,  Loss: 6.3508, Time: 3.82s, Token/s: 134.10
Epoch: 0, Step: 6037, Batch(micro): 6037, Batch (considering grad accum): 754,  Loss: 6.0052, Time: 3.74s, Token/s: 136.89
Epoch: 0, Step: 6038, Batch(micro): 6038, Batch (considering grad accum): 754,  Loss: 6.2043, Time: 3.46s, Token/s: 147.77
Epoch: 0, Step: 6039, Batch(micro): 6039, Batch (considering grad accum): 754,  Loss: 6.1743, Time: 23.00s, Token/s: 22.26
Epoch: 0, Step: 6040, Batch(micro): 6040, Batch (considering grad accum): 755,  Loss: 5.9334, Time: 7.33s, Token/s: 69.83
Epoch: 0, Step: 6041, Batch(micro): 6041, Batch (considering grad accum): 755,  Loss: 6.3280, Time: 3.86s, Token/s: 132.70
Epoch: 0, Step: 6042, Batch(micro): 6042, Batch (considering grad accum): 755,  Loss: 6.3516, Time: 3.55s, Token/s: 144.12
Epoch: 0, Step: 6043, Batch(micro): 6043, Batch (considering grad accum): 755,  Loss: 6.7144, Time: 3.73s, Token/s: 137.45
Epoch: 0, Step: 6044, Batch(micro): 6044, Batch (considering grad accum): 755,  Loss: 6.4202, Time: 3.59s, Token/s: 142.53
Epoch: 0, Step: 6045, Batch(micro): 6045, Batch (considering grad accum): 755,  Loss: 6.3189, Time: 3.73s, Token/s: 137.41
Epoch: 0, Step: 6046, Batch(micro): 6046, Batch (considering grad accum): 755,  Loss: 6.4060, Time: 3.80s, Token/s: 134.61
Epoch: 0, Step: 6047, Batch(micro): 6047, Batch (considering grad accum): 755,  Loss: 6.3335, Time: 22.35s, Token/s: 22.91
Epoch: 0, Step: 6048, Batch(micro): 6048, Batch (considering grad accum): 756,  Loss: 6.3855, Time: 5.79s, Token/s: 88.47
Epoch: 0, Step: 6049, Batch(micro): 6049, Batch (considering grad accum): 756,  Loss: 6.4009, Time: 4.08s, Token/s: 125.45
Epoch: 0, Step: 6050, Batch(micro): 6050, Batch (considering grad accum): 756,  Loss: 6.4058, Time: 3.93s, Token/s: 130.23
Epoch: 0, Step: 6051, Batch(micro): 6051, Batch (considering grad accum): 756,  Loss: 6.6491, Time: 3.48s, Token/s: 147.06
Epoch: 0, Step: 6052, Batch(micro): 6052, Batch (considering grad accum): 756,  Loss: 6.6242, Time: 3.71s, Token/s: 138.18
Epoch: 0, Step: 6053, Batch(micro): 6053, Batch (considering grad accum): 756,  Loss: 5.5530, Time: 3.49s, Token/s: 146.82
Epoch: 0, Step: 6054, Batch(micro): 6054, Batch (considering grad accum): 756,  Loss: 6.0534, Time: 3.63s, Token/s: 141.00
Epoch: 0, Step: 6055, Batch(micro): 6055, Batch (considering grad accum): 756,  Loss: 6.2589, Time: 25.72s, Token/s: 19.91
Epoch: 0, Step: 6056, Batch(micro): 6056, Batch (considering grad accum): 757,  Loss: 6.3176, Time: 7.40s, Token/s: 69.18
Epoch: 0, Step: 6057, Batch(micro): 6057, Batch (considering grad accum): 757,  Loss: 6.1984, Time: 4.02s, Token/s: 127.33
Epoch: 0, Step: 6058, Batch(micro): 6058, Batch (considering grad accum): 757,  Loss: 6.5809, Time: 3.57s, Token/s: 143.29
Epoch: 0, Step: 6059, Batch(micro): 6059, Batch (considering grad accum): 757,  Loss: 5.1804, Time: 3.59s, Token/s: 142.46
Epoch: 0, Step: 6060, Batch(micro): 6060, Batch (considering grad accum): 757,  Loss: 7.0140, Time: 4.34s, Token/s: 117.97
Epoch: 0, Step: 6061, Batch(micro): 6061, Batch (considering grad accum): 757,  Loss: 7.5968, Time: 3.37s, Token/s: 151.84
Epoch: 0, Step: 6062, Batch(micro): 6062, Batch (considering grad accum): 757,  Loss: 6.6338, Time: 3.45s, Token/s: 148.22
Epoch: 0, Step: 6063, Batch(micro): 6063, Batch (considering grad accum): 757,  Loss: 6.1604, Time: 24.29s, Token/s: 21.08
Epoch: 0, Step: 6064, Batch(micro): 6064, Batch (considering grad accum): 758,  Loss: 6.3888, Time: 7.47s, Token/s: 68.54
Epoch: 0, Step: 6065, Batch(micro): 6065, Batch (considering grad accum): 758,  Loss: 6.7370, Time: 3.80s, Token/s: 134.71
Epoch: 0, Step: 6066, Batch(micro): 6066, Batch (considering grad accum): 758,  Loss: 6.5541, Time: 3.48s, Token/s: 147.00
Epoch: 0, Step: 6067, Batch(micro): 6067, Batch (considering grad accum): 758,  Loss: 5.6560, Time: 3.34s, Token/s: 153.30
Epoch: 0, Step: 6068, Batch(micro): 6068, Batch (considering grad accum): 758,  Loss: 6.9763, Time: 3.35s, Token/s: 152.73
Epoch: 0, Step: 6069, Batch(micro): 6069, Batch (considering grad accum): 758,  Loss: 6.0770, Time: 3.95s, Token/s: 129.63
Epoch: 0, Step: 6070, Batch(micro): 6070, Batch (considering grad accum): 758,  Loss: 6.0435, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 6071, Batch(micro): 6071, Batch (considering grad accum): 758,  Loss: 6.7775, Time: 27.53s, Token/s: 18.60
Epoch: 0, Step: 6072, Batch(micro): 6072, Batch (considering grad accum): 759,  Loss: 5.1252, Time: 8.24s, Token/s: 62.16
Epoch: 0, Step: 6073, Batch(micro): 6073, Batch (considering grad accum): 759,  Loss: 6.5297, Time: 3.81s, Token/s: 134.47
Epoch: 0, Step: 6074, Batch(micro): 6074, Batch (considering grad accum): 759,  Loss: 6.4657, Time: 3.43s, Token/s: 149.10
Epoch: 0, Step: 6075, Batch(micro): 6075, Batch (considering grad accum): 759,  Loss: 6.2026, Time: 3.36s, Token/s: 152.51
Epoch: 0, Step: 6076, Batch(micro): 6076, Batch (considering grad accum): 759,  Loss: 6.5356, Time: 3.28s, Token/s: 155.96
Epoch: 0, Step: 6077, Batch(micro): 6077, Batch (considering grad accum): 759,  Loss: 7.8269, Time: 3.30s, Token/s: 155.20
Epoch: 0, Step: 6078, Batch(micro): 6078, Batch (considering grad accum): 759,  Loss: 6.3738, Time: 3.35s, Token/s: 153.06
Epoch: 0, Step: 6079, Batch(micro): 6079, Batch (considering grad accum): 759,  Loss: 6.1468, Time: 24.49s, Token/s: 20.90
Epoch: 0, Step: 6080, Batch(micro): 6080, Batch (considering grad accum): 760,  Loss: 6.0908, Time: 6.51s, Token/s: 78.64
Epoch: 0, Step: 6081, Batch(micro): 6081, Batch (considering grad accum): 760,  Loss: 5.6522, Time: 3.74s, Token/s: 137.07
Epoch: 0, Step: 6082, Batch(micro): 6082, Batch (considering grad accum): 760,  Loss: 6.1292, Time: 3.17s, Token/s: 161.51
Epoch: 0, Step: 6083, Batch(micro): 6083, Batch (considering grad accum): 760,  Loss: 6.4448, Time: 3.21s, Token/s: 159.31
Epoch: 0, Step: 6084, Batch(micro): 6084, Batch (considering grad accum): 760,  Loss: 5.4583, Time: 3.18s, Token/s: 161.12
Epoch: 0, Step: 6085, Batch(micro): 6085, Batch (considering grad accum): 760,  Loss: 5.6761, Time: 3.21s, Token/s: 159.33
Epoch: 0, Step: 6086, Batch(micro): 6086, Batch (considering grad accum): 760,  Loss: 6.4617, Time: 3.53s, Token/s: 145.07
Epoch: 0, Step: 6087, Batch(micro): 6087, Batch (considering grad accum): 760,  Loss: 6.4387, Time: 21.00s, Token/s: 24.38
Epoch: 0, Step: 6088, Batch(micro): 6088, Batch (considering grad accum): 761,  Loss: 5.5537, Time: 8.58s, Token/s: 59.69
Epoch: 0, Step: 6089, Batch(micro): 6089, Batch (considering grad accum): 761,  Loss: 5.5954, Time: 4.27s, Token/s: 119.80
Epoch: 0, Step: 6090, Batch(micro): 6090, Batch (considering grad accum): 761,  Loss: 5.2776, Time: 3.32s, Token/s: 154.14
Epoch: 0, Step: 6091, Batch(micro): 6091, Batch (considering grad accum): 761,  Loss: 5.4841, Time: 3.17s, Token/s: 161.56
Epoch: 0, Step: 6092, Batch(micro): 6092, Batch (considering grad accum): 761,  Loss: 5.8699, Time: 3.13s, Token/s: 163.51
Epoch: 0, Step: 6093, Batch(micro): 6093, Batch (considering grad accum): 761,  Loss: 6.1518, Time: 3.17s, Token/s: 161.33
Epoch: 0, Step: 6094, Batch(micro): 6094, Batch (considering grad accum): 761,  Loss: 8.6909, Time: 3.18s, Token/s: 160.88
Epoch: 0, Step: 6095, Batch(micro): 6095, Batch (considering grad accum): 761,  Loss: 6.8907, Time: 22.08s, Token/s: 23.19
Epoch: 0, Step: 6096, Batch(micro): 6096, Batch (considering grad accum): 762,  Loss: 6.6434, Time: 6.96s, Token/s: 73.60
Epoch: 0, Step: 6097, Batch(micro): 6097, Batch (considering grad accum): 762,  Loss: 6.2196, Time: 3.81s, Token/s: 134.27
Epoch: 0, Step: 6098, Batch(micro): 6098, Batch (considering grad accum): 762,  Loss: 6.3945, Time: 3.32s, Token/s: 154.09
Epoch: 0, Step: 6099, Batch(micro): 6099, Batch (considering grad accum): 762,  Loss: 5.5552, Time: 3.37s, Token/s: 152.08
Updating MLP bias
Epoch: 0, Step: 6100, Batch(micro): 6100, Batch (considering grad accum): 762,  Loss: 5.9876, Time: 3.45s, Token/s: 148.51
Epoch: 0, Step: 6101, Batch(micro): 6101, Batch (considering grad accum): 762,  Loss: 6.8448, Time: 3.65s, Token/s: 140.32
Epoch: 0, Step: 6102, Batch(micro): 6102, Batch (considering grad accum): 762,  Loss: 6.2926, Time: 3.45s, Token/s: 148.38
Epoch: 0, Step: 6103, Batch(micro): 6103, Batch (considering grad accum): 762,  Loss: 6.1012, Time: 24.74s, Token/s: 20.69
Epoch: 0, Step: 6104, Batch(micro): 6104, Batch (considering grad accum): 763,  Loss: 6.5698, Time: 8.16s, Token/s: 62.71
Epoch: 0, Step: 6105, Batch(micro): 6105, Batch (considering grad accum): 763,  Loss: 6.8052, Time: 4.00s, Token/s: 127.88
Epoch: 0, Step: 6106, Batch(micro): 6106, Batch (considering grad accum): 763,  Loss: 6.0978, Time: 3.50s, Token/s: 146.48
Epoch: 0, Step: 6107, Batch(micro): 6107, Batch (considering grad accum): 763,  Loss: 6.2078, Time: 3.55s, Token/s: 144.35
Epoch: 0, Step: 6108, Batch(micro): 6108, Batch (considering grad accum): 763,  Loss: 6.3573, Time: 3.54s, Token/s: 144.73
Epoch: 0, Step: 6109, Batch(micro): 6109, Batch (considering grad accum): 763,  Loss: 6.0927, Time: 3.32s, Token/s: 154.24
Epoch: 0, Step: 6110, Batch(micro): 6110, Batch (considering grad accum): 763,  Loss: 5.9485, Time: 3.47s, Token/s: 147.62
Epoch: 0, Step: 6111, Batch(micro): 6111, Batch (considering grad accum): 763,  Loss: 5.8552, Time: 23.23s, Token/s: 22.04
Epoch: 0, Step: 6112, Batch(micro): 6112, Batch (considering grad accum): 764,  Loss: 6.3206, Time: 7.28s, Token/s: 70.32
Epoch: 0, Step: 6113, Batch(micro): 6113, Batch (considering grad accum): 764,  Loss: 6.3010, Time: 3.74s, Token/s: 136.99
Epoch: 0, Step: 6114, Batch(micro): 6114, Batch (considering grad accum): 764,  Loss: 6.8150, Time: 3.55s, Token/s: 144.04
Epoch: 0, Step: 6115, Batch(micro): 6115, Batch (considering grad accum): 764,  Loss: 6.5108, Time: 3.24s, Token/s: 158.26
Epoch: 0, Step: 6116, Batch(micro): 6116, Batch (considering grad accum): 764,  Loss: 6.6175, Time: 3.19s, Token/s: 160.63
Epoch: 0, Step: 6117, Batch(micro): 6117, Batch (considering grad accum): 764,  Loss: 6.4691, Time: 3.33s, Token/s: 153.69
Epoch: 0, Step: 6118, Batch(micro): 6118, Batch (considering grad accum): 764,  Loss: 6.3601, Time: 3.01s, Token/s: 169.89
Epoch: 0, Step: 6119, Batch(micro): 6119, Batch (considering grad accum): 764,  Loss: 6.1370, Time: 18.53s, Token/s: 27.64
Epoch: 0, Step: 6120, Batch(micro): 6120, Batch (considering grad accum): 765,  Loss: 6.4941, Time: 5.93s, Token/s: 86.33
Epoch: 0, Step: 6121, Batch(micro): 6121, Batch (considering grad accum): 765,  Loss: 5.6267, Time: 3.71s, Token/s: 137.87
Epoch: 0, Step: 6122, Batch(micro): 6122, Batch (considering grad accum): 765,  Loss: 5.8823, Time: 3.32s, Token/s: 154.06
Epoch: 0, Step: 6123, Batch(micro): 6123, Batch (considering grad accum): 765,  Loss: 6.1414, Time: 3.66s, Token/s: 140.01
Epoch: 0, Step: 6124, Batch(micro): 6124, Batch (considering grad accum): 765,  Loss: 6.3027, Time: 3.57s, Token/s: 143.24
Epoch: 0, Step: 6125, Batch(micro): 6125, Batch (considering grad accum): 765,  Loss: 5.6594, Time: 3.56s, Token/s: 143.78
Epoch: 0, Step: 6126, Batch(micro): 6126, Batch (considering grad accum): 765,  Loss: 7.0164, Time: 3.99s, Token/s: 128.30
Epoch: 0, Step: 6127, Batch(micro): 6127, Batch (considering grad accum): 765,  Loss: 6.1533, Time: 18.36s, Token/s: 27.89
Epoch: 0, Step: 6128, Batch(micro): 6128, Batch (considering grad accum): 766,  Loss: 6.4350, Time: 9.53s, Token/s: 53.75
Epoch: 0, Step: 6129, Batch(micro): 6129, Batch (considering grad accum): 766,  Loss: 6.1047, Time: 3.87s, Token/s: 132.41
Epoch: 0, Step: 6130, Batch(micro): 6130, Batch (considering grad accum): 766,  Loss: 5.9135, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 6131, Batch(micro): 6131, Batch (considering grad accum): 766,  Loss: 5.9018, Time: 3.65s, Token/s: 140.38
Epoch: 0, Step: 6132, Batch(micro): 6132, Batch (considering grad accum): 766,  Loss: 5.7144, Time: 3.31s, Token/s: 154.52
Epoch: 0, Step: 6133, Batch(micro): 6133, Batch (considering grad accum): 766,  Loss: 6.8006, Time: 3.21s, Token/s: 159.46
Epoch: 0, Step: 6134, Batch(micro): 6134, Batch (considering grad accum): 766,  Loss: 7.1473, Time: 3.31s, Token/s: 154.46
Epoch: 0, Step: 6135, Batch(micro): 6135, Batch (considering grad accum): 766,  Loss: 6.9077, Time: 18.85s, Token/s: 27.17
Epoch: 0, Step: 6136, Batch(micro): 6136, Batch (considering grad accum): 767,  Loss: 6.0230, Time: 6.22s, Token/s: 82.28
Epoch: 0, Step: 6137, Batch(micro): 6137, Batch (considering grad accum): 767,  Loss: 6.3962, Time: 3.89s, Token/s: 131.52
Epoch: 0, Step: 6138, Batch(micro): 6138, Batch (considering grad accum): 767,  Loss: 6.2709, Time: 3.42s, Token/s: 149.86
Epoch: 0, Step: 6139, Batch(micro): 6139, Batch (considering grad accum): 767,  Loss: 6.3404, Time: 3.29s, Token/s: 155.83
Epoch: 0, Step: 6140, Batch(micro): 6140, Batch (considering grad accum): 767,  Loss: 5.8919, Time: 3.45s, Token/s: 148.36
Epoch: 0, Step: 6141, Batch(micro): 6141, Batch (considering grad accum): 767,  Loss: 6.3955, Time: 3.32s, Token/s: 154.04
Epoch: 0, Step: 6142, Batch(micro): 6142, Batch (considering grad accum): 767,  Loss: 6.2842, Time: 3.16s, Token/s: 161.87
Epoch: 0, Step: 6143, Batch(micro): 6143, Batch (considering grad accum): 767,  Loss: 5.7212, Time: 17.58s, Token/s: 29.12
Epoch: 0, Step: 6144, Batch(micro): 6144, Batch (considering grad accum): 768,  Loss: 6.8514, Time: 5.79s, Token/s: 88.48
Epoch: 0, Step: 6145, Batch(micro): 6145, Batch (considering grad accum): 768,  Loss: 6.4246, Time: 3.83s, Token/s: 133.81
Epoch: 0, Step: 6146, Batch(micro): 6146, Batch (considering grad accum): 768,  Loss: 7.0934, Time: 3.63s, Token/s: 140.92
Epoch: 0, Step: 6147, Batch(micro): 6147, Batch (considering grad accum): 768,  Loss: 6.1594, Time: 3.76s, Token/s: 136.27
Epoch: 0, Step: 6148, Batch(micro): 6148, Batch (considering grad accum): 768,  Loss: 5.7629, Time: 3.84s, Token/s: 133.28
Epoch: 0, Step: 6149, Batch(micro): 6149, Batch (considering grad accum): 768,  Loss: 5.9718, Time: 3.20s, Token/s: 159.81
Epoch: 0, Step: 6150, Batch(micro): 6150, Batch (considering grad accum): 768,  Loss: 6.2392, Time: 3.41s, Token/s: 150.10
Epoch: 0, Step: 6151, Batch(micro): 6151, Batch (considering grad accum): 768,  Loss: 6.4148, Time: 18.93s, Token/s: 27.05
Epoch: 0, Step: 6152, Batch(micro): 6152, Batch (considering grad accum): 769,  Loss: 6.1052, Time: 6.27s, Token/s: 81.63
Epoch: 0, Step: 6153, Batch(micro): 6153, Batch (considering grad accum): 769,  Loss: 5.9792, Time: 4.08s, Token/s: 125.54
Epoch: 0, Step: 6154, Batch(micro): 6154, Batch (considering grad accum): 769,  Loss: 6.2836, Time: 3.91s, Token/s: 130.87
Epoch: 0, Step: 6155, Batch(micro): 6155, Batch (considering grad accum): 769,  Loss: 6.2287, Time: 3.47s, Token/s: 147.52
Epoch: 0, Step: 6156, Batch(micro): 6156, Batch (considering grad accum): 769,  Loss: 6.0158, Time: 3.79s, Token/s: 135.13
Epoch: 0, Step: 6157, Batch(micro): 6157, Batch (considering grad accum): 769,  Loss: 5.5006, Time: 3.38s, Token/s: 151.68
Epoch: 0, Step: 6158, Batch(micro): 6158, Batch (considering grad accum): 769,  Loss: 5.9306, Time: 3.46s, Token/s: 147.88
Epoch: 0, Step: 6159, Batch(micro): 6159, Batch (considering grad accum): 769,  Loss: 6.8362, Time: 18.42s, Token/s: 27.79
Epoch: 0, Step: 6160, Batch(micro): 6160, Batch (considering grad accum): 770,  Loss: 7.0293, Time: 5.70s, Token/s: 89.80
Epoch: 0, Step: 6161, Batch(micro): 6161, Batch (considering grad accum): 770,  Loss: 6.4469, Time: 4.01s, Token/s: 127.65
Epoch: 0, Step: 6162, Batch(micro): 6162, Batch (considering grad accum): 770,  Loss: 6.3775, Time: 3.28s, Token/s: 155.90
Epoch: 0, Step: 6163, Batch(micro): 6163, Batch (considering grad accum): 770,  Loss: 6.4395, Time: 3.48s, Token/s: 147.25
Epoch: 0, Step: 6164, Batch(micro): 6164, Batch (considering grad accum): 770,  Loss: 5.9871, Time: 3.36s, Token/s: 152.44
Epoch: 0, Step: 6165, Batch(micro): 6165, Batch (considering grad accum): 770,  Loss: 5.9747, Time: 3.73s, Token/s: 137.17
Epoch: 0, Step: 6166, Batch(micro): 6166, Batch (considering grad accum): 770,  Loss: 6.8445, Time: 3.76s, Token/s: 136.01
Epoch: 0, Step: 6167, Batch(micro): 6167, Batch (considering grad accum): 770,  Loss: 6.5602, Time: 19.89s, Token/s: 25.74
Epoch: 0, Step: 6168, Batch(micro): 6168, Batch (considering grad accum): 771,  Loss: 6.2100, Time: 6.71s, Token/s: 76.35
Epoch: 0, Step: 6169, Batch(micro): 6169, Batch (considering grad accum): 771,  Loss: 5.6427, Time: 3.76s, Token/s: 136.04
Epoch: 0, Step: 6170, Batch(micro): 6170, Batch (considering grad accum): 771,  Loss: 5.9682, Time: 3.63s, Token/s: 141.01
Epoch: 0, Step: 6171, Batch(micro): 6171, Batch (considering grad accum): 771,  Loss: 6.9726, Time: 3.51s, Token/s: 145.96
Epoch: 0, Step: 6172, Batch(micro): 6172, Batch (considering grad accum): 771,  Loss: 7.4327, Time: 3.29s, Token/s: 155.43
Epoch: 0, Step: 6173, Batch(micro): 6173, Batch (considering grad accum): 771,  Loss: 7.4624, Time: 3.19s, Token/s: 160.28
Epoch: 0, Step: 6174, Batch(micro): 6174, Batch (considering grad accum): 771,  Loss: 5.8724, Time: 3.14s, Token/s: 162.94
Epoch: 0, Step: 6175, Batch(micro): 6175, Batch (considering grad accum): 771,  Loss: 6.1425, Time: 18.87s, Token/s: 27.14
Epoch: 0, Step: 6176, Batch(micro): 6176, Batch (considering grad accum): 772,  Loss: 6.9508, Time: 6.89s, Token/s: 74.29
Epoch: 0, Step: 6177, Batch(micro): 6177, Batch (considering grad accum): 772,  Loss: 6.0916, Time: 3.76s, Token/s: 136.04
Epoch: 0, Step: 6178, Batch(micro): 6178, Batch (considering grad accum): 772,  Loss: 5.9381, Time: 3.34s, Token/s: 153.20
Epoch: 0, Step: 6179, Batch(micro): 6179, Batch (considering grad accum): 772,  Loss: 5.6726, Time: 3.36s, Token/s: 152.22
Epoch: 0, Step: 6180, Batch(micro): 6180, Batch (considering grad accum): 772,  Loss: 6.1137, Time: 3.46s, Token/s: 148.12
Epoch: 0, Step: 6181, Batch(micro): 6181, Batch (considering grad accum): 772,  Loss: 6.8956, Time: 3.71s, Token/s: 138.04
Epoch: 0, Step: 6182, Batch(micro): 6182, Batch (considering grad accum): 772,  Loss: 6.3911, Time: 3.57s, Token/s: 143.61
Epoch: 0, Step: 6183, Batch(micro): 6183, Batch (considering grad accum): 772,  Loss: 6.0478, Time: 19.89s, Token/s: 25.75
Epoch: 0, Step: 6184, Batch(micro): 6184, Batch (considering grad accum): 773,  Loss: 6.6148, Time: 7.35s, Token/s: 69.66
Epoch: 0, Step: 6185, Batch(micro): 6185, Batch (considering grad accum): 773,  Loss: 6.2681, Time: 3.60s, Token/s: 142.25
Epoch: 0, Step: 6186, Batch(micro): 6186, Batch (considering grad accum): 773,  Loss: 7.0426, Time: 3.36s, Token/s: 152.47
Epoch: 0, Step: 6187, Batch(micro): 6187, Batch (considering grad accum): 773,  Loss: 5.9776, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 6188, Batch(micro): 6188, Batch (considering grad accum): 773,  Loss: 6.6840, Time: 3.60s, Token/s: 142.40
Epoch: 0, Step: 6189, Batch(micro): 6189, Batch (considering grad accum): 773,  Loss: 6.0064, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 6190, Batch(micro): 6190, Batch (considering grad accum): 773,  Loss: 6.3542, Time: 3.53s, Token/s: 145.12
Epoch: 0, Step: 6191, Batch(micro): 6191, Batch (considering grad accum): 773,  Loss: 5.6609, Time: 18.64s, Token/s: 27.47
Epoch: 0, Step: 6192, Batch(micro): 6192, Batch (considering grad accum): 774,  Loss: 6.1770, Time: 6.25s, Token/s: 81.97
Epoch: 0, Step: 6193, Batch(micro): 6193, Batch (considering grad accum): 774,  Loss: 6.7915, Time: 4.12s, Token/s: 124.39
Epoch: 0, Step: 6194, Batch(micro): 6194, Batch (considering grad accum): 774,  Loss: 6.3941, Time: 3.71s, Token/s: 138.09
Epoch: 0, Step: 6195, Batch(micro): 6195, Batch (considering grad accum): 774,  Loss: 6.8859, Time: 3.50s, Token/s: 146.37
Epoch: 0, Step: 6196, Batch(micro): 6196, Batch (considering grad accum): 774,  Loss: 6.5717, Time: 3.50s, Token/s: 146.40
Epoch: 0, Step: 6197, Batch(micro): 6197, Batch (considering grad accum): 774,  Loss: 6.1673, Time: 3.43s, Token/s: 149.14
Epoch: 0, Step: 6198, Batch(micro): 6198, Batch (considering grad accum): 774,  Loss: 6.5728, Time: 3.17s, Token/s: 161.39
Epoch: 0, Step: 6199, Batch(micro): 6199, Batch (considering grad accum): 774,  Loss: 5.4372, Time: 18.19s, Token/s: 28.15
Updating MLP bias
Epoch: 0, Step: 6200, Batch(micro): 6200, Batch (considering grad accum): 775,  Loss: 5.6000, Time: 6.07s, Token/s: 84.31
Epoch: 0, Step: 6201, Batch(micro): 6201, Batch (considering grad accum): 775,  Loss: 6.2899, Time: 3.87s, Token/s: 132.27
Epoch: 0, Step: 6202, Batch(micro): 6202, Batch (considering grad accum): 775,  Loss: 5.9360, Time: 3.38s, Token/s: 151.43
Epoch: 0, Step: 6203, Batch(micro): 6203, Batch (considering grad accum): 775,  Loss: 6.0271, Time: 3.48s, Token/s: 147.11
Epoch: 0, Step: 6204, Batch(micro): 6204, Batch (considering grad accum): 775,  Loss: 6.2772, Time: 3.74s, Token/s: 136.91
Epoch: 0, Step: 6205, Batch(micro): 6205, Batch (considering grad accum): 775,  Loss: 6.0272, Time: 3.91s, Token/s: 130.84
Epoch: 0, Step: 6206, Batch(micro): 6206, Batch (considering grad accum): 775,  Loss: 6.1299, Time: 3.52s, Token/s: 145.50
Epoch: 0, Step: 6207, Batch(micro): 6207, Batch (considering grad accum): 775,  Loss: 5.9242, Time: 17.66s, Token/s: 28.99
Epoch: 0, Step: 6208, Batch(micro): 6208, Batch (considering grad accum): 776,  Loss: 5.9805, Time: 6.36s, Token/s: 80.45
Epoch: 0, Step: 6209, Batch(micro): 6209, Batch (considering grad accum): 776,  Loss: 5.7310, Time: 3.82s, Token/s: 134.16
Epoch: 0, Step: 6210, Batch(micro): 6210, Batch (considering grad accum): 776,  Loss: 5.7711, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 6211, Batch(micro): 6211, Batch (considering grad accum): 776,  Loss: 4.9643, Time: 3.55s, Token/s: 144.34
Epoch: 0, Step: 6212, Batch(micro): 6212, Batch (considering grad accum): 776,  Loss: 6.9927, Time: 3.88s, Token/s: 131.94
Epoch: 0, Step: 6213, Batch(micro): 6213, Batch (considering grad accum): 776,  Loss: 6.4634, Time: 3.29s, Token/s: 155.45
Epoch: 0, Step: 6214, Batch(micro): 6214, Batch (considering grad accum): 776,  Loss: 5.5286, Time: 3.23s, Token/s: 158.57
Epoch: 0, Step: 6215, Batch(micro): 6215, Batch (considering grad accum): 776,  Loss: 5.8124, Time: 18.94s, Token/s: 27.03
Epoch: 0, Step: 6216, Batch(micro): 6216, Batch (considering grad accum): 777,  Loss: 6.3953, Time: 6.70s, Token/s: 76.39
Epoch: 0, Step: 6217, Batch(micro): 6217, Batch (considering grad accum): 777,  Loss: 6.7906, Time: 3.69s, Token/s: 138.57
Epoch: 0, Step: 6218, Batch(micro): 6218, Batch (considering grad accum): 777,  Loss: 6.6310, Time: 3.24s, Token/s: 157.82
Epoch: 0, Step: 6219, Batch(micro): 6219, Batch (considering grad accum): 777,  Loss: 6.0933, Time: 3.21s, Token/s: 159.30
Epoch: 0, Step: 6220, Batch(micro): 6220, Batch (considering grad accum): 777,  Loss: 6.8402, Time: 3.24s, Token/s: 158.21
Epoch: 0, Step: 6221, Batch(micro): 6221, Batch (considering grad accum): 777,  Loss: 5.9335, Time: 3.31s, Token/s: 154.56
Epoch: 0, Step: 6222, Batch(micro): 6222, Batch (considering grad accum): 777,  Loss: 6.5863, Time: 3.26s, Token/s: 157.07
Epoch: 0, Step: 6223, Batch(micro): 6223, Batch (considering grad accum): 777,  Loss: 6.3705, Time: 23.36s, Token/s: 21.91
Epoch: 0, Step: 6224, Batch(micro): 6224, Batch (considering grad accum): 778,  Loss: 6.4368, Time: 6.74s, Token/s: 75.96
Epoch: 0, Step: 6225, Batch(micro): 6225, Batch (considering grad accum): 778,  Loss: 5.7766, Time: 3.88s, Token/s: 132.10
Epoch: 0, Step: 6226, Batch(micro): 6226, Batch (considering grad accum): 778,  Loss: 6.0655, Time: 3.52s, Token/s: 145.47
Epoch: 0, Step: 6227, Batch(micro): 6227, Batch (considering grad accum): 778,  Loss: 6.0371, Time: 3.49s, Token/s: 146.60
Epoch: 0, Step: 6228, Batch(micro): 6228, Batch (considering grad accum): 778,  Loss: 6.2984, Time: 3.41s, Token/s: 150.12
Epoch: 0, Step: 6229, Batch(micro): 6229, Batch (considering grad accum): 778,  Loss: 6.3935, Time: 3.38s, Token/s: 151.46
Epoch: 0, Step: 6230, Batch(micro): 6230, Batch (considering grad accum): 778,  Loss: 5.8921, Time: 3.23s, Token/s: 158.55
Epoch: 0, Step: 6231, Batch(micro): 6231, Batch (considering grad accum): 778,  Loss: 7.3278, Time: 22.80s, Token/s: 22.45
Epoch: 0, Step: 6232, Batch(micro): 6232, Batch (considering grad accum): 779,  Loss: 6.2426, Time: 6.59s, Token/s: 77.70
Epoch: 0, Step: 6233, Batch(micro): 6233, Batch (considering grad accum): 779,  Loss: 5.8689, Time: 3.66s, Token/s: 140.05
Epoch: 0, Step: 6234, Batch(micro): 6234, Batch (considering grad accum): 779,  Loss: 5.8686, Time: 3.23s, Token/s: 158.69
Epoch: 0, Step: 6235, Batch(micro): 6235, Batch (considering grad accum): 779,  Loss: 7.0765, Time: 3.26s, Token/s: 156.84
Epoch: 0, Step: 6236, Batch(micro): 6236, Batch (considering grad accum): 779,  Loss: 6.2340, Time: 4.39s, Token/s: 116.52
Epoch: 0, Step: 6237, Batch(micro): 6237, Batch (considering grad accum): 779,  Loss: 6.1876, Time: 3.03s, Token/s: 168.78
Epoch: 0, Step: 6238, Batch(micro): 6238, Batch (considering grad accum): 779,  Loss: 6.1333, Time: 3.07s, Token/s: 166.60
Epoch: 0, Step: 6239, Batch(micro): 6239, Batch (considering grad accum): 779,  Loss: 6.5295, Time: 23.36s, Token/s: 21.92
Epoch: 0, Step: 6240, Batch(micro): 6240, Batch (considering grad accum): 780,  Loss: 6.4513, Time: 6.88s, Token/s: 74.41
Epoch: 0, Step: 6241, Batch(micro): 6241, Batch (considering grad accum): 780,  Loss: 5.5655, Time: 4.17s, Token/s: 122.72
Epoch: 0, Step: 6242, Batch(micro): 6242, Batch (considering grad accum): 780,  Loss: 6.0958, Time: 3.61s, Token/s: 141.83
Epoch: 0, Step: 6243, Batch(micro): 6243, Batch (considering grad accum): 780,  Loss: 6.3667, Time: 3.37s, Token/s: 151.89
Epoch: 0, Step: 6244, Batch(micro): 6244, Batch (considering grad accum): 780,  Loss: 7.2118, Time: 3.44s, Token/s: 148.90
Epoch: 0, Step: 6245, Batch(micro): 6245, Batch (considering grad accum): 780,  Loss: 7.3297, Time: 3.39s, Token/s: 150.95
Epoch: 0, Step: 6246, Batch(micro): 6246, Batch (considering grad accum): 780,  Loss: 6.4083, Time: 3.52s, Token/s: 145.42
Epoch: 0, Step: 6247, Batch(micro): 6247, Batch (considering grad accum): 780,  Loss: 5.9489, Time: 23.19s, Token/s: 22.08
Epoch: 0, Step: 6248, Batch(micro): 6248, Batch (considering grad accum): 781,  Loss: 5.5729, Time: 6.99s, Token/s: 73.24
Epoch: 0, Step: 6249, Batch(micro): 6249, Batch (considering grad accum): 781,  Loss: 6.9877, Time: 3.75s, Token/s: 136.60
Epoch: 0, Step: 6250, Batch(micro): 6250, Batch (considering grad accum): 781,  Loss: 6.7970, Time: 3.19s, Token/s: 160.50
Epoch: 0, Step: 6251, Batch(micro): 6251, Batch (considering grad accum): 781,  Loss: 6.1401, Time: 3.51s, Token/s: 145.82
Epoch: 0, Step: 6252, Batch(micro): 6252, Batch (considering grad accum): 781,  Loss: 6.1607, Time: 3.42s, Token/s: 149.83
Epoch: 0, Step: 6253, Batch(micro): 6253, Batch (considering grad accum): 781,  Loss: 5.6560, Time: 3.46s, Token/s: 147.79
Epoch: 0, Step: 6254, Batch(micro): 6254, Batch (considering grad accum): 781,  Loss: 5.3394, Time: 3.61s, Token/s: 141.64
Epoch: 0, Step: 6255, Batch(micro): 6255, Batch (considering grad accum): 781,  Loss: 5.9790, Time: 23.94s, Token/s: 21.39
Epoch: 0, Step: 6256, Batch(micro): 6256, Batch (considering grad accum): 782,  Loss: 6.1268, Time: 8.50s, Token/s: 60.21
Epoch: 0, Step: 6257, Batch(micro): 6257, Batch (considering grad accum): 782,  Loss: 6.2736, Time: 3.31s, Token/s: 154.57
Epoch: 0, Step: 6258, Batch(micro): 6258, Batch (considering grad accum): 782,  Loss: 6.3280, Time: 3.24s, Token/s: 158.13
Epoch: 0, Step: 6259, Batch(micro): 6259, Batch (considering grad accum): 782,  Loss: 6.2368, Time: 3.20s, Token/s: 159.76
Epoch: 0, Step: 6260, Batch(micro): 6260, Batch (considering grad accum): 782,  Loss: 6.4958, Time: 3.43s, Token/s: 149.17
Epoch: 0, Step: 6261, Batch(micro): 6261, Batch (considering grad accum): 782,  Loss: 5.8170, Time: 3.69s, Token/s: 138.61
Epoch: 0, Step: 6262, Batch(micro): 6262, Batch (considering grad accum): 782,  Loss: 6.0761, Time: 3.51s, Token/s: 146.06
Epoch: 0, Step: 6263, Batch(micro): 6263, Batch (considering grad accum): 782,  Loss: 6.2049, Time: 25.21s, Token/s: 20.31
Epoch: 0, Step: 6264, Batch(micro): 6264, Batch (considering grad accum): 783,  Loss: 7.7382, Time: 7.19s, Token/s: 71.21
Epoch: 0, Step: 6265, Batch(micro): 6265, Batch (considering grad accum): 783,  Loss: 6.8546, Time: 3.77s, Token/s: 135.91
Epoch: 0, Step: 6266, Batch(micro): 6266, Batch (considering grad accum): 783,  Loss: 6.9792, Time: 3.47s, Token/s: 147.75
Epoch: 0, Step: 6267, Batch(micro): 6267, Batch (considering grad accum): 783,  Loss: 6.6345, Time: 3.51s, Token/s: 145.72
Epoch: 0, Step: 6268, Batch(micro): 6268, Batch (considering grad accum): 783,  Loss: 6.2727, Time: 3.32s, Token/s: 154.15
Epoch: 0, Step: 6269, Batch(micro): 6269, Batch (considering grad accum): 783,  Loss: 5.7821, Time: 3.47s, Token/s: 147.56
Epoch: 0, Step: 6270, Batch(micro): 6270, Batch (considering grad accum): 783,  Loss: 5.7519, Time: 3.55s, Token/s: 144.36
Epoch: 0, Step: 6271, Batch(micro): 6271, Batch (considering grad accum): 783,  Loss: 6.3269, Time: 21.82s, Token/s: 23.47
Epoch: 0, Step: 6272, Batch(micro): 6272, Batch (considering grad accum): 784,  Loss: 5.5328, Time: 6.56s, Token/s: 78.04
Epoch: 0, Step: 6273, Batch(micro): 6273, Batch (considering grad accum): 784,  Loss: 6.4357, Time: 3.79s, Token/s: 135.20
Epoch: 0, Step: 6274, Batch(micro): 6274, Batch (considering grad accum): 784,  Loss: 6.2102, Time: 3.26s, Token/s: 156.95
Epoch: 0, Step: 6275, Batch(micro): 6275, Batch (considering grad accum): 784,  Loss: 6.6422, Time: 3.32s, Token/s: 154.29
Epoch: 0, Step: 6276, Batch(micro): 6276, Batch (considering grad accum): 784,  Loss: 6.3796, Time: 3.27s, Token/s: 156.40
Epoch: 0, Step: 6277, Batch(micro): 6277, Batch (considering grad accum): 784,  Loss: 5.9396, Time: 3.65s, Token/s: 140.18
Epoch: 0, Step: 6278, Batch(micro): 6278, Batch (considering grad accum): 784,  Loss: 6.2470, Time: 3.44s, Token/s: 148.63
Epoch: 0, Step: 6279, Batch(micro): 6279, Batch (considering grad accum): 784,  Loss: 6.0112, Time: 23.43s, Token/s: 21.85
Epoch: 0, Step: 6280, Batch(micro): 6280, Batch (considering grad accum): 785,  Loss: 5.8494, Time: 7.47s, Token/s: 68.55
Epoch: 0, Step: 6281, Batch(micro): 6281, Batch (considering grad accum): 785,  Loss: 5.4956, Time: 3.86s, Token/s: 132.73
Epoch: 0, Step: 6282, Batch(micro): 6282, Batch (considering grad accum): 785,  Loss: 6.0899, Time: 3.49s, Token/s: 146.84
Epoch: 0, Step: 6283, Batch(micro): 6283, Batch (considering grad accum): 785,  Loss: 6.7433, Time: 3.60s, Token/s: 142.15
Epoch: 0, Step: 6284, Batch(micro): 6284, Batch (considering grad accum): 785,  Loss: 6.3441, Time: 3.88s, Token/s: 131.96
Epoch: 0, Step: 6285, Batch(micro): 6285, Batch (considering grad accum): 785,  Loss: 5.8650, Time: 3.39s, Token/s: 151.10
Epoch: 0, Step: 6286, Batch(micro): 6286, Batch (considering grad accum): 785,  Loss: 6.1607, Time: 3.36s, Token/s: 152.33
Epoch: 0, Step: 6287, Batch(micro): 6287, Batch (considering grad accum): 785,  Loss: 5.7456, Time: 23.57s, Token/s: 21.72
Epoch: 0, Step: 6288, Batch(micro): 6288, Batch (considering grad accum): 786,  Loss: 5.7227, Time: 8.30s, Token/s: 61.68
Epoch: 0, Step: 6289, Batch(micro): 6289, Batch (considering grad accum): 786,  Loss: 5.7838, Time: 3.78s, Token/s: 135.51
Epoch: 0, Step: 6290, Batch(micro): 6290, Batch (considering grad accum): 786,  Loss: 6.3040, Time: 3.29s, Token/s: 155.72
Epoch: 0, Step: 6291, Batch(micro): 6291, Batch (considering grad accum): 786,  Loss: 5.7029, Time: 3.51s, Token/s: 146.07
Epoch: 0, Step: 6292, Batch(micro): 6292, Batch (considering grad accum): 786,  Loss: 6.6824, Time: 3.60s, Token/s: 142.30
Epoch: 0, Step: 6293, Batch(micro): 6293, Batch (considering grad accum): 786,  Loss: 6.9009, Time: 3.39s, Token/s: 151.17
Epoch: 0, Step: 6294, Batch(micro): 6294, Batch (considering grad accum): 786,  Loss: 6.2275, Time: 3.44s, Token/s: 148.65
Epoch: 0, Step: 6295, Batch(micro): 6295, Batch (considering grad accum): 786,  Loss: 6.1411, Time: 24.39s, Token/s: 21.00
Epoch: 0, Step: 6296, Batch(micro): 6296, Batch (considering grad accum): 787,  Loss: 6.2069, Time: 6.96s, Token/s: 73.54
Epoch: 0, Step: 6297, Batch(micro): 6297, Batch (considering grad accum): 787,  Loss: 6.3546, Time: 4.04s, Token/s: 126.80
Epoch: 0, Step: 6298, Batch(micro): 6298, Batch (considering grad accum): 787,  Loss: 5.9771, Time: 3.99s, Token/s: 128.36
Epoch: 0, Step: 6299, Batch(micro): 6299, Batch (considering grad accum): 787,  Loss: 5.7478, Time: 3.88s, Token/s: 132.11
Updating MLP bias
Epoch: 0, Step: 6300, Batch(micro): 6300, Batch (considering grad accum): 787,  Loss: 6.4361, Time: 3.38s, Token/s: 151.32
Epoch: 0, Step: 6301, Batch(micro): 6301, Batch (considering grad accum): 787,  Loss: 5.9471, Time: 3.66s, Token/s: 140.01
Epoch: 0, Step: 6302, Batch(micro): 6302, Batch (considering grad accum): 787,  Loss: 6.0470, Time: 3.83s, Token/s: 133.72
Epoch: 0, Step: 6303, Batch(micro): 6303, Batch (considering grad accum): 787,  Loss: 6.5453, Time: 23.58s, Token/s: 21.71
Epoch: 0, Step: 6304, Batch(micro): 6304, Batch (considering grad accum): 788,  Loss: 6.6311, Time: 8.50s, Token/s: 60.25
Epoch: 0, Step: 6305, Batch(micro): 6305, Batch (considering grad accum): 788,  Loss: 6.1115, Time: 3.88s, Token/s: 131.91
Epoch: 0, Step: 6306, Batch(micro): 6306, Batch (considering grad accum): 788,  Loss: 6.6464, Time: 3.63s, Token/s: 141.06
Epoch: 0, Step: 6307, Batch(micro): 6307, Batch (considering grad accum): 788,  Loss: 6.3049, Time: 3.94s, Token/s: 130.08
Epoch: 0, Step: 6308, Batch(micro): 6308, Batch (considering grad accum): 788,  Loss: 6.2665, Time: 3.53s, Token/s: 144.87
Epoch: 0, Step: 6309, Batch(micro): 6309, Batch (considering grad accum): 788,  Loss: 5.8751, Time: 3.63s, Token/s: 140.85
Epoch: 0, Step: 6310, Batch(micro): 6310, Batch (considering grad accum): 788,  Loss: 6.6580, Time: 3.49s, Token/s: 146.70
Epoch: 0, Step: 6311, Batch(micro): 6311, Batch (considering grad accum): 788,  Loss: 6.5686, Time: 26.58s, Token/s: 19.27
Epoch: 0, Step: 6312, Batch(micro): 6312, Batch (considering grad accum): 789,  Loss: 6.2057, Time: 7.42s, Token/s: 68.97
Epoch: 0, Step: 6313, Batch(micro): 6313, Batch (considering grad accum): 789,  Loss: 6.5053, Time: 3.90s, Token/s: 131.14
Epoch: 0, Step: 6314, Batch(micro): 6314, Batch (considering grad accum): 789,  Loss: 6.0869, Time: 3.53s, Token/s: 145.03
Epoch: 0, Step: 6315, Batch(micro): 6315, Batch (considering grad accum): 789,  Loss: 5.9942, Time: 3.67s, Token/s: 139.69
Epoch: 0, Step: 6316, Batch(micro): 6316, Batch (considering grad accum): 789,  Loss: 6.2792, Time: 3.59s, Token/s: 142.73
Epoch: 0, Step: 6317, Batch(micro): 6317, Batch (considering grad accum): 789,  Loss: 7.1942, Time: 3.66s, Token/s: 139.76
Epoch: 0, Step: 6318, Batch(micro): 6318, Batch (considering grad accum): 789,  Loss: 6.9799, Time: 3.57s, Token/s: 143.33
Epoch: 0, Step: 6319, Batch(micro): 6319, Batch (considering grad accum): 789,  Loss: 5.7287, Time: 24.00s, Token/s: 21.33
Epoch: 0, Step: 6320, Batch(micro): 6320, Batch (considering grad accum): 790,  Loss: 6.1568, Time: 6.33s, Token/s: 80.92
Epoch: 0, Step: 6321, Batch(micro): 6321, Batch (considering grad accum): 790,  Loss: 5.8566, Time: 3.76s, Token/s: 136.25
Epoch: 0, Step: 6322, Batch(micro): 6322, Batch (considering grad accum): 790,  Loss: 6.1426, Time: 3.38s, Token/s: 151.54
Epoch: 0, Step: 6323, Batch(micro): 6323, Batch (considering grad accum): 790,  Loss: 6.7442, Time: 3.42s, Token/s: 149.56
Epoch: 0, Step: 6324, Batch(micro): 6324, Batch (considering grad accum): 790,  Loss: 6.4987, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 6325, Batch(micro): 6325, Batch (considering grad accum): 790,  Loss: 5.9772, Time: 3.47s, Token/s: 147.70
Epoch: 0, Step: 6326, Batch(micro): 6326, Batch (considering grad accum): 790,  Loss: 5.6503, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 6327, Batch(micro): 6327, Batch (considering grad accum): 790,  Loss: 5.7139, Time: 22.53s, Token/s: 22.73
Epoch: 0, Step: 6328, Batch(micro): 6328, Batch (considering grad accum): 791,  Loss: 6.3001, Time: 6.26s, Token/s: 81.85
Epoch: 0, Step: 6329, Batch(micro): 6329, Batch (considering grad accum): 791,  Loss: 6.3332, Time: 3.87s, Token/s: 132.19
Epoch: 0, Step: 6330, Batch(micro): 6330, Batch (considering grad accum): 791,  Loss: 6.7030, Time: 3.63s, Token/s: 141.22
Epoch: 0, Step: 6331, Batch(micro): 6331, Batch (considering grad accum): 791,  Loss: 5.9284, Time: 3.61s, Token/s: 141.95
Epoch: 0, Step: 6332, Batch(micro): 6332, Batch (considering grad accum): 791,  Loss: 5.2558, Time: 3.42s, Token/s: 149.53
Epoch: 0, Step: 6333, Batch(micro): 6333, Batch (considering grad accum): 791,  Loss: 5.6963, Time: 3.37s, Token/s: 152.11
Epoch: 0, Step: 6334, Batch(micro): 6334, Batch (considering grad accum): 791,  Loss: 6.5221, Time: 3.60s, Token/s: 142.33
Epoch: 0, Step: 6335, Batch(micro): 6335, Batch (considering grad accum): 791,  Loss: 5.8970, Time: 21.21s, Token/s: 24.15
Epoch: 0, Step: 6336, Batch(micro): 6336, Batch (considering grad accum): 792,  Loss: 6.3222, Time: 6.46s, Token/s: 79.24
Epoch: 0, Step: 6337, Batch(micro): 6337, Batch (considering grad accum): 792,  Loss: 5.9431, Time: 3.73s, Token/s: 137.25
Epoch: 0, Step: 6338, Batch(micro): 6338, Batch (considering grad accum): 792,  Loss: 5.8327, Time: 3.54s, Token/s: 144.52
Epoch: 0, Step: 6339, Batch(micro): 6339, Batch (considering grad accum): 792,  Loss: 5.4540, Time: 3.49s, Token/s: 146.76
Epoch: 0, Step: 6340, Batch(micro): 6340, Batch (considering grad accum): 792,  Loss: 6.5409, Time: 3.22s, Token/s: 159.00
Epoch: 0, Step: 6341, Batch(micro): 6341, Batch (considering grad accum): 792,  Loss: 7.1213, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 6342, Batch(micro): 6342, Batch (considering grad accum): 792,  Loss: 7.0678, Time: 3.43s, Token/s: 149.37
Epoch: 0, Step: 6343, Batch(micro): 6343, Batch (considering grad accum): 792,  Loss: 6.3402, Time: 21.45s, Token/s: 23.87
Epoch: 0, Step: 6344, Batch(micro): 6344, Batch (considering grad accum): 793,  Loss: 5.2635, Time: 6.60s, Token/s: 77.52
Epoch: 0, Step: 6345, Batch(micro): 6345, Batch (considering grad accum): 793,  Loss: 5.7126, Time: 3.76s, Token/s: 136.25
Epoch: 0, Step: 6346, Batch(micro): 6346, Batch (considering grad accum): 793,  Loss: 6.2876, Time: 3.32s, Token/s: 154.23
Epoch: 0, Step: 6347, Batch(micro): 6347, Batch (considering grad accum): 793,  Loss: 6.5593, Time: 3.29s, Token/s: 155.85
Epoch: 0, Step: 6348, Batch(micro): 6348, Batch (considering grad accum): 793,  Loss: 6.8649, Time: 3.25s, Token/s: 157.71
Epoch: 0, Step: 6349, Batch(micro): 6349, Batch (considering grad accum): 793,  Loss: 6.7083, Time: 3.20s, Token/s: 159.87
Epoch: 0, Step: 6350, Batch(micro): 6350, Batch (considering grad accum): 793,  Loss: 6.4062, Time: 3.60s, Token/s: 142.13
Epoch: 0, Step: 6351, Batch(micro): 6351, Batch (considering grad accum): 793,  Loss: 6.1117, Time: 24.79s, Token/s: 20.65
Epoch: 0, Step: 6352, Batch(micro): 6352, Batch (considering grad accum): 794,  Loss: 6.5977, Time: 5.84s, Token/s: 87.66
Epoch: 0, Step: 6353, Batch(micro): 6353, Batch (considering grad accum): 794,  Loss: 6.1908, Time: 4.09s, Token/s: 125.28
Epoch: 0, Step: 6354, Batch(micro): 6354, Batch (considering grad accum): 794,  Loss: 5.8785, Time: 3.62s, Token/s: 141.56
Epoch: 0, Step: 6355, Batch(micro): 6355, Batch (considering grad accum): 794,  Loss: 5.7496, Time: 3.29s, Token/s: 155.48
Epoch: 0, Step: 6356, Batch(micro): 6356, Batch (considering grad accum): 794,  Loss: 5.3178, Time: 3.77s, Token/s: 135.87
Epoch: 0, Step: 6357, Batch(micro): 6357, Batch (considering grad accum): 794,  Loss: 6.0634, Time: 3.65s, Token/s: 140.14
Epoch: 0, Step: 6358, Batch(micro): 6358, Batch (considering grad accum): 794,  Loss: 5.8649, Time: 3.59s, Token/s: 142.81
Epoch: 0, Step: 6359, Batch(micro): 6359, Batch (considering grad accum): 794,  Loss: 6.0062, Time: 24.31s, Token/s: 21.06
Epoch: 0, Step: 6360, Batch(micro): 6360, Batch (considering grad accum): 795,  Loss: 6.1661, Time: 7.21s, Token/s: 71.06
Epoch: 0, Step: 6361, Batch(micro): 6361, Batch (considering grad accum): 795,  Loss: 6.5774, Time: 3.87s, Token/s: 132.33
Epoch: 0, Step: 6362, Batch(micro): 6362, Batch (considering grad accum): 795,  Loss: 6.5703, Time: 3.45s, Token/s: 148.55
Epoch: 0, Step: 6363, Batch(micro): 6363, Batch (considering grad accum): 795,  Loss: 6.7097, Time: 3.70s, Token/s: 138.41
Epoch: 0, Step: 6364, Batch(micro): 6364, Batch (considering grad accum): 795,  Loss: 6.3523, Time: 3.65s, Token/s: 140.19
Epoch: 0, Step: 6365, Batch(micro): 6365, Batch (considering grad accum): 795,  Loss: 6.1365, Time: 3.44s, Token/s: 148.72
Epoch: 0, Step: 6366, Batch(micro): 6366, Batch (considering grad accum): 795,  Loss: 6.0477, Time: 3.58s, Token/s: 143.15
Epoch: 0, Step: 6367, Batch(micro): 6367, Batch (considering grad accum): 795,  Loss: 6.9127, Time: 24.20s, Token/s: 21.16
Epoch: 0, Step: 6368, Batch(micro): 6368, Batch (considering grad accum): 796,  Loss: 5.7527, Time: 6.98s, Token/s: 73.37
Epoch: 0, Step: 6369, Batch(micro): 6369, Batch (considering grad accum): 796,  Loss: 6.5388, Time: 3.86s, Token/s: 132.74
Epoch: 0, Step: 6370, Batch(micro): 6370, Batch (considering grad accum): 796,  Loss: 6.3811, Time: 3.49s, Token/s: 146.54
Epoch: 0, Step: 6371, Batch(micro): 6371, Batch (considering grad accum): 796,  Loss: 5.4449, Time: 3.48s, Token/s: 147.20
Epoch: 0, Step: 6372, Batch(micro): 6372, Batch (considering grad accum): 796,  Loss: 6.0742, Time: 3.70s, Token/s: 138.28
Epoch: 0, Step: 6373, Batch(micro): 6373, Batch (considering grad accum): 796,  Loss: 6.2375, Time: 3.68s, Token/s: 139.30
Epoch: 0, Step: 6374, Batch(micro): 6374, Batch (considering grad accum): 796,  Loss: 6.1197, Time: 3.65s, Token/s: 140.19
Epoch: 0, Step: 6375, Batch(micro): 6375, Batch (considering grad accum): 796,  Loss: 6.5062, Time: 23.66s, Token/s: 21.64
Epoch: 0, Step: 6376, Batch(micro): 6376, Batch (considering grad accum): 797,  Loss: 6.3989, Time: 7.00s, Token/s: 73.13
Epoch: 0, Step: 6377, Batch(micro): 6377, Batch (considering grad accum): 797,  Loss: 7.0768, Time: 3.85s, Token/s: 133.08
Epoch: 0, Step: 6378, Batch(micro): 6378, Batch (considering grad accum): 797,  Loss: 5.7146, Time: 3.15s, Token/s: 162.33
Epoch: 0, Step: 6379, Batch(micro): 6379, Batch (considering grad accum): 797,  Loss: 5.7585, Time: 3.20s, Token/s: 160.16
Epoch: 0, Step: 6380, Batch(micro): 6380, Batch (considering grad accum): 797,  Loss: 6.0821, Time: 3.20s, Token/s: 160.05
Epoch: 0, Step: 6381, Batch(micro): 6381, Batch (considering grad accum): 797,  Loss: 6.2957, Time: 3.29s, Token/s: 155.69
Epoch: 0, Step: 6382, Batch(micro): 6382, Batch (considering grad accum): 797,  Loss: 5.9431, Time: 3.24s, Token/s: 158.06
Epoch: 0, Step: 6383, Batch(micro): 6383, Batch (considering grad accum): 797,  Loss: 5.9948, Time: 27.58s, Token/s: 18.56
Epoch: 0, Step: 6384, Batch(micro): 6384, Batch (considering grad accum): 798,  Loss: 5.9025, Time: 7.80s, Token/s: 65.67
Epoch: 0, Step: 6385, Batch(micro): 6385, Batch (considering grad accum): 798,  Loss: 6.4865, Time: 4.08s, Token/s: 125.54
Epoch: 0, Step: 6386, Batch(micro): 6386, Batch (considering grad accum): 798,  Loss: 6.6537, Time: 3.89s, Token/s: 131.75
Epoch: 0, Step: 6387, Batch(micro): 6387, Batch (considering grad accum): 798,  Loss: 6.5038, Time: 3.52s, Token/s: 145.29
Epoch: 0, Step: 6388, Batch(micro): 6388, Batch (considering grad accum): 798,  Loss: 6.3497, Time: 3.18s, Token/s: 161.20
Epoch: 0, Step: 6389, Batch(micro): 6389, Batch (considering grad accum): 798,  Loss: 5.8688, Time: 3.19s, Token/s: 160.63
Epoch: 0, Step: 6390, Batch(micro): 6390, Batch (considering grad accum): 798,  Loss: 6.1633, Time: 3.61s, Token/s: 141.94
Epoch: 0, Step: 6391, Batch(micro): 6391, Batch (considering grad accum): 798,  Loss: 5.9732, Time: 22.73s, Token/s: 22.52
Epoch: 0, Step: 6392, Batch(micro): 6392, Batch (considering grad accum): 799,  Loss: 6.2657, Time: 7.21s, Token/s: 71.05
Epoch: 0, Step: 6393, Batch(micro): 6393, Batch (considering grad accum): 799,  Loss: 6.4094, Time: 3.95s, Token/s: 129.65
Epoch: 0, Step: 6394, Batch(micro): 6394, Batch (considering grad accum): 799,  Loss: 6.0155, Time: 3.74s, Token/s: 136.96
Epoch: 0, Step: 6395, Batch(micro): 6395, Batch (considering grad accum): 799,  Loss: 6.1197, Time: 3.58s, Token/s: 142.86
Epoch: 0, Step: 6396, Batch(micro): 6396, Batch (considering grad accum): 799,  Loss: 5.8284, Time: 3.52s, Token/s: 145.34
Epoch: 0, Step: 6397, Batch(micro): 6397, Batch (considering grad accum): 799,  Loss: 6.0448, Time: 3.24s, Token/s: 158.18
Epoch: 0, Step: 6398, Batch(micro): 6398, Batch (considering grad accum): 799,  Loss: 6.7645, Time: 3.26s, Token/s: 157.04
Epoch: 0, Step: 6399, Batch(micro): 6399, Batch (considering grad accum): 799,  Loss: 6.7587, Time: 22.75s, Token/s: 22.50
Updating MLP bias
Epoch: 0, Step: 6400, Batch(micro): 6400, Batch (considering grad accum): 800,  Loss: 7.1581, Time: 7.33s, Token/s: 69.82
Epoch: 0, Step: 6401, Batch(micro): 6401, Batch (considering grad accum): 800,  Loss: 6.3176, Time: 4.03s, Token/s: 127.17
Epoch: 0, Step: 6402, Batch(micro): 6402, Batch (considering grad accum): 800,  Loss: 6.9216, Time: 3.66s, Token/s: 139.74
Epoch: 0, Step: 6403, Batch(micro): 6403, Batch (considering grad accum): 800,  Loss: 6.4440, Time: 3.31s, Token/s: 154.78
Epoch: 0, Step: 6404, Batch(micro): 6404, Batch (considering grad accum): 800,  Loss: 6.4443, Time: 3.50s, Token/s: 146.23
Epoch: 0, Step: 6405, Batch(micro): 6405, Batch (considering grad accum): 800,  Loss: 6.0339, Time: 3.21s, Token/s: 159.31
Epoch: 0, Step: 6406, Batch(micro): 6406, Batch (considering grad accum): 800,  Loss: 6.1122, Time: 3.66s, Token/s: 139.76
Epoch: 0, Step: 6407, Batch(micro): 6407, Batch (considering grad accum): 800,  Loss: 5.9024, Time: 20.65s, Token/s: 24.80
Epoch: 0, Step: 6408, Batch(micro): 6408, Batch (considering grad accum): 801,  Loss: 6.7467, Time: 7.56s, Token/s: 67.73
Epoch: 0, Step: 6409, Batch(micro): 6409, Batch (considering grad accum): 801,  Loss: 6.8727, Time: 3.99s, Token/s: 128.37
Epoch: 0, Step: 6410, Batch(micro): 6410, Batch (considering grad accum): 801,  Loss: 6.5541, Time: 4.10s, Token/s: 124.87
Epoch: 0, Step: 6411, Batch(micro): 6411, Batch (considering grad accum): 801,  Loss: 6.3196, Time: 3.60s, Token/s: 142.24
Epoch: 0, Step: 6412, Batch(micro): 6412, Batch (considering grad accum): 801,  Loss: 7.0061, Time: 3.91s, Token/s: 130.84
Epoch: 0, Step: 6413, Batch(micro): 6413, Batch (considering grad accum): 801,  Loss: 7.1006, Time: 3.50s, Token/s: 146.22
Epoch: 0, Step: 6414, Batch(micro): 6414, Batch (considering grad accum): 801,  Loss: 6.3340, Time: 4.17s, Token/s: 122.79
Epoch: 0, Step: 6415, Batch(micro): 6415, Batch (considering grad accum): 801,  Loss: 6.3439, Time: 23.21s, Token/s: 22.06
Epoch: 0, Step: 6416, Batch(micro): 6416, Batch (considering grad accum): 802,  Loss: 6.4676, Time: 8.96s, Token/s: 57.11
Epoch: 0, Step: 6417, Batch(micro): 6417, Batch (considering grad accum): 802,  Loss: 5.9963, Time: 3.70s, Token/s: 138.49
Epoch: 0, Step: 6418, Batch(micro): 6418, Batch (considering grad accum): 802,  Loss: 5.6690, Time: 3.49s, Token/s: 146.62
Epoch: 0, Step: 6419, Batch(micro): 6419, Batch (considering grad accum): 802,  Loss: 5.9936, Time: 3.39s, Token/s: 151.19
Epoch: 0, Step: 6420, Batch(micro): 6420, Batch (considering grad accum): 802,  Loss: 6.3308, Time: 3.47s, Token/s: 147.74
Epoch: 0, Step: 6421, Batch(micro): 6421, Batch (considering grad accum): 802,  Loss: 5.6926, Time: 3.52s, Token/s: 145.64
Epoch: 0, Step: 6422, Batch(micro): 6422, Batch (considering grad accum): 802,  Loss: 6.1571, Time: 3.57s, Token/s: 143.28
Epoch: 0, Step: 6423, Batch(micro): 6423, Batch (considering grad accum): 802,  Loss: 6.1967, Time: 19.31s, Token/s: 26.52
Epoch: 0, Step: 6424, Batch(micro): 6424, Batch (considering grad accum): 803,  Loss: 6.3114, Time: 6.27s, Token/s: 81.65
Epoch: 0, Step: 6425, Batch(micro): 6425, Batch (considering grad accum): 803,  Loss: 6.4364, Time: 3.77s, Token/s: 135.98
Epoch: 0, Step: 6426, Batch(micro): 6426, Batch (considering grad accum): 803,  Loss: 6.4648, Time: 3.69s, Token/s: 138.72
Epoch: 0, Step: 6427, Batch(micro): 6427, Batch (considering grad accum): 803,  Loss: 6.0432, Time: 3.66s, Token/s: 140.07
Epoch: 0, Step: 6428, Batch(micro): 6428, Batch (considering grad accum): 803,  Loss: 6.0010, Time: 3.36s, Token/s: 152.31
Epoch: 0, Step: 6429, Batch(micro): 6429, Batch (considering grad accum): 803,  Loss: 6.3164, Time: 3.78s, Token/s: 135.48
Epoch: 0, Step: 6430, Batch(micro): 6430, Batch (considering grad accum): 803,  Loss: 6.4524, Time: 3.48s, Token/s: 147.32
Epoch: 0, Step: 6431, Batch(micro): 6431, Batch (considering grad accum): 803,  Loss: 5.8437, Time: 17.29s, Token/s: 29.61
Epoch: 0, Step: 6432, Batch(micro): 6432, Batch (considering grad accum): 804,  Loss: 6.2788, Time: 5.92s, Token/s: 86.42
Epoch: 0, Step: 6433, Batch(micro): 6433, Batch (considering grad accum): 804,  Loss: 6.5502, Time: 3.58s, Token/s: 142.88
Epoch: 0, Step: 6434, Batch(micro): 6434, Batch (considering grad accum): 804,  Loss: 6.6140, Time: 3.17s, Token/s: 161.47
Epoch: 0, Step: 6435, Batch(micro): 6435, Batch (considering grad accum): 804,  Loss: 7.8568, Time: 3.15s, Token/s: 162.66
Epoch: 0, Step: 6436, Batch(micro): 6436, Batch (considering grad accum): 804,  Loss: 7.9280, Time: 3.35s, Token/s: 152.91
Epoch: 0, Step: 6437, Batch(micro): 6437, Batch (considering grad accum): 804,  Loss: 6.0647, Time: 3.31s, Token/s: 154.82
Epoch: 0, Step: 6438, Batch(micro): 6438, Batch (considering grad accum): 804,  Loss: 5.6926, Time: 3.27s, Token/s: 156.58
Epoch: 0, Step: 6439, Batch(micro): 6439, Batch (considering grad accum): 804,  Loss: 6.8023, Time: 19.06s, Token/s: 26.86
Epoch: 0, Step: 6440, Batch(micro): 6440, Batch (considering grad accum): 805,  Loss: 6.0523, Time: 7.89s, Token/s: 64.92
Epoch: 0, Step: 6441, Batch(micro): 6441, Batch (considering grad accum): 805,  Loss: 6.7866, Time: 4.09s, Token/s: 125.27
Epoch: 0, Step: 6442, Batch(micro): 6442, Batch (considering grad accum): 805,  Loss: 6.8311, Time: 3.72s, Token/s: 137.54
Epoch: 0, Step: 6443, Batch(micro): 6443, Batch (considering grad accum): 805,  Loss: 5.2845, Time: 3.42s, Token/s: 149.49
Epoch: 0, Step: 6444, Batch(micro): 6444, Batch (considering grad accum): 805,  Loss: 5.8930, Time: 3.42s, Token/s: 149.85
Epoch: 0, Step: 6445, Batch(micro): 6445, Batch (considering grad accum): 805,  Loss: 5.5423, Time: 3.60s, Token/s: 142.41
Epoch: 0, Step: 6446, Batch(micro): 6446, Batch (considering grad accum): 805,  Loss: 6.5092, Time: 3.40s, Token/s: 150.46
Epoch: 0, Step: 6447, Batch(micro): 6447, Batch (considering grad accum): 805,  Loss: 5.6359, Time: 18.39s, Token/s: 27.84
Epoch: 0, Step: 6448, Batch(micro): 6448, Batch (considering grad accum): 806,  Loss: 6.0100, Time: 7.40s, Token/s: 69.22
Epoch: 0, Step: 6449, Batch(micro): 6449, Batch (considering grad accum): 806,  Loss: 6.3689, Time: 3.76s, Token/s: 136.04
Epoch: 0, Step: 6450, Batch(micro): 6450, Batch (considering grad accum): 806,  Loss: 6.4021, Time: 3.23s, Token/s: 158.29
Epoch: 0, Step: 6451, Batch(micro): 6451, Batch (considering grad accum): 806,  Loss: 6.1332, Time: 3.51s, Token/s: 146.02
Epoch: 0, Step: 6452, Batch(micro): 6452, Batch (considering grad accum): 806,  Loss: 6.4907, Time: 3.75s, Token/s: 136.39
Epoch: 0, Step: 6453, Batch(micro): 6453, Batch (considering grad accum): 806,  Loss: 6.1250, Time: 3.42s, Token/s: 149.91
Epoch: 0, Step: 6454, Batch(micro): 6454, Batch (considering grad accum): 806,  Loss: 6.4450, Time: 3.52s, Token/s: 145.56
Epoch: 0, Step: 6455, Batch(micro): 6455, Batch (considering grad accum): 806,  Loss: 6.4487, Time: 18.18s, Token/s: 28.16
Epoch: 0, Step: 6456, Batch(micro): 6456, Batch (considering grad accum): 807,  Loss: 6.3002, Time: 5.95s, Token/s: 86.02
Epoch: 0, Step: 6457, Batch(micro): 6457, Batch (considering grad accum): 807,  Loss: 5.8073, Time: 4.08s, Token/s: 125.52
Epoch: 0, Step: 6458, Batch(micro): 6458, Batch (considering grad accum): 807,  Loss: 6.3782, Time: 3.85s, Token/s: 133.15
Epoch: 0, Step: 6459, Batch(micro): 6459, Batch (considering grad accum): 807,  Loss: 6.0125, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 6460, Batch(micro): 6460, Batch (considering grad accum): 807,  Loss: 7.2552, Time: 3.32s, Token/s: 154.16
Epoch: 0, Step: 6461, Batch(micro): 6461, Batch (considering grad accum): 807,  Loss: 6.8982, Time: 3.57s, Token/s: 143.55
Epoch: 0, Step: 6462, Batch(micro): 6462, Batch (considering grad accum): 807,  Loss: 6.6475, Time: 3.39s, Token/s: 150.93
Epoch: 0, Step: 6463, Batch(micro): 6463, Batch (considering grad accum): 807,  Loss: 6.2188, Time: 18.07s, Token/s: 28.33
Epoch: 0, Step: 6464, Batch(micro): 6464, Batch (considering grad accum): 808,  Loss: 6.3099, Time: 6.79s, Token/s: 75.45
Epoch: 0, Step: 6465, Batch(micro): 6465, Batch (considering grad accum): 808,  Loss: 5.7931, Time: 3.83s, Token/s: 133.78
Epoch: 0, Step: 6466, Batch(micro): 6466, Batch (considering grad accum): 808,  Loss: 5.6294, Time: 3.37s, Token/s: 151.95
Epoch: 0, Step: 6467, Batch(micro): 6467, Batch (considering grad accum): 808,  Loss: 6.1762, Time: 3.42s, Token/s: 149.87
Epoch: 0, Step: 6468, Batch(micro): 6468, Batch (considering grad accum): 808,  Loss: 6.0581, Time: 3.74s, Token/s: 136.74
Epoch: 0, Step: 6469, Batch(micro): 6469, Batch (considering grad accum): 808,  Loss: 5.4795, Time: 3.56s, Token/s: 143.62
Epoch: 0, Step: 6470, Batch(micro): 6470, Batch (considering grad accum): 808,  Loss: 6.1254, Time: 3.80s, Token/s: 134.75
Epoch: 0, Step: 6471, Batch(micro): 6471, Batch (considering grad accum): 808,  Loss: 6.9972, Time: 19.16s, Token/s: 26.72
Epoch: 0, Step: 6472, Batch(micro): 6472, Batch (considering grad accum): 809,  Loss: 5.6985, Time: 6.32s, Token/s: 81.01
Epoch: 0, Step: 6473, Batch(micro): 6473, Batch (considering grad accum): 809,  Loss: 5.8497, Time: 3.85s, Token/s: 133.13
Epoch: 0, Step: 6474, Batch(micro): 6474, Batch (considering grad accum): 809,  Loss: 5.8563, Time: 3.68s, Token/s: 139.29
Epoch: 0, Step: 6475, Batch(micro): 6475, Batch (considering grad accum): 809,  Loss: 6.2141, Time: 3.42s, Token/s: 149.51
Epoch: 0, Step: 6476, Batch(micro): 6476, Batch (considering grad accum): 809,  Loss: 6.4477, Time: 3.53s, Token/s: 144.90
Epoch: 0, Step: 6477, Batch(micro): 6477, Batch (considering grad accum): 809,  Loss: 6.2467, Time: 3.38s, Token/s: 151.61
Epoch: 0, Step: 6478, Batch(micro): 6478, Batch (considering grad accum): 809,  Loss: 6.1493, Time: 3.53s, Token/s: 145.19
Epoch: 0, Step: 6479, Batch(micro): 6479, Batch (considering grad accum): 809,  Loss: 5.8288, Time: 20.02s, Token/s: 25.58
Epoch: 0, Step: 6480, Batch(micro): 6480, Batch (considering grad accum): 810,  Loss: 6.3963, Time: 6.98s, Token/s: 73.35
Epoch: 0, Step: 6481, Batch(micro): 6481, Batch (considering grad accum): 810,  Loss: 6.0699, Time: 3.46s, Token/s: 147.91
Epoch: 0, Step: 6482, Batch(micro): 6482, Batch (considering grad accum): 810,  Loss: 6.7255, Time: 3.47s, Token/s: 147.43
Epoch: 0, Step: 6483, Batch(micro): 6483, Batch (considering grad accum): 810,  Loss: 6.4915, Time: 4.07s, Token/s: 125.78
Epoch: 0, Step: 6484, Batch(micro): 6484, Batch (considering grad accum): 810,  Loss: 5.6715, Time: 3.62s, Token/s: 141.56
Epoch: 0, Step: 6485, Batch(micro): 6485, Batch (considering grad accum): 810,  Loss: 6.4820, Time: 3.21s, Token/s: 159.33
Epoch: 0, Step: 6486, Batch(micro): 6486, Batch (considering grad accum): 810,  Loss: 6.9676, Time: 3.56s, Token/s: 143.93
Epoch: 0, Step: 6487, Batch(micro): 6487, Batch (considering grad accum): 810,  Loss: 5.1621, Time: 25.45s, Token/s: 20.12
Epoch: 0, Step: 6488, Batch(micro): 6488, Batch (considering grad accum): 811,  Loss: 6.3835, Time: 8.96s, Token/s: 57.14
Epoch: 0, Step: 6489, Batch(micro): 6489, Batch (considering grad accum): 811,  Loss: 5.9033, Time: 3.57s, Token/s: 143.34
Epoch: 0, Step: 6490, Batch(micro): 6490, Batch (considering grad accum): 811,  Loss: 6.4617, Time: 3.52s, Token/s: 145.27
Epoch: 0, Step: 6491, Batch(micro): 6491, Batch (considering grad accum): 811,  Loss: 5.7432, Time: 3.60s, Token/s: 142.32
Epoch: 0, Step: 6492, Batch(micro): 6492, Batch (considering grad accum): 811,  Loss: 6.3430, Time: 3.69s, Token/s: 138.89
Epoch: 0, Step: 6493, Batch(micro): 6493, Batch (considering grad accum): 811,  Loss: 5.9326, Time: 3.86s, Token/s: 132.65
Epoch: 0, Step: 6494, Batch(micro): 6494, Batch (considering grad accum): 811,  Loss: 6.2930, Time: 3.55s, Token/s: 144.05
Epoch: 0, Step: 6495, Batch(micro): 6495, Batch (considering grad accum): 811,  Loss: 5.8962, Time: 24.85s, Token/s: 20.61
Epoch: 0, Step: 6496, Batch(micro): 6496, Batch (considering grad accum): 812,  Loss: 6.2599, Time: 6.99s, Token/s: 73.25
Epoch: 0, Step: 6497, Batch(micro): 6497, Batch (considering grad accum): 812,  Loss: 6.1008, Time: 3.92s, Token/s: 130.62
Epoch: 0, Step: 6498, Batch(micro): 6498, Batch (considering grad accum): 812,  Loss: 6.7847, Time: 3.58s, Token/s: 143.14
Epoch: 0, Step: 6499, Batch(micro): 6499, Batch (considering grad accum): 812,  Loss: 6.2097, Time: 3.66s, Token/s: 139.78
Updating MLP bias
Epoch: 0, Step: 6500, Batch(micro): 6500, Batch (considering grad accum): 812,  Loss: 6.8516, Time: 3.32s, Token/s: 154.42
Epoch: 0, Step: 6501, Batch(micro): 6501, Batch (considering grad accum): 812,  Loss: 5.8731, Time: 3.20s, Token/s: 159.89
Epoch: 0, Step: 6502, Batch(micro): 6502, Batch (considering grad accum): 812,  Loss: 6.2175, Time: 3.21s, Token/s: 159.38
Epoch: 0, Step: 6503, Batch(micro): 6503, Batch (considering grad accum): 812,  Loss: 5.6632, Time: 23.66s, Token/s: 21.64
Epoch: 0, Step: 6504, Batch(micro): 6504, Batch (considering grad accum): 813,  Loss: 6.3033, Time: 7.38s, Token/s: 69.42
Epoch: 0, Step: 6505, Batch(micro): 6505, Batch (considering grad accum): 813,  Loss: 6.6394, Time: 4.10s, Token/s: 124.80
Epoch: 0, Step: 6506, Batch(micro): 6506, Batch (considering grad accum): 813,  Loss: 6.2282, Time: 3.75s, Token/s: 136.47
Epoch: 0, Step: 6507, Batch(micro): 6507, Batch (considering grad accum): 813,  Loss: 6.1120, Time: 3.73s, Token/s: 137.23
Epoch: 0, Step: 6508, Batch(micro): 6508, Batch (considering grad accum): 813,  Loss: 6.2735, Time: 3.33s, Token/s: 153.64
Epoch: 0, Step: 6509, Batch(micro): 6509, Batch (considering grad accum): 813,  Loss: 6.6050, Time: 3.24s, Token/s: 157.82
Epoch: 0, Step: 6510, Batch(micro): 6510, Batch (considering grad accum): 813,  Loss: 5.6801, Time: 3.35s, Token/s: 152.82
Epoch: 0, Step: 6511, Batch(micro): 6511, Batch (considering grad accum): 813,  Loss: 5.7326, Time: 21.74s, Token/s: 23.55
Epoch: 0, Step: 6512, Batch(micro): 6512, Batch (considering grad accum): 814,  Loss: 6.1998, Time: 7.40s, Token/s: 69.17
Epoch: 0, Step: 6513, Batch(micro): 6513, Batch (considering grad accum): 814,  Loss: 6.0868, Time: 4.11s, Token/s: 124.61
Epoch: 0, Step: 6514, Batch(micro): 6514, Batch (considering grad accum): 814,  Loss: 6.3342, Time: 3.46s, Token/s: 148.10
Epoch: 0, Step: 6515, Batch(micro): 6515, Batch (considering grad accum): 814,  Loss: 5.7161, Time: 3.46s, Token/s: 147.96
Epoch: 0, Step: 6516, Batch(micro): 6516, Batch (considering grad accum): 814,  Loss: 6.0074, Time: 3.64s, Token/s: 140.57
Epoch: 0, Step: 6517, Batch(micro): 6517, Batch (considering grad accum): 814,  Loss: 6.3758, Time: 3.62s, Token/s: 141.27
Epoch: 0, Step: 6518, Batch(micro): 6518, Batch (considering grad accum): 814,  Loss: 7.7948, Time: 3.76s, Token/s: 136.26
Epoch: 0, Step: 6519, Batch(micro): 6519, Batch (considering grad accum): 814,  Loss: 6.1768, Time: 22.37s, Token/s: 22.88
Epoch: 0, Step: 6520, Batch(micro): 6520, Batch (considering grad accum): 815,  Loss: 5.9433, Time: 6.69s, Token/s: 76.52
Epoch: 0, Step: 6521, Batch(micro): 6521, Batch (considering grad accum): 815,  Loss: 6.2377, Time: 4.10s, Token/s: 124.86
Epoch: 0, Step: 6522, Batch(micro): 6522, Batch (considering grad accum): 815,  Loss: 6.6388, Time: 3.70s, Token/s: 138.28
Epoch: 0, Step: 6523, Batch(micro): 6523, Batch (considering grad accum): 815,  Loss: 5.7637, Time: 3.49s, Token/s: 146.55
Epoch: 0, Step: 6524, Batch(micro): 6524, Batch (considering grad accum): 815,  Loss: 5.5444, Time: 3.26s, Token/s: 156.89
Epoch: 0, Step: 6525, Batch(micro): 6525, Batch (considering grad accum): 815,  Loss: 5.9822, Time: 3.29s, Token/s: 155.54
Epoch: 0, Step: 6526, Batch(micro): 6526, Batch (considering grad accum): 815,  Loss: 6.4696, Time: 3.37s, Token/s: 151.97
Epoch: 0, Step: 6527, Batch(micro): 6527, Batch (considering grad accum): 815,  Loss: 6.5019, Time: 22.84s, Token/s: 22.42
Epoch: 0, Step: 6528, Batch(micro): 6528, Batch (considering grad accum): 816,  Loss: 5.7432, Time: 9.48s, Token/s: 54.00
Epoch: 0, Step: 6529, Batch(micro): 6529, Batch (considering grad accum): 816,  Loss: 5.4981, Time: 3.27s, Token/s: 156.38
Epoch: 0, Step: 6530, Batch(micro): 6530, Batch (considering grad accum): 816,  Loss: 6.4648, Time: 3.49s, Token/s: 146.80
Epoch: 0, Step: 6531, Batch(micro): 6531, Batch (considering grad accum): 816,  Loss: 5.8474, Time: 3.63s, Token/s: 140.86
Epoch: 0, Step: 6532, Batch(micro): 6532, Batch (considering grad accum): 816,  Loss: 5.9011, Time: 3.42s, Token/s: 149.52
Epoch: 0, Step: 6533, Batch(micro): 6533, Batch (considering grad accum): 816,  Loss: 6.5716, Time: 3.50s, Token/s: 146.20
Epoch: 0, Step: 6534, Batch(micro): 6534, Batch (considering grad accum): 816,  Loss: 6.2068, Time: 3.42s, Token/s: 149.56
Epoch: 0, Step: 6535, Batch(micro): 6535, Batch (considering grad accum): 816,  Loss: 5.8720, Time: 24.92s, Token/s: 20.54
Epoch: 0, Step: 6536, Batch(micro): 6536, Batch (considering grad accum): 817,  Loss: 6.6724, Time: 7.37s, Token/s: 69.43
Epoch: 0, Step: 6537, Batch(micro): 6537, Batch (considering grad accum): 817,  Loss: 5.8077, Time: 3.82s, Token/s: 134.15
Epoch: 0, Step: 6538, Batch(micro): 6538, Batch (considering grad accum): 817,  Loss: 6.0259, Time: 3.53s, Token/s: 144.93
Epoch: 0, Step: 6539, Batch(micro): 6539, Batch (considering grad accum): 817,  Loss: 6.5107, Time: 3.60s, Token/s: 142.31
Epoch: 0, Step: 6540, Batch(micro): 6540, Batch (considering grad accum): 817,  Loss: 6.6771, Time: 3.90s, Token/s: 131.16
Epoch: 0, Step: 6541, Batch(micro): 6541, Batch (considering grad accum): 817,  Loss: 6.2486, Time: 4.51s, Token/s: 113.40
Epoch: 0, Step: 6542, Batch(micro): 6542, Batch (considering grad accum): 817,  Loss: 6.5632, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 6543, Batch(micro): 6543, Batch (considering grad accum): 817,  Loss: 6.1658, Time: 25.98s, Token/s: 19.71
Epoch: 0, Step: 6544, Batch(micro): 6544, Batch (considering grad accum): 818,  Loss: 5.8618, Time: 9.11s, Token/s: 56.21
Epoch: 0, Step: 6545, Batch(micro): 6545, Batch (considering grad accum): 818,  Loss: 6.4126, Time: 4.32s, Token/s: 118.44
Epoch: 0, Step: 6546, Batch(micro): 6546, Batch (considering grad accum): 818,  Loss: 5.9910, Time: 3.61s, Token/s: 141.75
Epoch: 0, Step: 6547, Batch(micro): 6547, Batch (considering grad accum): 818,  Loss: 5.0912, Time: 3.80s, Token/s: 134.65
Epoch: 0, Step: 6548, Batch(micro): 6548, Batch (considering grad accum): 818,  Loss: 5.3424, Time: 3.32s, Token/s: 154.18
Epoch: 0, Step: 6549, Batch(micro): 6549, Batch (considering grad accum): 818,  Loss: 5.8129, Time: 3.58s, Token/s: 142.90
Epoch: 0, Step: 6550, Batch(micro): 6550, Batch (considering grad accum): 818,  Loss: 6.3751, Time: 3.34s, Token/s: 153.07
Epoch: 0, Step: 6551, Batch(micro): 6551, Batch (considering grad accum): 818,  Loss: 5.8802, Time: 24.41s, Token/s: 20.98
Epoch: 0, Step: 6552, Batch(micro): 6552, Batch (considering grad accum): 819,  Loss: 6.1933, Time: 7.39s, Token/s: 69.32
Epoch: 0, Step: 6553, Batch(micro): 6553, Batch (considering grad accum): 819,  Loss: 6.1847, Time: 3.98s, Token/s: 128.62
Epoch: 0, Step: 6554, Batch(micro): 6554, Batch (considering grad accum): 819,  Loss: 6.3979, Time: 3.68s, Token/s: 139.14
Epoch: 0, Step: 6555, Batch(micro): 6555, Batch (considering grad accum): 819,  Loss: 5.8851, Time: 3.28s, Token/s: 155.88
Epoch: 0, Step: 6556, Batch(micro): 6556, Batch (considering grad accum): 819,  Loss: 5.7912, Time: 3.75s, Token/s: 136.41
Epoch: 0, Step: 6557, Batch(micro): 6557, Batch (considering grad accum): 819,  Loss: 6.2426, Time: 3.54s, Token/s: 144.57
Epoch: 0, Step: 6558, Batch(micro): 6558, Batch (considering grad accum): 819,  Loss: 6.1934, Time: 3.49s, Token/s: 146.58
Epoch: 0, Step: 6559, Batch(micro): 6559, Batch (considering grad accum): 819,  Loss: 6.3205, Time: 21.27s, Token/s: 24.07
Epoch: 0, Step: 6560, Batch(micro): 6560, Batch (considering grad accum): 820,  Loss: 6.8686, Time: 7.30s, Token/s: 70.15
Epoch: 0, Step: 6561, Batch(micro): 6561, Batch (considering grad accum): 820,  Loss: 6.1014, Time: 4.29s, Token/s: 119.41
Epoch: 0, Step: 6562, Batch(micro): 6562, Batch (considering grad accum): 820,  Loss: 5.9935, Time: 3.51s, Token/s: 145.84
Epoch: 0, Step: 6563, Batch(micro): 6563, Batch (considering grad accum): 820,  Loss: 5.9045, Time: 3.31s, Token/s: 154.46
Epoch: 0, Step: 6564, Batch(micro): 6564, Batch (considering grad accum): 820,  Loss: 6.2572, Time: 3.41s, Token/s: 150.32
Epoch: 0, Step: 6565, Batch(micro): 6565, Batch (considering grad accum): 820,  Loss: 5.7865, Time: 3.76s, Token/s: 136.07
Epoch: 0, Step: 6566, Batch(micro): 6566, Batch (considering grad accum): 820,  Loss: 5.9700, Time: 3.66s, Token/s: 139.85
Epoch: 0, Step: 6567, Batch(micro): 6567, Batch (considering grad accum): 820,  Loss: 6.3136, Time: 21.70s, Token/s: 23.60
Epoch: 0, Step: 6568, Batch(micro): 6568, Batch (considering grad accum): 821,  Loss: 5.9354, Time: 7.16s, Token/s: 71.53
Epoch: 0, Step: 6569, Batch(micro): 6569, Batch (considering grad accum): 821,  Loss: 6.1206, Time: 3.33s, Token/s: 153.96
Epoch: 0, Step: 6570, Batch(micro): 6570, Batch (considering grad accum): 821,  Loss: 6.2284, Time: 3.20s, Token/s: 160.22
Epoch: 0, Step: 6571, Batch(micro): 6571, Batch (considering grad accum): 821,  Loss: 6.0468, Time: 3.39s, Token/s: 151.01
Epoch: 0, Step: 6572, Batch(micro): 6572, Batch (considering grad accum): 821,  Loss: 6.4257, Time: 3.40s, Token/s: 150.40
Epoch: 0, Step: 6573, Batch(micro): 6573, Batch (considering grad accum): 821,  Loss: 5.7721, Time: 3.57s, Token/s: 143.38
Epoch: 0, Step: 6574, Batch(micro): 6574, Batch (considering grad accum): 821,  Loss: 6.3098, Time: 3.63s, Token/s: 140.90
Epoch: 0, Step: 6575, Batch(micro): 6575, Batch (considering grad accum): 821,  Loss: 5.5820, Time: 24.37s, Token/s: 21.01
Epoch: 0, Step: 6576, Batch(micro): 6576, Batch (considering grad accum): 822,  Loss: 5.9267, Time: 7.36s, Token/s: 69.54
Epoch: 0, Step: 6577, Batch(micro): 6577, Batch (considering grad accum): 822,  Loss: 6.2558, Time: 3.80s, Token/s: 134.88
Epoch: 0, Step: 6578, Batch(micro): 6578, Batch (considering grad accum): 822,  Loss: 6.2416, Time: 3.53s, Token/s: 145.10
Epoch: 0, Step: 6579, Batch(micro): 6579, Batch (considering grad accum): 822,  Loss: 5.6402, Time: 3.44s, Token/s: 148.86
Epoch: 0, Step: 6580, Batch(micro): 6580, Batch (considering grad accum): 822,  Loss: 5.7517, Time: 3.35s, Token/s: 152.70
Epoch: 0, Step: 6581, Batch(micro): 6581, Batch (considering grad accum): 822,  Loss: 5.7381, Time: 3.54s, Token/s: 144.58
Epoch: 0, Step: 6582, Batch(micro): 6582, Batch (considering grad accum): 822,  Loss: 5.5794, Time: 3.82s, Token/s: 134.03
Epoch: 0, Step: 6583, Batch(micro): 6583, Batch (considering grad accum): 822,  Loss: 6.0183, Time: 27.57s, Token/s: 18.57
Epoch: 0, Step: 6584, Batch(micro): 6584, Batch (considering grad accum): 823,  Loss: 6.5605, Time: 7.87s, Token/s: 65.06
Epoch: 0, Step: 6585, Batch(micro): 6585, Batch (considering grad accum): 823,  Loss: 6.7973, Time: 3.97s, Token/s: 128.98
Epoch: 0, Step: 6586, Batch(micro): 6586, Batch (considering grad accum): 823,  Loss: 6.4897, Time: 3.50s, Token/s: 146.13
Epoch: 0, Step: 6587, Batch(micro): 6587, Batch (considering grad accum): 823,  Loss: 6.4635, Time: 3.43s, Token/s: 149.16
Epoch: 0, Step: 6588, Batch(micro): 6588, Batch (considering grad accum): 823,  Loss: 6.8464, Time: 3.41s, Token/s: 150.32
Epoch: 0, Step: 6589, Batch(micro): 6589, Batch (considering grad accum): 823,  Loss: 6.1158, Time: 3.28s, Token/s: 156.01
Epoch: 0, Step: 6590, Batch(micro): 6590, Batch (considering grad accum): 823,  Loss: 5.2954, Time: 3.27s, Token/s: 156.57
Epoch: 0, Step: 6591, Batch(micro): 6591, Batch (considering grad accum): 823,  Loss: 7.2252, Time: 22.65s, Token/s: 22.60
Epoch: 0, Step: 6592, Batch(micro): 6592, Batch (considering grad accum): 824,  Loss: 6.4866, Time: 8.75s, Token/s: 58.52
Epoch: 0, Step: 6593, Batch(micro): 6593, Batch (considering grad accum): 824,  Loss: 6.5390, Time: 3.62s, Token/s: 141.26
Epoch: 0, Step: 6594, Batch(micro): 6594, Batch (considering grad accum): 824,  Loss: 6.4863, Time: 3.33s, Token/s: 153.89
Epoch: 0, Step: 6595, Batch(micro): 6595, Batch (considering grad accum): 824,  Loss: 6.0099, Time: 3.51s, Token/s: 145.87
Epoch: 0, Step: 6596, Batch(micro): 6596, Batch (considering grad accum): 824,  Loss: 6.3697, Time: 3.56s, Token/s: 143.81
Epoch: 0, Step: 6597, Batch(micro): 6597, Batch (considering grad accum): 824,  Loss: 6.8070, Time: 3.61s, Token/s: 141.81
Epoch: 0, Step: 6598, Batch(micro): 6598, Batch (considering grad accum): 824,  Loss: 5.3115, Time: 3.52s, Token/s: 145.61
Epoch: 0, Step: 6599, Batch(micro): 6599, Batch (considering grad accum): 824,  Loss: 6.1507, Time: 23.05s, Token/s: 22.22
Updating MLP bias
Epoch: 0, Step: 6600, Batch(micro): 6600, Batch (considering grad accum): 825,  Loss: 6.2093, Time: 7.65s, Token/s: 66.91
Epoch: 0, Step: 6601, Batch(micro): 6601, Batch (considering grad accum): 825,  Loss: 7.0930, Time: 4.20s, Token/s: 121.76
Epoch: 0, Step: 6602, Batch(micro): 6602, Batch (considering grad accum): 825,  Loss: 5.5682, Time: 3.73s, Token/s: 137.13
Epoch: 0, Step: 6603, Batch(micro): 6603, Batch (considering grad accum): 825,  Loss: 5.9939, Time: 3.48s, Token/s: 147.08
Epoch: 0, Step: 6604, Batch(micro): 6604, Batch (considering grad accum): 825,  Loss: 6.7314, Time: 3.37s, Token/s: 151.99
Epoch: 0, Step: 6605, Batch(micro): 6605, Batch (considering grad accum): 825,  Loss: 6.7451, Time: 3.63s, Token/s: 140.92
Epoch: 0, Step: 6606, Batch(micro): 6606, Batch (considering grad accum): 825,  Loss: 6.0618, Time: 3.24s, Token/s: 158.22
Epoch: 0, Step: 6607, Batch(micro): 6607, Batch (considering grad accum): 825,  Loss: 5.9174, Time: 22.68s, Token/s: 22.57
Epoch: 0, Step: 6608, Batch(micro): 6608, Batch (considering grad accum): 826,  Loss: 6.2434, Time: 6.28s, Token/s: 81.51
Epoch: 0, Step: 6609, Batch(micro): 6609, Batch (considering grad accum): 826,  Loss: 6.5315, Time: 3.64s, Token/s: 140.74
Epoch: 0, Step: 6610, Batch(micro): 6610, Batch (considering grad accum): 826,  Loss: 6.3293, Time: 3.63s, Token/s: 140.98
Epoch: 0, Step: 6611, Batch(micro): 6611, Batch (considering grad accum): 826,  Loss: 6.1225, Time: 3.66s, Token/s: 139.99
Epoch: 0, Step: 6612, Batch(micro): 6612, Batch (considering grad accum): 826,  Loss: 5.9220, Time: 3.53s, Token/s: 145.21
Epoch: 0, Step: 6613, Batch(micro): 6613, Batch (considering grad accum): 826,  Loss: 5.6817, Time: 3.57s, Token/s: 143.60
Epoch: 0, Step: 6614, Batch(micro): 6614, Batch (considering grad accum): 826,  Loss: 6.1937, Time: 3.50s, Token/s: 146.33
Epoch: 0, Step: 6615, Batch(micro): 6615, Batch (considering grad accum): 826,  Loss: 6.5154, Time: 22.29s, Token/s: 22.97
Epoch: 0, Step: 6616, Batch(micro): 6616, Batch (considering grad accum): 827,  Loss: 6.2359, Time: 6.89s, Token/s: 74.28
Epoch: 0, Step: 6617, Batch(micro): 6617, Batch (considering grad accum): 827,  Loss: 5.6036, Time: 3.88s, Token/s: 131.99
Epoch: 0, Step: 6618, Batch(micro): 6618, Batch (considering grad accum): 827,  Loss: 5.9341, Time: 3.80s, Token/s: 134.65
Epoch: 0, Step: 6619, Batch(micro): 6619, Batch (considering grad accum): 827,  Loss: 6.7831, Time: 3.57s, Token/s: 143.47
Epoch: 0, Step: 6620, Batch(micro): 6620, Batch (considering grad accum): 827,  Loss: 6.6048, Time: 3.38s, Token/s: 151.49
Epoch: 0, Step: 6621, Batch(micro): 6621, Batch (considering grad accum): 827,  Loss: 5.9106, Time: 3.80s, Token/s: 134.76
Epoch: 0, Step: 6622, Batch(micro): 6622, Batch (considering grad accum): 827,  Loss: 5.9963, Time: 3.60s, Token/s: 142.29
Epoch: 0, Step: 6623, Batch(micro): 6623, Batch (considering grad accum): 827,  Loss: 5.8824, Time: 22.02s, Token/s: 23.25
Epoch: 0, Step: 6624, Batch(micro): 6624, Batch (considering grad accum): 828,  Loss: 5.9288, Time: 9.00s, Token/s: 56.90
Epoch: 0, Step: 6625, Batch(micro): 6625, Batch (considering grad accum): 828,  Loss: 5.8740, Time: 3.94s, Token/s: 129.98
Epoch: 0, Step: 6626, Batch(micro): 6626, Batch (considering grad accum): 828,  Loss: 6.2248, Time: 3.59s, Token/s: 142.46
Epoch: 0, Step: 6627, Batch(micro): 6627, Batch (considering grad accum): 828,  Loss: 6.3745, Time: 3.36s, Token/s: 152.42
Epoch: 0, Step: 6628, Batch(micro): 6628, Batch (considering grad accum): 828,  Loss: 6.2946, Time: 3.45s, Token/s: 148.48
Epoch: 0, Step: 6629, Batch(micro): 6629, Batch (considering grad accum): 828,  Loss: 6.0436, Time: 3.39s, Token/s: 150.84
Epoch: 0, Step: 6630, Batch(micro): 6630, Batch (considering grad accum): 828,  Loss: 6.4073, Time: 3.51s, Token/s: 145.91
Epoch: 0, Step: 6631, Batch(micro): 6631, Batch (considering grad accum): 828,  Loss: 5.8849, Time: 24.89s, Token/s: 20.57
Epoch: 0, Step: 6632, Batch(micro): 6632, Batch (considering grad accum): 829,  Loss: 6.0854, Time: 6.56s, Token/s: 78.06
Epoch: 0, Step: 6633, Batch(micro): 6633, Batch (considering grad accum): 829,  Loss: 5.9459, Time: 3.81s, Token/s: 134.53
Epoch: 0, Step: 6634, Batch(micro): 6634, Batch (considering grad accum): 829,  Loss: 7.4100, Time: 4.16s, Token/s: 123.04
Epoch: 0, Step: 6635, Batch(micro): 6635, Batch (considering grad accum): 829,  Loss: 5.7469, Time: 3.56s, Token/s: 143.70
Epoch: 0, Step: 6636, Batch(micro): 6636, Batch (considering grad accum): 829,  Loss: 6.2887, Time: 3.26s, Token/s: 157.25
Epoch: 0, Step: 6637, Batch(micro): 6637, Batch (considering grad accum): 829,  Loss: 6.0868, Time: 3.21s, Token/s: 159.66
Epoch: 0, Step: 6638, Batch(micro): 6638, Batch (considering grad accum): 829,  Loss: 6.1836, Time: 3.21s, Token/s: 159.39
Epoch: 0, Step: 6639, Batch(micro): 6639, Batch (considering grad accum): 829,  Loss: 6.1682, Time: 28.40s, Token/s: 18.03
Epoch: 0, Step: 6640, Batch(micro): 6640, Batch (considering grad accum): 830,  Loss: 6.4829, Time: 8.61s, Token/s: 59.45
Epoch: 0, Step: 6641, Batch(micro): 6641, Batch (considering grad accum): 830,  Loss: 6.1816, Time: 3.82s, Token/s: 134.15
Epoch: 0, Step: 6642, Batch(micro): 6642, Batch (considering grad accum): 830,  Loss: 5.9888, Time: 3.51s, Token/s: 145.85
Epoch: 0, Step: 6643, Batch(micro): 6643, Batch (considering grad accum): 830,  Loss: 5.7303, Time: 3.43s, Token/s: 149.11
Epoch: 0, Step: 6644, Batch(micro): 6644, Batch (considering grad accum): 830,  Loss: 6.7695, Time: 3.19s, Token/s: 160.68
Epoch: 0, Step: 6645, Batch(micro): 6645, Batch (considering grad accum): 830,  Loss: 6.4047, Time: 3.51s, Token/s: 145.71
Epoch: 0, Step: 6646, Batch(micro): 6646, Batch (considering grad accum): 830,  Loss: 6.5402, Time: 3.60s, Token/s: 142.23
Epoch: 0, Step: 6647, Batch(micro): 6647, Batch (considering grad accum): 830,  Loss: 5.6834, Time: 24.44s, Token/s: 20.95
Epoch: 0, Step: 6648, Batch(micro): 6648, Batch (considering grad accum): 831,  Loss: 6.1938, Time: 6.87s, Token/s: 74.50
Epoch: 0, Step: 6649, Batch(micro): 6649, Batch (considering grad accum): 831,  Loss: 5.2702, Time: 3.62s, Token/s: 141.61
Epoch: 0, Step: 6650, Batch(micro): 6650, Batch (considering grad accum): 831,  Loss: 5.3297, Time: 3.30s, Token/s: 154.98
Epoch: 0, Step: 6651, Batch(micro): 6651, Batch (considering grad accum): 831,  Loss: 5.6000, Time: 3.21s, Token/s: 159.29
Epoch: 0, Step: 6652, Batch(micro): 6652, Batch (considering grad accum): 831,  Loss: 6.0055, Time: 4.16s, Token/s: 122.97
Epoch: 0, Step: 6653, Batch(micro): 6653, Batch (considering grad accum): 831,  Loss: 6.1021, Time: 3.57s, Token/s: 143.50
Epoch: 0, Step: 6654, Batch(micro): 6654, Batch (considering grad accum): 831,  Loss: 5.6217, Time: 3.65s, Token/s: 140.21
Epoch: 0, Step: 6655, Batch(micro): 6655, Batch (considering grad accum): 831,  Loss: 5.9711, Time: 22.25s, Token/s: 23.02
Epoch: 0, Step: 6656, Batch(micro): 6656, Batch (considering grad accum): 832,  Loss: 5.9397, Time: 8.97s, Token/s: 57.10
Epoch: 0, Step: 6657, Batch(micro): 6657, Batch (considering grad accum): 832,  Loss: 5.8116, Time: 3.31s, Token/s: 154.52
Epoch: 0, Step: 6658, Batch(micro): 6658, Batch (considering grad accum): 832,  Loss: 6.2279, Time: 3.24s, Token/s: 157.89
Epoch: 0, Step: 6659, Batch(micro): 6659, Batch (considering grad accum): 832,  Loss: 7.0254, Time: 3.21s, Token/s: 159.40
Epoch: 0, Step: 6660, Batch(micro): 6660, Batch (considering grad accum): 832,  Loss: 6.5191, Time: 3.17s, Token/s: 161.50
Epoch: 0, Step: 6661, Batch(micro): 6661, Batch (considering grad accum): 832,  Loss: 5.8208, Time: 4.36s, Token/s: 117.47
Epoch: 0, Step: 6662, Batch(micro): 6662, Batch (considering grad accum): 832,  Loss: 6.6399, Time: 3.49s, Token/s: 146.65
Epoch: 0, Step: 6663, Batch(micro): 6663, Batch (considering grad accum): 832,  Loss: 6.5030, Time: 25.88s, Token/s: 19.78
Epoch: 0, Step: 6664, Batch(micro): 6664, Batch (considering grad accum): 833,  Loss: 6.3351, Time: 7.95s, Token/s: 64.44
Epoch: 0, Step: 6665, Batch(micro): 6665, Batch (considering grad accum): 833,  Loss: 6.1228, Time: 3.69s, Token/s: 138.58
Epoch: 0, Step: 6666, Batch(micro): 6666, Batch (considering grad accum): 833,  Loss: 6.0810, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 6667, Batch(micro): 6667, Batch (considering grad accum): 833,  Loss: 6.3141, Time: 3.71s, Token/s: 137.99
Epoch: 0, Step: 6668, Batch(micro): 6668, Batch (considering grad accum): 833,  Loss: 6.4472, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 6669, Batch(micro): 6669, Batch (considering grad accum): 833,  Loss: 6.1623, Time: 3.49s, Token/s: 146.87
Epoch: 0, Step: 6670, Batch(micro): 6670, Batch (considering grad accum): 833,  Loss: 6.0964, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 6671, Batch(micro): 6671, Batch (considering grad accum): 833,  Loss: 6.1524, Time: 23.12s, Token/s: 22.15
Epoch: 0, Step: 6672, Batch(micro): 6672, Batch (considering grad accum): 834,  Loss: 5.7491, Time: 6.84s, Token/s: 74.89
Epoch: 0, Step: 6673, Batch(micro): 6673, Batch (considering grad accum): 834,  Loss: 5.8967, Time: 4.11s, Token/s: 124.70
Epoch: 0, Step: 6674, Batch(micro): 6674, Batch (considering grad accum): 834,  Loss: 6.0746, Time: 3.71s, Token/s: 138.17
Epoch: 0, Step: 6675, Batch(micro): 6675, Batch (considering grad accum): 834,  Loss: 6.3724, Time: 3.59s, Token/s: 142.75
Epoch: 0, Step: 6676, Batch(micro): 6676, Batch (considering grad accum): 834,  Loss: 5.7295, Time: 3.75s, Token/s: 136.58
Epoch: 0, Step: 6677, Batch(micro): 6677, Batch (considering grad accum): 834,  Loss: 6.3961, Time: 3.48s, Token/s: 147.27
Epoch: 0, Step: 6678, Batch(micro): 6678, Batch (considering grad accum): 834,  Loss: 6.0228, Time: 3.81s, Token/s: 134.32
Epoch: 0, Step: 6679, Batch(micro): 6679, Batch (considering grad accum): 834,  Loss: 5.8966, Time: 18.53s, Token/s: 27.63
Epoch: 0, Step: 6680, Batch(micro): 6680, Batch (considering grad accum): 835,  Loss: 5.8951, Time: 6.81s, Token/s: 75.16
Epoch: 0, Step: 6681, Batch(micro): 6681, Batch (considering grad accum): 835,  Loss: 6.2951, Time: 3.78s, Token/s: 135.54
Epoch: 0, Step: 6682, Batch(micro): 6682, Batch (considering grad accum): 835,  Loss: 5.9169, Time: 3.28s, Token/s: 156.04
Epoch: 0, Step: 6683, Batch(micro): 6683, Batch (considering grad accum): 835,  Loss: 6.1108, Time: 3.71s, Token/s: 138.00
Epoch: 0, Step: 6684, Batch(micro): 6684, Batch (considering grad accum): 835,  Loss: 6.2746, Time: 3.29s, Token/s: 155.82
Epoch: 0, Step: 6685, Batch(micro): 6685, Batch (considering grad accum): 835,  Loss: 6.0998, Time: 3.21s, Token/s: 159.60
Epoch: 0, Step: 6686, Batch(micro): 6686, Batch (considering grad accum): 835,  Loss: 6.1244, Time: 3.39s, Token/s: 150.88
Epoch: 0, Step: 6687, Batch(micro): 6687, Batch (considering grad accum): 835,  Loss: 6.1316, Time: 19.39s, Token/s: 26.40
Epoch: 0, Step: 6688, Batch(micro): 6688, Batch (considering grad accum): 836,  Loss: 6.1523, Time: 6.77s, Token/s: 75.59
Epoch: 0, Step: 6689, Batch(micro): 6689, Batch (considering grad accum): 836,  Loss: 6.0896, Time: 3.76s, Token/s: 136.10
Epoch: 0, Step: 6690, Batch(micro): 6690, Batch (considering grad accum): 836,  Loss: 6.4860, Time: 3.22s, Token/s: 158.97
Epoch: 0, Step: 6691, Batch(micro): 6691, Batch (considering grad accum): 836,  Loss: 6.0098, Time: 3.83s, Token/s: 133.57
Epoch: 0, Step: 6692, Batch(micro): 6692, Batch (considering grad accum): 836,  Loss: 6.3094, Time: 3.39s, Token/s: 151.24
Epoch: 0, Step: 6693, Batch(micro): 6693, Batch (considering grad accum): 836,  Loss: 6.0957, Time: 3.58s, Token/s: 142.89
Epoch: 0, Step: 6694, Batch(micro): 6694, Batch (considering grad accum): 836,  Loss: 6.3405, Time: 3.46s, Token/s: 147.88
Epoch: 0, Step: 6695, Batch(micro): 6695, Batch (considering grad accum): 836,  Loss: 6.4162, Time: 18.09s, Token/s: 28.31
Epoch: 0, Step: 6696, Batch(micro): 6696, Batch (considering grad accum): 837,  Loss: 5.6259, Time: 6.27s, Token/s: 81.64
Epoch: 0, Step: 6697, Batch(micro): 6697, Batch (considering grad accum): 837,  Loss: 6.3139, Time: 4.31s, Token/s: 118.81
Epoch: 0, Step: 6698, Batch(micro): 6698, Batch (considering grad accum): 837,  Loss: 6.6145, Time: 3.66s, Token/s: 139.79
Epoch: 0, Step: 6699, Batch(micro): 6699, Batch (considering grad accum): 837,  Loss: 6.6819, Time: 3.49s, Token/s: 146.55
Updating MLP bias
Epoch: 0, Step: 6700, Batch(micro): 6700, Batch (considering grad accum): 837,  Loss: 7.1454, Time: 3.22s, Token/s: 159.17
Epoch: 0, Step: 6701, Batch(micro): 6701, Batch (considering grad accum): 837,  Loss: 5.0449, Time: 3.42s, Token/s: 149.66
Epoch: 0, Step: 6702, Batch(micro): 6702, Batch (considering grad accum): 837,  Loss: 5.9765, Time: 3.20s, Token/s: 159.79
Epoch: 0, Step: 6703, Batch(micro): 6703, Batch (considering grad accum): 837,  Loss: 5.6894, Time: 18.45s, Token/s: 27.75
Epoch: 0, Step: 6704, Batch(micro): 6704, Batch (considering grad accum): 838,  Loss: 5.7053, Time: 6.38s, Token/s: 80.20
Epoch: 0, Step: 6705, Batch(micro): 6705, Batch (considering grad accum): 838,  Loss: 6.2108, Time: 3.88s, Token/s: 131.98
Epoch: 0, Step: 6706, Batch(micro): 6706, Batch (considering grad accum): 838,  Loss: 6.1327, Time: 3.58s, Token/s: 142.87
Epoch: 0, Step: 6707, Batch(micro): 6707, Batch (considering grad accum): 838,  Loss: 5.7928, Time: 3.41s, Token/s: 150.07
Epoch: 0, Step: 6708, Batch(micro): 6708, Batch (considering grad accum): 838,  Loss: 4.8796, Time: 3.23s, Token/s: 158.37
Epoch: 0, Step: 6709, Batch(micro): 6709, Batch (considering grad accum): 838,  Loss: 6.2527, Time: 3.62s, Token/s: 141.34
Epoch: 0, Step: 6710, Batch(micro): 6710, Batch (considering grad accum): 838,  Loss: 6.5358, Time: 3.54s, Token/s: 144.74
Epoch: 0, Step: 6711, Batch(micro): 6711, Batch (considering grad accum): 838,  Loss: 5.5222, Time: 18.81s, Token/s: 27.21
Epoch: 0, Step: 6712, Batch(micro): 6712, Batch (considering grad accum): 839,  Loss: 5.9445, Time: 6.44s, Token/s: 79.45
Epoch: 0, Step: 6713, Batch(micro): 6713, Batch (considering grad accum): 839,  Loss: 6.4444, Time: 3.72s, Token/s: 137.69
Epoch: 0, Step: 6714, Batch(micro): 6714, Batch (considering grad accum): 839,  Loss: 6.6612, Time: 3.36s, Token/s: 152.21
Epoch: 0, Step: 6715, Batch(micro): 6715, Batch (considering grad accum): 839,  Loss: 6.1789, Time: 3.22s, Token/s: 159.20
Epoch: 0, Step: 6716, Batch(micro): 6716, Batch (considering grad accum): 839,  Loss: 6.1665, Time: 3.20s, Token/s: 160.00
Epoch: 0, Step: 6717, Batch(micro): 6717, Batch (considering grad accum): 839,  Loss: 6.7448, Time: 3.28s, Token/s: 155.87
Epoch: 0, Step: 6718, Batch(micro): 6718, Batch (considering grad accum): 839,  Loss: 6.9742, Time: 3.25s, Token/s: 157.32
Epoch: 0, Step: 6719, Batch(micro): 6719, Batch (considering grad accum): 839,  Loss: 6.5788, Time: 20.74s, Token/s: 24.69
Epoch: 0, Step: 6720, Batch(micro): 6720, Batch (considering grad accum): 840,  Loss: 5.9944, Time: 7.36s, Token/s: 69.53
Epoch: 0, Step: 6721, Batch(micro): 6721, Batch (considering grad accum): 840,  Loss: 6.0817, Time: 4.12s, Token/s: 124.23
Epoch: 0, Step: 6722, Batch(micro): 6722, Batch (considering grad accum): 840,  Loss: 7.1017, Time: 3.24s, Token/s: 158.24
Epoch: 0, Step: 6723, Batch(micro): 6723, Batch (considering grad accum): 840,  Loss: 5.8725, Time: 3.20s, Token/s: 160.06
Epoch: 0, Step: 6724, Batch(micro): 6724, Batch (considering grad accum): 840,  Loss: 6.1968, Time: 3.16s, Token/s: 161.98
Epoch: 0, Step: 6725, Batch(micro): 6725, Batch (considering grad accum): 840,  Loss: 5.8894, Time: 3.23s, Token/s: 158.44
Epoch: 0, Step: 6726, Batch(micro): 6726, Batch (considering grad accum): 840,  Loss: 5.6146, Time: 3.30s, Token/s: 155.29
Epoch: 0, Step: 6727, Batch(micro): 6727, Batch (considering grad accum): 840,  Loss: 6.1048, Time: 20.76s, Token/s: 24.66
Epoch: 0, Step: 6728, Batch(micro): 6728, Batch (considering grad accum): 841,  Loss: 5.8741, Time: 8.51s, Token/s: 60.14
Epoch: 0, Step: 6729, Batch(micro): 6729, Batch (considering grad accum): 841,  Loss: 5.9986, Time: 3.79s, Token/s: 135.19
Epoch: 0, Step: 6730, Batch(micro): 6730, Batch (considering grad accum): 841,  Loss: 5.9454, Time: 3.15s, Token/s: 162.59
Epoch: 0, Step: 6731, Batch(micro): 6731, Batch (considering grad accum): 841,  Loss: 7.1025, Time: 3.26s, Token/s: 157.15
Epoch: 0, Step: 6732, Batch(micro): 6732, Batch (considering grad accum): 841,  Loss: 6.5375, Time: 3.27s, Token/s: 156.57
Epoch: 0, Step: 6733, Batch(micro): 6733, Batch (considering grad accum): 841,  Loss: 6.4368, Time: 3.42s, Token/s: 149.54
Epoch: 0, Step: 6734, Batch(micro): 6734, Batch (considering grad accum): 841,  Loss: 5.9238, Time: 3.50s, Token/s: 146.28
Epoch: 0, Step: 6735, Batch(micro): 6735, Batch (considering grad accum): 841,  Loss: 5.5740, Time: 27.14s, Token/s: 18.86
Epoch: 0, Step: 6736, Batch(micro): 6736, Batch (considering grad accum): 842,  Loss: 6.1707, Time: 8.01s, Token/s: 63.89
Epoch: 0, Step: 6737, Batch(micro): 6737, Batch (considering grad accum): 842,  Loss: 5.9698, Time: 4.19s, Token/s: 122.17
Epoch: 0, Step: 6738, Batch(micro): 6738, Batch (considering grad accum): 842,  Loss: 6.1817, Time: 3.57s, Token/s: 143.55
Epoch: 0, Step: 6739, Batch(micro): 6739, Batch (considering grad accum): 842,  Loss: 6.0061, Time: 3.21s, Token/s: 159.29
Epoch: 0, Step: 6740, Batch(micro): 6740, Batch (considering grad accum): 842,  Loss: 5.4974, Time: 3.47s, Token/s: 147.53
Epoch: 0, Step: 6741, Batch(micro): 6741, Batch (considering grad accum): 842,  Loss: 6.1634, Time: 3.30s, Token/s: 155.30
Epoch: 0, Step: 6742, Batch(micro): 6742, Batch (considering grad accum): 842,  Loss: 6.0349, Time: 3.28s, Token/s: 156.05
Epoch: 0, Step: 6743, Batch(micro): 6743, Batch (considering grad accum): 842,  Loss: 6.3995, Time: 22.91s, Token/s: 22.34
Epoch: 0, Step: 6744, Batch(micro): 6744, Batch (considering grad accum): 843,  Loss: 5.7648, Time: 7.36s, Token/s: 69.58
Epoch: 0, Step: 6745, Batch(micro): 6745, Batch (considering grad accum): 843,  Loss: 5.2255, Time: 3.78s, Token/s: 135.57
Epoch: 0, Step: 6746, Batch(micro): 6746, Batch (considering grad accum): 843,  Loss: 5.9931, Time: 3.17s, Token/s: 161.37
Epoch: 0, Step: 6747, Batch(micro): 6747, Batch (considering grad accum): 843,  Loss: 5.4072, Time: 3.25s, Token/s: 157.66
Epoch: 0, Step: 6748, Batch(micro): 6748, Batch (considering grad accum): 843,  Loss: 5.8511, Time: 3.29s, Token/s: 155.82
Epoch: 0, Step: 6749, Batch(micro): 6749, Batch (considering grad accum): 843,  Loss: 6.4295, Time: 3.39s, Token/s: 151.22
Epoch: 0, Step: 6750, Batch(micro): 6750, Batch (considering grad accum): 843,  Loss: 6.1863, Time: 3.29s, Token/s: 155.70
Epoch: 0, Step: 6751, Batch(micro): 6751, Batch (considering grad accum): 843,  Loss: 5.9213, Time: 21.04s, Token/s: 24.33
Epoch: 0, Step: 6752, Batch(micro): 6752, Batch (considering grad accum): 844,  Loss: 5.6982, Time: 6.90s, Token/s: 74.24
Epoch: 0, Step: 6753, Batch(micro): 6753, Batch (considering grad accum): 844,  Loss: 6.3810, Time: 3.80s, Token/s: 134.90
Epoch: 0, Step: 6754, Batch(micro): 6754, Batch (considering grad accum): 844,  Loss: 5.2918, Time: 3.68s, Token/s: 139.30
Epoch: 0, Step: 6755, Batch(micro): 6755, Batch (considering grad accum): 844,  Loss: 5.8681, Time: 3.63s, Token/s: 140.93
Epoch: 0, Step: 6756, Batch(micro): 6756, Batch (considering grad accum): 844,  Loss: 6.1047, Time: 3.50s, Token/s: 146.23
Epoch: 0, Step: 6757, Batch(micro): 6757, Batch (considering grad accum): 844,  Loss: 6.1908, Time: 3.76s, Token/s: 136.11
Epoch: 0, Step: 6758, Batch(micro): 6758, Batch (considering grad accum): 844,  Loss: 6.0692, Time: 3.61s, Token/s: 141.87
Epoch: 0, Step: 6759, Batch(micro): 6759, Batch (considering grad accum): 844,  Loss: 6.6361, Time: 22.39s, Token/s: 22.87
Epoch: 0, Step: 6760, Batch(micro): 6760, Batch (considering grad accum): 845,  Loss: 6.7318, Time: 7.06s, Token/s: 72.54
Epoch: 0, Step: 6761, Batch(micro): 6761, Batch (considering grad accum): 845,  Loss: 6.0205, Time: 3.85s, Token/s: 133.11
Epoch: 0, Step: 6762, Batch(micro): 6762, Batch (considering grad accum): 845,  Loss: 5.7165, Time: 3.38s, Token/s: 151.35
Epoch: 0, Step: 6763, Batch(micro): 6763, Batch (considering grad accum): 845,  Loss: 5.9861, Time: 4.11s, Token/s: 124.61
Epoch: 0, Step: 6764, Batch(micro): 6764, Batch (considering grad accum): 845,  Loss: 5.4191, Time: 3.54s, Token/s: 144.60
Epoch: 0, Step: 6765, Batch(micro): 6765, Batch (considering grad accum): 845,  Loss: 7.4092, Time: 3.50s, Token/s: 146.18
Epoch: 0, Step: 6766, Batch(micro): 6766, Batch (considering grad accum): 845,  Loss: 7.8813, Time: 3.36s, Token/s: 152.41
Epoch: 0, Step: 6767, Batch(micro): 6767, Batch (considering grad accum): 845,  Loss: 6.1918, Time: 22.16s, Token/s: 23.11
Epoch: 0, Step: 6768, Batch(micro): 6768, Batch (considering grad accum): 846,  Loss: 6.2173, Time: 6.53s, Token/s: 78.38
Epoch: 0, Step: 6769, Batch(micro): 6769, Batch (considering grad accum): 846,  Loss: 6.6337, Time: 3.59s, Token/s: 142.69
Epoch: 0, Step: 6770, Batch(micro): 6770, Batch (considering grad accum): 846,  Loss: 7.1202, Time: 3.48s, Token/s: 147.12
Epoch: 0, Step: 6771, Batch(micro): 6771, Batch (considering grad accum): 846,  Loss: 6.1721, Time: 3.45s, Token/s: 148.59
Epoch: 0, Step: 6772, Batch(micro): 6772, Batch (considering grad accum): 846,  Loss: 6.3342, Time: 3.30s, Token/s: 155.36
Epoch: 0, Step: 6773, Batch(micro): 6773, Batch (considering grad accum): 846,  Loss: 7.4108, Time: 3.92s, Token/s: 130.58
Epoch: 0, Step: 6774, Batch(micro): 6774, Batch (considering grad accum): 846,  Loss: 7.0349, Time: 3.82s, Token/s: 134.16
Epoch: 0, Step: 6775, Batch(micro): 6775, Batch (considering grad accum): 846,  Loss: 6.7390, Time: 24.18s, Token/s: 21.17
Epoch: 0, Step: 6776, Batch(micro): 6776, Batch (considering grad accum): 847,  Loss: 6.9093, Time: 6.99s, Token/s: 73.21
Epoch: 0, Step: 6777, Batch(micro): 6777, Batch (considering grad accum): 847,  Loss: 6.6789, Time: 4.45s, Token/s: 115.04
Epoch: 0, Step: 6778, Batch(micro): 6778, Batch (considering grad accum): 847,  Loss: 6.5176, Time: 3.58s, Token/s: 142.84
Epoch: 0, Step: 6779, Batch(micro): 6779, Batch (considering grad accum): 847,  Loss: 6.5385, Time: 3.61s, Token/s: 142.01
Epoch: 0, Step: 6780, Batch(micro): 6780, Batch (considering grad accum): 847,  Loss: 5.9728, Time: 3.57s, Token/s: 143.37
Epoch: 0, Step: 6781, Batch(micro): 6781, Batch (considering grad accum): 847,  Loss: 5.7765, Time: 3.27s, Token/s: 156.55
Epoch: 0, Step: 6782, Batch(micro): 6782, Batch (considering grad accum): 847,  Loss: 6.6571, Time: 3.29s, Token/s: 155.39
Epoch: 0, Step: 6783, Batch(micro): 6783, Batch (considering grad accum): 847,  Loss: 5.7285, Time: 24.81s, Token/s: 20.64
Epoch: 0, Step: 6784, Batch(micro): 6784, Batch (considering grad accum): 848,  Loss: 5.5728, Time: 7.46s, Token/s: 68.60
Epoch: 0, Step: 6785, Batch(micro): 6785, Batch (considering grad accum): 848,  Loss: 6.1422, Time: 3.83s, Token/s: 133.83
Epoch: 0, Step: 6786, Batch(micro): 6786, Batch (considering grad accum): 848,  Loss: 6.0420, Time: 3.21s, Token/s: 159.53
Epoch: 0, Step: 6787, Batch(micro): 6787, Batch (considering grad accum): 848,  Loss: 5.9603, Time: 3.27s, Token/s: 156.58
Epoch: 0, Step: 6788, Batch(micro): 6788, Batch (considering grad accum): 848,  Loss: 6.8736, Time: 3.41s, Token/s: 150.24
Epoch: 0, Step: 6789, Batch(micro): 6789, Batch (considering grad accum): 848,  Loss: 6.4107, Time: 3.44s, Token/s: 148.94
Epoch: 0, Step: 6790, Batch(micro): 6790, Batch (considering grad accum): 848,  Loss: 6.5915, Time: 3.61s, Token/s: 141.98
Epoch: 0, Step: 6791, Batch(micro): 6791, Batch (considering grad accum): 848,  Loss: 6.8965, Time: 23.09s, Token/s: 22.17
Epoch: 0, Step: 6792, Batch(micro): 6792, Batch (considering grad accum): 849,  Loss: 6.6306, Time: 6.48s, Token/s: 78.98
Epoch: 0, Step: 6793, Batch(micro): 6793, Batch (considering grad accum): 849,  Loss: 6.6339, Time: 3.75s, Token/s: 136.51
Epoch: 0, Step: 6794, Batch(micro): 6794, Batch (considering grad accum): 849,  Loss: 6.1044, Time: 3.29s, Token/s: 155.75
Epoch: 0, Step: 6795, Batch(micro): 6795, Batch (considering grad accum): 849,  Loss: 5.4647, Time: 3.42s, Token/s: 149.81
Epoch: 0, Step: 6796, Batch(micro): 6796, Batch (considering grad accum): 849,  Loss: 5.8217, Time: 3.42s, Token/s: 149.54
Epoch: 0, Step: 6797, Batch(micro): 6797, Batch (considering grad accum): 849,  Loss: 5.7272, Time: 3.57s, Token/s: 143.52
Epoch: 0, Step: 6798, Batch(micro): 6798, Batch (considering grad accum): 849,  Loss: 7.3543, Time: 3.52s, Token/s: 145.41
Epoch: 0, Step: 6799, Batch(micro): 6799, Batch (considering grad accum): 849,  Loss: 6.3692, Time: 24.19s, Token/s: 21.17
Updating MLP bias
Epoch: 0, Step: 6800, Batch(micro): 6800, Batch (considering grad accum): 850,  Loss: 6.3099, Time: 7.39s, Token/s: 69.25
Epoch: 0, Step: 6801, Batch(micro): 6801, Batch (considering grad accum): 850,  Loss: 6.0168, Time: 4.22s, Token/s: 121.27
Epoch: 0, Step: 6802, Batch(micro): 6802, Batch (considering grad accum): 850,  Loss: 6.3763, Time: 3.62s, Token/s: 141.46
Epoch: 0, Step: 6803, Batch(micro): 6803, Batch (considering grad accum): 850,  Loss: 7.3840, Time: 3.44s, Token/s: 148.63
Epoch: 0, Step: 6804, Batch(micro): 6804, Batch (considering grad accum): 850,  Loss: 6.6556, Time: 3.37s, Token/s: 151.71
Epoch: 0, Step: 6805, Batch(micro): 6805, Batch (considering grad accum): 850,  Loss: 6.7923, Time: 3.42s, Token/s: 149.75
Epoch: 0, Step: 6806, Batch(micro): 6806, Batch (considering grad accum): 850,  Loss: 6.5965, Time: 3.38s, Token/s: 151.30
Epoch: 0, Step: 6807, Batch(micro): 6807, Batch (considering grad accum): 850,  Loss: 7.1080, Time: 20.92s, Token/s: 24.48
Epoch: 0, Step: 6808, Batch(micro): 6808, Batch (considering grad accum): 851,  Loss: 7.5380, Time: 5.89s, Token/s: 86.94
Epoch: 0, Step: 6809, Batch(micro): 6809, Batch (considering grad accum): 851,  Loss: 6.6140, Time: 4.26s, Token/s: 120.24
Epoch: 0, Step: 6810, Batch(micro): 6810, Batch (considering grad accum): 851,  Loss: 5.7709, Time: 3.76s, Token/s: 136.24
Epoch: 0, Step: 6811, Batch(micro): 6811, Batch (considering grad accum): 851,  Loss: 5.6609, Time: 3.73s, Token/s: 137.15
Epoch: 0, Step: 6812, Batch(micro): 6812, Batch (considering grad accum): 851,  Loss: 5.5765, Time: 3.51s, Token/s: 145.91
Epoch: 0, Step: 6813, Batch(micro): 6813, Batch (considering grad accum): 851,  Loss: 6.0913, Time: 3.54s, Token/s: 144.83
Epoch: 0, Step: 6814, Batch(micro): 6814, Batch (considering grad accum): 851,  Loss: 5.8315, Time: 3.41s, Token/s: 150.28
Epoch: 0, Step: 6815, Batch(micro): 6815, Batch (considering grad accum): 851,  Loss: 6.0290, Time: 24.03s, Token/s: 21.30
Epoch: 0, Step: 6816, Batch(micro): 6816, Batch (considering grad accum): 852,  Loss: 5.7760, Time: 9.10s, Token/s: 56.28
Epoch: 0, Step: 6817, Batch(micro): 6817, Batch (considering grad accum): 852,  Loss: 5.6792, Time: 4.08s, Token/s: 125.52
Epoch: 0, Step: 6818, Batch(micro): 6818, Batch (considering grad accum): 852,  Loss: 5.2174, Time: 3.69s, Token/s: 138.86
Epoch: 0, Step: 6819, Batch(micro): 6819, Batch (considering grad accum): 852,  Loss: 6.0902, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 6820, Batch(micro): 6820, Batch (considering grad accum): 852,  Loss: 6.3847, Time: 3.16s, Token/s: 161.95
Epoch: 0, Step: 6821, Batch(micro): 6821, Batch (considering grad accum): 852,  Loss: 6.8832, Time: 3.29s, Token/s: 155.43
Epoch: 0, Step: 6822, Batch(micro): 6822, Batch (considering grad accum): 852,  Loss: 6.3141, Time: 3.30s, Token/s: 154.98
Epoch: 0, Step: 6823, Batch(micro): 6823, Batch (considering grad accum): 852,  Loss: 6.9312, Time: 22.34s, Token/s: 22.92
Epoch: 0, Step: 6824, Batch(micro): 6824, Batch (considering grad accum): 853,  Loss: 6.9022, Time: 9.55s, Token/s: 53.62
Epoch: 0, Step: 6825, Batch(micro): 6825, Batch (considering grad accum): 853,  Loss: 6.7584, Time: 3.39s, Token/s: 151.22
Epoch: 0, Step: 6826, Batch(micro): 6826, Batch (considering grad accum): 853,  Loss: 6.1591, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 6827, Batch(micro): 6827, Batch (considering grad accum): 853,  Loss: 6.3475, Time: 3.33s, Token/s: 153.54
Epoch: 0, Step: 6828, Batch(micro): 6828, Batch (considering grad accum): 853,  Loss: 5.9247, Time: 3.15s, Token/s: 162.77
Epoch: 0, Step: 6829, Batch(micro): 6829, Batch (considering grad accum): 853,  Loss: 5.8734, Time: 3.18s, Token/s: 161.11
Epoch: 0, Step: 6830, Batch(micro): 6830, Batch (considering grad accum): 853,  Loss: 6.1150, Time: 3.22s, Token/s: 159.08
Epoch: 0, Step: 6831, Batch(micro): 6831, Batch (considering grad accum): 853,  Loss: 6.0169, Time: 20.44s, Token/s: 25.05
Epoch: 0, Step: 6832, Batch(micro): 6832, Batch (considering grad accum): 854,  Loss: 6.0283, Time: 6.61s, Token/s: 77.40
Epoch: 0, Step: 6833, Batch(micro): 6833, Batch (considering grad accum): 854,  Loss: 5.9816, Time: 3.86s, Token/s: 132.50
Epoch: 0, Step: 6834, Batch(micro): 6834, Batch (considering grad accum): 854,  Loss: 6.1439, Time: 4.09s, Token/s: 125.25
Epoch: 0, Step: 6835, Batch(micro): 6835, Batch (considering grad accum): 854,  Loss: 6.1579, Time: 3.61s, Token/s: 141.96
Epoch: 0, Step: 6836, Batch(micro): 6836, Batch (considering grad accum): 854,  Loss: 5.6448, Time: 3.64s, Token/s: 140.77
Epoch: 0, Step: 6837, Batch(micro): 6837, Batch (considering grad accum): 854,  Loss: 5.8490, Time: 3.42s, Token/s: 149.71
Epoch: 0, Step: 6838, Batch(micro): 6838, Batch (considering grad accum): 854,  Loss: 6.9372, Time: 3.35s, Token/s: 152.83
Epoch: 0, Step: 6839, Batch(micro): 6839, Batch (considering grad accum): 854,  Loss: 6.7513, Time: 21.97s, Token/s: 23.31
Epoch: 0, Step: 6840, Batch(micro): 6840, Batch (considering grad accum): 855,  Loss: 6.6938, Time: 6.20s, Token/s: 82.58
Epoch: 0, Step: 6841, Batch(micro): 6841, Batch (considering grad accum): 855,  Loss: 5.8540, Time: 3.52s, Token/s: 145.49
Epoch: 0, Step: 6842, Batch(micro): 6842, Batch (considering grad accum): 855,  Loss: 6.1895, Time: 3.74s, Token/s: 136.91
Epoch: 0, Step: 6843, Batch(micro): 6843, Batch (considering grad accum): 855,  Loss: 5.5965, Time: 3.73s, Token/s: 137.26
Epoch: 0, Step: 6844, Batch(micro): 6844, Batch (considering grad accum): 855,  Loss: 6.0078, Time: 3.27s, Token/s: 156.57
Epoch: 0, Step: 6845, Batch(micro): 6845, Batch (considering grad accum): 855,  Loss: 5.7035, Time: 3.66s, Token/s: 139.79
Epoch: 0, Step: 6846, Batch(micro): 6846, Batch (considering grad accum): 855,  Loss: 6.2522, Time: 3.60s, Token/s: 142.27
Epoch: 0, Step: 6847, Batch(micro): 6847, Batch (considering grad accum): 855,  Loss: 5.4538, Time: 23.43s, Token/s: 21.85
Epoch: 0, Step: 6848, Batch(micro): 6848, Batch (considering grad accum): 856,  Loss: 6.2172, Time: 7.99s, Token/s: 64.11
Epoch: 0, Step: 6849, Batch(micro): 6849, Batch (considering grad accum): 856,  Loss: 5.7276, Time: 4.13s, Token/s: 123.89
Epoch: 0, Step: 6850, Batch(micro): 6850, Batch (considering grad accum): 856,  Loss: 6.8483, Time: 3.77s, Token/s: 135.84
Epoch: 0, Step: 6851, Batch(micro): 6851, Batch (considering grad accum): 856,  Loss: 7.7066, Time: 3.44s, Token/s: 148.74
Epoch: 0, Step: 6852, Batch(micro): 6852, Batch (considering grad accum): 856,  Loss: 6.4662, Time: 3.50s, Token/s: 146.13
Epoch: 0, Step: 6853, Batch(micro): 6853, Batch (considering grad accum): 856,  Loss: 6.0722, Time: 3.63s, Token/s: 140.94
Epoch: 0, Step: 6854, Batch(micro): 6854, Batch (considering grad accum): 856,  Loss: 7.0132, Time: 3.49s, Token/s: 146.68
Epoch: 0, Step: 6855, Batch(micro): 6855, Batch (considering grad accum): 856,  Loss: 6.0602, Time: 21.80s, Token/s: 23.49
Epoch: 0, Step: 6856, Batch(micro): 6856, Batch (considering grad accum): 857,  Loss: 5.9132, Time: 8.95s, Token/s: 57.23
Epoch: 0, Step: 6857, Batch(micro): 6857, Batch (considering grad accum): 857,  Loss: 6.2389, Time: 3.26s, Token/s: 157.01
Epoch: 0, Step: 6858, Batch(micro): 6858, Batch (considering grad accum): 857,  Loss: 6.1220, Time: 3.30s, Token/s: 155.37
Epoch: 0, Step: 6859, Batch(micro): 6859, Batch (considering grad accum): 857,  Loss: 6.3989, Time: 3.23s, Token/s: 158.62
Epoch: 0, Step: 6860, Batch(micro): 6860, Batch (considering grad accum): 857,  Loss: 5.5838, Time: 3.23s, Token/s: 158.48
Epoch: 0, Step: 6861, Batch(micro): 6861, Batch (considering grad accum): 857,  Loss: 5.7736, Time: 3.33s, Token/s: 153.54
Epoch: 0, Step: 6862, Batch(micro): 6862, Batch (considering grad accum): 857,  Loss: 5.9256, Time: 3.37s, Token/s: 151.85
Epoch: 0, Step: 6863, Batch(micro): 6863, Batch (considering grad accum): 857,  Loss: 6.0773, Time: 23.63s, Token/s: 21.67
Epoch: 0, Step: 6864, Batch(micro): 6864, Batch (considering grad accum): 858,  Loss: 4.8762, Time: 7.43s, Token/s: 68.87
Epoch: 0, Step: 6865, Batch(micro): 6865, Batch (considering grad accum): 858,  Loss: 5.7306, Time: 3.89s, Token/s: 131.68
Epoch: 0, Step: 6866, Batch(micro): 6866, Batch (considering grad accum): 858,  Loss: 5.9987, Time: 3.24s, Token/s: 158.00
Epoch: 0, Step: 6867, Batch(micro): 6867, Batch (considering grad accum): 858,  Loss: 5.5120, Time: 3.21s, Token/s: 159.65
Epoch: 0, Step: 6868, Batch(micro): 6868, Batch (considering grad accum): 858,  Loss: 5.9899, Time: 3.17s, Token/s: 161.54
Epoch: 0, Step: 6869, Batch(micro): 6869, Batch (considering grad accum): 858,  Loss: 5.9523, Time: 3.29s, Token/s: 155.72
Epoch: 0, Step: 6870, Batch(micro): 6870, Batch (considering grad accum): 858,  Loss: 5.7490, Time: 3.30s, Token/s: 155.02
Epoch: 0, Step: 6871, Batch(micro): 6871, Batch (considering grad accum): 858,  Loss: 6.2539, Time: 22.19s, Token/s: 23.07
Epoch: 0, Step: 6872, Batch(micro): 6872, Batch (considering grad accum): 859,  Loss: 6.1404, Time: 6.76s, Token/s: 75.79
Epoch: 0, Step: 6873, Batch(micro): 6873, Batch (considering grad accum): 859,  Loss: 6.8984, Time: 3.68s, Token/s: 139.15
Epoch: 0, Step: 6874, Batch(micro): 6874, Batch (considering grad accum): 859,  Loss: 6.2365, Time: 3.24s, Token/s: 158.26
Epoch: 0, Step: 6875, Batch(micro): 6875, Batch (considering grad accum): 859,  Loss: 6.1918, Time: 3.14s, Token/s: 163.11
Epoch: 0, Step: 6876, Batch(micro): 6876, Batch (considering grad accum): 859,  Loss: 5.7769, Time: 3.32s, Token/s: 154.31
Epoch: 0, Step: 6877, Batch(micro): 6877, Batch (considering grad accum): 859,  Loss: 6.0002, Time: 3.41s, Token/s: 150.01
Epoch: 0, Step: 6878, Batch(micro): 6878, Batch (considering grad accum): 859,  Loss: 5.8565, Time: 3.90s, Token/s: 131.27
Epoch: 0, Step: 6879, Batch(micro): 6879, Batch (considering grad accum): 859,  Loss: 6.2516, Time: 22.37s, Token/s: 22.88
Epoch: 0, Step: 6880, Batch(micro): 6880, Batch (considering grad accum): 860,  Loss: 6.5769, Time: 6.61s, Token/s: 77.44
Epoch: 0, Step: 6881, Batch(micro): 6881, Batch (considering grad accum): 860,  Loss: 6.2572, Time: 3.69s, Token/s: 138.68
Epoch: 0, Step: 6882, Batch(micro): 6882, Batch (considering grad accum): 860,  Loss: 6.1978, Time: 3.22s, Token/s: 159.19
Epoch: 0, Step: 6883, Batch(micro): 6883, Batch (considering grad accum): 860,  Loss: 6.0020, Time: 3.36s, Token/s: 152.44
Epoch: 0, Step: 6884, Batch(micro): 6884, Batch (considering grad accum): 860,  Loss: 6.2782, Time: 3.53s, Token/s: 145.23
Epoch: 0, Step: 6885, Batch(micro): 6885, Batch (considering grad accum): 860,  Loss: 6.1803, Time: 3.58s, Token/s: 143.15
Epoch: 0, Step: 6886, Batch(micro): 6886, Batch (considering grad accum): 860,  Loss: 6.4268, Time: 3.56s, Token/s: 143.63
Epoch: 0, Step: 6887, Batch(micro): 6887, Batch (considering grad accum): 860,  Loss: 6.2705, Time: 22.03s, Token/s: 23.24
Epoch: 0, Step: 6888, Batch(micro): 6888, Batch (considering grad accum): 861,  Loss: 6.4632, Time: 7.75s, Token/s: 66.04
Epoch: 0, Step: 6889, Batch(micro): 6889, Batch (considering grad accum): 861,  Loss: 6.9033, Time: 3.73s, Token/s: 137.32
Epoch: 0, Step: 6890, Batch(micro): 6890, Batch (considering grad accum): 861,  Loss: 6.4359, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 6891, Batch(micro): 6891, Batch (considering grad accum): 861,  Loss: 5.8822, Time: 3.33s, Token/s: 153.72
Epoch: 0, Step: 6892, Batch(micro): 6892, Batch (considering grad accum): 861,  Loss: 6.0352, Time: 3.54s, Token/s: 144.84
Epoch: 0, Step: 6893, Batch(micro): 6893, Batch (considering grad accum): 861,  Loss: 5.8751, Time: 3.53s, Token/s: 145.13
Epoch: 0, Step: 6894, Batch(micro): 6894, Batch (considering grad accum): 861,  Loss: 6.2703, Time: 3.93s, Token/s: 130.33
Epoch: 0, Step: 6895, Batch(micro): 6895, Batch (considering grad accum): 861,  Loss: 5.4460, Time: 17.34s, Token/s: 29.52
Epoch: 0, Step: 6896, Batch(micro): 6896, Batch (considering grad accum): 862,  Loss: 5.5268, Time: 6.19s, Token/s: 82.76
Epoch: 0, Step: 6897, Batch(micro): 6897, Batch (considering grad accum): 862,  Loss: 6.2104, Time: 4.09s, Token/s: 125.33
Epoch: 0, Step: 6898, Batch(micro): 6898, Batch (considering grad accum): 862,  Loss: 5.9949, Time: 3.29s, Token/s: 155.85
Epoch: 0, Step: 6899, Batch(micro): 6899, Batch (considering grad accum): 862,  Loss: 6.1350, Time: 3.74s, Token/s: 137.05
Updating MLP bias
Epoch: 0, Step: 6900, Batch(micro): 6900, Batch (considering grad accum): 862,  Loss: 5.7031, Time: 3.62s, Token/s: 141.41
Epoch: 0, Step: 6901, Batch(micro): 6901, Batch (considering grad accum): 862,  Loss: 5.6752, Time: 3.45s, Token/s: 148.52
Epoch: 0, Step: 6902, Batch(micro): 6902, Batch (considering grad accum): 862,  Loss: 5.6053, Time: 3.21s, Token/s: 159.36
Epoch: 0, Step: 6903, Batch(micro): 6903, Batch (considering grad accum): 862,  Loss: 6.0008, Time: 19.49s, Token/s: 26.26
Epoch: 0, Step: 6904, Batch(micro): 6904, Batch (considering grad accum): 863,  Loss: 5.7414, Time: 6.47s, Token/s: 79.08
Epoch: 0, Step: 6905, Batch(micro): 6905, Batch (considering grad accum): 863,  Loss: 6.3837, Time: 3.68s, Token/s: 138.94
Epoch: 0, Step: 6906, Batch(micro): 6906, Batch (considering grad accum): 863,  Loss: 6.4105, Time: 3.92s, Token/s: 130.64
Epoch: 0, Step: 6907, Batch(micro): 6907, Batch (considering grad accum): 863,  Loss: 6.0654, Time: 3.46s, Token/s: 147.84
Epoch: 0, Step: 6908, Batch(micro): 6908, Batch (considering grad accum): 863,  Loss: 6.2420, Time: 3.57s, Token/s: 143.29
Epoch: 0, Step: 6909, Batch(micro): 6909, Batch (considering grad accum): 863,  Loss: 6.3388, Time: 3.62s, Token/s: 141.28
Epoch: 0, Step: 6910, Batch(micro): 6910, Batch (considering grad accum): 863,  Loss: 6.0714, Time: 3.66s, Token/s: 139.99
Epoch: 0, Step: 6911, Batch(micro): 6911, Batch (considering grad accum): 863,  Loss: 6.4289, Time: 18.18s, Token/s: 28.17
Epoch: 0, Step: 6912, Batch(micro): 6912, Batch (considering grad accum): 864,  Loss: 6.0758, Time: 6.60s, Token/s: 77.57
Epoch: 0, Step: 6913, Batch(micro): 6913, Batch (considering grad accum): 864,  Loss: 5.8467, Time: 3.71s, Token/s: 137.96
Epoch: 0, Step: 6914, Batch(micro): 6914, Batch (considering grad accum): 864,  Loss: 6.0944, Time: 3.31s, Token/s: 154.58
Epoch: 0, Step: 6915, Batch(micro): 6915, Batch (considering grad accum): 864,  Loss: 6.3007, Time: 3.37s, Token/s: 151.97
Epoch: 0, Step: 6916, Batch(micro): 6916, Batch (considering grad accum): 864,  Loss: 6.7112, Time: 3.40s, Token/s: 150.44
Epoch: 0, Step: 6917, Batch(micro): 6917, Batch (considering grad accum): 864,  Loss: 6.4984, Time: 3.52s, Token/s: 145.38
Epoch: 0, Step: 6918, Batch(micro): 6918, Batch (considering grad accum): 864,  Loss: 6.4707, Time: 3.45s, Token/s: 148.34
Epoch: 0, Step: 6919, Batch(micro): 6919, Batch (considering grad accum): 864,  Loss: 5.7713, Time: 18.94s, Token/s: 27.04
Epoch: 0, Step: 6920, Batch(micro): 6920, Batch (considering grad accum): 865,  Loss: 6.6129, Time: 6.69s, Token/s: 76.49
Epoch: 0, Step: 6921, Batch(micro): 6921, Batch (considering grad accum): 865,  Loss: 5.9692, Time: 3.61s, Token/s: 141.84
Epoch: 0, Step: 6922, Batch(micro): 6922, Batch (considering grad accum): 865,  Loss: 6.1731, Time: 3.76s, Token/s: 136.33
Epoch: 0, Step: 6923, Batch(micro): 6923, Batch (considering grad accum): 865,  Loss: 5.5879, Time: 3.32s, Token/s: 154.45
Epoch: 0, Step: 6924, Batch(micro): 6924, Batch (considering grad accum): 865,  Loss: 5.6853, Time: 3.67s, Token/s: 139.65
Epoch: 0, Step: 6925, Batch(micro): 6925, Batch (considering grad accum): 865,  Loss: 5.3882, Time: 3.65s, Token/s: 140.24
Epoch: 0, Step: 6926, Batch(micro): 6926, Batch (considering grad accum): 865,  Loss: 6.5360, Time: 3.39s, Token/s: 151.18
Epoch: 0, Step: 6927, Batch(micro): 6927, Batch (considering grad accum): 865,  Loss: 6.0477, Time: 18.72s, Token/s: 27.35
Epoch: 0, Step: 6928, Batch(micro): 6928, Batch (considering grad accum): 866,  Loss: 5.7820, Time: 6.64s, Token/s: 77.15
Epoch: 0, Step: 6929, Batch(micro): 6929, Batch (considering grad accum): 866,  Loss: 6.6734, Time: 3.55s, Token/s: 144.12
Epoch: 0, Step: 6930, Batch(micro): 6930, Batch (considering grad accum): 866,  Loss: 5.8321, Time: 3.62s, Token/s: 141.28
Epoch: 0, Step: 6931, Batch(micro): 6931, Batch (considering grad accum): 866,  Loss: 5.7137, Time: 3.74s, Token/s: 137.04
Epoch: 0, Step: 6932, Batch(micro): 6932, Batch (considering grad accum): 866,  Loss: 5.8939, Time: 3.58s, Token/s: 142.88
Epoch: 0, Step: 6933, Batch(micro): 6933, Batch (considering grad accum): 866,  Loss: 5.7407, Time: 3.44s, Token/s: 148.74
Epoch: 0, Step: 6934, Batch(micro): 6934, Batch (considering grad accum): 866,  Loss: 6.5597, Time: 3.73s, Token/s: 137.13
Epoch: 0, Step: 6935, Batch(micro): 6935, Batch (considering grad accum): 866,  Loss: 6.2971, Time: 17.41s, Token/s: 29.40
Epoch: 0, Step: 6936, Batch(micro): 6936, Batch (considering grad accum): 867,  Loss: 6.5449, Time: 6.34s, Token/s: 80.74
Epoch: 0, Step: 6937, Batch(micro): 6937, Batch (considering grad accum): 867,  Loss: 6.9098, Time: 4.05s, Token/s: 126.45
Epoch: 0, Step: 6938, Batch(micro): 6938, Batch (considering grad accum): 867,  Loss: 7.0581, Time: 3.85s, Token/s: 133.08
Epoch: 0, Step: 6939, Batch(micro): 6939, Batch (considering grad accum): 867,  Loss: 6.2439, Time: 3.72s, Token/s: 137.50
Epoch: 0, Step: 6940, Batch(micro): 6940, Batch (considering grad accum): 867,  Loss: 6.4303, Time: 3.53s, Token/s: 144.86
Epoch: 0, Step: 6941, Batch(micro): 6941, Batch (considering grad accum): 867,  Loss: 5.6098, Time: 3.35s, Token/s: 152.93
Epoch: 0, Step: 6942, Batch(micro): 6942, Batch (considering grad accum): 867,  Loss: 5.8805, Time: 3.16s, Token/s: 161.78
Epoch: 0, Step: 6943, Batch(micro): 6943, Batch (considering grad accum): 867,  Loss: 6.1938, Time: 18.67s, Token/s: 27.43
Epoch: 0, Step: 6944, Batch(micro): 6944, Batch (considering grad accum): 868,  Loss: 6.8501, Time: 6.63s, Token/s: 77.19
Epoch: 0, Step: 6945, Batch(micro): 6945, Batch (considering grad accum): 868,  Loss: 5.8732, Time: 3.74s, Token/s: 136.85
Epoch: 0, Step: 6946, Batch(micro): 6946, Batch (considering grad accum): 868,  Loss: 5.5162, Time: 3.67s, Token/s: 139.42
Epoch: 0, Step: 6947, Batch(micro): 6947, Batch (considering grad accum): 868,  Loss: 5.7183, Time: 3.55s, Token/s: 144.34
Epoch: 0, Step: 6948, Batch(micro): 6948, Batch (considering grad accum): 868,  Loss: 6.5051, Time: 3.49s, Token/s: 146.76
Epoch: 0, Step: 6949, Batch(micro): 6949, Batch (considering grad accum): 868,  Loss: 6.1719, Time: 3.46s, Token/s: 147.87
Epoch: 0, Step: 6950, Batch(micro): 6950, Batch (considering grad accum): 868,  Loss: 6.2331, Time: 3.91s, Token/s: 131.04
Epoch: 0, Step: 6951, Batch(micro): 6951, Batch (considering grad accum): 868,  Loss: 6.0422, Time: 20.10s, Token/s: 25.47
Epoch: 0, Step: 6952, Batch(micro): 6952, Batch (considering grad accum): 869,  Loss: 6.3156, Time: 6.70s, Token/s: 76.43
Epoch: 0, Step: 6953, Batch(micro): 6953, Batch (considering grad accum): 869,  Loss: 5.8357, Time: 3.98s, Token/s: 128.56
Epoch: 0, Step: 6954, Batch(micro): 6954, Batch (considering grad accum): 869,  Loss: 6.5476, Time: 3.47s, Token/s: 147.45
Epoch: 0, Step: 6955, Batch(micro): 6955, Batch (considering grad accum): 869,  Loss: 6.7563, Time: 3.27s, Token/s: 156.64
Epoch: 0, Step: 6956, Batch(micro): 6956, Batch (considering grad accum): 869,  Loss: 5.6422, Time: 3.15s, Token/s: 162.41
Epoch: 0, Step: 6957, Batch(micro): 6957, Batch (considering grad accum): 869,  Loss: 6.1642, Time: 3.23s, Token/s: 158.40
Epoch: 0, Step: 6958, Batch(micro): 6958, Batch (considering grad accum): 869,  Loss: 5.8387, Time: 3.28s, Token/s: 156.21
Epoch: 0, Step: 6959, Batch(micro): 6959, Batch (considering grad accum): 869,  Loss: 7.3589, Time: 18.22s, Token/s: 28.09
Epoch: 0, Step: 6960, Batch(micro): 6960, Batch (considering grad accum): 870,  Loss: 7.5773, Time: 6.60s, Token/s: 77.62
Epoch: 0, Step: 6961, Batch(micro): 6961, Batch (considering grad accum): 870,  Loss: 6.2129, Time: 3.91s, Token/s: 131.03
Epoch: 0, Step: 6962, Batch(micro): 6962, Batch (considering grad accum): 870,  Loss: 6.2794, Time: 3.84s, Token/s: 133.22
Epoch: 0, Step: 6963, Batch(micro): 6963, Batch (considering grad accum): 870,  Loss: 6.2379, Time: 3.24s, Token/s: 157.89
Epoch: 0, Step: 6964, Batch(micro): 6964, Batch (considering grad accum): 870,  Loss: 5.8498, Time: 3.43s, Token/s: 149.47
Epoch: 0, Step: 6965, Batch(micro): 6965, Batch (considering grad accum): 870,  Loss: 6.4865, Time: 3.21s, Token/s: 159.68
Epoch: 0, Step: 6966, Batch(micro): 6966, Batch (considering grad accum): 870,  Loss: 5.7552, Time: 3.18s, Token/s: 160.98
Epoch: 0, Step: 6967, Batch(micro): 6967, Batch (considering grad accum): 870,  Loss: 6.3192, Time: 18.11s, Token/s: 28.27
Epoch: 0, Step: 6968, Batch(micro): 6968, Batch (considering grad accum): 871,  Loss: 5.6843, Time: 6.66s, Token/s: 76.84
Epoch: 0, Step: 6969, Batch(micro): 6969, Batch (considering grad accum): 871,  Loss: 6.1317, Time: 3.90s, Token/s: 131.25
Epoch: 0, Step: 6970, Batch(micro): 6970, Batch (considering grad accum): 871,  Loss: 6.0496, Time: 3.58s, Token/s: 143.14
Epoch: 0, Step: 6971, Batch(micro): 6971, Batch (considering grad accum): 871,  Loss: 6.1621, Time: 3.66s, Token/s: 140.02
Epoch: 0, Step: 6972, Batch(micro): 6972, Batch (considering grad accum): 871,  Loss: 5.9125, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 6973, Batch(micro): 6973, Batch (considering grad accum): 871,  Loss: 6.0901, Time: 3.68s, Token/s: 139.27
Epoch: 0, Step: 6974, Batch(micro): 6974, Batch (considering grad accum): 871,  Loss: 6.3107, Time: 3.23s, Token/s: 158.72
Epoch: 0, Step: 6975, Batch(micro): 6975, Batch (considering grad accum): 871,  Loss: 6.1689, Time: 17.57s, Token/s: 29.15
Epoch: 0, Step: 6976, Batch(micro): 6976, Batch (considering grad accum): 872,  Loss: 6.4983, Time: 6.23s, Token/s: 82.17
Epoch: 0, Step: 6977, Batch(micro): 6977, Batch (considering grad accum): 872,  Loss: 6.2852, Time: 3.50s, Token/s: 146.35
Epoch: 0, Step: 6978, Batch(micro): 6978, Batch (considering grad accum): 872,  Loss: 6.9267, Time: 3.43s, Token/s: 149.18
Epoch: 0, Step: 6979, Batch(micro): 6979, Batch (considering grad accum): 872,  Loss: 5.9524, Time: 3.90s, Token/s: 131.25
Epoch: 0, Step: 6980, Batch(micro): 6980, Batch (considering grad accum): 872,  Loss: 6.4350, Time: 3.43s, Token/s: 149.32
Epoch: 0, Step: 6981, Batch(micro): 6981, Batch (considering grad accum): 872,  Loss: 5.5251, Time: 3.34s, Token/s: 153.45
Epoch: 0, Step: 6982, Batch(micro): 6982, Batch (considering grad accum): 872,  Loss: 5.7343, Time: 3.70s, Token/s: 138.52
Epoch: 0, Step: 6983, Batch(micro): 6983, Batch (considering grad accum): 872,  Loss: 7.5888, Time: 16.91s, Token/s: 30.28
Epoch: 0, Step: 6984, Batch(micro): 6984, Batch (considering grad accum): 873,  Loss: 7.7869, Time: 6.26s, Token/s: 81.79
Epoch: 0, Step: 6985, Batch(micro): 6985, Batch (considering grad accum): 873,  Loss: 6.7856, Time: 3.88s, Token/s: 132.06
Epoch: 0, Step: 6986, Batch(micro): 6986, Batch (considering grad accum): 873,  Loss: 6.6477, Time: 3.56s, Token/s: 143.78
Epoch: 0, Step: 6987, Batch(micro): 6987, Batch (considering grad accum): 873,  Loss: 5.7208, Time: 3.51s, Token/s: 145.90
Epoch: 0, Step: 6988, Batch(micro): 6988, Batch (considering grad accum): 873,  Loss: 5.7776, Time: 3.61s, Token/s: 141.66
Epoch: 0, Step: 6989, Batch(micro): 6989, Batch (considering grad accum): 873,  Loss: 5.5747, Time: 3.79s, Token/s: 135.20
Epoch: 0, Step: 6990, Batch(micro): 6990, Batch (considering grad accum): 873,  Loss: 5.4787, Time: 3.47s, Token/s: 147.53
Epoch: 0, Step: 6991, Batch(micro): 6991, Batch (considering grad accum): 873,  Loss: 5.4162, Time: 23.41s, Token/s: 21.87
Epoch: 0, Step: 6992, Batch(micro): 6992, Batch (considering grad accum): 874,  Loss: 6.2283, Time: 7.79s, Token/s: 65.73
Epoch: 0, Step: 6993, Batch(micro): 6993, Batch (considering grad accum): 874,  Loss: 6.5687, Time: 3.76s, Token/s: 136.14
Epoch: 0, Step: 6994, Batch(micro): 6994, Batch (considering grad accum): 874,  Loss: 6.0602, Time: 3.57s, Token/s: 143.58
Epoch: 0, Step: 6995, Batch(micro): 6995, Batch (considering grad accum): 874,  Loss: 6.1085, Time: 3.47s, Token/s: 147.44
Epoch: 0, Step: 6996, Batch(micro): 6996, Batch (considering grad accum): 874,  Loss: 5.4827, Time: 3.40s, Token/s: 150.70
Epoch: 0, Step: 6997, Batch(micro): 6997, Batch (considering grad accum): 874,  Loss: 6.2875, Time: 3.45s, Token/s: 148.42
Epoch: 0, Step: 6998, Batch(micro): 6998, Batch (considering grad accum): 874,  Loss: 5.7402, Time: 3.41s, Token/s: 150.01
Epoch: 0, Step: 6999, Batch(micro): 6999, Batch (considering grad accum): 874,  Loss: 5.6084, Time: 23.30s, Token/s: 21.97
Updating MLP bias
Epoch: 0, Step: 7000, Batch(micro): 7000, Batch (considering grad accum): 875,  Loss: 6.2164, Time: 6.25s, Token/s: 81.90
Saved checkpoint at step 7000
What is Gravity?
* This situation with your body) = 20:




As a moment. These concepts and 185:
Epoch: 0, Step: 7001, Batch(micro): 7001, Batch (considering grad accum): 875,  Loss: 6.0177, Time: 14.63s, Token/s: 35.00
Epoch: 0, Step: 7002, Batch(micro): 7002, Batch (considering grad accum): 875,  Loss: 7.6964, Time: 4.14s, Token/s: 123.57
Epoch: 0, Step: 7003, Batch(micro): 7003, Batch (considering grad accum): 875,  Loss: 6.7113, Time: 3.50s, Token/s: 146.09
Epoch: 0, Step: 7004, Batch(micro): 7004, Batch (considering grad accum): 875,  Loss: 5.7750, Time: 3.51s, Token/s: 146.03
Epoch: 0, Step: 7005, Batch(micro): 7005, Batch (considering grad accum): 875,  Loss: 5.4338, Time: 3.33s, Token/s: 153.74
Epoch: 0, Step: 7006, Batch(micro): 7006, Batch (considering grad accum): 875,  Loss: 5.1038, Time: 3.29s, Token/s: 155.59
Epoch: 0, Step: 7007, Batch(micro): 7007, Batch (considering grad accum): 875,  Loss: 6.3037, Time: 25.18s, Token/s: 20.33
Epoch: 0, Step: 7008, Batch(micro): 7008, Batch (considering grad accum): 876,  Loss: 5.6518, Time: 8.77s, Token/s: 58.35
Epoch: 0, Step: 7009, Batch(micro): 7009, Batch (considering grad accum): 876,  Loss: 5.3772, Time: 3.36s, Token/s: 152.44
Epoch: 0, Step: 7010, Batch(micro): 7010, Batch (considering grad accum): 876,  Loss: 5.8362, Time: 3.17s, Token/s: 161.63
Epoch: 0, Step: 7011, Batch(micro): 7011, Batch (considering grad accum): 876,  Loss: 6.3162, Time: 3.15s, Token/s: 162.31
Epoch: 0, Step: 7012, Batch(micro): 7012, Batch (considering grad accum): 876,  Loss: 6.1561, Time: 3.95s, Token/s: 129.76
Epoch: 0, Step: 7013, Batch(micro): 7013, Batch (considering grad accum): 876,  Loss: 6.2371, Time: 3.57s, Token/s: 143.46
Epoch: 0, Step: 7014, Batch(micro): 7014, Batch (considering grad accum): 876,  Loss: 6.7235, Time: 4.04s, Token/s: 126.73
Epoch: 0, Step: 7015, Batch(micro): 7015, Batch (considering grad accum): 876,  Loss: 4.9357, Time: 23.42s, Token/s: 21.87
Epoch: 0, Step: 7016, Batch(micro): 7016, Batch (considering grad accum): 877,  Loss: 5.4049, Time: 6.75s, Token/s: 75.88
Epoch: 0, Step: 7017, Batch(micro): 7017, Batch (considering grad accum): 877,  Loss: 6.4217, Time: 3.75s, Token/s: 136.45
Epoch: 0, Step: 7018, Batch(micro): 7018, Batch (considering grad accum): 877,  Loss: 5.5111, Time: 3.44s, Token/s: 148.69
Epoch: 0, Step: 7019, Batch(micro): 7019, Batch (considering grad accum): 877,  Loss: 5.5260, Time: 3.48s, Token/s: 147.25
Epoch: 0, Step: 7020, Batch(micro): 7020, Batch (considering grad accum): 877,  Loss: 6.3939, Time: 3.52s, Token/s: 145.55
Epoch: 0, Step: 7021, Batch(micro): 7021, Batch (considering grad accum): 877,  Loss: 5.8228, Time: 4.22s, Token/s: 121.38
Epoch: 0, Step: 7022, Batch(micro): 7022, Batch (considering grad accum): 877,  Loss: 5.7881, Time: 3.45s, Token/s: 148.49
Epoch: 0, Step: 7023, Batch(micro): 7023, Batch (considering grad accum): 877,  Loss: 5.7120, Time: 27.10s, Token/s: 18.89
Epoch: 0, Step: 7024, Batch(micro): 7024, Batch (considering grad accum): 878,  Loss: 6.4370, Time: 7.79s, Token/s: 65.72
Epoch: 0, Step: 7025, Batch(micro): 7025, Batch (considering grad accum): 878,  Loss: 6.5906, Time: 3.72s, Token/s: 137.77
Epoch: 0, Step: 7026, Batch(micro): 7026, Batch (considering grad accum): 878,  Loss: 6.1963, Time: 3.29s, Token/s: 155.41
Epoch: 0, Step: 7027, Batch(micro): 7027, Batch (considering grad accum): 878,  Loss: 6.7178, Time: 3.31s, Token/s: 154.88
Epoch: 0, Step: 7028, Batch(micro): 7028, Batch (considering grad accum): 878,  Loss: 6.5824, Time: 3.36s, Token/s: 152.45
Epoch: 0, Step: 7029, Batch(micro): 7029, Batch (considering grad accum): 878,  Loss: 6.2654, Time: 3.51s, Token/s: 145.92
Epoch: 0, Step: 7030, Batch(micro): 7030, Batch (considering grad accum): 878,  Loss: 6.5090, Time: 3.53s, Token/s: 145.01
Epoch: 0, Step: 7031, Batch(micro): 7031, Batch (considering grad accum): 878,  Loss: 5.7858, Time: 25.08s, Token/s: 20.42
Epoch: 0, Step: 7032, Batch(micro): 7032, Batch (considering grad accum): 879,  Loss: 5.9912, Time: 8.02s, Token/s: 63.86
Epoch: 0, Step: 7033, Batch(micro): 7033, Batch (considering grad accum): 879,  Loss: 6.8057, Time: 3.72s, Token/s: 137.49
Epoch: 0, Step: 7034, Batch(micro): 7034, Batch (considering grad accum): 879,  Loss: 5.8182, Time: 3.20s, Token/s: 159.92
Epoch: 0, Step: 7035, Batch(micro): 7035, Batch (considering grad accum): 879,  Loss: 6.1387, Time: 3.36s, Token/s: 152.31
Epoch: 0, Step: 7036, Batch(micro): 7036, Batch (considering grad accum): 879,  Loss: 6.9277, Time: 3.30s, Token/s: 155.23
Epoch: 0, Step: 7037, Batch(micro): 7037, Batch (considering grad accum): 879,  Loss: 6.2710, Time: 3.32s, Token/s: 154.19
Epoch: 0, Step: 7038, Batch(micro): 7038, Batch (considering grad accum): 879,  Loss: 6.4880, Time: 3.33s, Token/s: 153.76
Epoch: 0, Step: 7039, Batch(micro): 7039, Batch (considering grad accum): 879,  Loss: 5.8157, Time: 25.17s, Token/s: 20.34
Epoch: 0, Step: 7040, Batch(micro): 7040, Batch (considering grad accum): 880,  Loss: 6.8966, Time: 7.80s, Token/s: 65.64
Epoch: 0, Step: 7041, Batch(micro): 7041, Batch (considering grad accum): 880,  Loss: 6.3090, Time: 3.63s, Token/s: 141.20
Epoch: 0, Step: 7042, Batch(micro): 7042, Batch (considering grad accum): 880,  Loss: 6.0760, Time: 3.62s, Token/s: 141.60
Epoch: 0, Step: 7043, Batch(micro): 7043, Batch (considering grad accum): 880,  Loss: 6.6972, Time: 3.67s, Token/s: 139.61
Epoch: 0, Step: 7044, Batch(micro): 7044, Batch (considering grad accum): 880,  Loss: 6.2647, Time: 3.48s, Token/s: 146.93
Epoch: 0, Step: 7045, Batch(micro): 7045, Batch (considering grad accum): 880,  Loss: 6.3937, Time: 3.38s, Token/s: 151.42
Epoch: 0, Step: 7046, Batch(micro): 7046, Batch (considering grad accum): 880,  Loss: 6.4935, Time: 3.65s, Token/s: 140.37
Epoch: 0, Step: 7047, Batch(micro): 7047, Batch (considering grad accum): 880,  Loss: 6.5187, Time: 23.47s, Token/s: 21.81
Epoch: 0, Step: 7048, Batch(micro): 7048, Batch (considering grad accum): 881,  Loss: 6.8311, Time: 6.75s, Token/s: 75.85
Epoch: 0, Step: 7049, Batch(micro): 7049, Batch (considering grad accum): 881,  Loss: 6.3031, Time: 3.67s, Token/s: 139.68
Epoch: 0, Step: 7050, Batch(micro): 7050, Batch (considering grad accum): 881,  Loss: 6.7493, Time: 3.52s, Token/s: 145.33
Epoch: 0, Step: 7051, Batch(micro): 7051, Batch (considering grad accum): 881,  Loss: 6.3696, Time: 3.67s, Token/s: 139.48
Epoch: 0, Step: 7052, Batch(micro): 7052, Batch (considering grad accum): 881,  Loss: 6.5210, Time: 3.84s, Token/s: 133.38
Epoch: 0, Step: 7053, Batch(micro): 7053, Batch (considering grad accum): 881,  Loss: 6.1561, Time: 3.45s, Token/s: 148.33
Epoch: 0, Step: 7054, Batch(micro): 7054, Batch (considering grad accum): 881,  Loss: 6.3376, Time: 3.55s, Token/s: 144.40
Epoch: 0, Step: 7055, Batch(micro): 7055, Batch (considering grad accum): 881,  Loss: 6.3354, Time: 25.31s, Token/s: 20.23
Epoch: 0, Step: 7056, Batch(micro): 7056, Batch (considering grad accum): 882,  Loss: 6.1229, Time: 6.90s, Token/s: 74.20
Epoch: 0, Step: 7057, Batch(micro): 7057, Batch (considering grad accum): 882,  Loss: 5.6474, Time: 3.83s, Token/s: 133.72
Epoch: 0, Step: 7058, Batch(micro): 7058, Batch (considering grad accum): 882,  Loss: 5.5377, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 7059, Batch(micro): 7059, Batch (considering grad accum): 882,  Loss: 6.0069, Time: 3.57s, Token/s: 143.43
Epoch: 0, Step: 7060, Batch(micro): 7060, Batch (considering grad accum): 882,  Loss: 5.9241, Time: 3.46s, Token/s: 148.00
Epoch: 0, Step: 7061, Batch(micro): 7061, Batch (considering grad accum): 882,  Loss: 6.8668, Time: 3.63s, Token/s: 141.07
Epoch: 0, Step: 7062, Batch(micro): 7062, Batch (considering grad accum): 882,  Loss: 6.6450, Time: 3.72s, Token/s: 137.49
Epoch: 0, Step: 7063, Batch(micro): 7063, Batch (considering grad accum): 882,  Loss: 6.1565, Time: 22.35s, Token/s: 22.91
Epoch: 0, Step: 7064, Batch(micro): 7064, Batch (considering grad accum): 883,  Loss: 5.8608, Time: 7.98s, Token/s: 64.16
Epoch: 0, Step: 7065, Batch(micro): 7065, Batch (considering grad accum): 883,  Loss: 6.2383, Time: 3.85s, Token/s: 133.05
Epoch: 0, Step: 7066, Batch(micro): 7066, Batch (considering grad accum): 883,  Loss: 6.8215, Time: 3.64s, Token/s: 140.83
Epoch: 0, Step: 7067, Batch(micro): 7067, Batch (considering grad accum): 883,  Loss: 6.4726, Time: 3.50s, Token/s: 146.25
Epoch: 0, Step: 7068, Batch(micro): 7068, Batch (considering grad accum): 883,  Loss: 6.3422, Time: 3.51s, Token/s: 145.81
Epoch: 0, Step: 7069, Batch(micro): 7069, Batch (considering grad accum): 883,  Loss: 6.1110, Time: 3.31s, Token/s: 154.54
Epoch: 0, Step: 7070, Batch(micro): 7070, Batch (considering grad accum): 883,  Loss: 5.8352, Time: 3.48s, Token/s: 146.92
Epoch: 0, Step: 7071, Batch(micro): 7071, Batch (considering grad accum): 883,  Loss: 6.1283, Time: 23.46s, Token/s: 21.83
Epoch: 0, Step: 7072, Batch(micro): 7072, Batch (considering grad accum): 884,  Loss: 5.9557, Time: 8.68s, Token/s: 59.01
Epoch: 0, Step: 7073, Batch(micro): 7073, Batch (considering grad accum): 884,  Loss: 5.7342, Time: 4.19s, Token/s: 122.21
Epoch: 0, Step: 7074, Batch(micro): 7074, Batch (considering grad accum): 884,  Loss: 6.2479, Time: 3.49s, Token/s: 146.57
Epoch: 0, Step: 7075, Batch(micro): 7075, Batch (considering grad accum): 884,  Loss: 6.0329, Time: 3.50s, Token/s: 146.40
Epoch: 0, Step: 7076, Batch(micro): 7076, Batch (considering grad accum): 884,  Loss: 5.3971, Time: 3.83s, Token/s: 133.83
Epoch: 0, Step: 7077, Batch(micro): 7077, Batch (considering grad accum): 884,  Loss: 6.1776, Time: 3.35s, Token/s: 152.73
Epoch: 0, Step: 7078, Batch(micro): 7078, Batch (considering grad accum): 884,  Loss: 6.8883, Time: 3.65s, Token/s: 140.18
Epoch: 0, Step: 7079, Batch(micro): 7079, Batch (considering grad accum): 884,  Loss: 6.3664, Time: 24.69s, Token/s: 20.74
Epoch: 0, Step: 7080, Batch(micro): 7080, Batch (considering grad accum): 885,  Loss: 6.9715, Time: 7.60s, Token/s: 67.39
Epoch: 0, Step: 7081, Batch(micro): 7081, Batch (considering grad accum): 885,  Loss: 6.1264, Time: 4.49s, Token/s: 114.01
Epoch: 0, Step: 7082, Batch(micro): 7082, Batch (considering grad accum): 885,  Loss: 6.1440, Time: 3.45s, Token/s: 148.58
Epoch: 0, Step: 7083, Batch(micro): 7083, Batch (considering grad accum): 885,  Loss: 6.6882, Time: 3.47s, Token/s: 147.38
Epoch: 0, Step: 7084, Batch(micro): 7084, Batch (considering grad accum): 885,  Loss: 6.6320, Time: 3.44s, Token/s: 149.05
Epoch: 0, Step: 7085, Batch(micro): 7085, Batch (considering grad accum): 885,  Loss: 6.1613, Time: 3.50s, Token/s: 146.14
Epoch: 0, Step: 7086, Batch(micro): 7086, Batch (considering grad accum): 885,  Loss: 5.6673, Time: 3.64s, Token/s: 140.79
Epoch: 0, Step: 7087, Batch(micro): 7087, Batch (considering grad accum): 885,  Loss: 5.9150, Time: 22.31s, Token/s: 22.95
Epoch: 0, Step: 7088, Batch(micro): 7088, Batch (considering grad accum): 886,  Loss: 5.7537, Time: 7.88s, Token/s: 64.95
Epoch: 0, Step: 7089, Batch(micro): 7089, Batch (considering grad accum): 886,  Loss: 5.4789, Time: 4.11s, Token/s: 124.72
Epoch: 0, Step: 7090, Batch(micro): 7090, Batch (considering grad accum): 886,  Loss: 5.8665, Time: 3.31s, Token/s: 154.45
Epoch: 0, Step: 7091, Batch(micro): 7091, Batch (considering grad accum): 886,  Loss: 6.4577, Time: 3.21s, Token/s: 159.68
Epoch: 0, Step: 7092, Batch(micro): 7092, Batch (considering grad accum): 886,  Loss: 6.1446, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 7093, Batch(micro): 7093, Batch (considering grad accum): 886,  Loss: 6.8194, Time: 3.74s, Token/s: 136.95
Epoch: 0, Step: 7094, Batch(micro): 7094, Batch (considering grad accum): 886,  Loss: 6.2769, Time: 3.56s, Token/s: 143.70
Epoch: 0, Step: 7095, Batch(micro): 7095, Batch (considering grad accum): 886,  Loss: 6.5661, Time: 22.45s, Token/s: 22.80
Epoch: 0, Step: 7096, Batch(micro): 7096, Batch (considering grad accum): 887,  Loss: 5.8527, Time: 6.47s, Token/s: 79.08
Epoch: 0, Step: 7097, Batch(micro): 7097, Batch (considering grad accum): 887,  Loss: 5.8794, Time: 4.09s, Token/s: 125.33
Epoch: 0, Step: 7098, Batch(micro): 7098, Batch (considering grad accum): 887,  Loss: 6.1244, Time: 3.52s, Token/s: 145.30
Epoch: 0, Step: 7099, Batch(micro): 7099, Batch (considering grad accum): 887,  Loss: 6.5232, Time: 3.52s, Token/s: 145.50
Updating MLP bias
Epoch: 0, Step: 7100, Batch(micro): 7100, Batch (considering grad accum): 887,  Loss: 6.0593, Time: 3.46s, Token/s: 148.18
Epoch: 0, Step: 7101, Batch(micro): 7101, Batch (considering grad accum): 887,  Loss: 6.2943, Time: 3.35s, Token/s: 152.98
Epoch: 0, Step: 7102, Batch(micro): 7102, Batch (considering grad accum): 887,  Loss: 6.9663, Time: 3.40s, Token/s: 150.66
Epoch: 0, Step: 7103, Batch(micro): 7103, Batch (considering grad accum): 887,  Loss: 6.3411, Time: 21.77s, Token/s: 23.52
Epoch: 0, Step: 7104, Batch(micro): 7104, Batch (considering grad accum): 888,  Loss: 7.1972, Time: 6.02s, Token/s: 85.11
Epoch: 0, Step: 7105, Batch(micro): 7105, Batch (considering grad accum): 888,  Loss: 5.9915, Time: 3.90s, Token/s: 131.36
Epoch: 0, Step: 7106, Batch(micro): 7106, Batch (considering grad accum): 888,  Loss: 6.2153, Time: 3.57s, Token/s: 143.28
Epoch: 0, Step: 7107, Batch(micro): 7107, Batch (considering grad accum): 888,  Loss: 7.5449, Time: 3.43s, Token/s: 149.19
Epoch: 0, Step: 7108, Batch(micro): 7108, Batch (considering grad accum): 888,  Loss: 7.0550, Time: 3.63s, Token/s: 141.07
Epoch: 0, Step: 7109, Batch(micro): 7109, Batch (considering grad accum): 888,  Loss: 6.3724, Time: 3.34s, Token/s: 153.25
Epoch: 0, Step: 7110, Batch(micro): 7110, Batch (considering grad accum): 888,  Loss: 6.2095, Time: 3.33s, Token/s: 153.92
Epoch: 0, Step: 7111, Batch(micro): 7111, Batch (considering grad accum): 888,  Loss: 6.0505, Time: 24.07s, Token/s: 21.27
Epoch: 0, Step: 7112, Batch(micro): 7112, Batch (considering grad accum): 889,  Loss: 6.3132, Time: 7.95s, Token/s: 64.37
Epoch: 0, Step: 7113, Batch(micro): 7113, Batch (considering grad accum): 889,  Loss: 5.9846, Time: 3.92s, Token/s: 130.64
Epoch: 0, Step: 7114, Batch(micro): 7114, Batch (considering grad accum): 889,  Loss: 6.2027, Time: 3.52s, Token/s: 145.29
Epoch: 0, Step: 7115, Batch(micro): 7115, Batch (considering grad accum): 889,  Loss: 5.9838, Time: 3.22s, Token/s: 159.12
Epoch: 0, Step: 7116, Batch(micro): 7116, Batch (considering grad accum): 889,  Loss: 5.9016, Time: 3.21s, Token/s: 159.51
Epoch: 0, Step: 7117, Batch(micro): 7117, Batch (considering grad accum): 889,  Loss: 5.8870, Time: 3.45s, Token/s: 148.35
Epoch: 0, Step: 7118, Batch(micro): 7118, Batch (considering grad accum): 889,  Loss: 6.4591, Time: 3.58s, Token/s: 142.94
Epoch: 0, Step: 7119, Batch(micro): 7119, Batch (considering grad accum): 889,  Loss: 5.9076, Time: 22.75s, Token/s: 22.51
Epoch: 0, Step: 7120, Batch(micro): 7120, Batch (considering grad accum): 890,  Loss: 6.1794, Time: 8.28s, Token/s: 61.80
Epoch: 0, Step: 7121, Batch(micro): 7121, Batch (considering grad accum): 890,  Loss: 6.3619, Time: 3.80s, Token/s: 134.76
Epoch: 0, Step: 7122, Batch(micro): 7122, Batch (considering grad accum): 890,  Loss: 6.2590, Time: 3.43s, Token/s: 149.15
Epoch: 0, Step: 7123, Batch(micro): 7123, Batch (considering grad accum): 890,  Loss: 6.1196, Time: 3.71s, Token/s: 137.83
Epoch: 0, Step: 7124, Batch(micro): 7124, Batch (considering grad accum): 890,  Loss: 6.2689, Time: 3.49s, Token/s: 146.69
Epoch: 0, Step: 7125, Batch(micro): 7125, Batch (considering grad accum): 890,  Loss: 5.9750, Time: 3.69s, Token/s: 138.92
Epoch: 0, Step: 7126, Batch(micro): 7126, Batch (considering grad accum): 890,  Loss: 6.2071, Time: 3.49s, Token/s: 146.56
Epoch: 0, Step: 7127, Batch(micro): 7127, Batch (considering grad accum): 890,  Loss: 6.0503, Time: 23.48s, Token/s: 21.81
Epoch: 0, Step: 7128, Batch(micro): 7128, Batch (considering grad accum): 891,  Loss: 5.8373, Time: 6.87s, Token/s: 74.52
Epoch: 0, Step: 7129, Batch(micro): 7129, Batch (considering grad accum): 891,  Loss: 6.5624, Time: 3.72s, Token/s: 137.75
Epoch: 0, Step: 7130, Batch(micro): 7130, Batch (considering grad accum): 891,  Loss: 5.9169, Time: 3.24s, Token/s: 157.78
Epoch: 0, Step: 7131, Batch(micro): 7131, Batch (considering grad accum): 891,  Loss: 6.1629, Time: 3.17s, Token/s: 161.35
Epoch: 0, Step: 7132, Batch(micro): 7132, Batch (considering grad accum): 891,  Loss: 6.3391, Time: 3.18s, Token/s: 160.87
Epoch: 0, Step: 7133, Batch(micro): 7133, Batch (considering grad accum): 891,  Loss: 5.8682, Time: 3.48s, Token/s: 147.04
Epoch: 0, Step: 7134, Batch(micro): 7134, Batch (considering grad accum): 891,  Loss: 5.8616, Time: 3.52s, Token/s: 145.37
Epoch: 0, Step: 7135, Batch(micro): 7135, Batch (considering grad accum): 891,  Loss: 6.3663, Time: 23.56s, Token/s: 21.73
Epoch: 0, Step: 7136, Batch(micro): 7136, Batch (considering grad accum): 892,  Loss: 6.5529, Time: 6.71s, Token/s: 76.31
Epoch: 0, Step: 7137, Batch(micro): 7137, Batch (considering grad accum): 892,  Loss: 5.9666, Time: 3.66s, Token/s: 139.84
Epoch: 0, Step: 7138, Batch(micro): 7138, Batch (considering grad accum): 892,  Loss: 6.3991, Time: 3.49s, Token/s: 146.58
Epoch: 0, Step: 7139, Batch(micro): 7139, Batch (considering grad accum): 892,  Loss: 6.9484, Time: 3.59s, Token/s: 142.72
Epoch: 0, Step: 7140, Batch(micro): 7140, Batch (considering grad accum): 892,  Loss: 6.0742, Time: 3.54s, Token/s: 144.62
Epoch: 0, Step: 7141, Batch(micro): 7141, Batch (considering grad accum): 892,  Loss: 6.9579, Time: 4.24s, Token/s: 120.68
Epoch: 0, Step: 7142, Batch(micro): 7142, Batch (considering grad accum): 892,  Loss: 5.8637, Time: 3.81s, Token/s: 134.47
Epoch: 0, Step: 7143, Batch(micro): 7143, Batch (considering grad accum): 892,  Loss: 6.0697, Time: 22.82s, Token/s: 22.44
Epoch: 0, Step: 7144, Batch(micro): 7144, Batch (considering grad accum): 893,  Loss: 5.4271, Time: 6.97s, Token/s: 73.44
Epoch: 0, Step: 7145, Batch(micro): 7145, Batch (considering grad accum): 893,  Loss: 5.8790, Time: 4.20s, Token/s: 121.82
Epoch: 0, Step: 7146, Batch(micro): 7146, Batch (considering grad accum): 893,  Loss: 5.8951, Time: 3.66s, Token/s: 139.96
Epoch: 0, Step: 7147, Batch(micro): 7147, Batch (considering grad accum): 893,  Loss: 5.9721, Time: 4.11s, Token/s: 124.44
Epoch: 0, Step: 7148, Batch(micro): 7148, Batch (considering grad accum): 893,  Loss: 5.9312, Time: 3.50s, Token/s: 146.47
Epoch: 0, Step: 7149, Batch(micro): 7149, Batch (considering grad accum): 893,  Loss: 5.8445, Time: 3.62s, Token/s: 141.37
Epoch: 0, Step: 7150, Batch(micro): 7150, Batch (considering grad accum): 893,  Loss: 6.3419, Time: 3.52s, Token/s: 145.45
Epoch: 0, Step: 7151, Batch(micro): 7151, Batch (considering grad accum): 893,  Loss: 6.3649, Time: 21.38s, Token/s: 23.95
Epoch: 0, Step: 7152, Batch(micro): 7152, Batch (considering grad accum): 894,  Loss: 6.8895, Time: 6.93s, Token/s: 73.90
Epoch: 0, Step: 7153, Batch(micro): 7153, Batch (considering grad accum): 894,  Loss: 6.8225, Time: 3.80s, Token/s: 134.90
Epoch: 0, Step: 7154, Batch(micro): 7154, Batch (considering grad accum): 894,  Loss: 5.6942, Time: 3.61s, Token/s: 141.80
Epoch: 0, Step: 7155, Batch(micro): 7155, Batch (considering grad accum): 894,  Loss: 6.2611, Time: 3.76s, Token/s: 136.03
Epoch: 0, Step: 7156, Batch(micro): 7156, Batch (considering grad accum): 894,  Loss: 6.0422, Time: 3.58s, Token/s: 143.15
Epoch: 0, Step: 7157, Batch(micro): 7157, Batch (considering grad accum): 894,  Loss: 6.3286, Time: 3.32s, Token/s: 154.13
Epoch: 0, Step: 7158, Batch(micro): 7158, Batch (considering grad accum): 894,  Loss: 6.4241, Time: 3.49s, Token/s: 146.91
Epoch: 0, Step: 7159, Batch(micro): 7159, Batch (considering grad accum): 894,  Loss: 6.4280, Time: 18.03s, Token/s: 28.40
Epoch: 0, Step: 7160, Batch(micro): 7160, Batch (considering grad accum): 895,  Loss: 6.6294, Time: 7.05s, Token/s: 72.61
Epoch: 0, Step: 7161, Batch(micro): 7161, Batch (considering grad accum): 895,  Loss: 5.2849, Time: 4.05s, Token/s: 126.38
Epoch: 0, Step: 7162, Batch(micro): 7162, Batch (considering grad accum): 895,  Loss: 6.1366, Time: 3.57s, Token/s: 143.29
Epoch: 0, Step: 7163, Batch(micro): 7163, Batch (considering grad accum): 895,  Loss: 5.1437, Time: 3.60s, Token/s: 142.34
Epoch: 0, Step: 7164, Batch(micro): 7164, Batch (considering grad accum): 895,  Loss: 5.3031, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 7165, Batch(micro): 7165, Batch (considering grad accum): 895,  Loss: 6.0562, Time: 3.63s, Token/s: 141.12
Epoch: 0, Step: 7166, Batch(micro): 7166, Batch (considering grad accum): 895,  Loss: 6.8344, Time: 3.76s, Token/s: 136.22
Epoch: 0, Step: 7167, Batch(micro): 7167, Batch (considering grad accum): 895,  Loss: 5.8073, Time: 18.86s, Token/s: 27.15
Epoch: 0, Step: 7168, Batch(micro): 7168, Batch (considering grad accum): 896,  Loss: 5.8105, Time: 6.61s, Token/s: 77.50
Epoch: 0, Step: 7169, Batch(micro): 7169, Batch (considering grad accum): 896,  Loss: 5.8151, Time: 3.61s, Token/s: 141.96
Epoch: 0, Step: 7170, Batch(micro): 7170, Batch (considering grad accum): 896,  Loss: 5.5895, Time: 3.32s, Token/s: 154.18
Epoch: 0, Step: 7171, Batch(micro): 7171, Batch (considering grad accum): 896,  Loss: 6.1128, Time: 3.45s, Token/s: 148.55
Epoch: 0, Step: 7172, Batch(micro): 7172, Batch (considering grad accum): 896,  Loss: 6.4141, Time: 3.68s, Token/s: 139.00
Epoch: 0, Step: 7173, Batch(micro): 7173, Batch (considering grad accum): 896,  Loss: 5.5909, Time: 3.28s, Token/s: 156.29
Epoch: 0, Step: 7174, Batch(micro): 7174, Batch (considering grad accum): 896,  Loss: 5.6974, Time: 3.32s, Token/s: 154.36
Epoch: 0, Step: 7175, Batch(micro): 7175, Batch (considering grad accum): 896,  Loss: 5.9403, Time: 17.57s, Token/s: 29.14
Epoch: 0, Step: 7176, Batch(micro): 7176, Batch (considering grad accum): 897,  Loss: 6.1070, Time: 6.54s, Token/s: 78.27
Epoch: 0, Step: 7177, Batch(micro): 7177, Batch (considering grad accum): 897,  Loss: 7.1494, Time: 3.88s, Token/s: 132.01
Epoch: 0, Step: 7178, Batch(micro): 7178, Batch (considering grad accum): 897,  Loss: 7.4589, Time: 3.59s, Token/s: 142.64
Epoch: 0, Step: 7179, Batch(micro): 7179, Batch (considering grad accum): 897,  Loss: 7.1107, Time: 3.83s, Token/s: 133.63
Epoch: 0, Step: 7180, Batch(micro): 7180, Batch (considering grad accum): 897,  Loss: 6.1863, Time: 3.44s, Token/s: 148.83
Epoch: 0, Step: 7181, Batch(micro): 7181, Batch (considering grad accum): 897,  Loss: 5.8179, Time: 3.86s, Token/s: 132.77
Epoch: 0, Step: 7182, Batch(micro): 7182, Batch (considering grad accum): 897,  Loss: 5.7353, Time: 3.69s, Token/s: 138.78
Epoch: 0, Step: 7183, Batch(micro): 7183, Batch (considering grad accum): 897,  Loss: 5.8095, Time: 18.01s, Token/s: 28.43
Epoch: 0, Step: 7184, Batch(micro): 7184, Batch (considering grad accum): 898,  Loss: 6.1442, Time: 6.53s, Token/s: 78.40
Epoch: 0, Step: 7185, Batch(micro): 7185, Batch (considering grad accum): 898,  Loss: 6.1315, Time: 4.00s, Token/s: 127.92
Epoch: 0, Step: 7186, Batch(micro): 7186, Batch (considering grad accum): 898,  Loss: 6.0704, Time: 3.55s, Token/s: 144.33
Epoch: 0, Step: 7187, Batch(micro): 7187, Batch (considering grad accum): 898,  Loss: 5.5435, Time: 3.47s, Token/s: 147.60
Epoch: 0, Step: 7188, Batch(micro): 7188, Batch (considering grad accum): 898,  Loss: 6.1600, Time: 3.50s, Token/s: 146.35
Epoch: 0, Step: 7189, Batch(micro): 7189, Batch (considering grad accum): 898,  Loss: 5.0868, Time: 3.57s, Token/s: 143.37
Epoch: 0, Step: 7190, Batch(micro): 7190, Batch (considering grad accum): 898,  Loss: 5.7229, Time: 3.58s, Token/s: 142.95
Epoch: 0, Step: 7191, Batch(micro): 7191, Batch (considering grad accum): 898,  Loss: 5.9366, Time: 18.26s, Token/s: 28.04
Epoch: 0, Step: 7192, Batch(micro): 7192, Batch (considering grad accum): 899,  Loss: 6.0881, Time: 6.85s, Token/s: 74.76
Epoch: 0, Step: 7193, Batch(micro): 7193, Batch (considering grad accum): 899,  Loss: 6.0559, Time: 3.64s, Token/s: 140.77
Epoch: 0, Step: 7194, Batch(micro): 7194, Batch (considering grad accum): 899,  Loss: 6.1968, Time: 3.20s, Token/s: 159.82
Epoch: 0, Step: 7195, Batch(micro): 7195, Batch (considering grad accum): 899,  Loss: 6.6249, Time: 3.21s, Token/s: 159.70
Epoch: 0, Step: 7196, Batch(micro): 7196, Batch (considering grad accum): 899,  Loss: 6.6501, Time: 3.21s, Token/s: 159.30
Epoch: 0, Step: 7197, Batch(micro): 7197, Batch (considering grad accum): 899,  Loss: 6.3294, Time: 3.48s, Token/s: 146.98
Epoch: 0, Step: 7198, Batch(micro): 7198, Batch (considering grad accum): 899,  Loss: 6.9072, Time: 3.67s, Token/s: 139.41
Epoch: 0, Step: 7199, Batch(micro): 7199, Batch (considering grad accum): 899,  Loss: 7.1642, Time: 19.03s, Token/s: 26.91
Updating MLP bias
Epoch: 0, Step: 7200, Batch(micro): 7200, Batch (considering grad accum): 900,  Loss: 6.5326, Time: 7.47s, Token/s: 68.52
Epoch: 0, Step: 7201, Batch(micro): 7201, Batch (considering grad accum): 900,  Loss: 6.1665, Time: 3.39s, Token/s: 150.90
Epoch: 0, Step: 7202, Batch(micro): 7202, Batch (considering grad accum): 900,  Loss: 6.3007, Time: 2.84s, Token/s: 180.49
Epoch: 0, Step: 7203, Batch(micro): 7203, Batch (considering grad accum): 900,  Loss: 6.1257, Time: 2.83s, Token/s: 181.10
Epoch: 0, Step: 7204, Batch(micro): 7204, Batch (considering grad accum): 900,  Loss: 6.2656, Time: 2.89s, Token/s: 177.07
Epoch: 0, Step: 7205, Batch(micro): 7205, Batch (considering grad accum): 900,  Loss: 6.3483, Time: 2.84s, Token/s: 180.37
Epoch: 0, Step: 7206, Batch(micro): 7206, Batch (considering grad accum): 900,  Loss: 5.7732, Time: 2.77s, Token/s: 184.84
Epoch: 0, Step: 7207, Batch(micro): 7207, Batch (considering grad accum): 900,  Loss: 5.3821, Time: 18.91s, Token/s: 27.07
Epoch: 0, Step: 7208, Batch(micro): 7208, Batch (considering grad accum): 901,  Loss: 5.6782, Time: 7.17s, Token/s: 71.42
Epoch: 0, Step: 7209, Batch(micro): 7209, Batch (considering grad accum): 901,  Loss: 5.9056, Time: 3.26s, Token/s: 157.10
Epoch: 0, Step: 7210, Batch(micro): 7210, Batch (considering grad accum): 901,  Loss: 5.9172, Time: 3.01s, Token/s: 170.29
Epoch: 0, Step: 7211, Batch(micro): 7211, Batch (considering grad accum): 901,  Loss: 6.5994, Time: 2.99s, Token/s: 171.48
Epoch: 0, Step: 7212, Batch(micro): 7212, Batch (considering grad accum): 901,  Loss: 5.9882, Time: 2.73s, Token/s: 187.41
Epoch: 0, Step: 7213, Batch(micro): 7213, Batch (considering grad accum): 901,  Loss: 6.0986, Time: 3.38s, Token/s: 151.53
Epoch: 0, Step: 7214, Batch(micro): 7214, Batch (considering grad accum): 901,  Loss: 5.7538, Time: 3.51s, Token/s: 146.06
Epoch: 0, Step: 7215, Batch(micro): 7215, Batch (considering grad accum): 901,  Loss: 5.7029, Time: 19.93s, Token/s: 25.69
Epoch: 0, Step: 7216, Batch(micro): 7216, Batch (considering grad accum): 902,  Loss: 6.4818, Time: 6.68s, Token/s: 76.68
Epoch: 0, Step: 7217, Batch(micro): 7217, Batch (considering grad accum): 902,  Loss: 6.0644, Time: 3.69s, Token/s: 138.91
Epoch: 0, Step: 7218, Batch(micro): 7218, Batch (considering grad accum): 902,  Loss: 6.2605, Time: 3.11s, Token/s: 164.49
Epoch: 0, Step: 7219, Batch(micro): 7219, Batch (considering grad accum): 902,  Loss: 7.0695, Time: 2.69s, Token/s: 190.57
Epoch: 0, Step: 7220, Batch(micro): 7220, Batch (considering grad accum): 902,  Loss: 7.6459, Time: 2.85s, Token/s: 179.66
Epoch: 0, Step: 7221, Batch(micro): 7221, Batch (considering grad accum): 902,  Loss: 5.9111, Time: 2.98s, Token/s: 171.77
Epoch: 0, Step: 7222, Batch(micro): 7222, Batch (considering grad accum): 902,  Loss: 6.1896, Time: 2.77s, Token/s: 184.61
Epoch: 0, Step: 7223, Batch(micro): 7223, Batch (considering grad accum): 902,  Loss: 6.4444, Time: 21.58s, Token/s: 23.73
Epoch: 0, Step: 7224, Batch(micro): 7224, Batch (considering grad accum): 903,  Loss: 6.1606, Time: 7.22s, Token/s: 70.87
Epoch: 0, Step: 7225, Batch(micro): 7225, Batch (considering grad accum): 903,  Loss: 6.1832, Time: 3.80s, Token/s: 134.65
Epoch: 0, Step: 7226, Batch(micro): 7226, Batch (considering grad accum): 903,  Loss: 5.6509, Time: 3.37s, Token/s: 151.89
Epoch: 0, Step: 7227, Batch(micro): 7227, Batch (considering grad accum): 903,  Loss: 5.7714, Time: 3.26s, Token/s: 157.11
Epoch: 0, Step: 7228, Batch(micro): 7228, Batch (considering grad accum): 903,  Loss: 6.3231, Time: 3.20s, Token/s: 159.85
Epoch: 0, Step: 7229, Batch(micro): 7229, Batch (considering grad accum): 903,  Loss: 6.3050, Time: 3.22s, Token/s: 158.93
Epoch: 0, Step: 7230, Batch(micro): 7230, Batch (considering grad accum): 903,  Loss: 6.1776, Time: 3.31s, Token/s: 154.73
Epoch: 0, Step: 7231, Batch(micro): 7231, Batch (considering grad accum): 903,  Loss: 6.4488, Time: 26.18s, Token/s: 19.56
Epoch: 0, Step: 7232, Batch(micro): 7232, Batch (considering grad accum): 904,  Loss: 6.2797, Time: 7.75s, Token/s: 66.06
Epoch: 0, Step: 7233, Batch(micro): 7233, Batch (considering grad accum): 904,  Loss: 7.0720, Time: 3.72s, Token/s: 137.53
Epoch: 0, Step: 7234, Batch(micro): 7234, Batch (considering grad accum): 904,  Loss: 6.0326, Time: 3.49s, Token/s: 146.70
Epoch: 0, Step: 7235, Batch(micro): 7235, Batch (considering grad accum): 904,  Loss: 5.7308, Time: 3.58s, Token/s: 142.99
Epoch: 0, Step: 7236, Batch(micro): 7236, Batch (considering grad accum): 904,  Loss: 5.3720, Time: 3.54s, Token/s: 144.57
Epoch: 0, Step: 7237, Batch(micro): 7237, Batch (considering grad accum): 904,  Loss: 5.5280, Time: 3.49s, Token/s: 146.57
Epoch: 0, Step: 7238, Batch(micro): 7238, Batch (considering grad accum): 904,  Loss: 5.5944, Time: 3.40s, Token/s: 150.58
Epoch: 0, Step: 7239, Batch(micro): 7239, Batch (considering grad accum): 904,  Loss: 5.7754, Time: 24.15s, Token/s: 21.20
Epoch: 0, Step: 7240, Batch(micro): 7240, Batch (considering grad accum): 905,  Loss: 7.1959, Time: 7.25s, Token/s: 70.62
Epoch: 0, Step: 7241, Batch(micro): 7241, Batch (considering grad accum): 905,  Loss: 6.3609, Time: 3.72s, Token/s: 137.56
Epoch: 0, Step: 7242, Batch(micro): 7242, Batch (considering grad accum): 905,  Loss: 5.8026, Time: 3.49s, Token/s: 146.72
Epoch: 0, Step: 7243, Batch(micro): 7243, Batch (considering grad accum): 905,  Loss: 6.0151, Time: 3.71s, Token/s: 138.17
Epoch: 0, Step: 7244, Batch(micro): 7244, Batch (considering grad accum): 905,  Loss: 5.3336, Time: 3.72s, Token/s: 137.68
Epoch: 0, Step: 7245, Batch(micro): 7245, Batch (considering grad accum): 905,  Loss: 5.6514, Time: 3.34s, Token/s: 153.25
Epoch: 0, Step: 7246, Batch(micro): 7246, Batch (considering grad accum): 905,  Loss: 5.9012, Time: 3.42s, Token/s: 149.67
Epoch: 0, Step: 7247, Batch(micro): 7247, Batch (considering grad accum): 905,  Loss: 6.0125, Time: 23.94s, Token/s: 21.39
Epoch: 0, Step: 7248, Batch(micro): 7248, Batch (considering grad accum): 906,  Loss: 6.0232, Time: 6.98s, Token/s: 73.39
Epoch: 0, Step: 7249, Batch(micro): 7249, Batch (considering grad accum): 906,  Loss: 5.6292, Time: 3.95s, Token/s: 129.59
Epoch: 0, Step: 7250, Batch(micro): 7250, Batch (considering grad accum): 906,  Loss: 6.3487, Time: 3.58s, Token/s: 143.17
Epoch: 0, Step: 7251, Batch(micro): 7251, Batch (considering grad accum): 906,  Loss: 6.9380, Time: 3.71s, Token/s: 137.99
Epoch: 0, Step: 7252, Batch(micro): 7252, Batch (considering grad accum): 906,  Loss: 5.9378, Time: 3.78s, Token/s: 135.49
Epoch: 0, Step: 7253, Batch(micro): 7253, Batch (considering grad accum): 906,  Loss: 5.6933, Time: 3.71s, Token/s: 137.87
Epoch: 0, Step: 7254, Batch(micro): 7254, Batch (considering grad accum): 906,  Loss: 6.0591, Time: 3.52s, Token/s: 145.50
Epoch: 0, Step: 7255, Batch(micro): 7255, Batch (considering grad accum): 906,  Loss: 6.4574, Time: 23.57s, Token/s: 21.72
Epoch: 0, Step: 7256, Batch(micro): 7256, Batch (considering grad accum): 907,  Loss: 6.2384, Time: 8.28s, Token/s: 61.81
Epoch: 0, Step: 7257, Batch(micro): 7257, Batch (considering grad accum): 907,  Loss: 6.5069, Time: 3.90s, Token/s: 131.25
Epoch: 0, Step: 7258, Batch(micro): 7258, Batch (considering grad accum): 907,  Loss: 6.1631, Time: 3.47s, Token/s: 147.60
Epoch: 0, Step: 7259, Batch(micro): 7259, Batch (considering grad accum): 907,  Loss: 6.5895, Time: 3.46s, Token/s: 147.85
Epoch: 0, Step: 7260, Batch(micro): 7260, Batch (considering grad accum): 907,  Loss: 6.6530, Time: 3.36s, Token/s: 152.60
Epoch: 0, Step: 7261, Batch(micro): 7261, Batch (considering grad accum): 907,  Loss: 6.2239, Time: 4.20s, Token/s: 121.87
Epoch: 0, Step: 7262, Batch(micro): 7262, Batch (considering grad accum): 907,  Loss: 6.7248, Time: 3.41s, Token/s: 149.93
Epoch: 0, Step: 7263, Batch(micro): 7263, Batch (considering grad accum): 907,  Loss: 6.8470, Time: 22.32s, Token/s: 22.94
Epoch: 0, Step: 7264, Batch(micro): 7264, Batch (considering grad accum): 908,  Loss: 5.7178, Time: 6.57s, Token/s: 77.97
Epoch: 0, Step: 7265, Batch(micro): 7265, Batch (considering grad accum): 908,  Loss: 6.0581, Time: 4.41s, Token/s: 116.06
Epoch: 0, Step: 7266, Batch(micro): 7266, Batch (considering grad accum): 908,  Loss: 5.7569, Time: 3.69s, Token/s: 138.59
Epoch: 0, Step: 7267, Batch(micro): 7267, Batch (considering grad accum): 908,  Loss: 5.6062, Time: 3.60s, Token/s: 142.09
Epoch: 0, Step: 7268, Batch(micro): 7268, Batch (considering grad accum): 908,  Loss: 6.1343, Time: 3.58s, Token/s: 143.19
Epoch: 0, Step: 7269, Batch(micro): 7269, Batch (considering grad accum): 908,  Loss: 6.1535, Time: 3.88s, Token/s: 131.86
Epoch: 0, Step: 7270, Batch(micro): 7270, Batch (considering grad accum): 908,  Loss: 6.1722, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 7271, Batch(micro): 7271, Batch (considering grad accum): 908,  Loss: 6.2814, Time: 23.17s, Token/s: 22.10
Epoch: 0, Step: 7272, Batch(micro): 7272, Batch (considering grad accum): 909,  Loss: 5.7721, Time: 7.75s, Token/s: 66.03
Epoch: 0, Step: 7273, Batch(micro): 7273, Batch (considering grad accum): 909,  Loss: 6.0300, Time: 3.85s, Token/s: 133.11
Epoch: 0, Step: 7274, Batch(micro): 7274, Batch (considering grad accum): 909,  Loss: 6.0129, Time: 3.39s, Token/s: 150.91
Epoch: 0, Step: 7275, Batch(micro): 7275, Batch (considering grad accum): 909,  Loss: 6.1028, Time: 3.54s, Token/s: 144.83
Epoch: 0, Step: 7276, Batch(micro): 7276, Batch (considering grad accum): 909,  Loss: 5.9196, Time: 3.41s, Token/s: 150.22
Epoch: 0, Step: 7277, Batch(micro): 7277, Batch (considering grad accum): 909,  Loss: 6.1624, Time: 3.70s, Token/s: 138.26
Epoch: 0, Step: 7278, Batch(micro): 7278, Batch (considering grad accum): 909,  Loss: 6.1743, Time: 3.53s, Token/s: 145.05
Epoch: 0, Step: 7279, Batch(micro): 7279, Batch (considering grad accum): 909,  Loss: 5.5777, Time: 23.56s, Token/s: 21.73
Epoch: 0, Step: 7280, Batch(micro): 7280, Batch (considering grad accum): 910,  Loss: 6.1781, Time: 8.23s, Token/s: 62.19
Epoch: 0, Step: 7281, Batch(micro): 7281, Batch (considering grad accum): 910,  Loss: 5.5686, Time: 3.44s, Token/s: 148.76
Epoch: 0, Step: 7282, Batch(micro): 7282, Batch (considering grad accum): 910,  Loss: 5.1801, Time: 3.52s, Token/s: 145.47
Epoch: 0, Step: 7283, Batch(micro): 7283, Batch (considering grad accum): 910,  Loss: 4.9637, Time: 3.55s, Token/s: 144.23
Epoch: 0, Step: 7284, Batch(micro): 7284, Batch (considering grad accum): 910,  Loss: 5.7968, Time: 4.00s, Token/s: 127.86
Epoch: 0, Step: 7285, Batch(micro): 7285, Batch (considering grad accum): 910,  Loss: 5.7314, Time: 3.55s, Token/s: 144.22
Epoch: 0, Step: 7286, Batch(micro): 7286, Batch (considering grad accum): 910,  Loss: 6.5634, Time: 3.64s, Token/s: 140.82
Epoch: 0, Step: 7287, Batch(micro): 7287, Batch (considering grad accum): 910,  Loss: 6.5629, Time: 22.87s, Token/s: 22.39
Epoch: 0, Step: 7288, Batch(micro): 7288, Batch (considering grad accum): 911,  Loss: 5.8793, Time: 7.37s, Token/s: 69.50
Epoch: 0, Step: 7289, Batch(micro): 7289, Batch (considering grad accum): 911,  Loss: 5.9495, Time: 3.78s, Token/s: 135.28
Epoch: 0, Step: 7290, Batch(micro): 7290, Batch (considering grad accum): 911,  Loss: 5.9484, Time: 3.50s, Token/s: 146.32
Epoch: 0, Step: 7291, Batch(micro): 7291, Batch (considering grad accum): 911,  Loss: 5.8291, Time: 3.42s, Token/s: 149.67
Epoch: 0, Step: 7292, Batch(micro): 7292, Batch (considering grad accum): 911,  Loss: 6.7732, Time: 3.65s, Token/s: 140.14
Epoch: 0, Step: 7293, Batch(micro): 7293, Batch (considering grad accum): 911,  Loss: 6.6997, Time: 3.75s, Token/s: 136.39
Epoch: 0, Step: 7294, Batch(micro): 7294, Batch (considering grad accum): 911,  Loss: 6.7885, Time: 3.59s, Token/s: 142.61
Epoch: 0, Step: 7295, Batch(micro): 7295, Batch (considering grad accum): 911,  Loss: 6.5187, Time: 23.50s, Token/s: 21.79
Epoch: 0, Step: 7296, Batch(micro): 7296, Batch (considering grad accum): 912,  Loss: 6.1852, Time: 7.70s, Token/s: 66.47
Epoch: 0, Step: 7297, Batch(micro): 7297, Batch (considering grad accum): 912,  Loss: 6.0207, Time: 3.92s, Token/s: 130.57
Epoch: 0, Step: 7298, Batch(micro): 7298, Batch (considering grad accum): 912,  Loss: 6.0342, Time: 3.73s, Token/s: 137.39
Epoch: 0, Step: 7299, Batch(micro): 7299, Batch (considering grad accum): 912,  Loss: 5.5105, Time: 3.93s, Token/s: 130.19
Updating MLP bias
Epoch: 0, Step: 7300, Batch(micro): 7300, Batch (considering grad accum): 912,  Loss: 5.6342, Time: 3.44s, Token/s: 148.95
Epoch: 0, Step: 7301, Batch(micro): 7301, Batch (considering grad accum): 912,  Loss: 6.7813, Time: 3.63s, Token/s: 141.13
Epoch: 0, Step: 7302, Batch(micro): 7302, Batch (considering grad accum): 912,  Loss: 6.4636, Time: 3.83s, Token/s: 133.69
Epoch: 0, Step: 7303, Batch(micro): 7303, Batch (considering grad accum): 912,  Loss: 5.7003, Time: 22.05s, Token/s: 23.22
Epoch: 0, Step: 7304, Batch(micro): 7304, Batch (considering grad accum): 913,  Loss: 6.1371, Time: 6.19s, Token/s: 82.76
Epoch: 0, Step: 7305, Batch(micro): 7305, Batch (considering grad accum): 913,  Loss: 5.9636, Time: 3.86s, Token/s: 132.77
Epoch: 0, Step: 7306, Batch(micro): 7306, Batch (considering grad accum): 913,  Loss: 5.9915, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 7307, Batch(micro): 7307, Batch (considering grad accum): 913,  Loss: 5.8289, Time: 3.59s, Token/s: 142.75
Epoch: 0, Step: 7308, Batch(micro): 7308, Batch (considering grad accum): 913,  Loss: 6.9442, Time: 3.69s, Token/s: 138.59
Epoch: 0, Step: 7309, Batch(micro): 7309, Batch (considering grad accum): 913,  Loss: 6.3935, Time: 3.20s, Token/s: 160.19
Epoch: 0, Step: 7310, Batch(micro): 7310, Batch (considering grad accum): 913,  Loss: 6.1993, Time: 3.22s, Token/s: 159.20
Epoch: 0, Step: 7311, Batch(micro): 7311, Batch (considering grad accum): 913,  Loss: 5.2898, Time: 23.45s, Token/s: 21.83
Epoch: 0, Step: 7312, Batch(micro): 7312, Batch (considering grad accum): 914,  Loss: 5.7388, Time: 8.75s, Token/s: 58.53
Epoch: 0, Step: 7313, Batch(micro): 7313, Batch (considering grad accum): 914,  Loss: 5.8412, Time: 3.93s, Token/s: 130.26
Epoch: 0, Step: 7314, Batch(micro): 7314, Batch (considering grad accum): 914,  Loss: 6.0127, Time: 3.57s, Token/s: 143.45
Epoch: 0, Step: 7315, Batch(micro): 7315, Batch (considering grad accum): 914,  Loss: 5.6780, Time: 3.38s, Token/s: 151.66
Epoch: 0, Step: 7316, Batch(micro): 7316, Batch (considering grad accum): 914,  Loss: 5.7518, Time: 3.26s, Token/s: 157.09
Epoch: 0, Step: 7317, Batch(micro): 7317, Batch (considering grad accum): 914,  Loss: 5.9097, Time: 3.17s, Token/s: 161.26
Epoch: 0, Step: 7318, Batch(micro): 7318, Batch (considering grad accum): 914,  Loss: 6.0447, Time: 3.21s, Token/s: 159.61
Epoch: 0, Step: 7319, Batch(micro): 7319, Batch (considering grad accum): 914,  Loss: 6.1046, Time: 25.18s, Token/s: 20.34
Epoch: 0, Step: 7320, Batch(micro): 7320, Batch (considering grad accum): 915,  Loss: 6.3172, Time: 7.63s, Token/s: 67.14
Epoch: 0, Step: 7321, Batch(micro): 7321, Batch (considering grad accum): 915,  Loss: 6.9189, Time: 3.98s, Token/s: 128.70
Epoch: 0, Step: 7322, Batch(micro): 7322, Batch (considering grad accum): 915,  Loss: 6.3040, Time: 3.85s, Token/s: 133.07
Epoch: 0, Step: 7323, Batch(micro): 7323, Batch (considering grad accum): 915,  Loss: 6.0774, Time: 3.70s, Token/s: 138.38
Epoch: 0, Step: 7324, Batch(micro): 7324, Batch (considering grad accum): 915,  Loss: 6.4073, Time: 3.54s, Token/s: 144.84
Epoch: 0, Step: 7325, Batch(micro): 7325, Batch (considering grad accum): 915,  Loss: 6.0409, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 7326, Batch(micro): 7326, Batch (considering grad accum): 915,  Loss: 5.8830, Time: 3.49s, Token/s: 146.63
Epoch: 0, Step: 7327, Batch(micro): 7327, Batch (considering grad accum): 915,  Loss: 5.9525, Time: 22.69s, Token/s: 22.56
Epoch: 0, Step: 7328, Batch(micro): 7328, Batch (considering grad accum): 916,  Loss: 6.6454, Time: 9.54s, Token/s: 53.65
Epoch: 0, Step: 7329, Batch(micro): 7329, Batch (considering grad accum): 916,  Loss: 6.3324, Time: 3.34s, Token/s: 153.48
Epoch: 0, Step: 7330, Batch(micro): 7330, Batch (considering grad accum): 916,  Loss: 5.9220, Time: 3.28s, Token/s: 156.32
Epoch: 0, Step: 7331, Batch(micro): 7331, Batch (considering grad accum): 916,  Loss: 6.1200, Time: 3.21s, Token/s: 159.65
Epoch: 0, Step: 7332, Batch(micro): 7332, Batch (considering grad accum): 916,  Loss: 6.3794, Time: 3.24s, Token/s: 157.93
Epoch: 0, Step: 7333, Batch(micro): 7333, Batch (considering grad accum): 916,  Loss: 6.1234, Time: 3.21s, Token/s: 159.44
Epoch: 0, Step: 7334, Batch(micro): 7334, Batch (considering grad accum): 916,  Loss: 5.9691, Time: 3.18s, Token/s: 160.97
Epoch: 0, Step: 7335, Batch(micro): 7335, Batch (considering grad accum): 916,  Loss: 6.8047, Time: 25.65s, Token/s: 19.96
Epoch: 0, Step: 7336, Batch(micro): 7336, Batch (considering grad accum): 917,  Loss: 5.9811, Time: 6.86s, Token/s: 74.61
Epoch: 0, Step: 7337, Batch(micro): 7337, Batch (considering grad accum): 917,  Loss: 6.6484, Time: 3.79s, Token/s: 135.20
Epoch: 0, Step: 7338, Batch(micro): 7338, Batch (considering grad accum): 917,  Loss: 6.1202, Time: 3.34s, Token/s: 153.24
Epoch: 0, Step: 7339, Batch(micro): 7339, Batch (considering grad accum): 917,  Loss: 6.0933, Time: 3.41s, Token/s: 150.33
Epoch: 0, Step: 7340, Batch(micro): 7340, Batch (considering grad accum): 917,  Loss: 5.9794, Time: 3.70s, Token/s: 138.52
Epoch: 0, Step: 7341, Batch(micro): 7341, Batch (considering grad accum): 917,  Loss: 5.8230, Time: 3.76s, Token/s: 136.07
Epoch: 0, Step: 7342, Batch(micro): 7342, Batch (considering grad accum): 917,  Loss: 6.3759, Time: 3.63s, Token/s: 141.07
Epoch: 0, Step: 7343, Batch(micro): 7343, Batch (considering grad accum): 917,  Loss: 5.8300, Time: 23.44s, Token/s: 21.84
Epoch: 0, Step: 7344, Batch(micro): 7344, Batch (considering grad accum): 918,  Loss: 6.0408, Time: 8.11s, Token/s: 63.13
Epoch: 0, Step: 7345, Batch(micro): 7345, Batch (considering grad accum): 918,  Loss: 6.3894, Time: 3.92s, Token/s: 130.55
Epoch: 0, Step: 7346, Batch(micro): 7346, Batch (considering grad accum): 918,  Loss: 7.1513, Time: 3.57s, Token/s: 143.37
Epoch: 0, Step: 7347, Batch(micro): 7347, Batch (considering grad accum): 918,  Loss: 6.3836, Time: 3.57s, Token/s: 143.50
Epoch: 0, Step: 7348, Batch(micro): 7348, Batch (considering grad accum): 918,  Loss: 7.1980, Time: 3.61s, Token/s: 141.94
Epoch: 0, Step: 7349, Batch(micro): 7349, Batch (considering grad accum): 918,  Loss: 7.4448, Time: 3.54s, Token/s: 144.47
Epoch: 0, Step: 7350, Batch(micro): 7350, Batch (considering grad accum): 918,  Loss: 5.9627, Time: 3.61s, Token/s: 141.65
Epoch: 0, Step: 7351, Batch(micro): 7351, Batch (considering grad accum): 918,  Loss: 5.0529, Time: 21.89s, Token/s: 23.39
Epoch: 0, Step: 7352, Batch(micro): 7352, Batch (considering grad accum): 919,  Loss: 5.5899, Time: 8.36s, Token/s: 61.22
Epoch: 0, Step: 7353, Batch(micro): 7353, Batch (considering grad accum): 919,  Loss: 5.7467, Time: 4.18s, Token/s: 122.38
Epoch: 0, Step: 7354, Batch(micro): 7354, Batch (considering grad accum): 919,  Loss: 6.1150, Time: 3.82s, Token/s: 133.98
Epoch: 0, Step: 7355, Batch(micro): 7355, Batch (considering grad accum): 919,  Loss: 5.4019, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 7356, Batch(micro): 7356, Batch (considering grad accum): 919,  Loss: 5.5004, Time: 3.45s, Token/s: 148.44
Epoch: 0, Step: 7357, Batch(micro): 7357, Batch (considering grad accum): 919,  Loss: 6.1186, Time: 3.39s, Token/s: 151.00
Epoch: 0, Step: 7358, Batch(micro): 7358, Batch (considering grad accum): 919,  Loss: 6.6659, Time: 3.39s, Token/s: 150.94
Epoch: 0, Step: 7359, Batch(micro): 7359, Batch (considering grad accum): 919,  Loss: 6.3453, Time: 25.06s, Token/s: 20.43
Epoch: 0, Step: 7360, Batch(micro): 7360, Batch (considering grad accum): 920,  Loss: 6.0943, Time: 7.08s, Token/s: 72.28
Epoch: 0, Step: 7361, Batch(micro): 7361, Batch (considering grad accum): 920,  Loss: 6.3386, Time: 3.93s, Token/s: 130.13
Epoch: 0, Step: 7362, Batch(micro): 7362, Batch (considering grad accum): 920,  Loss: 5.7719, Time: 3.51s, Token/s: 145.85
Epoch: 0, Step: 7363, Batch(micro): 7363, Batch (considering grad accum): 920,  Loss: 6.2274, Time: 3.68s, Token/s: 139.18
Epoch: 0, Step: 7364, Batch(micro): 7364, Batch (considering grad accum): 920,  Loss: 5.8195, Time: 3.47s, Token/s: 147.42
Epoch: 0, Step: 7365, Batch(micro): 7365, Batch (considering grad accum): 920,  Loss: 5.2081, Time: 3.41s, Token/s: 150.19
Epoch: 0, Step: 7366, Batch(micro): 7366, Batch (considering grad accum): 920,  Loss: 5.9083, Time: 3.47s, Token/s: 147.63
Epoch: 0, Step: 7367, Batch(micro): 7367, Batch (considering grad accum): 920,  Loss: 5.6114, Time: 22.41s, Token/s: 22.85
Epoch: 0, Step: 7368, Batch(micro): 7368, Batch (considering grad accum): 921,  Loss: 6.3169, Time: 6.14s, Token/s: 83.35
Epoch: 0, Step: 7369, Batch(micro): 7369, Batch (considering grad accum): 921,  Loss: 6.5155, Time: 3.73s, Token/s: 137.09
Epoch: 0, Step: 7370, Batch(micro): 7370, Batch (considering grad accum): 921,  Loss: 6.3832, Time: 3.23s, Token/s: 158.29
Epoch: 0, Step: 7371, Batch(micro): 7371, Batch (considering grad accum): 921,  Loss: 6.5222, Time: 3.66s, Token/s: 140.00
Epoch: 0, Step: 7372, Batch(micro): 7372, Batch (considering grad accum): 921,  Loss: 5.9837, Time: 3.35s, Token/s: 153.01
Epoch: 0, Step: 7373, Batch(micro): 7373, Batch (considering grad accum): 921,  Loss: 5.9736, Time: 3.92s, Token/s: 130.59
Epoch: 0, Step: 7374, Batch(micro): 7374, Batch (considering grad accum): 921,  Loss: 6.8482, Time: 3.67s, Token/s: 139.36
Epoch: 0, Step: 7375, Batch(micro): 7375, Batch (considering grad accum): 921,  Loss: 6.5324, Time: 18.51s, Token/s: 27.67
Epoch: 0, Step: 7376, Batch(micro): 7376, Batch (considering grad accum): 922,  Loss: 6.1527, Time: 6.24s, Token/s: 82.03
Epoch: 0, Step: 7377, Batch(micro): 7377, Batch (considering grad accum): 922,  Loss: 6.2274, Time: 3.67s, Token/s: 139.56
Epoch: 0, Step: 7378, Batch(micro): 7378, Batch (considering grad accum): 922,  Loss: 6.5868, Time: 3.47s, Token/s: 147.37
Epoch: 0, Step: 7379, Batch(micro): 7379, Batch (considering grad accum): 922,  Loss: 7.0374, Time: 3.59s, Token/s: 142.48
Epoch: 0, Step: 7380, Batch(micro): 7380, Batch (considering grad accum): 922,  Loss: 6.5337, Time: 3.43s, Token/s: 149.30
Epoch: 0, Step: 7381, Batch(micro): 7381, Batch (considering grad accum): 922,  Loss: 6.0723, Time: 3.55s, Token/s: 144.30
Epoch: 0, Step: 7382, Batch(micro): 7382, Batch (considering grad accum): 922,  Loss: 5.7177, Time: 3.78s, Token/s: 135.52
Epoch: 0, Step: 7383, Batch(micro): 7383, Batch (considering grad accum): 922,  Loss: 6.1629, Time: 19.09s, Token/s: 26.82
Epoch: 0, Step: 7384, Batch(micro): 7384, Batch (considering grad accum): 923,  Loss: 7.2368, Time: 6.86s, Token/s: 74.64
Epoch: 0, Step: 7385, Batch(micro): 7385, Batch (considering grad accum): 923,  Loss: 6.7612, Time: 4.42s, Token/s: 115.77
Epoch: 0, Step: 7386, Batch(micro): 7386, Batch (considering grad accum): 923,  Loss: 7.4614, Time: 4.01s, Token/s: 127.54
Epoch: 0, Step: 7387, Batch(micro): 7387, Batch (considering grad accum): 923,  Loss: 6.2282, Time: 3.89s, Token/s: 131.77
Epoch: 0, Step: 7388, Batch(micro): 7388, Batch (considering grad accum): 923,  Loss: 5.5803, Time: 3.26s, Token/s: 157.28
Epoch: 0, Step: 7389, Batch(micro): 7389, Batch (considering grad accum): 923,  Loss: 5.8075, Time: 3.38s, Token/s: 151.31
Epoch: 0, Step: 7390, Batch(micro): 7390, Batch (considering grad accum): 923,  Loss: 6.1685, Time: 3.94s, Token/s: 129.85
Epoch: 0, Step: 7391, Batch(micro): 7391, Batch (considering grad accum): 923,  Loss: 5.5956, Time: 20.78s, Token/s: 24.64
Epoch: 0, Step: 7392, Batch(micro): 7392, Batch (considering grad accum): 924,  Loss: 5.3352, Time: 7.66s, Token/s: 66.81
Epoch: 0, Step: 7393, Batch(micro): 7393, Batch (considering grad accum): 924,  Loss: 5.5338, Time: 3.79s, Token/s: 135.25
Epoch: 0, Step: 7394, Batch(micro): 7394, Batch (considering grad accum): 924,  Loss: 5.7711, Time: 3.27s, Token/s: 156.50
Epoch: 0, Step: 7395, Batch(micro): 7395, Batch (considering grad accum): 924,  Loss: 6.3410, Time: 3.20s, Token/s: 160.06
Epoch: 0, Step: 7396, Batch(micro): 7396, Batch (considering grad accum): 924,  Loss: 6.0438, Time: 3.23s, Token/s: 158.66
Epoch: 0, Step: 7397, Batch(micro): 7397, Batch (considering grad accum): 924,  Loss: 5.7610, Time: 3.41s, Token/s: 150.32
Epoch: 0, Step: 7398, Batch(micro): 7398, Batch (considering grad accum): 924,  Loss: 6.3375, Time: 3.61s, Token/s: 141.87
Epoch: 0, Step: 7399, Batch(micro): 7399, Batch (considering grad accum): 924,  Loss: 5.4522, Time: 21.19s, Token/s: 24.16
Updating MLP bias
Epoch: 0, Step: 7400, Batch(micro): 7400, Batch (considering grad accum): 925,  Loss: 5.6170, Time: 7.92s, Token/s: 64.62
Epoch: 0, Step: 7401, Batch(micro): 7401, Batch (considering grad accum): 925,  Loss: 5.4946, Time: 3.96s, Token/s: 129.28
Epoch: 0, Step: 7402, Batch(micro): 7402, Batch (considering grad accum): 925,  Loss: 6.6103, Time: 3.50s, Token/s: 146.20
Epoch: 0, Step: 7403, Batch(micro): 7403, Batch (considering grad accum): 925,  Loss: 6.9075, Time: 3.53s, Token/s: 145.02
Epoch: 0, Step: 7404, Batch(micro): 7404, Batch (considering grad accum): 925,  Loss: 6.2123, Time: 3.50s, Token/s: 146.45
Epoch: 0, Step: 7405, Batch(micro): 7405, Batch (considering grad accum): 925,  Loss: 6.2630, Time: 3.64s, Token/s: 140.78
Epoch: 0, Step: 7406, Batch(micro): 7406, Batch (considering grad accum): 925,  Loss: 5.9442, Time: 3.51s, Token/s: 145.84
Epoch: 0, Step: 7407, Batch(micro): 7407, Batch (considering grad accum): 925,  Loss: 6.2597, Time: 19.19s, Token/s: 26.68
Epoch: 0, Step: 7408, Batch(micro): 7408, Batch (considering grad accum): 926,  Loss: 6.5055, Time: 6.26s, Token/s: 81.82
Epoch: 0, Step: 7409, Batch(micro): 7409, Batch (considering grad accum): 926,  Loss: 6.2531, Time: 3.68s, Token/s: 139.09
Epoch: 0, Step: 7410, Batch(micro): 7410, Batch (considering grad accum): 926,  Loss: 6.4073, Time: 3.14s, Token/s: 162.86
Epoch: 0, Step: 7411, Batch(micro): 7411, Batch (considering grad accum): 926,  Loss: 6.8568, Time: 3.16s, Token/s: 162.03
Epoch: 0, Step: 7412, Batch(micro): 7412, Batch (considering grad accum): 926,  Loss: 6.3830, Time: 3.22s, Token/s: 159.14
Epoch: 0, Step: 7413, Batch(micro): 7413, Batch (considering grad accum): 926,  Loss: 5.5210, Time: 3.34s, Token/s: 153.49
Epoch: 0, Step: 7414, Batch(micro): 7414, Batch (considering grad accum): 926,  Loss: 6.0021, Time: 3.50s, Token/s: 146.41
Epoch: 0, Step: 7415, Batch(micro): 7415, Batch (considering grad accum): 926,  Loss: 6.6484, Time: 19.53s, Token/s: 26.21
Epoch: 0, Step: 7416, Batch(micro): 7416, Batch (considering grad accum): 927,  Loss: 5.9097, Time: 7.15s, Token/s: 71.62
Epoch: 0, Step: 7417, Batch(micro): 7417, Batch (considering grad accum): 927,  Loss: 5.8292, Time: 3.97s, Token/s: 128.88
Epoch: 0, Step: 7418, Batch(micro): 7418, Batch (considering grad accum): 927,  Loss: 5.9717, Time: 3.53s, Token/s: 145.07
Epoch: 0, Step: 7419, Batch(micro): 7419, Batch (considering grad accum): 927,  Loss: 5.7900, Time: 3.80s, Token/s: 134.58
Epoch: 0, Step: 7420, Batch(micro): 7420, Batch (considering grad accum): 927,  Loss: 5.6113, Time: 3.49s, Token/s: 146.75
Epoch: 0, Step: 7421, Batch(micro): 7421, Batch (considering grad accum): 927,  Loss: 5.8842, Time: 3.65s, Token/s: 140.29
Epoch: 0, Step: 7422, Batch(micro): 7422, Batch (considering grad accum): 927,  Loss: 5.4037, Time: 3.38s, Token/s: 151.36
Epoch: 0, Step: 7423, Batch(micro): 7423, Batch (considering grad accum): 927,  Loss: 5.8150, Time: 18.58s, Token/s: 27.56
Epoch: 0, Step: 7424, Batch(micro): 7424, Batch (considering grad accum): 928,  Loss: 6.2300, Time: 5.95s, Token/s: 86.10
Epoch: 0, Step: 7425, Batch(micro): 7425, Batch (considering grad accum): 928,  Loss: 5.6973, Time: 4.09s, Token/s: 125.06
Epoch: 0, Step: 7426, Batch(micro): 7426, Batch (considering grad accum): 928,  Loss: 5.9181, Time: 3.35s, Token/s: 152.75
Epoch: 0, Step: 7427, Batch(micro): 7427, Batch (considering grad accum): 928,  Loss: 6.3495, Time: 3.61s, Token/s: 141.82
Epoch: 0, Step: 7428, Batch(micro): 7428, Batch (considering grad accum): 928,  Loss: 6.1127, Time: 3.64s, Token/s: 140.48
Epoch: 0, Step: 7429, Batch(micro): 7429, Batch (considering grad accum): 928,  Loss: 5.7253, Time: 3.35s, Token/s: 152.67
Epoch: 0, Step: 7430, Batch(micro): 7430, Batch (considering grad accum): 928,  Loss: 6.1467, Time: 3.66s, Token/s: 139.78
Epoch: 0, Step: 7431, Batch(micro): 7431, Batch (considering grad accum): 928,  Loss: 6.2678, Time: 18.65s, Token/s: 27.45
Epoch: 0, Step: 7432, Batch(micro): 7432, Batch (considering grad accum): 929,  Loss: 5.4434, Time: 6.49s, Token/s: 78.83
Epoch: 0, Step: 7433, Batch(micro): 7433, Batch (considering grad accum): 929,  Loss: 5.6680, Time: 3.92s, Token/s: 130.50
Epoch: 0, Step: 7434, Batch(micro): 7434, Batch (considering grad accum): 929,  Loss: 6.0646, Time: 3.84s, Token/s: 133.27
Epoch: 0, Step: 7435, Batch(micro): 7435, Batch (considering grad accum): 929,  Loss: 6.0921, Time: 3.41s, Token/s: 150.25
Epoch: 0, Step: 7436, Batch(micro): 7436, Batch (considering grad accum): 929,  Loss: 5.4004, Time: 3.66s, Token/s: 139.91
Epoch: 0, Step: 7437, Batch(micro): 7437, Batch (considering grad accum): 929,  Loss: 5.2494, Time: 3.61s, Token/s: 141.69
Epoch: 0, Step: 7438, Batch(micro): 7438, Batch (considering grad accum): 929,  Loss: 5.3715, Time: 3.33s, Token/s: 153.72
Epoch: 0, Step: 7439, Batch(micro): 7439, Batch (considering grad accum): 929,  Loss: 5.9393, Time: 21.41s, Token/s: 23.92
Epoch: 0, Step: 7440, Batch(micro): 7440, Batch (considering grad accum): 930,  Loss: 6.8362, Time: 6.73s, Token/s: 76.03
Epoch: 0, Step: 7441, Batch(micro): 7441, Batch (considering grad accum): 930,  Loss: 6.5387, Time: 3.79s, Token/s: 135.02
Epoch: 0, Step: 7442, Batch(micro): 7442, Batch (considering grad accum): 930,  Loss: 5.7654, Time: 3.56s, Token/s: 143.71
Epoch: 0, Step: 7443, Batch(micro): 7443, Batch (considering grad accum): 930,  Loss: 6.0961, Time: 3.47s, Token/s: 147.34
Epoch: 0, Step: 7444, Batch(micro): 7444, Batch (considering grad accum): 930,  Loss: 6.0447, Time: 3.67s, Token/s: 139.41
Epoch: 0, Step: 7445, Batch(micro): 7445, Batch (considering grad accum): 930,  Loss: 6.3147, Time: 3.58s, Token/s: 143.06
Epoch: 0, Step: 7446, Batch(micro): 7446, Batch (considering grad accum): 930,  Loss: 5.5895, Time: 3.30s, Token/s: 155.37
Epoch: 0, Step: 7447, Batch(micro): 7447, Batch (considering grad accum): 930,  Loss: 6.1464, Time: 18.50s, Token/s: 27.67
Epoch: 0, Step: 7448, Batch(micro): 7448, Batch (considering grad accum): 931,  Loss: 5.6287, Time: 6.34s, Token/s: 80.71
Epoch: 0, Step: 7449, Batch(micro): 7449, Batch (considering grad accum): 931,  Loss: 6.1519, Time: 3.65s, Token/s: 140.12
Epoch: 0, Step: 7450, Batch(micro): 7450, Batch (considering grad accum): 931,  Loss: 5.8810, Time: 3.18s, Token/s: 160.88
Epoch: 0, Step: 7451, Batch(micro): 7451, Batch (considering grad accum): 931,  Loss: 6.2353, Time: 3.24s, Token/s: 158.03
Epoch: 0, Step: 7452, Batch(micro): 7452, Batch (considering grad accum): 931,  Loss: 6.6012, Time: 3.86s, Token/s: 132.57
Epoch: 0, Step: 7453, Batch(micro): 7453, Batch (considering grad accum): 931,  Loss: 6.3998, Time: 3.92s, Token/s: 130.63
Epoch: 0, Step: 7454, Batch(micro): 7454, Batch (considering grad accum): 931,  Loss: 6.1495, Time: 3.42s, Token/s: 149.76
Epoch: 0, Step: 7455, Batch(micro): 7455, Batch (considering grad accum): 931,  Loss: 5.0548, Time: 20.58s, Token/s: 24.88
Epoch: 0, Step: 7456, Batch(micro): 7456, Batch (considering grad accum): 932,  Loss: 5.7410, Time: 7.39s, Token/s: 69.31
Epoch: 0, Step: 7457, Batch(micro): 7457, Batch (considering grad accum): 932,  Loss: 6.9612, Time: 4.27s, Token/s: 119.81
Epoch: 0, Step: 7458, Batch(micro): 7458, Batch (considering grad accum): 932,  Loss: 6.3063, Time: 3.60s, Token/s: 142.06
Epoch: 0, Step: 7459, Batch(micro): 7459, Batch (considering grad accum): 932,  Loss: 5.9800, Time: 3.41s, Token/s: 150.36
Epoch: 0, Step: 7460, Batch(micro): 7460, Batch (considering grad accum): 932,  Loss: 5.9747, Time: 3.44s, Token/s: 148.66
Epoch: 0, Step: 7461, Batch(micro): 7461, Batch (considering grad accum): 932,  Loss: 6.3435, Time: 3.51s, Token/s: 145.81
Epoch: 0, Step: 7462, Batch(micro): 7462, Batch (considering grad accum): 932,  Loss: 6.1770, Time: 3.40s, Token/s: 150.49
Epoch: 0, Step: 7463, Batch(micro): 7463, Batch (considering grad accum): 932,  Loss: 5.8256, Time: 18.91s, Token/s: 27.07
Epoch: 0, Step: 7464, Batch(micro): 7464, Batch (considering grad accum): 933,  Loss: 5.7870, Time: 6.62s, Token/s: 77.36
Epoch: 0, Step: 7465, Batch(micro): 7465, Batch (considering grad accum): 933,  Loss: 5.4266, Time: 4.25s, Token/s: 120.59
Epoch: 0, Step: 7466, Batch(micro): 7466, Batch (considering grad accum): 933,  Loss: 5.7926, Time: 3.47s, Token/s: 147.55
Epoch: 0, Step: 7467, Batch(micro): 7467, Batch (considering grad accum): 933,  Loss: 6.0001, Time: 3.61s, Token/s: 141.83
Epoch: 0, Step: 7468, Batch(micro): 7468, Batch (considering grad accum): 933,  Loss: 6.2373, Time: 3.96s, Token/s: 129.18
Epoch: 0, Step: 7469, Batch(micro): 7469, Batch (considering grad accum): 933,  Loss: 6.1265, Time: 3.63s, Token/s: 141.10
Epoch: 0, Step: 7470, Batch(micro): 7470, Batch (considering grad accum): 933,  Loss: 5.7666, Time: 3.59s, Token/s: 142.65
Epoch: 0, Step: 7471, Batch(micro): 7471, Batch (considering grad accum): 933,  Loss: 6.7265, Time: 18.95s, Token/s: 27.02
Epoch: 0, Step: 7472, Batch(micro): 7472, Batch (considering grad accum): 934,  Loss: 6.2106, Time: 6.10s, Token/s: 83.90
Epoch: 0, Step: 7473, Batch(micro): 7473, Batch (considering grad accum): 934,  Loss: 6.4770, Time: 3.84s, Token/s: 133.47
Epoch: 0, Step: 7474, Batch(micro): 7474, Batch (considering grad accum): 934,  Loss: 6.7400, Time: 3.65s, Token/s: 140.29
Epoch: 0, Step: 7475, Batch(micro): 7475, Batch (considering grad accum): 934,  Loss: 6.5950, Time: 3.57s, Token/s: 143.58
Epoch: 0, Step: 7476, Batch(micro): 7476, Batch (considering grad accum): 934,  Loss: 6.0918, Time: 3.40s, Token/s: 150.64
Epoch: 0, Step: 7477, Batch(micro): 7477, Batch (considering grad accum): 934,  Loss: 6.5826, Time: 3.48s, Token/s: 147.30
Epoch: 0, Step: 7478, Batch(micro): 7478, Batch (considering grad accum): 934,  Loss: 5.6485, Time: 3.50s, Token/s: 146.15
Epoch: 0, Step: 7479, Batch(micro): 7479, Batch (considering grad accum): 934,  Loss: 5.8455, Time: 23.47s, Token/s: 21.82
Epoch: 0, Step: 7480, Batch(micro): 7480, Batch (considering grad accum): 935,  Loss: 5.9203, Time: 6.85s, Token/s: 74.77
Epoch: 0, Step: 7481, Batch(micro): 7481, Batch (considering grad accum): 935,  Loss: 5.6269, Time: 4.08s, Token/s: 125.55
Epoch: 0, Step: 7482, Batch(micro): 7482, Batch (considering grad accum): 935,  Loss: 5.8095, Time: 3.82s, Token/s: 134.18
Epoch: 0, Step: 7483, Batch(micro): 7483, Batch (considering grad accum): 935,  Loss: 5.8144, Time: 3.93s, Token/s: 130.44
Epoch: 0, Step: 7484, Batch(micro): 7484, Batch (considering grad accum): 935,  Loss: 5.6764, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 7485, Batch(micro): 7485, Batch (considering grad accum): 935,  Loss: 5.5097, Time: 3.43s, Token/s: 149.27
Epoch: 0, Step: 7486, Batch(micro): 7486, Batch (considering grad accum): 935,  Loss: 5.6054, Time: 3.30s, Token/s: 155.11
Epoch: 0, Step: 7487, Batch(micro): 7487, Batch (considering grad accum): 935,  Loss: 5.9592, Time: 24.16s, Token/s: 21.19
Epoch: 0, Step: 7488, Batch(micro): 7488, Batch (considering grad accum): 936,  Loss: 6.1813, Time: 6.87s, Token/s: 74.58
Epoch: 0, Step: 7489, Batch(micro): 7489, Batch (considering grad accum): 936,  Loss: 7.0772, Time: 4.10s, Token/s: 124.75
Epoch: 0, Step: 7490, Batch(micro): 7490, Batch (considering grad accum): 936,  Loss: 5.9633, Time: 3.52s, Token/s: 145.47
Epoch: 0, Step: 7491, Batch(micro): 7491, Batch (considering grad accum): 936,  Loss: 5.9261, Time: 4.06s, Token/s: 125.99
Epoch: 0, Step: 7492, Batch(micro): 7492, Batch (considering grad accum): 936,  Loss: 6.5271, Time: 3.46s, Token/s: 148.13
Epoch: 0, Step: 7493, Batch(micro): 7493, Batch (considering grad accum): 936,  Loss: 5.8097, Time: 3.57s, Token/s: 143.53
Epoch: 0, Step: 7494, Batch(micro): 7494, Batch (considering grad accum): 936,  Loss: 5.8351, Time: 3.39s, Token/s: 150.86
Epoch: 0, Step: 7495, Batch(micro): 7495, Batch (considering grad accum): 936,  Loss: 6.0711, Time: 24.74s, Token/s: 20.69
Epoch: 0, Step: 7496, Batch(micro): 7496, Batch (considering grad accum): 937,  Loss: 5.4780, Time: 8.55s, Token/s: 59.86
Epoch: 0, Step: 7497, Batch(micro): 7497, Batch (considering grad accum): 937,  Loss: 5.6448, Time: 4.03s, Token/s: 127.12
Epoch: 0, Step: 7498, Batch(micro): 7498, Batch (considering grad accum): 937,  Loss: 5.6162, Time: 3.35s, Token/s: 152.95
Epoch: 0, Step: 7499, Batch(micro): 7499, Batch (considering grad accum): 937,  Loss: 5.0268, Time: 3.53s, Token/s: 145.23
Updating MLP bias
Epoch: 0, Step: 7500, Batch(micro): 7500, Batch (considering grad accum): 937,  Loss: 5.2256, Time: 3.58s, Token/s: 142.96
Epoch: 0, Step: 7501, Batch(micro): 7501, Batch (considering grad accum): 937,  Loss: 5.8959, Time: 3.63s, Token/s: 141.15
Epoch: 0, Step: 7502, Batch(micro): 7502, Batch (considering grad accum): 937,  Loss: 6.4440, Time: 3.34s, Token/s: 153.42
Epoch: 0, Step: 7503, Batch(micro): 7503, Batch (considering grad accum): 937,  Loss: 5.6200, Time: 25.69s, Token/s: 19.93
Epoch: 0, Step: 7504, Batch(micro): 7504, Batch (considering grad accum): 938,  Loss: 6.2277, Time: 6.38s, Token/s: 80.27
Epoch: 0, Step: 7505, Batch(micro): 7505, Batch (considering grad accum): 938,  Loss: 6.6338, Time: 3.84s, Token/s: 133.49
Epoch: 0, Step: 7506, Batch(micro): 7506, Batch (considering grad accum): 938,  Loss: 5.9719, Time: 4.10s, Token/s: 124.81
Epoch: 0, Step: 7507, Batch(micro): 7507, Batch (considering grad accum): 938,  Loss: 6.3108, Time: 3.53s, Token/s: 144.87
Epoch: 0, Step: 7508, Batch(micro): 7508, Batch (considering grad accum): 938,  Loss: 6.0389, Time: 3.42s, Token/s: 149.71
Epoch: 0, Step: 7509, Batch(micro): 7509, Batch (considering grad accum): 938,  Loss: 6.0548, Time: 3.31s, Token/s: 154.84
Epoch: 0, Step: 7510, Batch(micro): 7510, Batch (considering grad accum): 938,  Loss: 5.7210, Time: 3.34s, Token/s: 153.31
Epoch: 0, Step: 7511, Batch(micro): 7511, Batch (considering grad accum): 938,  Loss: 5.9252, Time: 24.29s, Token/s: 21.08
Epoch: 0, Step: 7512, Batch(micro): 7512, Batch (considering grad accum): 939,  Loss: 5.8229, Time: 6.84s, Token/s: 74.86
Epoch: 0, Step: 7513, Batch(micro): 7513, Batch (considering grad accum): 939,  Loss: 6.6754, Time: 3.88s, Token/s: 132.13
Epoch: 0, Step: 7514, Batch(micro): 7514, Batch (considering grad accum): 939,  Loss: 6.6794, Time: 3.48s, Token/s: 147.24
Epoch: 0, Step: 7515, Batch(micro): 7515, Batch (considering grad accum): 939,  Loss: 6.4304, Time: 3.38s, Token/s: 151.33
Epoch: 0, Step: 7516, Batch(micro): 7516, Batch (considering grad accum): 939,  Loss: 7.6118, Time: 3.40s, Token/s: 150.59
Epoch: 0, Step: 7517, Batch(micro): 7517, Batch (considering grad accum): 939,  Loss: 5.8760, Time: 3.43s, Token/s: 149.35
Epoch: 0, Step: 7518, Batch(micro): 7518, Batch (considering grad accum): 939,  Loss: 5.8606, Time: 3.48s, Token/s: 147.31
Epoch: 0, Step: 7519, Batch(micro): 7519, Batch (considering grad accum): 939,  Loss: 6.2887, Time: 24.78s, Token/s: 20.66
Epoch: 0, Step: 7520, Batch(micro): 7520, Batch (considering grad accum): 940,  Loss: 6.3894, Time: 7.27s, Token/s: 70.38
Epoch: 0, Step: 7521, Batch(micro): 7521, Batch (considering grad accum): 940,  Loss: 6.5055, Time: 3.65s, Token/s: 140.33
Epoch: 0, Step: 7522, Batch(micro): 7522, Batch (considering grad accum): 940,  Loss: 5.9307, Time: 3.65s, Token/s: 140.19
Epoch: 0, Step: 7523, Batch(micro): 7523, Batch (considering grad accum): 940,  Loss: 6.4373, Time: 3.19s, Token/s: 160.50
Epoch: 0, Step: 7524, Batch(micro): 7524, Batch (considering grad accum): 940,  Loss: 6.5162, Time: 3.27s, Token/s: 156.58
Epoch: 0, Step: 7525, Batch(micro): 7525, Batch (considering grad accum): 940,  Loss: 6.6997, Time: 3.24s, Token/s: 157.86
Epoch: 0, Step: 7526, Batch(micro): 7526, Batch (considering grad accum): 940,  Loss: 6.2440, Time: 3.19s, Token/s: 160.29
Epoch: 0, Step: 7527, Batch(micro): 7527, Batch (considering grad accum): 940,  Loss: 5.9366, Time: 24.46s, Token/s: 20.93
Epoch: 0, Step: 7528, Batch(micro): 7528, Batch (considering grad accum): 941,  Loss: 6.2937, Time: 7.31s, Token/s: 70.06
Epoch: 0, Step: 7529, Batch(micro): 7529, Batch (considering grad accum): 941,  Loss: 7.2376, Time: 4.09s, Token/s: 125.04
Epoch: 0, Step: 7530, Batch(micro): 7530, Batch (considering grad accum): 941,  Loss: 7.4424, Time: 3.39s, Token/s: 151.14
Epoch: 0, Step: 7531, Batch(micro): 7531, Batch (considering grad accum): 941,  Loss: 6.0202, Time: 3.30s, Token/s: 155.10
Epoch: 0, Step: 7532, Batch(micro): 7532, Batch (considering grad accum): 941,  Loss: 5.4232, Time: 3.73s, Token/s: 137.18
Epoch: 0, Step: 7533, Batch(micro): 7533, Batch (considering grad accum): 941,  Loss: 5.7570, Time: 3.56s, Token/s: 143.77
Epoch: 0, Step: 7534, Batch(micro): 7534, Batch (considering grad accum): 941,  Loss: 5.2483, Time: 3.21s, Token/s: 159.45
Epoch: 0, Step: 7535, Batch(micro): 7535, Batch (considering grad accum): 941,  Loss: 5.9505, Time: 22.22s, Token/s: 23.05
Epoch: 0, Step: 7536, Batch(micro): 7536, Batch (considering grad accum): 942,  Loss: 6.2060, Time: 6.83s, Token/s: 74.99
Epoch: 0, Step: 7537, Batch(micro): 7537, Batch (considering grad accum): 942,  Loss: 6.4888, Time: 3.77s, Token/s: 135.92
Epoch: 0, Step: 7538, Batch(micro): 7538, Batch (considering grad accum): 942,  Loss: 6.1286, Time: 3.30s, Token/s: 155.09
Epoch: 0, Step: 7539, Batch(micro): 7539, Batch (considering grad accum): 942,  Loss: 6.2718, Time: 3.33s, Token/s: 153.71
Epoch: 0, Step: 7540, Batch(micro): 7540, Batch (considering grad accum): 942,  Loss: 6.0975, Time: 3.33s, Token/s: 153.74
Epoch: 0, Step: 7541, Batch(micro): 7541, Batch (considering grad accum): 942,  Loss: 5.3913, Time: 3.30s, Token/s: 155.19
Epoch: 0, Step: 7542, Batch(micro): 7542, Batch (considering grad accum): 942,  Loss: 5.7193, Time: 3.59s, Token/s: 142.67
Epoch: 0, Step: 7543, Batch(micro): 7543, Batch (considering grad accum): 942,  Loss: 5.5856, Time: 23.47s, Token/s: 21.81
Epoch: 0, Step: 7544, Batch(micro): 7544, Batch (considering grad accum): 943,  Loss: 5.8825, Time: 8.98s, Token/s: 57.02
Epoch: 0, Step: 7545, Batch(micro): 7545, Batch (considering grad accum): 943,  Loss: 5.9250, Time: 3.33s, Token/s: 153.64
Epoch: 0, Step: 7546, Batch(micro): 7546, Batch (considering grad accum): 943,  Loss: 5.8573, Time: 3.37s, Token/s: 151.99
Epoch: 0, Step: 7547, Batch(micro): 7547, Batch (considering grad accum): 943,  Loss: 6.1604, Time: 3.66s, Token/s: 139.74
Epoch: 0, Step: 7548, Batch(micro): 7548, Batch (considering grad accum): 943,  Loss: 5.2046, Time: 3.50s, Token/s: 146.12
Epoch: 0, Step: 7549, Batch(micro): 7549, Batch (considering grad accum): 943,  Loss: 5.7207, Time: 3.54s, Token/s: 144.65
Epoch: 0, Step: 7550, Batch(micro): 7550, Batch (considering grad accum): 943,  Loss: 5.7904, Time: 3.32s, Token/s: 154.22
Epoch: 0, Step: 7551, Batch(micro): 7551, Batch (considering grad accum): 943,  Loss: 5.5994, Time: 23.31s, Token/s: 21.97
Epoch: 0, Step: 7552, Batch(micro): 7552, Batch (considering grad accum): 944,  Loss: 6.2163, Time: 5.08s, Token/s: 100.76
Epoch: 0, Step: 7553, Batch(micro): 7553, Batch (considering grad accum): 944,  Loss: 6.2413, Time: 3.83s, Token/s: 133.69
Epoch: 0, Step: 7554, Batch(micro): 7554, Batch (considering grad accum): 944,  Loss: 6.1479, Time: 3.41s, Token/s: 150.33
Epoch: 0, Step: 7555, Batch(micro): 7555, Batch (considering grad accum): 944,  Loss: 5.5913, Time: 3.50s, Token/s: 146.31
Epoch: 0, Step: 7556, Batch(micro): 7556, Batch (considering grad accum): 944,  Loss: 5.9574, Time: 3.52s, Token/s: 145.57
Epoch: 0, Step: 7557, Batch(micro): 7557, Batch (considering grad accum): 944,  Loss: 6.3373, Time: 3.45s, Token/s: 148.56
Epoch: 0, Step: 7558, Batch(micro): 7558, Batch (considering grad accum): 944,  Loss: 6.8595, Time: 3.63s, Token/s: 140.97
Epoch: 0, Step: 7559, Batch(micro): 7559, Batch (considering grad accum): 944,  Loss: 6.7307, Time: 22.08s, Token/s: 23.19
Epoch: 0, Step: 7560, Batch(micro): 7560, Batch (considering grad accum): 945,  Loss: 5.1546, Time: 7.39s, Token/s: 69.27
Epoch: 0, Step: 7561, Batch(micro): 7561, Batch (considering grad accum): 945,  Loss: 6.1020, Time: 3.97s, Token/s: 128.86
Epoch: 0, Step: 7562, Batch(micro): 7562, Batch (considering grad accum): 945,  Loss: 5.9483, Time: 3.53s, Token/s: 145.15
Epoch: 0, Step: 7563, Batch(micro): 7563, Batch (considering grad accum): 945,  Loss: 6.2563, Time: 3.45s, Token/s: 148.58
Epoch: 0, Step: 7564, Batch(micro): 7564, Batch (considering grad accum): 945,  Loss: 6.5042, Time: 3.53s, Token/s: 145.10
Epoch: 0, Step: 7565, Batch(micro): 7565, Batch (considering grad accum): 945,  Loss: 5.5435, Time: 3.51s, Token/s: 146.00
Epoch: 0, Step: 7566, Batch(micro): 7566, Batch (considering grad accum): 945,  Loss: 5.3632, Time: 3.60s, Token/s: 142.24
Epoch: 0, Step: 7567, Batch(micro): 7567, Batch (considering grad accum): 945,  Loss: 6.0623, Time: 26.80s, Token/s: 19.10
Epoch: 0, Step: 7568, Batch(micro): 7568, Batch (considering grad accum): 946,  Loss: 6.1243, Time: 10.09s, Token/s: 50.73
Epoch: 0, Step: 7569, Batch(micro): 7569, Batch (considering grad accum): 946,  Loss: 5.8049, Time: 3.91s, Token/s: 131.07
Epoch: 0, Step: 7570, Batch(micro): 7570, Batch (considering grad accum): 946,  Loss: 5.6794, Time: 3.57s, Token/s: 143.50
Epoch: 0, Step: 7571, Batch(micro): 7571, Batch (considering grad accum): 946,  Loss: 5.7069, Time: 3.56s, Token/s: 143.95
Epoch: 0, Step: 7572, Batch(micro): 7572, Batch (considering grad accum): 946,  Loss: 5.4928, Time: 3.51s, Token/s: 146.01
Epoch: 0, Step: 7573, Batch(micro): 7573, Batch (considering grad accum): 946,  Loss: 5.7030, Time: 3.80s, Token/s: 134.82
Epoch: 0, Step: 7574, Batch(micro): 7574, Batch (considering grad accum): 946,  Loss: 6.1624, Time: 3.42s, Token/s: 149.93
Epoch: 0, Step: 7575, Batch(micro): 7575, Batch (considering grad accum): 946,  Loss: 5.9165, Time: 22.41s, Token/s: 22.85
Epoch: 0, Step: 7576, Batch(micro): 7576, Batch (considering grad accum): 947,  Loss: 7.5710, Time: 7.40s, Token/s: 69.18
Epoch: 0, Step: 7577, Batch(micro): 7577, Batch (considering grad accum): 947,  Loss: 6.0307, Time: 3.78s, Token/s: 135.32
Epoch: 0, Step: 7578, Batch(micro): 7578, Batch (considering grad accum): 947,  Loss: 6.4039, Time: 3.42s, Token/s: 149.81
Epoch: 0, Step: 7579, Batch(micro): 7579, Batch (considering grad accum): 947,  Loss: 5.9333, Time: 3.31s, Token/s: 154.64
Epoch: 0, Step: 7580, Batch(micro): 7580, Batch (considering grad accum): 947,  Loss: 5.7730, Time: 3.41s, Token/s: 150.17
Epoch: 0, Step: 7581, Batch(micro): 7581, Batch (considering grad accum): 947,  Loss: 5.7660, Time: 3.54s, Token/s: 144.56
Epoch: 0, Step: 7582, Batch(micro): 7582, Batch (considering grad accum): 947,  Loss: 5.7438, Time: 3.27s, Token/s: 156.77
Epoch: 0, Step: 7583, Batch(micro): 7583, Batch (considering grad accum): 947,  Loss: 5.7704, Time: 22.15s, Token/s: 23.12
Epoch: 0, Step: 7584, Batch(micro): 7584, Batch (considering grad accum): 948,  Loss: 5.7946, Time: 7.48s, Token/s: 68.46
Epoch: 0, Step: 7585, Batch(micro): 7585, Batch (considering grad accum): 948,  Loss: 5.5831, Time: 3.85s, Token/s: 132.82
Epoch: 0, Step: 7586, Batch(micro): 7586, Batch (considering grad accum): 948,  Loss: 5.8927, Time: 3.69s, Token/s: 138.88
Epoch: 0, Step: 7587, Batch(micro): 7587, Batch (considering grad accum): 948,  Loss: 5.9919, Time: 3.61s, Token/s: 141.94
Epoch: 0, Step: 7588, Batch(micro): 7588, Batch (considering grad accum): 948,  Loss: 5.9176, Time: 3.67s, Token/s: 139.65
Epoch: 0, Step: 7589, Batch(micro): 7589, Batch (considering grad accum): 948,  Loss: 5.5907, Time: 3.23s, Token/s: 158.72
Epoch: 0, Step: 7590, Batch(micro): 7590, Batch (considering grad accum): 948,  Loss: 5.9777, Time: 3.38s, Token/s: 151.47
Epoch: 0, Step: 7591, Batch(micro): 7591, Batch (considering grad accum): 948,  Loss: 5.6362, Time: 24.00s, Token/s: 21.33
Epoch: 0, Step: 7592, Batch(micro): 7592, Batch (considering grad accum): 949,  Loss: 5.6590, Time: 8.30s, Token/s: 61.68
Epoch: 0, Step: 7593, Batch(micro): 7593, Batch (considering grad accum): 949,  Loss: 5.4534, Time: 3.88s, Token/s: 131.94
Epoch: 0, Step: 7594, Batch(micro): 7594, Batch (considering grad accum): 949,  Loss: 6.3107, Time: 3.54s, Token/s: 144.48
Epoch: 0, Step: 7595, Batch(micro): 7595, Batch (considering grad accum): 949,  Loss: 6.1467, Time: 3.43s, Token/s: 149.20
Epoch: 0, Step: 7596, Batch(micro): 7596, Batch (considering grad accum): 949,  Loss: 5.8446, Time: 3.31s, Token/s: 154.71
Epoch: 0, Step: 7597, Batch(micro): 7597, Batch (considering grad accum): 949,  Loss: 6.5920, Time: 3.26s, Token/s: 157.29
Epoch: 0, Step: 7598, Batch(micro): 7598, Batch (considering grad accum): 949,  Loss: 6.4689, Time: 3.26s, Token/s: 157.09
Epoch: 0, Step: 7599, Batch(micro): 7599, Batch (considering grad accum): 949,  Loss: 5.9716, Time: 23.62s, Token/s: 21.67
Updating MLP bias
Epoch: 0, Step: 7600, Batch(micro): 7600, Batch (considering grad accum): 950,  Loss: 6.1575, Time: 7.60s, Token/s: 67.39
Epoch: 0, Step: 7601, Batch(micro): 7601, Batch (considering grad accum): 950,  Loss: 6.4782, Time: 4.09s, Token/s: 125.08
Epoch: 0, Step: 7602, Batch(micro): 7602, Batch (considering grad accum): 950,  Loss: 7.3982, Time: 3.64s, Token/s: 140.83
Epoch: 0, Step: 7603, Batch(micro): 7603, Batch (considering grad accum): 950,  Loss: 6.9852, Time: 3.36s, Token/s: 152.48
Epoch: 0, Step: 7604, Batch(micro): 7604, Batch (considering grad accum): 950,  Loss: 5.7917, Time: 3.51s, Token/s: 145.91
Epoch: 0, Step: 7605, Batch(micro): 7605, Batch (considering grad accum): 950,  Loss: 4.3161, Time: 3.46s, Token/s: 148.01
Epoch: 0, Step: 7606, Batch(micro): 7606, Batch (considering grad accum): 950,  Loss: 4.8642, Time: 3.37s, Token/s: 151.82
Epoch: 0, Step: 7607, Batch(micro): 7607, Batch (considering grad accum): 950,  Loss: 5.4015, Time: 24.85s, Token/s: 20.61
Epoch: 0, Step: 7608, Batch(micro): 7608, Batch (considering grad accum): 951,  Loss: 6.1878, Time: 7.85s, Token/s: 65.21
Epoch: 0, Step: 7609, Batch(micro): 7609, Batch (considering grad accum): 951,  Loss: 6.2119, Time: 3.87s, Token/s: 132.18
Epoch: 0, Step: 7610, Batch(micro): 7610, Batch (considering grad accum): 951,  Loss: 5.7821, Time: 3.60s, Token/s: 142.19
Epoch: 0, Step: 7611, Batch(micro): 7611, Batch (considering grad accum): 951,  Loss: 6.3408, Time: 3.70s, Token/s: 138.43
Epoch: 0, Step: 7612, Batch(micro): 7612, Batch (considering grad accum): 951,  Loss: 6.2088, Time: 3.44s, Token/s: 148.80
Epoch: 0, Step: 7613, Batch(micro): 7613, Batch (considering grad accum): 951,  Loss: 5.9721, Time: 3.33s, Token/s: 153.85
Epoch: 0, Step: 7614, Batch(micro): 7614, Batch (considering grad accum): 951,  Loss: 4.6883, Time: 3.52s, Token/s: 145.40
Epoch: 0, Step: 7615, Batch(micro): 7615, Batch (considering grad accum): 951,  Loss: 5.1691, Time: 25.97s, Token/s: 19.71
Epoch: 0, Step: 7616, Batch(micro): 7616, Batch (considering grad accum): 952,  Loss: 6.3962, Time: 9.28s, Token/s: 55.20
Epoch: 0, Step: 7617, Batch(micro): 7617, Batch (considering grad accum): 952,  Loss: 6.0087, Time: 3.92s, Token/s: 130.56
Epoch: 0, Step: 7618, Batch(micro): 7618, Batch (considering grad accum): 952,  Loss: 6.3805, Time: 3.56s, Token/s: 143.89
Epoch: 0, Step: 7619, Batch(micro): 7619, Batch (considering grad accum): 952,  Loss: 6.4027, Time: 3.58s, Token/s: 143.02
Epoch: 0, Step: 7620, Batch(micro): 7620, Batch (considering grad accum): 952,  Loss: 6.3502, Time: 3.18s, Token/s: 161.14
Epoch: 0, Step: 7621, Batch(micro): 7621, Batch (considering grad accum): 952,  Loss: 6.1359, Time: 3.43s, Token/s: 149.16
Epoch: 0, Step: 7622, Batch(micro): 7622, Batch (considering grad accum): 952,  Loss: 6.4989, Time: 3.20s, Token/s: 159.86
Epoch: 0, Step: 7623, Batch(micro): 7623, Batch (considering grad accum): 952,  Loss: 6.4362, Time: 27.23s, Token/s: 18.80
Epoch: 0, Step: 7624, Batch(micro): 7624, Batch (considering grad accum): 953,  Loss: 5.6683, Time: 8.68s, Token/s: 58.96
Epoch: 0, Step: 7625, Batch(micro): 7625, Batch (considering grad accum): 953,  Loss: 5.8875, Time: 3.70s, Token/s: 138.50
Epoch: 0, Step: 7626, Batch(micro): 7626, Batch (considering grad accum): 953,  Loss: 6.5470, Time: 3.35s, Token/s: 152.96
Epoch: 0, Step: 7627, Batch(micro): 7627, Batch (considering grad accum): 953,  Loss: 5.5249, Time: 3.34s, Token/s: 153.19
Epoch: 0, Step: 7628, Batch(micro): 7628, Batch (considering grad accum): 953,  Loss: 5.6946, Time: 3.41s, Token/s: 150.33
Epoch: 0, Step: 7629, Batch(micro): 7629, Batch (considering grad accum): 953,  Loss: 5.9184, Time: 3.26s, Token/s: 157.09
Epoch: 0, Step: 7630, Batch(micro): 7630, Batch (considering grad accum): 953,  Loss: 6.2436, Time: 3.67s, Token/s: 139.54
Epoch: 0, Step: 7631, Batch(micro): 7631, Batch (considering grad accum): 953,  Loss: 6.4033, Time: 21.58s, Token/s: 23.72
Epoch: 0, Step: 7632, Batch(micro): 7632, Batch (considering grad accum): 954,  Loss: 6.0041, Time: 7.64s, Token/s: 66.99
Epoch: 0, Step: 7633, Batch(micro): 7633, Batch (considering grad accum): 954,  Loss: 5.8366, Time: 4.05s, Token/s: 126.55
Epoch: 0, Step: 7634, Batch(micro): 7634, Batch (considering grad accum): 954,  Loss: 6.2866, Time: 3.50s, Token/s: 146.21
Epoch: 0, Step: 7635, Batch(micro): 7635, Batch (considering grad accum): 954,  Loss: 6.1408, Time: 3.81s, Token/s: 134.40
Epoch: 0, Step: 7636, Batch(micro): 7636, Batch (considering grad accum): 954,  Loss: 6.3922, Time: 3.48s, Token/s: 147.03
Epoch: 0, Step: 7637, Batch(micro): 7637, Batch (considering grad accum): 954,  Loss: 6.9764, Time: 3.52s, Token/s: 145.58
Epoch: 0, Step: 7638, Batch(micro): 7638, Batch (considering grad accum): 954,  Loss: 5.9978, Time: 4.59s, Token/s: 111.49
Epoch: 0, Step: 7639, Batch(micro): 7639, Batch (considering grad accum): 954,  Loss: 5.7667, Time: 22.31s, Token/s: 22.95
Epoch: 0, Step: 7640, Batch(micro): 7640, Batch (considering grad accum): 955,  Loss: 6.7966, Time: 7.51s, Token/s: 68.14
Epoch: 0, Step: 7641, Batch(micro): 7641, Batch (considering grad accum): 955,  Loss: 7.0442, Time: 3.91s, Token/s: 130.85
Epoch: 0, Step: 7642, Batch(micro): 7642, Batch (considering grad accum): 955,  Loss: 5.7056, Time: 3.79s, Token/s: 135.15
Epoch: 0, Step: 7643, Batch(micro): 7643, Batch (considering grad accum): 955,  Loss: 6.1508, Time: 3.51s, Token/s: 146.06
Epoch: 0, Step: 7644, Batch(micro): 7644, Batch (considering grad accum): 955,  Loss: 6.2722, Time: 3.49s, Token/s: 146.64
Epoch: 0, Step: 7645, Batch(micro): 7645, Batch (considering grad accum): 955,  Loss: 5.9242, Time: 3.19s, Token/s: 160.33
Epoch: 0, Step: 7646, Batch(micro): 7646, Batch (considering grad accum): 955,  Loss: 6.1827, Time: 3.19s, Token/s: 160.34
Epoch: 0, Step: 7647, Batch(micro): 7647, Batch (considering grad accum): 955,  Loss: 5.3863, Time: 20.96s, Token/s: 24.43
Epoch: 0, Step: 7648, Batch(micro): 7648, Batch (considering grad accum): 956,  Loss: 6.2792, Time: 7.38s, Token/s: 69.39
Epoch: 0, Step: 7649, Batch(micro): 7649, Batch (considering grad accum): 956,  Loss: 6.4607, Time: 3.83s, Token/s: 133.65
Epoch: 0, Step: 7650, Batch(micro): 7650, Batch (considering grad accum): 956,  Loss: 6.2788, Time: 4.18s, Token/s: 122.62
Epoch: 0, Step: 7651, Batch(micro): 7651, Batch (considering grad accum): 956,  Loss: 5.9883, Time: 3.64s, Token/s: 140.57
Epoch: 0, Step: 7652, Batch(micro): 7652, Batch (considering grad accum): 956,  Loss: 6.1076, Time: 3.49s, Token/s: 146.81
Epoch: 0, Step: 7653, Batch(micro): 7653, Batch (considering grad accum): 956,  Loss: 6.0491, Time: 3.61s, Token/s: 141.81
Epoch: 0, Step: 7654, Batch(micro): 7654, Batch (considering grad accum): 956,  Loss: 5.8152, Time: 3.60s, Token/s: 142.39
Epoch: 0, Step: 7655, Batch(micro): 7655, Batch (considering grad accum): 956,  Loss: 5.4522, Time: 25.94s, Token/s: 19.74
Epoch: 0, Step: 7656, Batch(micro): 7656, Batch (considering grad accum): 957,  Loss: 5.2575, Time: 7.57s, Token/s: 67.65
Epoch: 0, Step: 7657, Batch(micro): 7657, Batch (considering grad accum): 957,  Loss: 6.1570, Time: 3.77s, Token/s: 135.93
Epoch: 0, Step: 7658, Batch(micro): 7658, Batch (considering grad accum): 957,  Loss: 6.8318, Time: 3.65s, Token/s: 140.10
Epoch: 0, Step: 7659, Batch(micro): 7659, Batch (considering grad accum): 957,  Loss: 6.7038, Time: 3.68s, Token/s: 139.18
Epoch: 0, Step: 7660, Batch(micro): 7660, Batch (considering grad accum): 957,  Loss: 6.0066, Time: 3.52s, Token/s: 145.56
Epoch: 0, Step: 7661, Batch(micro): 7661, Batch (considering grad accum): 957,  Loss: 6.3028, Time: 3.43s, Token/s: 149.47
Epoch: 0, Step: 7662, Batch(micro): 7662, Batch (considering grad accum): 957,  Loss: 6.1630, Time: 3.45s, Token/s: 148.38
Epoch: 0, Step: 7663, Batch(micro): 7663, Batch (considering grad accum): 957,  Loss: 5.8796, Time: 24.63s, Token/s: 20.79
Epoch: 0, Step: 7664, Batch(micro): 7664, Batch (considering grad accum): 958,  Loss: 6.0201, Time: 6.12s, Token/s: 83.62
Epoch: 0, Step: 7665, Batch(micro): 7665, Batch (considering grad accum): 958,  Loss: 5.7248, Time: 3.72s, Token/s: 137.53
Epoch: 0, Step: 7666, Batch(micro): 7666, Batch (considering grad accum): 958,  Loss: 5.9324, Time: 3.28s, Token/s: 156.30
Epoch: 0, Step: 7667, Batch(micro): 7667, Batch (considering grad accum): 958,  Loss: 6.2812, Time: 3.33s, Token/s: 153.64
Epoch: 0, Step: 7668, Batch(micro): 7668, Batch (considering grad accum): 958,  Loss: 6.4481, Time: 3.42s, Token/s: 149.86
Epoch: 0, Step: 7669, Batch(micro): 7669, Batch (considering grad accum): 958,  Loss: 5.1916, Time: 3.42s, Token/s: 149.66
Epoch: 0, Step: 7670, Batch(micro): 7670, Batch (considering grad accum): 958,  Loss: 6.0994, Time: 3.27s, Token/s: 156.35
Epoch: 0, Step: 7671, Batch(micro): 7671, Batch (considering grad accum): 958,  Loss: 6.8027, Time: 20.00s, Token/s: 25.60
Epoch: 0, Step: 7672, Batch(micro): 7672, Batch (considering grad accum): 959,  Loss: 6.8311, Time: 9.06s, Token/s: 56.52
Epoch: 0, Step: 7673, Batch(micro): 7673, Batch (considering grad accum): 959,  Loss: 7.3121, Time: 3.77s, Token/s: 135.94
Epoch: 0, Step: 7674, Batch(micro): 7674, Batch (considering grad accum): 959,  Loss: 6.9205, Time: 3.63s, Token/s: 141.11
Epoch: 0, Step: 7675, Batch(micro): 7675, Batch (considering grad accum): 959,  Loss: 5.8935, Time: 3.42s, Token/s: 149.73
Epoch: 0, Step: 7676, Batch(micro): 7676, Batch (considering grad accum): 959,  Loss: 6.1490, Time: 3.49s, Token/s: 146.64
Epoch: 0, Step: 7677, Batch(micro): 7677, Batch (considering grad accum): 959,  Loss: 5.6488, Time: 3.31s, Token/s: 154.66
Epoch: 0, Step: 7678, Batch(micro): 7678, Batch (considering grad accum): 959,  Loss: 6.8145, Time: 3.36s, Token/s: 152.42
Epoch: 0, Step: 7679, Batch(micro): 7679, Batch (considering grad accum): 959,  Loss: 5.6815, Time: 18.04s, Token/s: 28.38
Epoch: 0, Step: 7680, Batch(micro): 7680, Batch (considering grad accum): 960,  Loss: 5.8361, Time: 6.54s, Token/s: 78.32
Epoch: 0, Step: 7681, Batch(micro): 7681, Batch (considering grad accum): 960,  Loss: 6.2754, Time: 3.81s, Token/s: 134.33
Epoch: 0, Step: 7682, Batch(micro): 7682, Batch (considering grad accum): 960,  Loss: 6.1290, Time: 3.46s, Token/s: 147.78
Epoch: 0, Step: 7683, Batch(micro): 7683, Batch (considering grad accum): 960,  Loss: 6.5941, Time: 4.03s, Token/s: 127.00
Epoch: 0, Step: 7684, Batch(micro): 7684, Batch (considering grad accum): 960,  Loss: 6.1334, Time: 3.36s, Token/s: 152.49
Epoch: 0, Step: 7685, Batch(micro): 7685, Batch (considering grad accum): 960,  Loss: 6.0270, Time: 3.43s, Token/s: 149.12
Epoch: 0, Step: 7686, Batch(micro): 7686, Batch (considering grad accum): 960,  Loss: 5.3862, Time: 3.45s, Token/s: 148.40
Epoch: 0, Step: 7687, Batch(micro): 7687, Batch (considering grad accum): 960,  Loss: 4.9494, Time: 19.80s, Token/s: 25.86
Epoch: 0, Step: 7688, Batch(micro): 7688, Batch (considering grad accum): 961,  Loss: 5.8557, Time: 6.44s, Token/s: 79.54
Epoch: 0, Step: 7689, Batch(micro): 7689, Batch (considering grad accum): 961,  Loss: 5.8266, Time: 3.95s, Token/s: 129.74
Epoch: 0, Step: 7690, Batch(micro): 7690, Batch (considering grad accum): 961,  Loss: 6.1122, Time: 3.43s, Token/s: 149.19
Epoch: 0, Step: 7691, Batch(micro): 7691, Batch (considering grad accum): 961,  Loss: 6.0871, Time: 3.32s, Token/s: 154.07
Epoch: 0, Step: 7692, Batch(micro): 7692, Batch (considering grad accum): 961,  Loss: 5.9219, Time: 3.49s, Token/s: 146.88
Epoch: 0, Step: 7693, Batch(micro): 7693, Batch (considering grad accum): 961,  Loss: 6.4479, Time: 3.66s, Token/s: 139.83
Epoch: 0, Step: 7694, Batch(micro): 7694, Batch (considering grad accum): 961,  Loss: 5.7087, Time: 3.39s, Token/s: 151.04
Epoch: 0, Step: 7695, Batch(micro): 7695, Batch (considering grad accum): 961,  Loss: 5.9781, Time: 17.59s, Token/s: 29.11
Epoch: 0, Step: 7696, Batch(micro): 7696, Batch (considering grad accum): 962,  Loss: 5.9162, Time: 7.14s, Token/s: 71.73
Epoch: 0, Step: 7697, Batch(micro): 7697, Batch (considering grad accum): 962,  Loss: 5.8848, Time: 3.51s, Token/s: 145.81
Epoch: 0, Step: 7698, Batch(micro): 7698, Batch (considering grad accum): 962,  Loss: 6.6564, Time: 3.37s, Token/s: 152.13
Epoch: 0, Step: 7699, Batch(micro): 7699, Batch (considering grad accum): 962,  Loss: 5.8706, Time: 3.32s, Token/s: 154.03
Updating MLP bias
Epoch: 0, Step: 7700, Batch(micro): 7700, Batch (considering grad accum): 962,  Loss: 5.3895, Time: 3.30s, Token/s: 155.04
Epoch: 0, Step: 7701, Batch(micro): 7701, Batch (considering grad accum): 962,  Loss: 7.0191, Time: 3.21s, Token/s: 159.52
Epoch: 0, Step: 7702, Batch(micro): 7702, Batch (considering grad accum): 962,  Loss: 6.5431, Time: 3.34s, Token/s: 153.09
Epoch: 0, Step: 7703, Batch(micro): 7703, Batch (considering grad accum): 962,  Loss: 6.7726, Time: 19.16s, Token/s: 26.73
Epoch: 0, Step: 7704, Batch(micro): 7704, Batch (considering grad accum): 963,  Loss: 5.7445, Time: 6.90s, Token/s: 74.24
Epoch: 0, Step: 7705, Batch(micro): 7705, Batch (considering grad accum): 963,  Loss: 5.6512, Time: 3.96s, Token/s: 129.29
Epoch: 0, Step: 7706, Batch(micro): 7706, Batch (considering grad accum): 963,  Loss: 5.5157, Time: 3.53s, Token/s: 145.08
Epoch: 0, Step: 7707, Batch(micro): 7707, Batch (considering grad accum): 963,  Loss: 6.5956, Time: 3.79s, Token/s: 134.96
Epoch: 0, Step: 7708, Batch(micro): 7708, Batch (considering grad accum): 963,  Loss: 6.3723, Time: 3.65s, Token/s: 140.22
Epoch: 0, Step: 7709, Batch(micro): 7709, Batch (considering grad accum): 963,  Loss: 6.1596, Time: 3.75s, Token/s: 136.58
Epoch: 0, Step: 7710, Batch(micro): 7710, Batch (considering grad accum): 963,  Loss: 6.4824, Time: 3.46s, Token/s: 148.08
Epoch: 0, Step: 7711, Batch(micro): 7711, Batch (considering grad accum): 963,  Loss: 6.4464, Time: 17.01s, Token/s: 30.10
Epoch: 0, Step: 7712, Batch(micro): 7712, Batch (considering grad accum): 964,  Loss: 5.9611, Time: 6.21s, Token/s: 82.39
Epoch: 0, Step: 7713, Batch(micro): 7713, Batch (considering grad accum): 964,  Loss: 5.5866, Time: 3.82s, Token/s: 133.97
Epoch: 0, Step: 7714, Batch(micro): 7714, Batch (considering grad accum): 964,  Loss: 5.5205, Time: 3.37s, Token/s: 151.96
Epoch: 0, Step: 7715, Batch(micro): 7715, Batch (considering grad accum): 964,  Loss: 6.1816, Time: 3.36s, Token/s: 152.32
Epoch: 0, Step: 7716, Batch(micro): 7716, Batch (considering grad accum): 964,  Loss: 7.5771, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 7717, Batch(micro): 7717, Batch (considering grad accum): 964,  Loss: 7.8362, Time: 3.67s, Token/s: 139.39
Epoch: 0, Step: 7718, Batch(micro): 7718, Batch (considering grad accum): 964,  Loss: 6.4752, Time: 3.40s, Token/s: 150.57
Epoch: 0, Step: 7719, Batch(micro): 7719, Batch (considering grad accum): 964,  Loss: 6.4037, Time: 18.35s, Token/s: 27.91
Epoch: 0, Step: 7720, Batch(micro): 7720, Batch (considering grad accum): 965,  Loss: 5.8267, Time: 6.43s, Token/s: 79.59
Epoch: 0, Step: 7721, Batch(micro): 7721, Batch (considering grad accum): 965,  Loss: 6.6462, Time: 3.92s, Token/s: 130.46
Epoch: 0, Step: 7722, Batch(micro): 7722, Batch (considering grad accum): 965,  Loss: 6.2804, Time: 3.60s, Token/s: 142.24
Epoch: 0, Step: 7723, Batch(micro): 7723, Batch (considering grad accum): 965,  Loss: 6.1097, Time: 3.60s, Token/s: 142.28
Epoch: 0, Step: 7724, Batch(micro): 7724, Batch (considering grad accum): 965,  Loss: 6.8509, Time: 3.58s, Token/s: 143.21
Epoch: 0, Step: 7725, Batch(micro): 7725, Batch (considering grad accum): 965,  Loss: 6.2452, Time: 3.89s, Token/s: 131.51
Epoch: 0, Step: 7726, Batch(micro): 7726, Batch (considering grad accum): 965,  Loss: 6.0763, Time: 3.44s, Token/s: 148.92
Epoch: 0, Step: 7727, Batch(micro): 7727, Batch (considering grad accum): 965,  Loss: 5.6250, Time: 25.69s, Token/s: 19.93
Epoch: 0, Step: 7728, Batch(micro): 7728, Batch (considering grad accum): 966,  Loss: 5.9507, Time: 8.00s, Token/s: 64.02
Epoch: 0, Step: 7729, Batch(micro): 7729, Batch (considering grad accum): 966,  Loss: 5.9986, Time: 3.74s, Token/s: 136.97
Epoch: 0, Step: 7730, Batch(micro): 7730, Batch (considering grad accum): 966,  Loss: 6.1685, Time: 3.47s, Token/s: 147.49
Epoch: 0, Step: 7731, Batch(micro): 7731, Batch (considering grad accum): 966,  Loss: 6.9309, Time: 3.36s, Token/s: 152.59
Epoch: 0, Step: 7732, Batch(micro): 7732, Batch (considering grad accum): 966,  Loss: 6.8691, Time: 3.42s, Token/s: 149.85
Epoch: 0, Step: 7733, Batch(micro): 7733, Batch (considering grad accum): 966,  Loss: 6.6858, Time: 3.58s, Token/s: 142.91
Epoch: 0, Step: 7734, Batch(micro): 7734, Batch (considering grad accum): 966,  Loss: 6.0304, Time: 3.50s, Token/s: 146.09
Epoch: 0, Step: 7735, Batch(micro): 7735, Batch (considering grad accum): 966,  Loss: 6.5571, Time: 23.56s, Token/s: 21.73
Epoch: 0, Step: 7736, Batch(micro): 7736, Batch (considering grad accum): 967,  Loss: 5.8377, Time: 6.26s, Token/s: 81.82
Epoch: 0, Step: 7737, Batch(micro): 7737, Batch (considering grad accum): 967,  Loss: 5.9157, Time: 3.73s, Token/s: 137.34
Epoch: 0, Step: 7738, Batch(micro): 7738, Batch (considering grad accum): 967,  Loss: 5.6915, Time: 3.52s, Token/s: 145.42
Epoch: 0, Step: 7739, Batch(micro): 7739, Batch (considering grad accum): 967,  Loss: 6.2208, Time: 3.58s, Token/s: 143.16
Epoch: 0, Step: 7740, Batch(micro): 7740, Batch (considering grad accum): 967,  Loss: 6.7013, Time: 3.35s, Token/s: 152.87
Epoch: 0, Step: 7741, Batch(micro): 7741, Batch (considering grad accum): 967,  Loss: 6.4263, Time: 3.64s, Token/s: 140.74
Epoch: 0, Step: 7742, Batch(micro): 7742, Batch (considering grad accum): 967,  Loss: 6.2051, Time: 3.56s, Token/s: 143.83
Epoch: 0, Step: 7743, Batch(micro): 7743, Batch (considering grad accum): 967,  Loss: 5.5038, Time: 26.07s, Token/s: 19.64
Epoch: 0, Step: 7744, Batch(micro): 7744, Batch (considering grad accum): 968,  Loss: 5.6921, Time: 6.78s, Token/s: 75.52
Epoch: 0, Step: 7745, Batch(micro): 7745, Batch (considering grad accum): 968,  Loss: 5.2862, Time: 3.83s, Token/s: 133.65
Epoch: 0, Step: 7746, Batch(micro): 7746, Batch (considering grad accum): 968,  Loss: 6.2828, Time: 3.30s, Token/s: 155.01
Epoch: 0, Step: 7747, Batch(micro): 7747, Batch (considering grad accum): 968,  Loss: 5.9226, Time: 3.40s, Token/s: 150.74
Epoch: 0, Step: 7748, Batch(micro): 7748, Batch (considering grad accum): 968,  Loss: 6.1788, Time: 3.96s, Token/s: 129.44
Epoch: 0, Step: 7749, Batch(micro): 7749, Batch (considering grad accum): 968,  Loss: 5.5089, Time: 3.44s, Token/s: 148.92
Epoch: 0, Step: 7750, Batch(micro): 7750, Batch (considering grad accum): 968,  Loss: 5.5855, Time: 3.65s, Token/s: 140.17
Epoch: 0, Step: 7751, Batch(micro): 7751, Batch (considering grad accum): 968,  Loss: 5.0223, Time: 25.93s, Token/s: 19.74
Epoch: 0, Step: 7752, Batch(micro): 7752, Batch (considering grad accum): 969,  Loss: 6.2213, Time: 7.58s, Token/s: 67.56
Epoch: 0, Step: 7753, Batch(micro): 7753, Batch (considering grad accum): 969,  Loss: 6.1523, Time: 3.50s, Token/s: 146.32
Epoch: 0, Step: 7754, Batch(micro): 7754, Batch (considering grad accum): 969,  Loss: 6.7081, Time: 3.10s, Token/s: 165.24
Epoch: 0, Step: 7755, Batch(micro): 7755, Batch (considering grad accum): 969,  Loss: 7.3294, Time: 3.15s, Token/s: 162.36
Epoch: 0, Step: 7756, Batch(micro): 7756, Batch (considering grad accum): 969,  Loss: 6.0835, Time: 3.12s, Token/s: 163.95
Epoch: 0, Step: 7757, Batch(micro): 7757, Batch (considering grad accum): 969,  Loss: 6.7986, Time: 3.02s, Token/s: 169.41
Epoch: 0, Step: 7758, Batch(micro): 7758, Batch (considering grad accum): 969,  Loss: 7.5195, Time: 2.99s, Token/s: 171.06
Epoch: 0, Step: 7759, Batch(micro): 7759, Batch (considering grad accum): 969,  Loss: 6.9120, Time: 22.27s, Token/s: 22.99
Epoch: 0, Step: 7760, Batch(micro): 7760, Batch (considering grad accum): 970,  Loss: 6.0306, Time: 7.34s, Token/s: 69.74
Epoch: 0, Step: 7761, Batch(micro): 7761, Batch (considering grad accum): 970,  Loss: 6.2643, Time: 4.53s, Token/s: 112.94
Epoch: 0, Step: 7762, Batch(micro): 7762, Batch (considering grad accum): 970,  Loss: 5.3795, Time: 3.57s, Token/s: 143.32
Epoch: 0, Step: 7763, Batch(micro): 7763, Batch (considering grad accum): 970,  Loss: 6.4140, Time: 3.59s, Token/s: 142.77
Epoch: 0, Step: 7764, Batch(micro): 7764, Batch (considering grad accum): 970,  Loss: 5.6524, Time: 3.34s, Token/s: 153.48
Epoch: 0, Step: 7765, Batch(micro): 7765, Batch (considering grad accum): 970,  Loss: 6.1489, Time: 3.06s, Token/s: 167.08
Epoch: 0, Step: 7766, Batch(micro): 7766, Batch (considering grad accum): 970,  Loss: 5.7424, Time: 3.01s, Token/s: 170.18
Epoch: 0, Step: 7767, Batch(micro): 7767, Batch (considering grad accum): 970,  Loss: 6.0463, Time: 22.93s, Token/s: 22.33
Epoch: 0, Step: 7768, Batch(micro): 7768, Batch (considering grad accum): 971,  Loss: 6.1619, Time: 7.27s, Token/s: 70.42
Epoch: 0, Step: 7769, Batch(micro): 7769, Batch (considering grad accum): 971,  Loss: 6.4572, Time: 3.71s, Token/s: 138.00
Epoch: 0, Step: 7770, Batch(micro): 7770, Batch (considering grad accum): 971,  Loss: 6.1572, Time: 3.60s, Token/s: 142.08
Epoch: 0, Step: 7771, Batch(micro): 7771, Batch (considering grad accum): 971,  Loss: 5.9849, Time: 3.26s, Token/s: 157.09
Epoch: 0, Step: 7772, Batch(micro): 7772, Batch (considering grad accum): 971,  Loss: 6.6102, Time: 3.28s, Token/s: 156.04
Epoch: 0, Step: 7773, Batch(micro): 7773, Batch (considering grad accum): 971,  Loss: 5.9277, Time: 3.25s, Token/s: 157.32
Epoch: 0, Step: 7774, Batch(micro): 7774, Batch (considering grad accum): 971,  Loss: 6.1624, Time: 3.11s, Token/s: 164.76
Epoch: 0, Step: 7775, Batch(micro): 7775, Batch (considering grad accum): 971,  Loss: 6.5128, Time: 22.79s, Token/s: 22.47
Epoch: 0, Step: 7776, Batch(micro): 7776, Batch (considering grad accum): 972,  Loss: 6.0355, Time: 6.84s, Token/s: 74.87
Epoch: 0, Step: 7777, Batch(micro): 7777, Batch (considering grad accum): 972,  Loss: 5.5490, Time: 3.72s, Token/s: 137.67
Epoch: 0, Step: 7778, Batch(micro): 7778, Batch (considering grad accum): 972,  Loss: 5.3781, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 7779, Batch(micro): 7779, Batch (considering grad accum): 972,  Loss: 5.8432, Time: 3.03s, Token/s: 169.05
Epoch: 0, Step: 7780, Batch(micro): 7780, Batch (considering grad accum): 972,  Loss: 5.6753, Time: 3.15s, Token/s: 162.39
Epoch: 0, Step: 7781, Batch(micro): 7781, Batch (considering grad accum): 972,  Loss: 6.1060, Time: 3.23s, Token/s: 158.32
Epoch: 0, Step: 7782, Batch(micro): 7782, Batch (considering grad accum): 972,  Loss: 4.9791, Time: 2.94s, Token/s: 174.05
Epoch: 0, Step: 7783, Batch(micro): 7783, Batch (considering grad accum): 972,  Loss: 5.0608, Time: 23.10s, Token/s: 22.17
Epoch: 0, Step: 7784, Batch(micro): 7784, Batch (considering grad accum): 973,  Loss: 6.4999, Time: 5.65s, Token/s: 90.69
Epoch: 0, Step: 7785, Batch(micro): 7785, Batch (considering grad accum): 973,  Loss: 5.9929, Time: 3.58s, Token/s: 143.09
Epoch: 0, Step: 7786, Batch(micro): 7786, Batch (considering grad accum): 973,  Loss: 6.3046, Time: 3.39s, Token/s: 150.88
Epoch: 0, Step: 7787, Batch(micro): 7787, Batch (considering grad accum): 973,  Loss: 6.3290, Time: 3.21s, Token/s: 159.30
Epoch: 0, Step: 7788, Batch(micro): 7788, Batch (considering grad accum): 973,  Loss: 6.7551, Time: 3.14s, Token/s: 163.17
Epoch: 0, Step: 7789, Batch(micro): 7789, Batch (considering grad accum): 973,  Loss: 6.5596, Time: 3.01s, Token/s: 170.02
Epoch: 0, Step: 7790, Batch(micro): 7790, Batch (considering grad accum): 973,  Loss: 6.0489, Time: 2.98s, Token/s: 171.61
Epoch: 0, Step: 7791, Batch(micro): 7791, Batch (considering grad accum): 973,  Loss: 6.2337, Time: 23.81s, Token/s: 21.50
Epoch: 0, Step: 7792, Batch(micro): 7792, Batch (considering grad accum): 974,  Loss: 5.4674, Time: 7.42s, Token/s: 69.03
Epoch: 0, Step: 7793, Batch(micro): 7793, Batch (considering grad accum): 974,  Loss: 5.8291, Time: 3.48s, Token/s: 147.14
Epoch: 0, Step: 7794, Batch(micro): 7794, Batch (considering grad accum): 974,  Loss: 5.8418, Time: 3.31s, Token/s: 154.64
Epoch: 0, Step: 7795, Batch(micro): 7795, Batch (considering grad accum): 974,  Loss: 5.6667, Time: 3.38s, Token/s: 151.43
Epoch: 0, Step: 7796, Batch(micro): 7796, Batch (considering grad accum): 974,  Loss: 5.6562, Time: 3.37s, Token/s: 152.01
Epoch: 0, Step: 7797, Batch(micro): 7797, Batch (considering grad accum): 974,  Loss: 5.8021, Time: 3.47s, Token/s: 147.72
Epoch: 0, Step: 7798, Batch(micro): 7798, Batch (considering grad accum): 974,  Loss: 6.0728, Time: 3.26s, Token/s: 157.19
Epoch: 0, Step: 7799, Batch(micro): 7799, Batch (considering grad accum): 974,  Loss: 6.0844, Time: 24.47s, Token/s: 20.92
Updating MLP bias
Epoch: 0, Step: 7800, Batch(micro): 7800, Batch (considering grad accum): 975,  Loss: 6.4450, Time: 7.81s, Token/s: 65.54
Epoch: 0, Step: 7801, Batch(micro): 7801, Batch (considering grad accum): 975,  Loss: 5.8924, Time: 3.45s, Token/s: 148.24
Epoch: 0, Step: 7802, Batch(micro): 7802, Batch (considering grad accum): 975,  Loss: 6.1402, Time: 3.18s, Token/s: 161.08
Epoch: 0, Step: 7803, Batch(micro): 7803, Batch (considering grad accum): 975,  Loss: 6.0541, Time: 3.17s, Token/s: 161.49
Epoch: 0, Step: 7804, Batch(micro): 7804, Batch (considering grad accum): 975,  Loss: 6.2818, Time: 3.49s, Token/s: 146.69
Epoch: 0, Step: 7805, Batch(micro): 7805, Batch (considering grad accum): 975,  Loss: 6.0828, Time: 3.49s, Token/s: 146.90
Epoch: 0, Step: 7806, Batch(micro): 7806, Batch (considering grad accum): 975,  Loss: 5.8365, Time: 2.92s, Token/s: 175.18
Epoch: 0, Step: 7807, Batch(micro): 7807, Batch (considering grad accum): 975,  Loss: 5.7129, Time: 22.40s, Token/s: 22.86
Epoch: 0, Step: 7808, Batch(micro): 7808, Batch (considering grad accum): 976,  Loss: 5.8282, Time: 7.25s, Token/s: 70.65
Epoch: 0, Step: 7809, Batch(micro): 7809, Batch (considering grad accum): 976,  Loss: 5.7527, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 7810, Batch(micro): 7810, Batch (considering grad accum): 976,  Loss: 6.1990, Time: 3.03s, Token/s: 169.03
Epoch: 0, Step: 7811, Batch(micro): 7811, Batch (considering grad accum): 976,  Loss: 6.0324, Time: 3.57s, Token/s: 143.51
Epoch: 0, Step: 7812, Batch(micro): 7812, Batch (considering grad accum): 976,  Loss: 5.6120, Time: 3.35s, Token/s: 152.67
Epoch: 0, Step: 7813, Batch(micro): 7813, Batch (considering grad accum): 976,  Loss: 5.8122, Time: 3.16s, Token/s: 161.85
Epoch: 0, Step: 7814, Batch(micro): 7814, Batch (considering grad accum): 976,  Loss: 5.6388, Time: 3.21s, Token/s: 159.48
Epoch: 0, Step: 7815, Batch(micro): 7815, Batch (considering grad accum): 976,  Loss: 5.6722, Time: 22.07s, Token/s: 23.20
Epoch: 0, Step: 7816, Batch(micro): 7816, Batch (considering grad accum): 977,  Loss: 6.4023, Time: 7.45s, Token/s: 68.69
Epoch: 0, Step: 7817, Batch(micro): 7817, Batch (considering grad accum): 977,  Loss: 6.4912, Time: 3.85s, Token/s: 132.99
Epoch: 0, Step: 7818, Batch(micro): 7818, Batch (considering grad accum): 977,  Loss: 6.0717, Time: 3.28s, Token/s: 156.28
Epoch: 0, Step: 7819, Batch(micro): 7819, Batch (considering grad accum): 977,  Loss: 6.3361, Time: 3.26s, Token/s: 156.89
Epoch: 0, Step: 7820, Batch(micro): 7820, Batch (considering grad accum): 977,  Loss: 6.7086, Time: 3.32s, Token/s: 154.15
Epoch: 0, Step: 7821, Batch(micro): 7821, Batch (considering grad accum): 977,  Loss: 6.3517, Time: 3.21s, Token/s: 159.75
Epoch: 0, Step: 7822, Batch(micro): 7822, Batch (considering grad accum): 977,  Loss: 6.2731, Time: 2.91s, Token/s: 176.20
Epoch: 0, Step: 7823, Batch(micro): 7823, Batch (considering grad accum): 977,  Loss: 5.7378, Time: 22.38s, Token/s: 22.88
Epoch: 0, Step: 7824, Batch(micro): 7824, Batch (considering grad accum): 978,  Loss: 5.7785, Time: 6.70s, Token/s: 76.43
Epoch: 0, Step: 7825, Batch(micro): 7825, Batch (considering grad accum): 978,  Loss: 5.4832, Time: 3.84s, Token/s: 133.36
Epoch: 0, Step: 7826, Batch(micro): 7826, Batch (considering grad accum): 978,  Loss: 5.9069, Time: 3.30s, Token/s: 155.37
Epoch: 0, Step: 7827, Batch(micro): 7827, Batch (considering grad accum): 978,  Loss: 5.8609, Time: 3.25s, Token/s: 157.39
Epoch: 0, Step: 7828, Batch(micro): 7828, Batch (considering grad accum): 978,  Loss: 6.1569, Time: 2.87s, Token/s: 178.18
Epoch: 0, Step: 7829, Batch(micro): 7829, Batch (considering grad accum): 978,  Loss: 5.7082, Time: 2.90s, Token/s: 176.61
Epoch: 0, Step: 7830, Batch(micro): 7830, Batch (considering grad accum): 978,  Loss: 6.0934, Time: 2.84s, Token/s: 180.00
Epoch: 0, Step: 7831, Batch(micro): 7831, Batch (considering grad accum): 978,  Loss: 5.9534, Time: 20.77s, Token/s: 24.65
Epoch: 0, Step: 7832, Batch(micro): 7832, Batch (considering grad accum): 979,  Loss: 6.1846, Time: 6.27s, Token/s: 81.62
Epoch: 0, Step: 7833, Batch(micro): 7833, Batch (considering grad accum): 979,  Loss: 6.0702, Time: 3.42s, Token/s: 149.62
Epoch: 0, Step: 7834, Batch(micro): 7834, Batch (considering grad accum): 979,  Loss: 6.0590, Time: 3.42s, Token/s: 149.63
Epoch: 0, Step: 7835, Batch(micro): 7835, Batch (considering grad accum): 979,  Loss: 6.7218, Time: 3.23s, Token/s: 158.41
Epoch: 0, Step: 7836, Batch(micro): 7836, Batch (considering grad accum): 979,  Loss: 5.8554, Time: 3.36s, Token/s: 152.28
Epoch: 0, Step: 7837, Batch(micro): 7837, Batch (considering grad accum): 979,  Loss: 5.2361, Time: 3.18s, Token/s: 161.17
Epoch: 0, Step: 7838, Batch(micro): 7838, Batch (considering grad accum): 979,  Loss: 5.8887, Time: 3.04s, Token/s: 168.57
Epoch: 0, Step: 7839, Batch(micro): 7839, Batch (considering grad accum): 979,  Loss: 5.9951, Time: 21.64s, Token/s: 23.66
Epoch: 0, Step: 7840, Batch(micro): 7840, Batch (considering grad accum): 980,  Loss: 6.3405, Time: 7.18s, Token/s: 71.29
Epoch: 0, Step: 7841, Batch(micro): 7841, Batch (considering grad accum): 980,  Loss: 5.7242, Time: 3.56s, Token/s: 143.65
Epoch: 0, Step: 7842, Batch(micro): 7842, Batch (considering grad accum): 980,  Loss: 6.2876, Time: 3.07s, Token/s: 166.62
Epoch: 0, Step: 7843, Batch(micro): 7843, Batch (considering grad accum): 980,  Loss: 6.0059, Time: 3.49s, Token/s: 146.70
Epoch: 0, Step: 7844, Batch(micro): 7844, Batch (considering grad accum): 980,  Loss: 6.1384, Time: 3.29s, Token/s: 155.47
Epoch: 0, Step: 7845, Batch(micro): 7845, Batch (considering grad accum): 980,  Loss: 5.7083, Time: 2.91s, Token/s: 175.76
Epoch: 0, Step: 7846, Batch(micro): 7846, Batch (considering grad accum): 980,  Loss: 5.9032, Time: 3.02s, Token/s: 169.36
Epoch: 0, Step: 7847, Batch(micro): 7847, Batch (considering grad accum): 980,  Loss: 5.6065, Time: 22.89s, Token/s: 22.37
Epoch: 0, Step: 7848, Batch(micro): 7848, Batch (considering grad accum): 981,  Loss: 5.6964, Time: 6.95s, Token/s: 73.72
Epoch: 0, Step: 7849, Batch(micro): 7849, Batch (considering grad accum): 981,  Loss: 5.9787, Time: 3.67s, Token/s: 139.69
Epoch: 0, Step: 7850, Batch(micro): 7850, Batch (considering grad accum): 981,  Loss: 7.0406, Time: 3.32s, Token/s: 154.16
Epoch: 0, Step: 7851, Batch(micro): 7851, Batch (considering grad accum): 981,  Loss: 6.7275, Time: 3.31s, Token/s: 154.59
Epoch: 0, Step: 7852, Batch(micro): 7852, Batch (considering grad accum): 981,  Loss: 5.8319, Time: 3.29s, Token/s: 155.55
Epoch: 0, Step: 7853, Batch(micro): 7853, Batch (considering grad accum): 981,  Loss: 5.4178, Time: 3.15s, Token/s: 162.46
Epoch: 0, Step: 7854, Batch(micro): 7854, Batch (considering grad accum): 981,  Loss: 5.7803, Time: 3.13s, Token/s: 163.61
Epoch: 0, Step: 7855, Batch(micro): 7855, Batch (considering grad accum): 981,  Loss: 6.1751, Time: 23.73s, Token/s: 21.57
Epoch: 0, Step: 7856, Batch(micro): 7856, Batch (considering grad accum): 982,  Loss: 6.0525, Time: 7.57s, Token/s: 67.59
Epoch: 0, Step: 7857, Batch(micro): 7857, Batch (considering grad accum): 982,  Loss: 6.1493, Time: 3.57s, Token/s: 143.47
Epoch: 0, Step: 7858, Batch(micro): 7858, Batch (considering grad accum): 982,  Loss: 6.1749, Time: 3.57s, Token/s: 143.28
Epoch: 0, Step: 7859, Batch(micro): 7859, Batch (considering grad accum): 982,  Loss: 6.1781, Time: 3.51s, Token/s: 145.93
Epoch: 0, Step: 7860, Batch(micro): 7860, Batch (considering grad accum): 982,  Loss: 4.5459, Time: 3.24s, Token/s: 158.16
Epoch: 0, Step: 7861, Batch(micro): 7861, Batch (considering grad accum): 982,  Loss: 6.0749, Time: 3.23s, Token/s: 158.62
Epoch: 0, Step: 7862, Batch(micro): 7862, Batch (considering grad accum): 982,  Loss: 5.4253, Time: 3.39s, Token/s: 151.20
Epoch: 0, Step: 7863, Batch(micro): 7863, Batch (considering grad accum): 982,  Loss: 6.2531, Time: 25.81s, Token/s: 19.84
Epoch: 0, Step: 7864, Batch(micro): 7864, Batch (considering grad accum): 983,  Loss: 5.7495, Time: 8.46s, Token/s: 60.49
Epoch: 0, Step: 7865, Batch(micro): 7865, Batch (considering grad accum): 983,  Loss: 6.3138, Time: 3.81s, Token/s: 134.47
Epoch: 0, Step: 7866, Batch(micro): 7866, Batch (considering grad accum): 983,  Loss: 6.2017, Time: 3.13s, Token/s: 163.75
Epoch: 0, Step: 7867, Batch(micro): 7867, Batch (considering grad accum): 983,  Loss: 6.6057, Time: 3.08s, Token/s: 166.10
Epoch: 0, Step: 7868, Batch(micro): 7868, Batch (considering grad accum): 983,  Loss: 6.0158, Time: 3.09s, Token/s: 165.51
Epoch: 0, Step: 7869, Batch(micro): 7869, Batch (considering grad accum): 983,  Loss: 6.6067, Time: 2.92s, Token/s: 175.46
Epoch: 0, Step: 7870, Batch(micro): 7870, Batch (considering grad accum): 983,  Loss: 6.4052, Time: 2.96s, Token/s: 172.89
Epoch: 0, Step: 7871, Batch(micro): 7871, Batch (considering grad accum): 983,  Loss: 5.6619, Time: 23.66s, Token/s: 21.64
Epoch: 0, Step: 7872, Batch(micro): 7872, Batch (considering grad accum): 984,  Loss: 5.8889, Time: 6.67s, Token/s: 76.80
Epoch: 0, Step: 7873, Batch(micro): 7873, Batch (considering grad accum): 984,  Loss: 6.3397, Time: 3.69s, Token/s: 138.78
Epoch: 0, Step: 7874, Batch(micro): 7874, Batch (considering grad accum): 984,  Loss: 6.7944, Time: 3.33s, Token/s: 153.96
Epoch: 0, Step: 7875, Batch(micro): 7875, Batch (considering grad accum): 984,  Loss: 6.7339, Time: 3.25s, Token/s: 157.46
Epoch: 0, Step: 7876, Batch(micro): 7876, Batch (considering grad accum): 984,  Loss: 5.4450, Time: 3.16s, Token/s: 162.20
Epoch: 0, Step: 7877, Batch(micro): 7877, Batch (considering grad accum): 984,  Loss: 5.4277, Time: 3.36s, Token/s: 152.54
Epoch: 0, Step: 7878, Batch(micro): 7878, Batch (considering grad accum): 984,  Loss: 5.6068, Time: 2.97s, Token/s: 172.41
Epoch: 0, Step: 7879, Batch(micro): 7879, Batch (considering grad accum): 984,  Loss: 6.1473, Time: 22.92s, Token/s: 22.34
Epoch: 0, Step: 7880, Batch(micro): 7880, Batch (considering grad accum): 985,  Loss: 5.7269, Time: 8.80s, Token/s: 58.17
Epoch: 0, Step: 7881, Batch(micro): 7881, Batch (considering grad accum): 985,  Loss: 5.6921, Time: 3.62s, Token/s: 141.26
Epoch: 0, Step: 7882, Batch(micro): 7882, Batch (considering grad accum): 985,  Loss: 5.8262, Time: 3.19s, Token/s: 160.68
Epoch: 0, Step: 7883, Batch(micro): 7883, Batch (considering grad accum): 985,  Loss: 5.5109, Time: 3.29s, Token/s: 155.73
Epoch: 0, Step: 7884, Batch(micro): 7884, Batch (considering grad accum): 985,  Loss: 6.0056, Time: 3.09s, Token/s: 165.94
Epoch: 0, Step: 7885, Batch(micro): 7885, Batch (considering grad accum): 985,  Loss: 6.0411, Time: 3.04s, Token/s: 168.51
Epoch: 0, Step: 7886, Batch(micro): 7886, Batch (considering grad accum): 985,  Loss: 5.9357, Time: 3.00s, Token/s: 170.46
Epoch: 0, Step: 7887, Batch(micro): 7887, Batch (considering grad accum): 985,  Loss: 6.4930, Time: 23.47s, Token/s: 21.81
Epoch: 0, Step: 7888, Batch(micro): 7888, Batch (considering grad accum): 986,  Loss: 5.9008, Time: 8.55s, Token/s: 59.92
Epoch: 0, Step: 7889, Batch(micro): 7889, Batch (considering grad accum): 986,  Loss: 5.6244, Time: 3.96s, Token/s: 129.24
Epoch: 0, Step: 7890, Batch(micro): 7890, Batch (considering grad accum): 986,  Loss: 5.5084, Time: 3.39s, Token/s: 150.93
Epoch: 0, Step: 7891, Batch(micro): 7891, Batch (considering grad accum): 986,  Loss: 5.7670, Time: 3.26s, Token/s: 157.13
Epoch: 0, Step: 7892, Batch(micro): 7892, Batch (considering grad accum): 986,  Loss: 7.8518, Time: 3.37s, Token/s: 151.82
Epoch: 0, Step: 7893, Batch(micro): 7893, Batch (considering grad accum): 986,  Loss: 6.5666, Time: 3.27s, Token/s: 156.70
Epoch: 0, Step: 7894, Batch(micro): 7894, Batch (considering grad accum): 986,  Loss: 6.2018, Time: 3.13s, Token/s: 163.82
Epoch: 0, Step: 7895, Batch(micro): 7895, Batch (considering grad accum): 986,  Loss: 6.1046, Time: 21.00s, Token/s: 24.38
Epoch: 0, Step: 7896, Batch(micro): 7896, Batch (considering grad accum): 987,  Loss: 6.1040, Time: 8.99s, Token/s: 56.95
Epoch: 0, Step: 7897, Batch(micro): 7897, Batch (considering grad accum): 987,  Loss: 5.7017, Time: 3.51s, Token/s: 145.68
Epoch: 0, Step: 7898, Batch(micro): 7898, Batch (considering grad accum): 987,  Loss: 6.1398, Time: 2.98s, Token/s: 171.62
Epoch: 0, Step: 7899, Batch(micro): 7899, Batch (considering grad accum): 987,  Loss: 5.8265, Time: 3.16s, Token/s: 162.14
Updating MLP bias
Epoch: 0, Step: 7900, Batch(micro): 7900, Batch (considering grad accum): 987,  Loss: 5.5072, Time: 3.10s, Token/s: 164.90
Epoch: 0, Step: 7901, Batch(micro): 7901, Batch (considering grad accum): 987,  Loss: 5.6044, Time: 3.02s, Token/s: 169.59
Epoch: 0, Step: 7902, Batch(micro): 7902, Batch (considering grad accum): 987,  Loss: 5.4558, Time: 3.00s, Token/s: 170.49
Epoch: 0, Step: 7903, Batch(micro): 7903, Batch (considering grad accum): 987,  Loss: 5.3745, Time: 23.97s, Token/s: 21.36
Epoch: 0, Step: 7904, Batch(micro): 7904, Batch (considering grad accum): 988,  Loss: 5.9589, Time: 6.83s, Token/s: 74.93
Epoch: 0, Step: 7905, Batch(micro): 7905, Batch (considering grad accum): 988,  Loss: 6.1386, Time: 3.65s, Token/s: 140.30
Epoch: 0, Step: 7906, Batch(micro): 7906, Batch (considering grad accum): 988,  Loss: 6.6111, Time: 3.38s, Token/s: 151.26
Epoch: 0, Step: 7907, Batch(micro): 7907, Batch (considering grad accum): 988,  Loss: 6.6101, Time: 3.30s, Token/s: 155.16
Epoch: 0, Step: 7908, Batch(micro): 7908, Batch (considering grad accum): 988,  Loss: 6.4777, Time: 3.42s, Token/s: 149.68
Epoch: 0, Step: 7909, Batch(micro): 7909, Batch (considering grad accum): 988,  Loss: 6.3773, Time: 3.40s, Token/s: 150.62
Epoch: 0, Step: 7910, Batch(micro): 7910, Batch (considering grad accum): 988,  Loss: 6.5896, Time: 3.13s, Token/s: 163.37
Epoch: 0, Step: 7911, Batch(micro): 7911, Batch (considering grad accum): 988,  Loss: 5.7332, Time: 22.30s, Token/s: 22.96
Epoch: 0, Step: 7912, Batch(micro): 7912, Batch (considering grad accum): 989,  Loss: 5.8059, Time: 8.48s, Token/s: 60.35
Epoch: 0, Step: 7913, Batch(micro): 7913, Batch (considering grad accum): 989,  Loss: 5.8993, Time: 3.73s, Token/s: 137.39
Epoch: 0, Step: 7914, Batch(micro): 7914, Batch (considering grad accum): 989,  Loss: 6.1277, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 7915, Batch(micro): 7915, Batch (considering grad accum): 989,  Loss: 5.7911, Time: 3.25s, Token/s: 157.59
Epoch: 0, Step: 7916, Batch(micro): 7916, Batch (considering grad accum): 989,  Loss: 5.9421, Time: 3.10s, Token/s: 165.37
Epoch: 0, Step: 7917, Batch(micro): 7917, Batch (considering grad accum): 989,  Loss: 6.2934, Time: 2.84s, Token/s: 180.20
Epoch: 0, Step: 7918, Batch(micro): 7918, Batch (considering grad accum): 989,  Loss: 6.0154, Time: 2.94s, Token/s: 174.36
Epoch: 0, Step: 7919, Batch(micro): 7919, Batch (considering grad accum): 989,  Loss: 5.7983, Time: 22.29s, Token/s: 22.97
Epoch: 0, Step: 7920, Batch(micro): 7920, Batch (considering grad accum): 990,  Loss: 6.0329, Time: 7.58s, Token/s: 67.56
Epoch: 0, Step: 7921, Batch(micro): 7921, Batch (considering grad accum): 990,  Loss: 5.6901, Time: 3.51s, Token/s: 145.71
Epoch: 0, Step: 7922, Batch(micro): 7922, Batch (considering grad accum): 990,  Loss: 6.1517, Time: 3.03s, Token/s: 169.02
Epoch: 0, Step: 7923, Batch(micro): 7923, Batch (considering grad accum): 990,  Loss: 6.1715, Time: 2.95s, Token/s: 173.53
Epoch: 0, Step: 7924, Batch(micro): 7924, Batch (considering grad accum): 990,  Loss: 5.5918, Time: 2.89s, Token/s: 176.98
Epoch: 0, Step: 7925, Batch(micro): 7925, Batch (considering grad accum): 990,  Loss: 4.9793, Time: 2.92s, Token/s: 175.40
Epoch: 0, Step: 7926, Batch(micro): 7926, Batch (considering grad accum): 990,  Loss: 6.2007, Time: 2.96s, Token/s: 173.16
Epoch: 0, Step: 7927, Batch(micro): 7927, Batch (considering grad accum): 990,  Loss: 5.6330, Time: 18.70s, Token/s: 27.37
Epoch: 0, Step: 7928, Batch(micro): 7928, Batch (considering grad accum): 991,  Loss: 5.0784, Time: 6.60s, Token/s: 77.53
Epoch: 0, Step: 7929, Batch(micro): 7929, Batch (considering grad accum): 991,  Loss: 5.3535, Time: 3.57s, Token/s: 143.59
Epoch: 0, Step: 7930, Batch(micro): 7930, Batch (considering grad accum): 991,  Loss: 5.8488, Time: 3.56s, Token/s: 143.90
Epoch: 0, Step: 7931, Batch(micro): 7931, Batch (considering grad accum): 991,  Loss: 6.9169, Time: 3.25s, Token/s: 157.44
Epoch: 0, Step: 7932, Batch(micro): 7932, Batch (considering grad accum): 991,  Loss: 5.9641, Time: 3.14s, Token/s: 163.02
Epoch: 0, Step: 7933, Batch(micro): 7933, Batch (considering grad accum): 991,  Loss: 6.1886, Time: 3.19s, Token/s: 160.26
Epoch: 0, Step: 7934, Batch(micro): 7934, Batch (considering grad accum): 991,  Loss: 5.3378, Time: 3.22s, Token/s: 158.88
Epoch: 0, Step: 7935, Batch(micro): 7935, Batch (considering grad accum): 991,  Loss: 5.5064, Time: 20.06s, Token/s: 25.52
Epoch: 0, Step: 7936, Batch(micro): 7936, Batch (considering grad accum): 992,  Loss: 6.8900, Time: 6.68s, Token/s: 76.66
Epoch: 0, Step: 7937, Batch(micro): 7937, Batch (considering grad accum): 992,  Loss: 5.5060, Time: 3.89s, Token/s: 131.62
Epoch: 0, Step: 7938, Batch(micro): 7938, Batch (considering grad accum): 992,  Loss: 6.0329, Time: 3.35s, Token/s: 153.04
Epoch: 0, Step: 7939, Batch(micro): 7939, Batch (considering grad accum): 992,  Loss: 6.3659, Time: 2.98s, Token/s: 171.53
Epoch: 0, Step: 7940, Batch(micro): 7940, Batch (considering grad accum): 992,  Loss: 6.4776, Time: 2.93s, Token/s: 174.87
Epoch: 0, Step: 7941, Batch(micro): 7941, Batch (considering grad accum): 992,  Loss: 6.5194, Time: 2.91s, Token/s: 175.70
Epoch: 0, Step: 7942, Batch(micro): 7942, Batch (considering grad accum): 992,  Loss: 6.1201, Time: 2.90s, Token/s: 176.31
Epoch: 0, Step: 7943, Batch(micro): 7943, Batch (considering grad accum): 992,  Loss: 6.6709, Time: 18.58s, Token/s: 27.56
Epoch: 0, Step: 7944, Batch(micro): 7944, Batch (considering grad accum): 993,  Loss: 6.3814, Time: 7.02s, Token/s: 72.91
Epoch: 0, Step: 7945, Batch(micro): 7945, Batch (considering grad accum): 993,  Loss: 6.4962, Time: 3.82s, Token/s: 134.15
Epoch: 0, Step: 7946, Batch(micro): 7946, Batch (considering grad accum): 993,  Loss: 5.5731, Time: 3.33s, Token/s: 153.64
Epoch: 0, Step: 7947, Batch(micro): 7947, Batch (considering grad accum): 993,  Loss: 5.9131, Time: 3.19s, Token/s: 160.42
Epoch: 0, Step: 7948, Batch(micro): 7948, Batch (considering grad accum): 993,  Loss: 6.3215, Time: 3.37s, Token/s: 151.82
Epoch: 0, Step: 7949, Batch(micro): 7949, Batch (considering grad accum): 993,  Loss: 6.2548, Time: 3.20s, Token/s: 159.88
Epoch: 0, Step: 7950, Batch(micro): 7950, Batch (considering grad accum): 993,  Loss: 6.5811, Time: 3.01s, Token/s: 169.99
Epoch: 0, Step: 7951, Batch(micro): 7951, Batch (considering grad accum): 993,  Loss: 5.8287, Time: 18.25s, Token/s: 28.05
Epoch: 0, Step: 7952, Batch(micro): 7952, Batch (considering grad accum): 994,  Loss: 6.7503, Time: 6.73s, Token/s: 76.05
Epoch: 0, Step: 7953, Batch(micro): 7953, Batch (considering grad accum): 994,  Loss: 6.6311, Time: 3.49s, Token/s: 146.74
Epoch: 0, Step: 7954, Batch(micro): 7954, Batch (considering grad accum): 994,  Loss: 5.9275, Time: 3.07s, Token/s: 166.61
Epoch: 0, Step: 7955, Batch(micro): 7955, Batch (considering grad accum): 994,  Loss: 5.8210, Time: 3.00s, Token/s: 170.81
Epoch: 0, Step: 7956, Batch(micro): 7956, Batch (considering grad accum): 994,  Loss: 5.9298, Time: 3.05s, Token/s: 167.93
Epoch: 0, Step: 7957, Batch(micro): 7957, Batch (considering grad accum): 994,  Loss: 6.0729, Time: 3.07s, Token/s: 166.75
Epoch: 0, Step: 7958, Batch(micro): 7958, Batch (considering grad accum): 994,  Loss: 6.0230, Time: 3.01s, Token/s: 169.93
Epoch: 0, Step: 7959, Batch(micro): 7959, Batch (considering grad accum): 994,  Loss: 6.2069, Time: 18.58s, Token/s: 27.56
Epoch: 0, Step: 7960, Batch(micro): 7960, Batch (considering grad accum): 995,  Loss: 5.4386, Time: 6.38s, Token/s: 80.23
Epoch: 0, Step: 7961, Batch(micro): 7961, Batch (considering grad accum): 995,  Loss: 5.1644, Time: 3.54s, Token/s: 144.67
Epoch: 0, Step: 7962, Batch(micro): 7962, Batch (considering grad accum): 995,  Loss: 5.7238, Time: 3.02s, Token/s: 169.54
Epoch: 0, Step: 7963, Batch(micro): 7963, Batch (considering grad accum): 995,  Loss: 6.3142, Time: 3.07s, Token/s: 167.00
Epoch: 0, Step: 7964, Batch(micro): 7964, Batch (considering grad accum): 995,  Loss: 5.6533, Time: 3.11s, Token/s: 164.85
Epoch: 0, Step: 7965, Batch(micro): 7965, Batch (considering grad accum): 995,  Loss: 5.7949, Time: 3.09s, Token/s: 165.84
Epoch: 0, Step: 7966, Batch(micro): 7966, Batch (considering grad accum): 995,  Loss: 5.6403, Time: 3.21s, Token/s: 159.72
Epoch: 0, Step: 7967, Batch(micro): 7967, Batch (considering grad accum): 995,  Loss: 5.7596, Time: 19.44s, Token/s: 26.34
Epoch: 0, Step: 7968, Batch(micro): 7968, Batch (considering grad accum): 996,  Loss: 5.8049, Time: 6.94s, Token/s: 73.75
Epoch: 0, Step: 7969, Batch(micro): 7969, Batch (considering grad accum): 996,  Loss: 7.1033, Time: 3.46s, Token/s: 147.78
Epoch: 0, Step: 7970, Batch(micro): 7970, Batch (considering grad accum): 996,  Loss: 6.6095, Time: 2.94s, Token/s: 174.30
Epoch: 0, Step: 7971, Batch(micro): 7971, Batch (considering grad accum): 996,  Loss: 6.0149, Time: 2.86s, Token/s: 178.95
Epoch: 0, Step: 7972, Batch(micro): 7972, Batch (considering grad accum): 996,  Loss: 6.0662, Time: 2.90s, Token/s: 176.30
Epoch: 0, Step: 7973, Batch(micro): 7973, Batch (considering grad accum): 996,  Loss: 5.8758, Time: 2.89s, Token/s: 176.99
Epoch: 0, Step: 7974, Batch(micro): 7974, Batch (considering grad accum): 996,  Loss: 6.9844, Time: 4.33s, Token/s: 118.30
Epoch: 0, Step: 7975, Batch(micro): 7975, Batch (considering grad accum): 996,  Loss: 8.0321, Time: 19.03s, Token/s: 26.90
Epoch: 0, Step: 7976, Batch(micro): 7976, Batch (considering grad accum): 997,  Loss: 6.1164, Time: 6.55s, Token/s: 78.15
Epoch: 0, Step: 7977, Batch(micro): 7977, Batch (considering grad accum): 997,  Loss: 5.6287, Time: 3.38s, Token/s: 151.65
Epoch: 0, Step: 7978, Batch(micro): 7978, Batch (considering grad accum): 997,  Loss: 6.3255, Time: 3.18s, Token/s: 161.17
Epoch: 0, Step: 7979, Batch(micro): 7979, Batch (considering grad accum): 997,  Loss: 5.4298, Time: 3.32s, Token/s: 154.43
Epoch: 0, Step: 7980, Batch(micro): 7980, Batch (considering grad accum): 997,  Loss: 5.6011, Time: 2.90s, Token/s: 176.35
Epoch: 0, Step: 7981, Batch(micro): 7981, Batch (considering grad accum): 997,  Loss: 7.1453, Time: 3.11s, Token/s: 164.84
Epoch: 0, Step: 7982, Batch(micro): 7982, Batch (considering grad accum): 997,  Loss: 6.4156, Time: 3.20s, Token/s: 159.81
Epoch: 0, Step: 7983, Batch(micro): 7983, Batch (considering grad accum): 997,  Loss: 6.6533, Time: 25.86s, Token/s: 19.80
Epoch: 0, Step: 7984, Batch(micro): 7984, Batch (considering grad accum): 998,  Loss: 6.7627, Time: 7.67s, Token/s: 66.73
Epoch: 0, Step: 7985, Batch(micro): 7985, Batch (considering grad accum): 998,  Loss: 5.9416, Time: 3.84s, Token/s: 133.38
Epoch: 0, Step: 7986, Batch(micro): 7986, Batch (considering grad accum): 998,  Loss: 5.5367, Time: 3.57s, Token/s: 143.36
Epoch: 0, Step: 7987, Batch(micro): 7987, Batch (considering grad accum): 998,  Loss: 5.9345, Time: 3.40s, Token/s: 150.59
Epoch: 0, Step: 7988, Batch(micro): 7988, Batch (considering grad accum): 998,  Loss: 6.4546, Time: 3.28s, Token/s: 156.00
Epoch: 0, Step: 7989, Batch(micro): 7989, Batch (considering grad accum): 998,  Loss: 6.2252, Time: 3.28s, Token/s: 155.91
Epoch: 0, Step: 7990, Batch(micro): 7990, Batch (considering grad accum): 998,  Loss: 5.3867, Time: 3.21s, Token/s: 159.34
Epoch: 0, Step: 7991, Batch(micro): 7991, Batch (considering grad accum): 998,  Loss: 5.4906, Time: 22.88s, Token/s: 22.38
Epoch: 0, Step: 7992, Batch(micro): 7992, Batch (considering grad accum): 999,  Loss: 6.2184, Time: 8.21s, Token/s: 62.39
Epoch: 0, Step: 7993, Batch(micro): 7993, Batch (considering grad accum): 999,  Loss: 5.8925, Time: 3.38s, Token/s: 151.51
Epoch: 0, Step: 7994, Batch(micro): 7994, Batch (considering grad accum): 999,  Loss: 5.2065, Time: 3.30s, Token/s: 155.06
Epoch: 0, Step: 7995, Batch(micro): 7995, Batch (considering grad accum): 999,  Loss: 6.3717, Time: 3.40s, Token/s: 150.68
Epoch: 0, Step: 7996, Batch(micro): 7996, Batch (considering grad accum): 999,  Loss: 6.2479, Time: 3.18s, Token/s: 160.91
Epoch: 0, Step: 7997, Batch(micro): 7997, Batch (considering grad accum): 999,  Loss: 6.7719, Time: 3.08s, Token/s: 166.20
Epoch: 0, Step: 7998, Batch(micro): 7998, Batch (considering grad accum): 999,  Loss: 6.0194, Time: 3.14s, Token/s: 163.14
Epoch: 0, Step: 7999, Batch(micro): 7999, Batch (considering grad accum): 999,  Loss: 5.9877, Time: 21.68s, Token/s: 23.62
Updating MLP bias
Epoch: 0, Step: 8000, Batch(micro): 8000, Batch (considering grad accum): 1000,  Loss: 5.8438, Time: 6.87s, Token/s: 74.56
Saved checkpoint at step 8000
What is Gravity? Maybe someday that would have heard about them, the fascinating history.

So, and community leaders like when I discovered.

So, or
Epoch: 0, Step: 8001, Batch(micro): 8001, Batch (considering grad accum): 1000,  Loss: 6.4338, Time: 15.15s, Token/s: 33.80
Epoch: 0, Step: 8002, Batch(micro): 8002, Batch (considering grad accum): 1000,  Loss: 6.7364, Time: 3.56s, Token/s: 143.70
Epoch: 0, Step: 8003, Batch(micro): 8003, Batch (considering grad accum): 1000,  Loss: 6.3492, Time: 3.17s, Token/s: 161.51
Epoch: 0, Step: 8004, Batch(micro): 8004, Batch (considering grad accum): 1000,  Loss: 7.2195, Time: 3.18s, Token/s: 161.13
Epoch: 0, Step: 8005, Batch(micro): 8005, Batch (considering grad accum): 1000,  Loss: 6.8018, Time: 3.14s, Token/s: 163.11
Epoch: 0, Step: 8006, Batch(micro): 8006, Batch (considering grad accum): 1000,  Loss: 6.5713, Time: 2.97s, Token/s: 172.13
Epoch: 0, Step: 8007, Batch(micro): 8007, Batch (considering grad accum): 1000,  Loss: 5.9846, Time: 25.30s, Token/s: 20.24
Epoch: 0, Step: 8008, Batch(micro): 8008, Batch (considering grad accum): 1001,  Loss: 5.6993, Time: 7.21s, Token/s: 70.99
Epoch: 0, Step: 8009, Batch(micro): 8009, Batch (considering grad accum): 1001,  Loss: 5.1853, Time: 3.59s, Token/s: 142.45
Epoch: 0, Step: 8010, Batch(micro): 8010, Batch (considering grad accum): 1001,  Loss: 5.4052, Time: 3.43s, Token/s: 149.07
Epoch: 0, Step: 8011, Batch(micro): 8011, Batch (considering grad accum): 1001,  Loss: 6.2403, Time: 3.86s, Token/s: 132.50
Epoch: 0, Step: 8012, Batch(micro): 8012, Batch (considering grad accum): 1001,  Loss: 6.2181, Time: 2.99s, Token/s: 171.23
Epoch: 0, Step: 8013, Batch(micro): 8013, Batch (considering grad accum): 1001,  Loss: 6.3892, Time: 3.12s, Token/s: 164.21
Epoch: 0, Step: 8014, Batch(micro): 8014, Batch (considering grad accum): 1001,  Loss: 5.9244, Time: 3.24s, Token/s: 157.93
Epoch: 0, Step: 8015, Batch(micro): 8015, Batch (considering grad accum): 1001,  Loss: 6.6006, Time: 24.01s, Token/s: 21.32
Epoch: 0, Step: 8016, Batch(micro): 8016, Batch (considering grad accum): 1002,  Loss: 6.4552, Time: 6.75s, Token/s: 75.85
Epoch: 0, Step: 8017, Batch(micro): 8017, Batch (considering grad accum): 1002,  Loss: 6.6971, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 8018, Batch(micro): 8018, Batch (considering grad accum): 1002,  Loss: 6.4163, Time: 3.41s, Token/s: 150.31
Epoch: 0, Step: 8019, Batch(micro): 8019, Batch (considering grad accum): 1002,  Loss: 6.4417, Time: 3.27s, Token/s: 156.60
Epoch: 0, Step: 8020, Batch(micro): 8020, Batch (considering grad accum): 1002,  Loss: 6.2857, Time: 3.18s, Token/s: 160.95
Epoch: 0, Step: 8021, Batch(micro): 8021, Batch (considering grad accum): 1002,  Loss: 5.5937, Time: 3.39s, Token/s: 150.94
Epoch: 0, Step: 8022, Batch(micro): 8022, Batch (considering grad accum): 1002,  Loss: 5.3423, Time: 3.88s, Token/s: 131.83
Epoch: 0, Step: 8023, Batch(micro): 8023, Batch (considering grad accum): 1002,  Loss: 6.0419, Time: 23.73s, Token/s: 21.58
Epoch: 0, Step: 8024, Batch(micro): 8024, Batch (considering grad accum): 1003,  Loss: 6.2340, Time: 6.88s, Token/s: 74.37
Epoch: 0, Step: 8025, Batch(micro): 8025, Batch (considering grad accum): 1003,  Loss: 5.8864, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 8026, Batch(micro): 8026, Batch (considering grad accum): 1003,  Loss: 5.8197, Time: 2.89s, Token/s: 177.07
Epoch: 0, Step: 8027, Batch(micro): 8027, Batch (considering grad accum): 1003,  Loss: 6.1291, Time: 2.92s, Token/s: 175.51
Epoch: 0, Step: 8028, Batch(micro): 8028, Batch (considering grad accum): 1003,  Loss: 6.9311, Time: 3.03s, Token/s: 168.96
Epoch: 0, Step: 8029, Batch(micro): 8029, Batch (considering grad accum): 1003,  Loss: 6.3564, Time: 3.12s, Token/s: 164.32
Epoch: 0, Step: 8030, Batch(micro): 8030, Batch (considering grad accum): 1003,  Loss: 6.0739, Time: 3.88s, Token/s: 131.88
Epoch: 0, Step: 8031, Batch(micro): 8031, Batch (considering grad accum): 1003,  Loss: 5.8191, Time: 22.53s, Token/s: 22.72
Epoch: 0, Step: 8032, Batch(micro): 8032, Batch (considering grad accum): 1004,  Loss: 5.9365, Time: 8.66s, Token/s: 59.12
Epoch: 0, Step: 8033, Batch(micro): 8033, Batch (considering grad accum): 1004,  Loss: 6.1627, Time: 3.59s, Token/s: 142.60
Epoch: 0, Step: 8034, Batch(micro): 8034, Batch (considering grad accum): 1004,  Loss: 5.7110, Time: 3.35s, Token/s: 152.86
Epoch: 0, Step: 8035, Batch(micro): 8035, Batch (considering grad accum): 1004,  Loss: 6.6703, Time: 3.48s, Token/s: 147.13
Epoch: 0, Step: 8036, Batch(micro): 8036, Batch (considering grad accum): 1004,  Loss: 5.9077, Time: 3.47s, Token/s: 147.70
Epoch: 0, Step: 8037, Batch(micro): 8037, Batch (considering grad accum): 1004,  Loss: 6.0682, Time: 3.25s, Token/s: 157.32
Epoch: 0, Step: 8038, Batch(micro): 8038, Batch (considering grad accum): 1004,  Loss: 5.8652, Time: 3.58s, Token/s: 143.13
Epoch: 0, Step: 8039, Batch(micro): 8039, Batch (considering grad accum): 1004,  Loss: 6.0435, Time: 25.21s, Token/s: 20.31
Epoch: 0, Step: 8040, Batch(micro): 8040, Batch (considering grad accum): 1005,  Loss: 5.7232, Time: 8.40s, Token/s: 60.92
Epoch: 0, Step: 8041, Batch(micro): 8041, Batch (considering grad accum): 1005,  Loss: 5.2751, Time: 3.60s, Token/s: 142.37
Epoch: 0, Step: 8042, Batch(micro): 8042, Batch (considering grad accum): 1005,  Loss: 5.4272, Time: 3.29s, Token/s: 155.62
Epoch: 0, Step: 8043, Batch(micro): 8043, Batch (considering grad accum): 1005,  Loss: 5.5984, Time: 3.19s, Token/s: 160.39
Epoch: 0, Step: 8044, Batch(micro): 8044, Batch (considering grad accum): 1005,  Loss: 5.8400, Time: 3.08s, Token/s: 166.24
Epoch: 0, Step: 8045, Batch(micro): 8045, Batch (considering grad accum): 1005,  Loss: 6.3736, Time: 3.15s, Token/s: 162.75
Epoch: 0, Step: 8046, Batch(micro): 8046, Batch (considering grad accum): 1005,  Loss: 6.1863, Time: 3.08s, Token/s: 166.37
Epoch: 0, Step: 8047, Batch(micro): 8047, Batch (considering grad accum): 1005,  Loss: 5.9195, Time: 22.92s, Token/s: 22.34
Epoch: 0, Step: 8048, Batch(micro): 8048, Batch (considering grad accum): 1006,  Loss: 5.9410, Time: 7.23s, Token/s: 70.85
Epoch: 0, Step: 8049, Batch(micro): 8049, Batch (considering grad accum): 1006,  Loss: 7.3535, Time: 2.96s, Token/s: 173.11
Epoch: 0, Step: 8050, Batch(micro): 8050, Batch (considering grad accum): 1006,  Loss: 7.0659, Time: 3.01s, Token/s: 170.06
Epoch: 0, Step: 8051, Batch(micro): 8051, Batch (considering grad accum): 1006,  Loss: 6.6908, Time: 2.99s, Token/s: 171.07
Epoch: 0, Step: 8052, Batch(micro): 8052, Batch (considering grad accum): 1006,  Loss: 6.2764, Time: 3.01s, Token/s: 170.05
Epoch: 0, Step: 8053, Batch(micro): 8053, Batch (considering grad accum): 1006,  Loss: 5.9993, Time: 3.03s, Token/s: 168.76
Epoch: 0, Step: 8054, Batch(micro): 8054, Batch (considering grad accum): 1006,  Loss: 6.1461, Time: 3.12s, Token/s: 164.07
Epoch: 0, Step: 8055, Batch(micro): 8055, Batch (considering grad accum): 1006,  Loss: 6.1861, Time: 21.21s, Token/s: 24.14
Epoch: 0, Step: 8056, Batch(micro): 8056, Batch (considering grad accum): 1007,  Loss: 6.1760, Time: 7.62s, Token/s: 67.16
Epoch: 0, Step: 8057, Batch(micro): 8057, Batch (considering grad accum): 1007,  Loss: 5.6291, Time: 3.50s, Token/s: 146.15
Epoch: 0, Step: 8058, Batch(micro): 8058, Batch (considering grad accum): 1007,  Loss: 5.9587, Time: 3.01s, Token/s: 170.02
Epoch: 0, Step: 8059, Batch(micro): 8059, Batch (considering grad accum): 1007,  Loss: 5.7993, Time: 3.02s, Token/s: 169.73
Epoch: 0, Step: 8060, Batch(micro): 8060, Batch (considering grad accum): 1007,  Loss: 5.6775, Time: 3.21s, Token/s: 159.57
Epoch: 0, Step: 8061, Batch(micro): 8061, Batch (considering grad accum): 1007,  Loss: 6.5175, Time: 3.23s, Token/s: 158.65
Epoch: 0, Step: 8062, Batch(micro): 8062, Batch (considering grad accum): 1007,  Loss: 6.1867, Time: 3.29s, Token/s: 155.39
Epoch: 0, Step: 8063, Batch(micro): 8063, Batch (considering grad accum): 1007,  Loss: 6.7726, Time: 24.09s, Token/s: 21.26
Epoch: 0, Step: 8064, Batch(micro): 8064, Batch (considering grad accum): 1008,  Loss: 5.9688, Time: 7.74s, Token/s: 66.16
Epoch: 0, Step: 8065, Batch(micro): 8065, Batch (considering grad accum): 1008,  Loss: 5.8095, Time: 3.52s, Token/s: 145.34
Epoch: 0, Step: 8066, Batch(micro): 8066, Batch (considering grad accum): 1008,  Loss: 6.0913, Time: 2.98s, Token/s: 171.86
Epoch: 0, Step: 8067, Batch(micro): 8067, Batch (considering grad accum): 1008,  Loss: 5.7601, Time: 3.38s, Token/s: 151.30
Epoch: 0, Step: 8068, Batch(micro): 8068, Batch (considering grad accum): 1008,  Loss: 5.9082, Time: 3.36s, Token/s: 152.35
Epoch: 0, Step: 8069, Batch(micro): 8069, Batch (considering grad accum): 1008,  Loss: 6.0552, Time: 3.31s, Token/s: 154.91
Epoch: 0, Step: 8070, Batch(micro): 8070, Batch (considering grad accum): 1008,  Loss: 6.7297, Time: 3.19s, Token/s: 160.56
Epoch: 0, Step: 8071, Batch(micro): 8071, Batch (considering grad accum): 1008,  Loss: 6.2004, Time: 23.06s, Token/s: 22.20
Epoch: 0, Step: 8072, Batch(micro): 8072, Batch (considering grad accum): 1009,  Loss: 5.3418, Time: 5.86s, Token/s: 87.39
Epoch: 0, Step: 8073, Batch(micro): 8073, Batch (considering grad accum): 1009,  Loss: 5.6782, Time: 3.43s, Token/s: 149.23
Epoch: 0, Step: 8074, Batch(micro): 8074, Batch (considering grad accum): 1009,  Loss: 5.7529, Time: 3.13s, Token/s: 163.61
Epoch: 0, Step: 8075, Batch(micro): 8075, Batch (considering grad accum): 1009,  Loss: 5.6987, Time: 3.32s, Token/s: 154.14
Epoch: 0, Step: 8076, Batch(micro): 8076, Batch (considering grad accum): 1009,  Loss: 6.1119, Time: 3.26s, Token/s: 156.90
Epoch: 0, Step: 8077, Batch(micro): 8077, Batch (considering grad accum): 1009,  Loss: 5.7071, Time: 3.10s, Token/s: 165.29
Epoch: 0, Step: 8078, Batch(micro): 8078, Batch (considering grad accum): 1009,  Loss: 6.4642, Time: 2.97s, Token/s: 172.63
Epoch: 0, Step: 8079, Batch(micro): 8079, Batch (considering grad accum): 1009,  Loss: 6.5092, Time: 24.50s, Token/s: 20.90
Epoch: 0, Step: 8080, Batch(micro): 8080, Batch (considering grad accum): 1010,  Loss: 6.7221, Time: 7.31s, Token/s: 70.06
Epoch: 0, Step: 8081, Batch(micro): 8081, Batch (considering grad accum): 1010,  Loss: 6.6618, Time: 3.55s, Token/s: 144.14
Epoch: 0, Step: 8082, Batch(micro): 8082, Batch (considering grad accum): 1010,  Loss: 5.3174, Time: 3.32s, Token/s: 154.44
Epoch: 0, Step: 8083, Batch(micro): 8083, Batch (considering grad accum): 1010,  Loss: 5.5698, Time: 3.48s, Token/s: 146.93
Epoch: 0, Step: 8084, Batch(micro): 8084, Batch (considering grad accum): 1010,  Loss: 5.7223, Time: 3.58s, Token/s: 142.95
Epoch: 0, Step: 8085, Batch(micro): 8085, Batch (considering grad accum): 1010,  Loss: 6.0259, Time: 3.37s, Token/s: 152.04
Epoch: 0, Step: 8086, Batch(micro): 8086, Batch (considering grad accum): 1010,  Loss: 5.5989, Time: 3.06s, Token/s: 167.40
Epoch: 0, Step: 8087, Batch(micro): 8087, Batch (considering grad accum): 1010,  Loss: 6.0467, Time: 23.04s, Token/s: 22.22
Epoch: 0, Step: 8088, Batch(micro): 8088, Batch (considering grad accum): 1011,  Loss: 5.5142, Time: 9.10s, Token/s: 56.24
Epoch: 0, Step: 8089, Batch(micro): 8089, Batch (considering grad accum): 1011,  Loss: 5.8319, Time: 3.85s, Token/s: 132.96
Epoch: 0, Step: 8090, Batch(micro): 8090, Batch (considering grad accum): 1011,  Loss: 5.6760, Time: 3.38s, Token/s: 151.59
Epoch: 0, Step: 8091, Batch(micro): 8091, Batch (considering grad accum): 1011,  Loss: 6.5149, Time: 3.16s, Token/s: 161.86
Epoch: 0, Step: 8092, Batch(micro): 8092, Batch (considering grad accum): 1011,  Loss: 6.2774, Time: 2.80s, Token/s: 183.13
Epoch: 0, Step: 8093, Batch(micro): 8093, Batch (considering grad accum): 1011,  Loss: 6.3697, Time: 2.90s, Token/s: 176.78
Epoch: 0, Step: 8094, Batch(micro): 8094, Batch (considering grad accum): 1011,  Loss: 5.9977, Time: 3.61s, Token/s: 141.66
Epoch: 0, Step: 8095, Batch(micro): 8095, Batch (considering grad accum): 1011,  Loss: 5.6514, Time: 21.68s, Token/s: 23.62
Epoch: 0, Step: 8096, Batch(micro): 8096, Batch (considering grad accum): 1012,  Loss: 5.7930, Time: 9.43s, Token/s: 54.30
Epoch: 0, Step: 8097, Batch(micro): 8097, Batch (considering grad accum): 1012,  Loss: 6.6726, Time: 3.17s, Token/s: 161.68
Epoch: 0, Step: 8098, Batch(micro): 8098, Batch (considering grad accum): 1012,  Loss: 6.1939, Time: 3.25s, Token/s: 157.57
Epoch: 0, Step: 8099, Batch(micro): 8099, Batch (considering grad accum): 1012,  Loss: 5.6622, Time: 3.52s, Token/s: 145.45
Updating MLP bias
Epoch: 0, Step: 8100, Batch(micro): 8100, Batch (considering grad accum): 1012,  Loss: 5.8792, Time: 3.41s, Token/s: 150.15
Epoch: 0, Step: 8101, Batch(micro): 8101, Batch (considering grad accum): 1012,  Loss: 6.1288, Time: 3.39s, Token/s: 151.04
Epoch: 0, Step: 8102, Batch(micro): 8102, Batch (considering grad accum): 1012,  Loss: 5.8323, Time: 3.46s, Token/s: 148.06
Epoch: 0, Step: 8103, Batch(micro): 8103, Batch (considering grad accum): 1012,  Loss: 6.5696, Time: 17.76s, Token/s: 28.82
Epoch: 0, Step: 8104, Batch(micro): 8104, Batch (considering grad accum): 1013,  Loss: 6.8843, Time: 6.21s, Token/s: 82.39
Epoch: 0, Step: 8105, Batch(micro): 8105, Batch (considering grad accum): 1013,  Loss: 6.8058, Time: 4.13s, Token/s: 124.05
Epoch: 0, Step: 8106, Batch(micro): 8106, Batch (considering grad accum): 1013,  Loss: 6.0517, Time: 3.64s, Token/s: 140.82
Epoch: 0, Step: 8107, Batch(micro): 8107, Batch (considering grad accum): 1013,  Loss: 5.9549, Time: 3.33s, Token/s: 153.54
Epoch: 0, Step: 8108, Batch(micro): 8108, Batch (considering grad accum): 1013,  Loss: 5.4878, Time: 3.71s, Token/s: 137.97
Epoch: 0, Step: 8109, Batch(micro): 8109, Batch (considering grad accum): 1013,  Loss: 6.0551, Time: 3.68s, Token/s: 139.30
Epoch: 0, Step: 8110, Batch(micro): 8110, Batch (considering grad accum): 1013,  Loss: 5.4966, Time: 3.91s, Token/s: 130.83
Epoch: 0, Step: 8111, Batch(micro): 8111, Batch (considering grad accum): 1013,  Loss: 5.9334, Time: 20.03s, Token/s: 25.56
Epoch: 0, Step: 8112, Batch(micro): 8112, Batch (considering grad accum): 1014,  Loss: 5.8242, Time: 8.18s, Token/s: 62.62
Epoch: 0, Step: 8113, Batch(micro): 8113, Batch (considering grad accum): 1014,  Loss: 6.4949, Time: 3.75s, Token/s: 136.36
Epoch: 0, Step: 8114, Batch(micro): 8114, Batch (considering grad accum): 1014,  Loss: 6.0617, Time: 3.39s, Token/s: 150.84
Epoch: 0, Step: 8115, Batch(micro): 8115, Batch (considering grad accum): 1014,  Loss: 5.7620, Time: 3.46s, Token/s: 148.01
Epoch: 0, Step: 8116, Batch(micro): 8116, Batch (considering grad accum): 1014,  Loss: 5.9373, Time: 3.56s, Token/s: 143.94
Epoch: 0, Step: 8117, Batch(micro): 8117, Batch (considering grad accum): 1014,  Loss: 6.4311, Time: 3.38s, Token/s: 151.70
Epoch: 0, Step: 8118, Batch(micro): 8118, Batch (considering grad accum): 1014,  Loss: 7.0203, Time: 3.64s, Token/s: 140.55
Epoch: 0, Step: 8119, Batch(micro): 8119, Batch (considering grad accum): 1014,  Loss: 5.9367, Time: 18.82s, Token/s: 27.20
Epoch: 0, Step: 8120, Batch(micro): 8120, Batch (considering grad accum): 1015,  Loss: 6.2989, Time: 7.02s, Token/s: 72.92
Epoch: 0, Step: 8121, Batch(micro): 8121, Batch (considering grad accum): 1015,  Loss: 6.9277, Time: 4.28s, Token/s: 119.61
Epoch: 0, Step: 8122, Batch(micro): 8122, Batch (considering grad accum): 1015,  Loss: 6.5973, Time: 3.56s, Token/s: 143.83
Epoch: 0, Step: 8123, Batch(micro): 8123, Batch (considering grad accum): 1015,  Loss: 6.4059, Time: 3.25s, Token/s: 157.51
Epoch: 0, Step: 8124, Batch(micro): 8124, Batch (considering grad accum): 1015,  Loss: 6.0070, Time: 4.14s, Token/s: 123.59
Epoch: 0, Step: 8125, Batch(micro): 8125, Batch (considering grad accum): 1015,  Loss: 5.3963, Time: 3.48s, Token/s: 147.12
Epoch: 0, Step: 8126, Batch(micro): 8126, Batch (considering grad accum): 1015,  Loss: 5.6172, Time: 3.29s, Token/s: 155.57
Epoch: 0, Step: 8127, Batch(micro): 8127, Batch (considering grad accum): 1015,  Loss: 6.3121, Time: 18.54s, Token/s: 27.62
Epoch: 0, Step: 8128, Batch(micro): 8128, Batch (considering grad accum): 1016,  Loss: 6.2299, Time: 6.09s, Token/s: 84.05
Epoch: 0, Step: 8129, Batch(micro): 8129, Batch (considering grad accum): 1016,  Loss: 5.9997, Time: 4.11s, Token/s: 124.48
Epoch: 0, Step: 8130, Batch(micro): 8130, Batch (considering grad accum): 1016,  Loss: 6.8527, Time: 3.95s, Token/s: 129.49
Epoch: 0, Step: 8131, Batch(micro): 8131, Batch (considering grad accum): 1016,  Loss: 5.5233, Time: 3.92s, Token/s: 130.65
Epoch: 0, Step: 8132, Batch(micro): 8132, Batch (considering grad accum): 1016,  Loss: 5.8484, Time: 3.76s, Token/s: 136.12
Epoch: 0, Step: 8133, Batch(micro): 8133, Batch (considering grad accum): 1016,  Loss: 6.3375, Time: 3.37s, Token/s: 151.80
Epoch: 0, Step: 8134, Batch(micro): 8134, Batch (considering grad accum): 1016,  Loss: 5.0664, Time: 3.72s, Token/s: 137.67
Epoch: 0, Step: 8135, Batch(micro): 8135, Batch (considering grad accum): 1016,  Loss: 5.5798, Time: 18.41s, Token/s: 27.80
Epoch: 0, Step: 8136, Batch(micro): 8136, Batch (considering grad accum): 1017,  Loss: 5.5813, Time: 6.82s, Token/s: 75.11
Epoch: 0, Step: 8137, Batch(micro): 8137, Batch (considering grad accum): 1017,  Loss: 5.6642, Time: 4.05s, Token/s: 126.32
Epoch: 0, Step: 8138, Batch(micro): 8138, Batch (considering grad accum): 1017,  Loss: 5.9103, Time: 3.56s, Token/s: 143.67
Epoch: 0, Step: 8139, Batch(micro): 8139, Batch (considering grad accum): 1017,  Loss: 6.2647, Time: 3.69s, Token/s: 138.70
Epoch: 0, Step: 8140, Batch(micro): 8140, Batch (considering grad accum): 1017,  Loss: 6.0381, Time: 3.60s, Token/s: 142.38
Epoch: 0, Step: 8141, Batch(micro): 8141, Batch (considering grad accum): 1017,  Loss: 5.7967, Time: 3.91s, Token/s: 130.83
Epoch: 0, Step: 8142, Batch(micro): 8142, Batch (considering grad accum): 1017,  Loss: 6.0184, Time: 3.60s, Token/s: 142.26
Epoch: 0, Step: 8143, Batch(micro): 8143, Batch (considering grad accum): 1017,  Loss: 6.4573, Time: 18.56s, Token/s: 27.59
Epoch: 0, Step: 8144, Batch(micro): 8144, Batch (considering grad accum): 1018,  Loss: 6.0201, Time: 6.19s, Token/s: 82.72
Epoch: 0, Step: 8145, Batch(micro): 8145, Batch (considering grad accum): 1018,  Loss: 6.5371, Time: 3.77s, Token/s: 135.92
Epoch: 0, Step: 8146, Batch(micro): 8146, Batch (considering grad accum): 1018,  Loss: 5.7076, Time: 3.66s, Token/s: 139.72
Epoch: 0, Step: 8147, Batch(micro): 8147, Batch (considering grad accum): 1018,  Loss: 5.8426, Time: 3.65s, Token/s: 140.28
Epoch: 0, Step: 8148, Batch(micro): 8148, Batch (considering grad accum): 1018,  Loss: 6.0215, Time: 3.47s, Token/s: 147.36
Epoch: 0, Step: 8149, Batch(micro): 8149, Batch (considering grad accum): 1018,  Loss: 6.5448, Time: 3.53s, Token/s: 144.86
Epoch: 0, Step: 8150, Batch(micro): 8150, Batch (considering grad accum): 1018,  Loss: 7.4932, Time: 3.48s, Token/s: 147.02
Epoch: 0, Step: 8151, Batch(micro): 8151, Batch (considering grad accum): 1018,  Loss: 6.1013, Time: 18.82s, Token/s: 27.21
Epoch: 0, Step: 8152, Batch(micro): 8152, Batch (considering grad accum): 1019,  Loss: 5.7240, Time: 6.72s, Token/s: 76.20
Epoch: 0, Step: 8153, Batch(micro): 8153, Batch (considering grad accum): 1019,  Loss: 5.6409, Time: 3.80s, Token/s: 134.78
Epoch: 0, Step: 8154, Batch(micro): 8154, Batch (considering grad accum): 1019,  Loss: 5.8936, Time: 3.71s, Token/s: 138.05
Epoch: 0, Step: 8155, Batch(micro): 8155, Batch (considering grad accum): 1019,  Loss: 6.4424, Time: 3.73s, Token/s: 137.32
Epoch: 0, Step: 8156, Batch(micro): 8156, Batch (considering grad accum): 1019,  Loss: 7.3222, Time: 3.52s, Token/s: 145.64
Epoch: 0, Step: 8157, Batch(micro): 8157, Batch (considering grad accum): 1019,  Loss: 5.7146, Time: 3.28s, Token/s: 155.89
Epoch: 0, Step: 8158, Batch(micro): 8158, Batch (considering grad accum): 1019,  Loss: 6.1426, Time: 3.49s, Token/s: 146.52
Epoch: 0, Step: 8159, Batch(micro): 8159, Batch (considering grad accum): 1019,  Loss: 5.8986, Time: 19.62s, Token/s: 26.10
Epoch: 0, Step: 8160, Batch(micro): 8160, Batch (considering grad accum): 1020,  Loss: 6.2057, Time: 6.70s, Token/s: 76.40
Epoch: 0, Step: 8161, Batch(micro): 8161, Batch (considering grad accum): 1020,  Loss: 6.6518, Time: 3.75s, Token/s: 136.69
Epoch: 0, Step: 8162, Batch(micro): 8162, Batch (considering grad accum): 1020,  Loss: 5.6178, Time: 3.54s, Token/s: 144.77
Epoch: 0, Step: 8163, Batch(micro): 8163, Batch (considering grad accum): 1020,  Loss: 5.9013, Time: 3.52s, Token/s: 145.38
Epoch: 0, Step: 8164, Batch(micro): 8164, Batch (considering grad accum): 1020,  Loss: 5.5996, Time: 3.65s, Token/s: 140.32
Epoch: 0, Step: 8165, Batch(micro): 8165, Batch (considering grad accum): 1020,  Loss: 5.9714, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 8166, Batch(micro): 8166, Batch (considering grad accum): 1020,  Loss: 6.5301, Time: 3.25s, Token/s: 157.49
Epoch: 0, Step: 8167, Batch(micro): 8167, Batch (considering grad accum): 1020,  Loss: 5.9480, Time: 17.93s, Token/s: 28.55
Epoch: 0, Step: 8168, Batch(micro): 8168, Batch (considering grad accum): 1021,  Loss: 5.4216, Time: 5.98s, Token/s: 85.60
Epoch: 0, Step: 8169, Batch(micro): 8169, Batch (considering grad accum): 1021,  Loss: 6.5964, Time: 4.20s, Token/s: 121.83
Epoch: 0, Step: 8170, Batch(micro): 8170, Batch (considering grad accum): 1021,  Loss: 5.6016, Time: 3.88s, Token/s: 131.97
Epoch: 0, Step: 8171, Batch(micro): 8171, Batch (considering grad accum): 1021,  Loss: 5.5281, Time: 3.38s, Token/s: 151.63
Epoch: 0, Step: 8172, Batch(micro): 8172, Batch (considering grad accum): 1021,  Loss: 6.6522, Time: 3.51s, Token/s: 145.97
Epoch: 0, Step: 8173, Batch(micro): 8173, Batch (considering grad accum): 1021,  Loss: 6.4208, Time: 3.47s, Token/s: 147.38
Epoch: 0, Step: 8174, Batch(micro): 8174, Batch (considering grad accum): 1021,  Loss: 6.8122, Time: 3.31s, Token/s: 154.86
Epoch: 0, Step: 8175, Batch(micro): 8175, Batch (considering grad accum): 1021,  Loss: 6.0630, Time: 18.94s, Token/s: 27.04
Epoch: 0, Step: 8176, Batch(micro): 8176, Batch (considering grad accum): 1022,  Loss: 5.9736, Time: 6.58s, Token/s: 77.86
Epoch: 0, Step: 8177, Batch(micro): 8177, Batch (considering grad accum): 1022,  Loss: 5.9020, Time: 3.67s, Token/s: 139.51
Epoch: 0, Step: 8178, Batch(micro): 8178, Batch (considering grad accum): 1022,  Loss: 5.9140, Time: 3.59s, Token/s: 142.77
Epoch: 0, Step: 8179, Batch(micro): 8179, Batch (considering grad accum): 1022,  Loss: 6.0626, Time: 3.52s, Token/s: 145.57
Epoch: 0, Step: 8180, Batch(micro): 8180, Batch (considering grad accum): 1022,  Loss: 5.8136, Time: 3.35s, Token/s: 152.77
Epoch: 0, Step: 8181, Batch(micro): 8181, Batch (considering grad accum): 1022,  Loss: 6.3792, Time: 3.85s, Token/s: 133.04
Epoch: 0, Step: 8182, Batch(micro): 8182, Batch (considering grad accum): 1022,  Loss: 5.7425, Time: 3.33s, Token/s: 153.64
Epoch: 0, Step: 8183, Batch(micro): 8183, Batch (considering grad accum): 1022,  Loss: 6.5126, Time: 18.98s, Token/s: 26.97
Epoch: 0, Step: 8184, Batch(micro): 8184, Batch (considering grad accum): 1023,  Loss: 6.3221, Time: 5.87s, Token/s: 87.20
Epoch: 0, Step: 8185, Batch(micro): 8185, Batch (considering grad accum): 1023,  Loss: 5.6349, Time: 3.81s, Token/s: 134.24
Epoch: 0, Step: 8186, Batch(micro): 8186, Batch (considering grad accum): 1023,  Loss: 6.1457, Time: 3.75s, Token/s: 136.46
Epoch: 0, Step: 8187, Batch(micro): 8187, Batch (considering grad accum): 1023,  Loss: 5.7393, Time: 3.41s, Token/s: 150.10
Epoch: 0, Step: 8188, Batch(micro): 8188, Batch (considering grad accum): 1023,  Loss: 6.0392, Time: 3.41s, Token/s: 150.23
Epoch: 0, Step: 8189, Batch(micro): 8189, Batch (considering grad accum): 1023,  Loss: 6.3517, Time: 3.34s, Token/s: 153.36
Epoch: 0, Step: 8190, Batch(micro): 8190, Batch (considering grad accum): 1023,  Loss: 5.5260, Time: 3.42s, Token/s: 149.61
Epoch: 0, Step: 8191, Batch(micro): 8191, Batch (considering grad accum): 1023,  Loss: 5.6258, Time: 18.37s, Token/s: 27.87
Epoch: 0, Step: 8192, Batch(micro): 8192, Batch (considering grad accum): 1024,  Loss: 5.9953, Time: 6.31s, Token/s: 81.10
Epoch: 0, Step: 8193, Batch(micro): 8193, Batch (considering grad accum): 1024,  Loss: 5.8137, Time: 3.90s, Token/s: 131.42
Epoch: 0, Step: 8194, Batch(micro): 8194, Batch (considering grad accum): 1024,  Loss: 5.7781, Time: 3.46s, Token/s: 148.16
Epoch: 0, Step: 8195, Batch(micro): 8195, Batch (considering grad accum): 1024,  Loss: 5.0695, Time: 3.47s, Token/s: 147.47
Epoch: 0, Step: 8196, Batch(micro): 8196, Batch (considering grad accum): 1024,  Loss: 5.9086, Time: 3.53s, Token/s: 144.84
Epoch: 0, Step: 8197, Batch(micro): 8197, Batch (considering grad accum): 1024,  Loss: 5.9324, Time: 3.71s, Token/s: 138.07
Epoch: 0, Step: 8198, Batch(micro): 8198, Batch (considering grad accum): 1024,  Loss: 6.5001, Time: 3.65s, Token/s: 140.20
Epoch: 0, Step: 8199, Batch(micro): 8199, Batch (considering grad accum): 1024,  Loss: 6.8573, Time: 18.23s, Token/s: 28.08
Updating MLP bias
Epoch: 0, Step: 8200, Batch(micro): 8200, Batch (considering grad accum): 1025,  Loss: 5.9715, Time: 6.52s, Token/s: 78.50
Epoch: 0, Step: 8201, Batch(micro): 8201, Batch (considering grad accum): 1025,  Loss: 5.3911, Time: 3.65s, Token/s: 140.44
Epoch: 0, Step: 8202, Batch(micro): 8202, Batch (considering grad accum): 1025,  Loss: 5.6521, Time: 3.29s, Token/s: 155.56
Epoch: 0, Step: 8203, Batch(micro): 8203, Batch (considering grad accum): 1025,  Loss: 5.9668, Time: 3.35s, Token/s: 152.80
Epoch: 0, Step: 8204, Batch(micro): 8204, Batch (considering grad accum): 1025,  Loss: 6.2794, Time: 3.41s, Token/s: 149.98
Epoch: 0, Step: 8205, Batch(micro): 8205, Batch (considering grad accum): 1025,  Loss: 6.6329, Time: 3.41s, Token/s: 150.03
Epoch: 0, Step: 8206, Batch(micro): 8206, Batch (considering grad accum): 1025,  Loss: 6.2063, Time: 3.49s, Token/s: 146.66
Epoch: 0, Step: 8207, Batch(micro): 8207, Batch (considering grad accum): 1025,  Loss: 6.4204, Time: 23.72s, Token/s: 21.59
Epoch: 0, Step: 8208, Batch(micro): 8208, Batch (considering grad accum): 1026,  Loss: 6.0169, Time: 9.95s, Token/s: 51.43
Epoch: 0, Step: 8209, Batch(micro): 8209, Batch (considering grad accum): 1026,  Loss: 6.3208, Time: 3.92s, Token/s: 130.73
Epoch: 0, Step: 8210, Batch(micro): 8210, Batch (considering grad accum): 1026,  Loss: 5.6836, Time: 3.31s, Token/s: 154.75
Epoch: 0, Step: 8211, Batch(micro): 8211, Batch (considering grad accum): 1026,  Loss: 6.1880, Time: 3.52s, Token/s: 145.25
Epoch: 0, Step: 8212, Batch(micro): 8212, Batch (considering grad accum): 1026,  Loss: 6.6005, Time: 3.33s, Token/s: 153.89
Epoch: 0, Step: 8213, Batch(micro): 8213, Batch (considering grad accum): 1026,  Loss: 6.2257, Time: 3.33s, Token/s: 153.53
Epoch: 0, Step: 8214, Batch(micro): 8214, Batch (considering grad accum): 1026,  Loss: 5.2545, Time: 3.44s, Token/s: 148.92
Epoch: 0, Step: 8215, Batch(micro): 8215, Batch (considering grad accum): 1026,  Loss: 6.0004, Time: 24.04s, Token/s: 21.29
Epoch: 0, Step: 8216, Batch(micro): 8216, Batch (considering grad accum): 1027,  Loss: 5.9017, Time: 6.06s, Token/s: 84.48
Epoch: 0, Step: 8217, Batch(micro): 8217, Batch (considering grad accum): 1027,  Loss: 6.4193, Time: 5.24s, Token/s: 97.71
Epoch: 0, Step: 8218, Batch(micro): 8218, Batch (considering grad accum): 1027,  Loss: 6.2417, Time: 3.98s, Token/s: 128.76
Epoch: 0, Step: 8219, Batch(micro): 8219, Batch (considering grad accum): 1027,  Loss: 5.5154, Time: 3.33s, Token/s: 153.98
Epoch: 0, Step: 8220, Batch(micro): 8220, Batch (considering grad accum): 1027,  Loss: 6.7250, Time: 3.14s, Token/s: 162.94
Epoch: 0, Step: 8221, Batch(micro): 8221, Batch (considering grad accum): 1027,  Loss: 6.5319, Time: 3.29s, Token/s: 155.50
Epoch: 0, Step: 8222, Batch(micro): 8222, Batch (considering grad accum): 1027,  Loss: 6.2634, Time: 3.23s, Token/s: 158.66
Epoch: 0, Step: 8223, Batch(micro): 8223, Batch (considering grad accum): 1027,  Loss: 6.6456, Time: 23.53s, Token/s: 21.76
Epoch: 0, Step: 8224, Batch(micro): 8224, Batch (considering grad accum): 1028,  Loss: 5.8352, Time: 7.50s, Token/s: 68.29
Epoch: 0, Step: 8225, Batch(micro): 8225, Batch (considering grad accum): 1028,  Loss: 6.3454, Time: 4.12s, Token/s: 124.36
Epoch: 0, Step: 8226, Batch(micro): 8226, Batch (considering grad accum): 1028,  Loss: 6.1811, Time: 3.60s, Token/s: 142.41
Epoch: 0, Step: 8227, Batch(micro): 8227, Batch (considering grad accum): 1028,  Loss: 5.7844, Time: 3.26s, Token/s: 157.28
Epoch: 0, Step: 8228, Batch(micro): 8228, Batch (considering grad accum): 1028,  Loss: 5.7962, Time: 3.25s, Token/s: 157.42
Epoch: 0, Step: 8229, Batch(micro): 8229, Batch (considering grad accum): 1028,  Loss: 5.7725, Time: 3.29s, Token/s: 155.68
Epoch: 0, Step: 8230, Batch(micro): 8230, Batch (considering grad accum): 1028,  Loss: 6.2172, Time: 3.42s, Token/s: 149.66
Epoch: 0, Step: 8231, Batch(micro): 8231, Batch (considering grad accum): 1028,  Loss: 6.4793, Time: 23.98s, Token/s: 21.35
Epoch: 0, Step: 8232, Batch(micro): 8232, Batch (considering grad accum): 1029,  Loss: 6.5222, Time: 6.66s, Token/s: 76.92
Epoch: 0, Step: 8233, Batch(micro): 8233, Batch (considering grad accum): 1029,  Loss: 5.5935, Time: 3.99s, Token/s: 128.22
Epoch: 0, Step: 8234, Batch(micro): 8234, Batch (considering grad accum): 1029,  Loss: 6.3122, Time: 3.94s, Token/s: 129.86
Epoch: 0, Step: 8235, Batch(micro): 8235, Batch (considering grad accum): 1029,  Loss: 6.4076, Time: 3.79s, Token/s: 135.08
Epoch: 0, Step: 8236, Batch(micro): 8236, Batch (considering grad accum): 1029,  Loss: 6.0085, Time: 3.61s, Token/s: 141.73
Epoch: 0, Step: 8237, Batch(micro): 8237, Batch (considering grad accum): 1029,  Loss: 5.6919, Time: 3.28s, Token/s: 156.15
Epoch: 0, Step: 8238, Batch(micro): 8238, Batch (considering grad accum): 1029,  Loss: 5.9841, Time: 3.25s, Token/s: 157.35
Epoch: 0, Step: 8239, Batch(micro): 8239, Batch (considering grad accum): 1029,  Loss: 5.7395, Time: 23.38s, Token/s: 21.90
Epoch: 0, Step: 8240, Batch(micro): 8240, Batch (considering grad accum): 1030,  Loss: 5.6634, Time: 7.96s, Token/s: 64.34
Epoch: 0, Step: 8241, Batch(micro): 8241, Batch (considering grad accum): 1030,  Loss: 5.7064, Time: 3.91s, Token/s: 130.94
Epoch: 0, Step: 8242, Batch(micro): 8242, Batch (considering grad accum): 1030,  Loss: 6.1537, Time: 3.64s, Token/s: 140.63
Epoch: 0, Step: 8243, Batch(micro): 8243, Batch (considering grad accum): 1030,  Loss: 5.1684, Time: 3.60s, Token/s: 142.40
Epoch: 0, Step: 8244, Batch(micro): 8244, Batch (considering grad accum): 1030,  Loss: 5.6455, Time: 3.71s, Token/s: 137.93
Epoch: 0, Step: 8245, Batch(micro): 8245, Batch (considering grad accum): 1030,  Loss: 6.2473, Time: 3.65s, Token/s: 140.30
Epoch: 0, Step: 8246, Batch(micro): 8246, Batch (considering grad accum): 1030,  Loss: 5.8002, Time: 3.38s, Token/s: 151.43
Epoch: 0, Step: 8247, Batch(micro): 8247, Batch (considering grad accum): 1030,  Loss: 6.0533, Time: 23.23s, Token/s: 22.04
Epoch: 0, Step: 8248, Batch(micro): 8248, Batch (considering grad accum): 1031,  Loss: 6.0344, Time: 8.74s, Token/s: 58.59
Epoch: 0, Step: 8249, Batch(micro): 8249, Batch (considering grad accum): 1031,  Loss: 5.6764, Time: 3.91s, Token/s: 130.90
Epoch: 0, Step: 8250, Batch(micro): 8250, Batch (considering grad accum): 1031,  Loss: 5.3840, Time: 3.53s, Token/s: 145.03
Epoch: 0, Step: 8251, Batch(micro): 8251, Batch (considering grad accum): 1031,  Loss: 5.2805, Time: 4.35s, Token/s: 117.65
Epoch: 0, Step: 8252, Batch(micro): 8252, Batch (considering grad accum): 1031,  Loss: 6.0953, Time: 4.03s, Token/s: 127.11
Epoch: 0, Step: 8253, Batch(micro): 8253, Batch (considering grad accum): 1031,  Loss: 5.5597, Time: 3.92s, Token/s: 130.48
Epoch: 0, Step: 8254, Batch(micro): 8254, Batch (considering grad accum): 1031,  Loss: 5.9753, Time: 3.47s, Token/s: 147.54
Epoch: 0, Step: 8255, Batch(micro): 8255, Batch (considering grad accum): 1031,  Loss: 5.1974, Time: 25.18s, Token/s: 20.33
Epoch: 0, Step: 8256, Batch(micro): 8256, Batch (considering grad accum): 1032,  Loss: 5.6737, Time: 8.74s, Token/s: 58.57
Epoch: 0, Step: 8257, Batch(micro): 8257, Batch (considering grad accum): 1032,  Loss: 6.5744, Time: 3.39s, Token/s: 151.19
Epoch: 0, Step: 8258, Batch(micro): 8258, Batch (considering grad accum): 1032,  Loss: 6.2039, Time: 3.23s, Token/s: 158.47
Epoch: 0, Step: 8259, Batch(micro): 8259, Batch (considering grad accum): 1032,  Loss: 6.1584, Time: 3.17s, Token/s: 161.59
Epoch: 0, Step: 8260, Batch(micro): 8260, Batch (considering grad accum): 1032,  Loss: 6.0437, Time: 3.41s, Token/s: 150.07
Epoch: 0, Step: 8261, Batch(micro): 8261, Batch (considering grad accum): 1032,  Loss: 5.6489, Time: 3.69s, Token/s: 138.60
Epoch: 0, Step: 8262, Batch(micro): 8262, Batch (considering grad accum): 1032,  Loss: 6.7152, Time: 3.86s, Token/s: 132.52
Epoch: 0, Step: 8263, Batch(micro): 8263, Batch (considering grad accum): 1032,  Loss: 6.1965, Time: 23.28s, Token/s: 21.99
Epoch: 0, Step: 8264, Batch(micro): 8264, Batch (considering grad accum): 1033,  Loss: 6.6440, Time: 6.77s, Token/s: 75.60
Epoch: 0, Step: 8265, Batch(micro): 8265, Batch (considering grad accum): 1033,  Loss: 6.2303, Time: 4.71s, Token/s: 108.69
Epoch: 0, Step: 8266, Batch(micro): 8266, Batch (considering grad accum): 1033,  Loss: 6.1946, Time: 3.55s, Token/s: 144.32
Epoch: 0, Step: 8267, Batch(micro): 8267, Batch (considering grad accum): 1033,  Loss: 5.6819, Time: 3.25s, Token/s: 157.73
Epoch: 0, Step: 8268, Batch(micro): 8268, Batch (considering grad accum): 1033,  Loss: 6.1924, Time: 3.29s, Token/s: 155.40
Epoch: 0, Step: 8269, Batch(micro): 8269, Batch (considering grad accum): 1033,  Loss: 6.1301, Time: 3.43s, Token/s: 149.35
Epoch: 0, Step: 8270, Batch(micro): 8270, Batch (considering grad accum): 1033,  Loss: 6.1641, Time: 3.70s, Token/s: 138.31
Epoch: 0, Step: 8271, Batch(micro): 8271, Batch (considering grad accum): 1033,  Loss: 6.1807, Time: 24.11s, Token/s: 21.24
Epoch: 0, Step: 8272, Batch(micro): 8272, Batch (considering grad accum): 1034,  Loss: 5.9922, Time: 7.22s, Token/s: 70.91
Epoch: 0, Step: 8273, Batch(micro): 8273, Batch (considering grad accum): 1034,  Loss: 6.6934, Time: 3.94s, Token/s: 129.90
Epoch: 0, Step: 8274, Batch(micro): 8274, Batch (considering grad accum): 1034,  Loss: 5.4491, Time: 3.79s, Token/s: 134.95
Epoch: 0, Step: 8275, Batch(micro): 8275, Batch (considering grad accum): 1034,  Loss: 6.1813, Time: 3.60s, Token/s: 142.04
Epoch: 0, Step: 8276, Batch(micro): 8276, Batch (considering grad accum): 1034,  Loss: 6.8246, Time: 3.54s, Token/s: 144.71
Epoch: 0, Step: 8277, Batch(micro): 8277, Batch (considering grad accum): 1034,  Loss: 6.6828, Time: 3.44s, Token/s: 148.69
Epoch: 0, Step: 8278, Batch(micro): 8278, Batch (considering grad accum): 1034,  Loss: 5.7593, Time: 3.31s, Token/s: 154.60
Epoch: 0, Step: 8279, Batch(micro): 8279, Batch (considering grad accum): 1034,  Loss: 6.3385, Time: 24.30s, Token/s: 21.07
Epoch: 0, Step: 8280, Batch(micro): 8280, Batch (considering grad accum): 1035,  Loss: 5.3748, Time: 8.97s, Token/s: 57.09
Epoch: 0, Step: 8281, Batch(micro): 8281, Batch (considering grad accum): 1035,  Loss: 6.3665, Time: 3.80s, Token/s: 134.77
Epoch: 0, Step: 8282, Batch(micro): 8282, Batch (considering grad accum): 1035,  Loss: 6.4760, Time: 3.38s, Token/s: 151.70
Epoch: 0, Step: 8283, Batch(micro): 8283, Batch (considering grad accum): 1035,  Loss: 5.9724, Time: 3.36s, Token/s: 152.54
Epoch: 0, Step: 8284, Batch(micro): 8284, Batch (considering grad accum): 1035,  Loss: 5.9971, Time: 3.54s, Token/s: 144.79
Epoch: 0, Step: 8285, Batch(micro): 8285, Batch (considering grad accum): 1035,  Loss: 6.4869, Time: 3.55s, Token/s: 144.28
Epoch: 0, Step: 8286, Batch(micro): 8286, Batch (considering grad accum): 1035,  Loss: 6.6568, Time: 3.53s, Token/s: 145.00
Epoch: 0, Step: 8287, Batch(micro): 8287, Batch (considering grad accum): 1035,  Loss: 5.9925, Time: 21.80s, Token/s: 23.48
Epoch: 0, Step: 8288, Batch(micro): 8288, Batch (considering grad accum): 1036,  Loss: 5.7500, Time: 9.75s, Token/s: 52.51
Epoch: 0, Step: 8289, Batch(micro): 8289, Batch (considering grad accum): 1036,  Loss: 5.7551, Time: 3.33s, Token/s: 153.94
Epoch: 0, Step: 8290, Batch(micro): 8290, Batch (considering grad accum): 1036,  Loss: 5.8230, Time: 3.23s, Token/s: 158.33
Epoch: 0, Step: 8291, Batch(micro): 8291, Batch (considering grad accum): 1036,  Loss: 6.0159, Time: 3.16s, Token/s: 162.11
Epoch: 0, Step: 8292, Batch(micro): 8292, Batch (considering grad accum): 1036,  Loss: 6.0252, Time: 3.31s, Token/s: 154.59
Epoch: 0, Step: 8293, Batch(micro): 8293, Batch (considering grad accum): 1036,  Loss: 6.0914, Time: 3.82s, Token/s: 134.18
Epoch: 0, Step: 8294, Batch(micro): 8294, Batch (considering grad accum): 1036,  Loss: 5.7564, Time: 3.81s, Token/s: 134.47
Epoch: 0, Step: 8295, Batch(micro): 8295, Batch (considering grad accum): 1036,  Loss: 5.0403, Time: 26.91s, Token/s: 19.03
Epoch: 0, Step: 8296, Batch(micro): 8296, Batch (considering grad accum): 1037,  Loss: 6.1116, Time: 7.16s, Token/s: 71.53
Epoch: 0, Step: 8297, Batch(micro): 8297, Batch (considering grad accum): 1037,  Loss: 6.0786, Time: 3.97s, Token/s: 128.83
Epoch: 0, Step: 8298, Batch(micro): 8298, Batch (considering grad accum): 1037,  Loss: 5.8889, Time: 3.73s, Token/s: 137.27
Epoch: 0, Step: 8299, Batch(micro): 8299, Batch (considering grad accum): 1037,  Loss: 6.5510, Time: 3.69s, Token/s: 138.82
Updating MLP bias
Epoch: 0, Step: 8300, Batch(micro): 8300, Batch (considering grad accum): 1037,  Loss: 5.8350, Time: 3.24s, Token/s: 158.25
Epoch: 0, Step: 8301, Batch(micro): 8301, Batch (considering grad accum): 1037,  Loss: 6.7486, Time: 2.88s, Token/s: 177.75
Epoch: 0, Step: 8302, Batch(micro): 8302, Batch (considering grad accum): 1037,  Loss: 5.6040, Time: 2.99s, Token/s: 171.09
Epoch: 0, Step: 8303, Batch(micro): 8303, Batch (considering grad accum): 1037,  Loss: 6.2419, Time: 26.40s, Token/s: 19.40
Epoch: 0, Step: 8304, Batch(micro): 8304, Batch (considering grad accum): 1038,  Loss: 6.6410, Time: 8.61s, Token/s: 59.44
Epoch: 0, Step: 8305, Batch(micro): 8305, Batch (considering grad accum): 1038,  Loss: 6.2051, Time: 3.85s, Token/s: 133.09
Epoch: 0, Step: 8306, Batch(micro): 8306, Batch (considering grad accum): 1038,  Loss: 5.8220, Time: 3.77s, Token/s: 135.74
Epoch: 0, Step: 8307, Batch(micro): 8307, Batch (considering grad accum): 1038,  Loss: 5.7257, Time: 3.60s, Token/s: 142.16
Epoch: 0, Step: 8308, Batch(micro): 8308, Batch (considering grad accum): 1038,  Loss: 6.0115, Time: 3.40s, Token/s: 150.67
Epoch: 0, Step: 8309, Batch(micro): 8309, Batch (considering grad accum): 1038,  Loss: 6.1311, Time: 3.61s, Token/s: 141.83
Epoch: 0, Step: 8310, Batch(micro): 8310, Batch (considering grad accum): 1038,  Loss: 6.3642, Time: 3.40s, Token/s: 150.55
Epoch: 0, Step: 8311, Batch(micro): 8311, Batch (considering grad accum): 1038,  Loss: 5.8003, Time: 25.26s, Token/s: 20.27
Epoch: 0, Step: 8312, Batch(micro): 8312, Batch (considering grad accum): 1039,  Loss: 6.3176, Time: 8.47s, Token/s: 60.47
Epoch: 0, Step: 8313, Batch(micro): 8313, Batch (considering grad accum): 1039,  Loss: 6.0770, Time: 3.37s, Token/s: 152.11
Epoch: 0, Step: 8314, Batch(micro): 8314, Batch (considering grad accum): 1039,  Loss: 5.4276, Time: 3.19s, Token/s: 160.43
Epoch: 0, Step: 8315, Batch(micro): 8315, Batch (considering grad accum): 1039,  Loss: 6.4073, Time: 3.17s, Token/s: 161.34
Epoch: 0, Step: 8316, Batch(micro): 8316, Batch (considering grad accum): 1039,  Loss: 5.6835, Time: 3.42s, Token/s: 149.83
Epoch: 0, Step: 8317, Batch(micro): 8317, Batch (considering grad accum): 1039,  Loss: 6.0468, Time: 3.42s, Token/s: 149.60
Epoch: 0, Step: 8318, Batch(micro): 8318, Batch (considering grad accum): 1039,  Loss: 6.2433, Time: 3.31s, Token/s: 154.63
Epoch: 0, Step: 8319, Batch(micro): 8319, Batch (considering grad accum): 1039,  Loss: 6.6793, Time: 23.86s, Token/s: 21.46
Epoch: 0, Step: 8320, Batch(micro): 8320, Batch (considering grad accum): 1040,  Loss: 5.7415, Time: 6.90s, Token/s: 74.18
Epoch: 0, Step: 8321, Batch(micro): 8321, Batch (considering grad accum): 1040,  Loss: 5.9731, Time: 3.92s, Token/s: 130.48
Epoch: 0, Step: 8322, Batch(micro): 8322, Batch (considering grad accum): 1040,  Loss: 6.0238, Time: 3.29s, Token/s: 155.80
Epoch: 0, Step: 8323, Batch(micro): 8323, Batch (considering grad accum): 1040,  Loss: 5.0200, Time: 3.24s, Token/s: 158.04
Epoch: 0, Step: 8324, Batch(micro): 8324, Batch (considering grad accum): 1040,  Loss: 5.0203, Time: 3.53s, Token/s: 145.12
Epoch: 0, Step: 8325, Batch(micro): 8325, Batch (considering grad accum): 1040,  Loss: 5.6866, Time: 3.74s, Token/s: 136.87
Epoch: 0, Step: 8326, Batch(micro): 8326, Batch (considering grad accum): 1040,  Loss: 5.5393, Time: 3.51s, Token/s: 145.98
Epoch: 0, Step: 8327, Batch(micro): 8327, Batch (considering grad accum): 1040,  Loss: 5.6974, Time: 23.38s, Token/s: 21.90
Epoch: 0, Step: 8328, Batch(micro): 8328, Batch (considering grad accum): 1041,  Loss: 5.9431, Time: 7.09s, Token/s: 72.20
Epoch: 0, Step: 8329, Batch(micro): 8329, Batch (considering grad accum): 1041,  Loss: 5.4287, Time: 3.77s, Token/s: 135.93
Epoch: 0, Step: 8330, Batch(micro): 8330, Batch (considering grad accum): 1041,  Loss: 6.1925, Time: 3.28s, Token/s: 156.29
Epoch: 0, Step: 8331, Batch(micro): 8331, Batch (considering grad accum): 1041,  Loss: 6.1292, Time: 3.38s, Token/s: 151.43
Epoch: 0, Step: 8332, Batch(micro): 8332, Batch (considering grad accum): 1041,  Loss: 5.4453, Time: 3.22s, Token/s: 158.82
Epoch: 0, Step: 8333, Batch(micro): 8333, Batch (considering grad accum): 1041,  Loss: 5.6611, Time: 3.39s, Token/s: 151.02
Epoch: 0, Step: 8334, Batch(micro): 8334, Batch (considering grad accum): 1041,  Loss: 5.2361, Time: 3.43s, Token/s: 149.39
Epoch: 0, Step: 8335, Batch(micro): 8335, Batch (considering grad accum): 1041,  Loss: 5.5785, Time: 22.15s, Token/s: 23.11
Epoch: 0, Step: 8336, Batch(micro): 8336, Batch (considering grad accum): 1042,  Loss: 5.1891, Time: 7.57s, Token/s: 67.61
Epoch: 0, Step: 8337, Batch(micro): 8337, Batch (considering grad accum): 1042,  Loss: 5.2678, Time: 4.06s, Token/s: 126.08
Epoch: 0, Step: 8338, Batch(micro): 8338, Batch (considering grad accum): 1042,  Loss: 6.6158, Time: 3.74s, Token/s: 137.04
Epoch: 0, Step: 8339, Batch(micro): 8339, Batch (considering grad accum): 1042,  Loss: 6.9057, Time: 3.52s, Token/s: 145.31
Epoch: 0, Step: 8340, Batch(micro): 8340, Batch (considering grad accum): 1042,  Loss: 7.2470, Time: 3.81s, Token/s: 134.34
Epoch: 0, Step: 8341, Batch(micro): 8341, Batch (considering grad accum): 1042,  Loss: 6.1487, Time: 3.56s, Token/s: 143.87
Epoch: 0, Step: 8342, Batch(micro): 8342, Batch (considering grad accum): 1042,  Loss: 5.9883, Time: 3.31s, Token/s: 154.50
Epoch: 0, Step: 8343, Batch(micro): 8343, Batch (considering grad accum): 1042,  Loss: 6.5020, Time: 23.12s, Token/s: 22.14
Epoch: 0, Step: 8344, Batch(micro): 8344, Batch (considering grad accum): 1043,  Loss: 5.8533, Time: 6.61s, Token/s: 77.47
Epoch: 0, Step: 8345, Batch(micro): 8345, Batch (considering grad accum): 1043,  Loss: 6.3399, Time: 4.03s, Token/s: 127.20
Epoch: 0, Step: 8346, Batch(micro): 8346, Batch (considering grad accum): 1043,  Loss: 5.8732, Time: 3.50s, Token/s: 146.21
Epoch: 0, Step: 8347, Batch(micro): 8347, Batch (considering grad accum): 1043,  Loss: 6.1748, Time: 3.23s, Token/s: 158.62
Epoch: 0, Step: 8348, Batch(micro): 8348, Batch (considering grad accum): 1043,  Loss: 5.9725, Time: 3.27s, Token/s: 156.61
Epoch: 0, Step: 8349, Batch(micro): 8349, Batch (considering grad accum): 1043,  Loss: 6.1399, Time: 3.21s, Token/s: 159.72
Epoch: 0, Step: 8350, Batch(micro): 8350, Batch (considering grad accum): 1043,  Loss: 5.8376, Time: 3.22s, Token/s: 159.24
Epoch: 0, Step: 8351, Batch(micro): 8351, Batch (considering grad accum): 1043,  Loss: 6.1194, Time: 23.42s, Token/s: 21.86
Epoch: 0, Step: 8352, Batch(micro): 8352, Batch (considering grad accum): 1044,  Loss: 5.7245, Time: 6.69s, Token/s: 76.58
Epoch: 0, Step: 8353, Batch(micro): 8353, Batch (considering grad accum): 1044,  Loss: 6.0279, Time: 3.84s, Token/s: 133.24
Epoch: 0, Step: 8354, Batch(micro): 8354, Batch (considering grad accum): 1044,  Loss: 5.7784, Time: 3.43s, Token/s: 149.48
Epoch: 0, Step: 8355, Batch(micro): 8355, Batch (considering grad accum): 1044,  Loss: 6.0528, Time: 3.27s, Token/s: 156.37
Epoch: 0, Step: 8356, Batch(micro): 8356, Batch (considering grad accum): 1044,  Loss: 5.9177, Time: 3.17s, Token/s: 161.36
Epoch: 0, Step: 8357, Batch(micro): 8357, Batch (considering grad accum): 1044,  Loss: 6.0615, Time: 3.25s, Token/s: 157.49
Epoch: 0, Step: 8358, Batch(micro): 8358, Batch (considering grad accum): 1044,  Loss: 6.2690, Time: 3.59s, Token/s: 142.63
Epoch: 0, Step: 8359, Batch(micro): 8359, Batch (considering grad accum): 1044,  Loss: 6.2081, Time: 23.76s, Token/s: 21.55
Epoch: 0, Step: 8360, Batch(micro): 8360, Batch (considering grad accum): 1045,  Loss: 6.4931, Time: 6.76s, Token/s: 75.70
Epoch: 0, Step: 8361, Batch(micro): 8361, Batch (considering grad accum): 1045,  Loss: 6.0176, Time: 3.55s, Token/s: 144.29
Epoch: 0, Step: 8362, Batch(micro): 8362, Batch (considering grad accum): 1045,  Loss: 5.6807, Time: 3.36s, Token/s: 152.56
Epoch: 0, Step: 8363, Batch(micro): 8363, Batch (considering grad accum): 1045,  Loss: 5.4189, Time: 3.50s, Token/s: 146.45
Epoch: 0, Step: 8364, Batch(micro): 8364, Batch (considering grad accum): 1045,  Loss: 6.1274, Time: 3.80s, Token/s: 134.80
Epoch: 0, Step: 8365, Batch(micro): 8365, Batch (considering grad accum): 1045,  Loss: 6.0053, Time: 3.55s, Token/s: 144.07
Epoch: 0, Step: 8366, Batch(micro): 8366, Batch (considering grad accum): 1045,  Loss: 5.9534, Time: 3.21s, Token/s: 159.59
Epoch: 0, Step: 8367, Batch(micro): 8367, Batch (considering grad accum): 1045,  Loss: 6.9724, Time: 22.42s, Token/s: 22.84
Epoch: 0, Step: 8368, Batch(micro): 8368, Batch (considering grad accum): 1046,  Loss: 6.1858, Time: 6.71s, Token/s: 76.30
Epoch: 0, Step: 8369, Batch(micro): 8369, Batch (considering grad accum): 1046,  Loss: 5.4467, Time: 4.84s, Token/s: 105.73
Epoch: 0, Step: 8370, Batch(micro): 8370, Batch (considering grad accum): 1046,  Loss: 5.3747, Time: 3.50s, Token/s: 146.15
Epoch: 0, Step: 8371, Batch(micro): 8371, Batch (considering grad accum): 1046,  Loss: 5.9981, Time: 3.22s, Token/s: 159.20
Epoch: 0, Step: 8372, Batch(micro): 8372, Batch (considering grad accum): 1046,  Loss: 5.1934, Time: 3.13s, Token/s: 163.79
Epoch: 0, Step: 8373, Batch(micro): 8373, Batch (considering grad accum): 1046,  Loss: 5.6202, Time: 3.19s, Token/s: 160.28
Epoch: 0, Step: 8374, Batch(micro): 8374, Batch (considering grad accum): 1046,  Loss: 5.9501, Time: 3.20s, Token/s: 160.13
Epoch: 0, Step: 8375, Batch(micro): 8375, Batch (considering grad accum): 1046,  Loss: 5.2785, Time: 22.59s, Token/s: 22.66
Epoch: 0, Step: 8376, Batch(micro): 8376, Batch (considering grad accum): 1047,  Loss: 5.7052, Time: 9.40s, Token/s: 54.46
Epoch: 0, Step: 8377, Batch(micro): 8377, Batch (considering grad accum): 1047,  Loss: 6.1371, Time: 3.40s, Token/s: 150.58
Epoch: 0, Step: 8378, Batch(micro): 8378, Batch (considering grad accum): 1047,  Loss: 6.6797, Time: 3.25s, Token/s: 157.64
Epoch: 0, Step: 8379, Batch(micro): 8379, Batch (considering grad accum): 1047,  Loss: 6.7185, Time: 3.15s, Token/s: 162.32
Epoch: 0, Step: 8380, Batch(micro): 8380, Batch (considering grad accum): 1047,  Loss: 5.7594, Time: 3.27s, Token/s: 156.46
Epoch: 0, Step: 8381, Batch(micro): 8381, Batch (considering grad accum): 1047,  Loss: 6.2846, Time: 3.27s, Token/s: 156.54
Epoch: 0, Step: 8382, Batch(micro): 8382, Batch (considering grad accum): 1047,  Loss: 6.1127, Time: 3.24s, Token/s: 157.83
Epoch: 0, Step: 8383, Batch(micro): 8383, Batch (considering grad accum): 1047,  Loss: 6.4391, Time: 24.25s, Token/s: 21.12
Epoch: 0, Step: 8384, Batch(micro): 8384, Batch (considering grad accum): 1048,  Loss: 6.8856, Time: 8.52s, Token/s: 60.11
Epoch: 0, Step: 8385, Batch(micro): 8385, Batch (considering grad accum): 1048,  Loss: 5.8035, Time: 3.84s, Token/s: 133.36
Epoch: 0, Step: 8386, Batch(micro): 8386, Batch (considering grad accum): 1048,  Loss: 7.0327, Time: 3.40s, Token/s: 150.78
Epoch: 0, Step: 8387, Batch(micro): 8387, Batch (considering grad accum): 1048,  Loss: 6.0943, Time: 3.49s, Token/s: 146.59
Epoch: 0, Step: 8388, Batch(micro): 8388, Batch (considering grad accum): 1048,  Loss: 6.2666, Time: 3.35s, Token/s: 152.69
Epoch: 0, Step: 8389, Batch(micro): 8389, Batch (considering grad accum): 1048,  Loss: 5.9329, Time: 3.52s, Token/s: 145.51
Epoch: 0, Step: 8390, Batch(micro): 8390, Batch (considering grad accum): 1048,  Loss: 6.2684, Time: 3.58s, Token/s: 142.99
Epoch: 0, Step: 8391, Batch(micro): 8391, Batch (considering grad accum): 1048,  Loss: 6.6392, Time: 22.96s, Token/s: 22.30
Epoch: 0, Step: 8392, Batch(micro): 8392, Batch (considering grad accum): 1049,  Loss: 6.5148, Time: 6.22s, Token/s: 82.33
Epoch: 0, Step: 8393, Batch(micro): 8393, Batch (considering grad accum): 1049,  Loss: 5.6895, Time: 3.77s, Token/s: 135.86
Epoch: 0, Step: 8394, Batch(micro): 8394, Batch (considering grad accum): 1049,  Loss: 5.7770, Time: 3.58s, Token/s: 143.02
Epoch: 0, Step: 8395, Batch(micro): 8395, Batch (considering grad accum): 1049,  Loss: 5.7372, Time: 4.18s, Token/s: 122.40
Epoch: 0, Step: 8396, Batch(micro): 8396, Batch (considering grad accum): 1049,  Loss: 5.9638, Time: 3.68s, Token/s: 139.27
Epoch: 0, Step: 8397, Batch(micro): 8397, Batch (considering grad accum): 1049,  Loss: 5.3079, Time: 3.25s, Token/s: 157.58
Epoch: 0, Step: 8398, Batch(micro): 8398, Batch (considering grad accum): 1049,  Loss: 5.8818, Time: 3.24s, Token/s: 158.03
Epoch: 0, Step: 8399, Batch(micro): 8399, Batch (considering grad accum): 1049,  Loss: 5.4963, Time: 18.09s, Token/s: 28.30
Updating MLP bias
Epoch: 0, Step: 8400, Batch(micro): 8400, Batch (considering grad accum): 1050,  Loss: 4.8359, Time: 6.60s, Token/s: 77.53
Epoch: 0, Step: 8401, Batch(micro): 8401, Batch (considering grad accum): 1050,  Loss: 5.6332, Time: 4.21s, Token/s: 121.58
Epoch: 0, Step: 8402, Batch(micro): 8402, Batch (considering grad accum): 1050,  Loss: 6.6935, Time: 4.00s, Token/s: 127.96
Epoch: 0, Step: 8403, Batch(micro): 8403, Batch (considering grad accum): 1050,  Loss: 5.5718, Time: 3.54s, Token/s: 144.47
Epoch: 0, Step: 8404, Batch(micro): 8404, Batch (considering grad accum): 1050,  Loss: 7.0680, Time: 3.58s, Token/s: 143.02
Epoch: 0, Step: 8405, Batch(micro): 8405, Batch (considering grad accum): 1050,  Loss: 4.5753, Time: 3.29s, Token/s: 155.74
Epoch: 0, Step: 8406, Batch(micro): 8406, Batch (considering grad accum): 1050,  Loss: 5.8287, Time: 3.27s, Token/s: 156.65
Epoch: 0, Step: 8407, Batch(micro): 8407, Batch (considering grad accum): 1050,  Loss: 5.6788, Time: 18.52s, Token/s: 27.65
Epoch: 0, Step: 8408, Batch(micro): 8408, Batch (considering grad accum): 1051,  Loss: 5.6920, Time: 6.64s, Token/s: 77.17
Epoch: 0, Step: 8409, Batch(micro): 8409, Batch (considering grad accum): 1051,  Loss: 5.7550, Time: 3.63s, Token/s: 140.91
Epoch: 0, Step: 8410, Batch(micro): 8410, Batch (considering grad accum): 1051,  Loss: 6.1896, Time: 3.22s, Token/s: 158.86
Epoch: 0, Step: 8411, Batch(micro): 8411, Batch (considering grad accum): 1051,  Loss: 6.9539, Time: 3.22s, Token/s: 159.25
Epoch: 0, Step: 8412, Batch(micro): 8412, Batch (considering grad accum): 1051,  Loss: 6.4594, Time: 3.23s, Token/s: 158.30
Epoch: 0, Step: 8413, Batch(micro): 8413, Batch (considering grad accum): 1051,  Loss: 5.9277, Time: 3.38s, Token/s: 151.46
Epoch: 0, Step: 8414, Batch(micro): 8414, Batch (considering grad accum): 1051,  Loss: 6.7169, Time: 3.53s, Token/s: 145.24
Epoch: 0, Step: 8415, Batch(micro): 8415, Batch (considering grad accum): 1051,  Loss: 6.2519, Time: 18.61s, Token/s: 27.51
Epoch: 0, Step: 8416, Batch(micro): 8416, Batch (considering grad accum): 1052,  Loss: 6.5618, Time: 6.14s, Token/s: 83.36
Epoch: 0, Step: 8417, Batch(micro): 8417, Batch (considering grad accum): 1052,  Loss: 7.1804, Time: 3.76s, Token/s: 136.14
Epoch: 0, Step: 8418, Batch(micro): 8418, Batch (considering grad accum): 1052,  Loss: 5.7469, Time: 3.22s, Token/s: 159.03
Epoch: 0, Step: 8419, Batch(micro): 8419, Batch (considering grad accum): 1052,  Loss: 5.2924, Time: 3.18s, Token/s: 160.98
Epoch: 0, Step: 8420, Batch(micro): 8420, Batch (considering grad accum): 1052,  Loss: 6.2825, Time: 3.18s, Token/s: 160.75
Epoch: 0, Step: 8421, Batch(micro): 8421, Batch (considering grad accum): 1052,  Loss: 7.4395, Time: 3.25s, Token/s: 157.45
Epoch: 0, Step: 8422, Batch(micro): 8422, Batch (considering grad accum): 1052,  Loss: 5.6385, Time: 3.20s, Token/s: 159.91
Epoch: 0, Step: 8423, Batch(micro): 8423, Batch (considering grad accum): 1052,  Loss: 5.5448, Time: 17.39s, Token/s: 29.43
Epoch: 0, Step: 8424, Batch(micro): 8424, Batch (considering grad accum): 1053,  Loss: 5.8081, Time: 6.49s, Token/s: 78.89
Epoch: 0, Step: 8425, Batch(micro): 8425, Batch (considering grad accum): 1053,  Loss: 5.0207, Time: 4.14s, Token/s: 123.71
Epoch: 0, Step: 8426, Batch(micro): 8426, Batch (considering grad accum): 1053,  Loss: 6.6332, Time: 3.50s, Token/s: 146.39
Epoch: 0, Step: 8427, Batch(micro): 8427, Batch (considering grad accum): 1053,  Loss: 6.2266, Time: 3.64s, Token/s: 140.51
Epoch: 0, Step: 8428, Batch(micro): 8428, Batch (considering grad accum): 1053,  Loss: 6.6808, Time: 3.69s, Token/s: 138.63
Epoch: 0, Step: 8429, Batch(micro): 8429, Batch (considering grad accum): 1053,  Loss: 5.6482, Time: 3.20s, Token/s: 159.79
Epoch: 0, Step: 8430, Batch(micro): 8430, Batch (considering grad accum): 1053,  Loss: 5.8214, Time: 3.36s, Token/s: 152.56
Epoch: 0, Step: 8431, Batch(micro): 8431, Batch (considering grad accum): 1053,  Loss: 6.1752, Time: 20.58s, Token/s: 24.88
Epoch: 0, Step: 8432, Batch(micro): 8432, Batch (considering grad accum): 1054,  Loss: 6.6493, Time: 7.89s, Token/s: 64.86
Epoch: 0, Step: 8433, Batch(micro): 8433, Batch (considering grad accum): 1054,  Loss: 6.0884, Time: 3.98s, Token/s: 128.49
Epoch: 0, Step: 8434, Batch(micro): 8434, Batch (considering grad accum): 1054,  Loss: 5.7332, Time: 3.69s, Token/s: 138.68
Epoch: 0, Step: 8435, Batch(micro): 8435, Batch (considering grad accum): 1054,  Loss: 5.9052, Time: 3.69s, Token/s: 138.78
Epoch: 0, Step: 8436, Batch(micro): 8436, Batch (considering grad accum): 1054,  Loss: 6.4315, Time: 3.52s, Token/s: 145.43
Epoch: 0, Step: 8437, Batch(micro): 8437, Batch (considering grad accum): 1054,  Loss: 5.9379, Time: 3.61s, Token/s: 141.71
Epoch: 0, Step: 8438, Batch(micro): 8438, Batch (considering grad accum): 1054,  Loss: 5.1225, Time: 3.40s, Token/s: 150.67
Epoch: 0, Step: 8439, Batch(micro): 8439, Batch (considering grad accum): 1054,  Loss: 5.3663, Time: 17.99s, Token/s: 28.45
Epoch: 0, Step: 8440, Batch(micro): 8440, Batch (considering grad accum): 1055,  Loss: 5.6917, Time: 6.39s, Token/s: 80.08
Epoch: 0, Step: 8441, Batch(micro): 8441, Batch (considering grad accum): 1055,  Loss: 6.0789, Time: 3.86s, Token/s: 132.49
Epoch: 0, Step: 8442, Batch(micro): 8442, Batch (considering grad accum): 1055,  Loss: 5.8372, Time: 3.32s, Token/s: 154.27
Epoch: 0, Step: 8443, Batch(micro): 8443, Batch (considering grad accum): 1055,  Loss: 6.1057, Time: 3.40s, Token/s: 150.39
Epoch: 0, Step: 8444, Batch(micro): 8444, Batch (considering grad accum): 1055,  Loss: 5.9536, Time: 3.54s, Token/s: 144.76
Epoch: 0, Step: 8445, Batch(micro): 8445, Batch (considering grad accum): 1055,  Loss: 6.1301, Time: 3.67s, Token/s: 139.60
Epoch: 0, Step: 8446, Batch(micro): 8446, Batch (considering grad accum): 1055,  Loss: 6.0555, Time: 3.41s, Token/s: 150.02
Epoch: 0, Step: 8447, Batch(micro): 8447, Batch (considering grad accum): 1055,  Loss: 6.0528, Time: 18.08s, Token/s: 28.32
Epoch: 0, Step: 8448, Batch(micro): 8448, Batch (considering grad accum): 1056,  Loss: 5.9333, Time: 7.56s, Token/s: 67.69
Epoch: 0, Step: 8449, Batch(micro): 8449, Batch (considering grad accum): 1056,  Loss: 5.9264, Time: 3.90s, Token/s: 131.31
Epoch: 0, Step: 8450, Batch(micro): 8450, Batch (considering grad accum): 1056,  Loss: 5.1661, Time: 3.64s, Token/s: 140.51
Epoch: 0, Step: 8451, Batch(micro): 8451, Batch (considering grad accum): 1056,  Loss: 5.8027, Time: 3.67s, Token/s: 139.42
Epoch: 0, Step: 8452, Batch(micro): 8452, Batch (considering grad accum): 1056,  Loss: 6.1228, Time: 3.39s, Token/s: 150.91
Epoch: 0, Step: 8453, Batch(micro): 8453, Batch (considering grad accum): 1056,  Loss: 6.4200, Time: 3.36s, Token/s: 152.19
Epoch: 0, Step: 8454, Batch(micro): 8454, Batch (considering grad accum): 1056,  Loss: 9.7145, Time: 3.49s, Token/s: 146.91
Epoch: 0, Step: 8455, Batch(micro): 8455, Batch (considering grad accum): 1056,  Loss: 10.8020, Time: 22.82s, Token/s: 22.44
Epoch: 0, Step: 8456, Batch(micro): 8456, Batch (considering grad accum): 1057,  Loss: 6.0541, Time: 6.86s, Token/s: 74.66
Epoch: 0, Step: 8457, Batch(micro): 8457, Batch (considering grad accum): 1057,  Loss: 5.9455, Time: 3.76s, Token/s: 136.24
Epoch: 0, Step: 8458, Batch(micro): 8458, Batch (considering grad accum): 1057,  Loss: 7.0408, Time: 3.20s, Token/s: 160.01
Epoch: 0, Step: 8459, Batch(micro): 8459, Batch (considering grad accum): 1057,  Loss: 6.6448, Time: 3.23s, Token/s: 158.71
Epoch: 0, Step: 8460, Batch(micro): 8460, Batch (considering grad accum): 1057,  Loss: 5.9544, Time: 3.38s, Token/s: 151.37
Epoch: 0, Step: 8461, Batch(micro): 8461, Batch (considering grad accum): 1057,  Loss: 6.0681, Time: 3.30s, Token/s: 155.25
Epoch: 0, Step: 8462, Batch(micro): 8462, Batch (considering grad accum): 1057,  Loss: 6.1643, Time: 3.79s, Token/s: 134.95
Epoch: 0, Step: 8463, Batch(micro): 8463, Batch (considering grad accum): 1057,  Loss: 5.9195, Time: 22.89s, Token/s: 22.37
Epoch: 0, Step: 8464, Batch(micro): 8464, Batch (considering grad accum): 1058,  Loss: 5.6307, Time: 7.22s, Token/s: 70.95
Epoch: 0, Step: 8465, Batch(micro): 8465, Batch (considering grad accum): 1058,  Loss: 5.4478, Time: 3.95s, Token/s: 129.54
Epoch: 0, Step: 8466, Batch(micro): 8466, Batch (considering grad accum): 1058,  Loss: 5.4464, Time: 3.40s, Token/s: 150.38
Epoch: 0, Step: 8467, Batch(micro): 8467, Batch (considering grad accum): 1058,  Loss: 5.8194, Time: 3.52s, Token/s: 145.61
Epoch: 0, Step: 8468, Batch(micro): 8468, Batch (considering grad accum): 1058,  Loss: 5.5567, Time: 3.46s, Token/s: 147.87
Epoch: 0, Step: 8469, Batch(micro): 8469, Batch (considering grad accum): 1058,  Loss: 5.0621, Time: 3.48s, Token/s: 147.06
Epoch: 0, Step: 8470, Batch(micro): 8470, Batch (considering grad accum): 1058,  Loss: 5.8880, Time: 3.74s, Token/s: 136.97
Epoch: 0, Step: 8471, Batch(micro): 8471, Batch (considering grad accum): 1058,  Loss: 5.6945, Time: 22.35s, Token/s: 22.90
Epoch: 0, Step: 8472, Batch(micro): 8472, Batch (considering grad accum): 1059,  Loss: 5.9711, Time: 8.02s, Token/s: 63.85
Epoch: 0, Step: 8473, Batch(micro): 8473, Batch (considering grad accum): 1059,  Loss: 6.4176, Time: 3.81s, Token/s: 134.22
Epoch: 0, Step: 8474, Batch(micro): 8474, Batch (considering grad accum): 1059,  Loss: 5.6074, Time: 3.64s, Token/s: 140.65
Epoch: 0, Step: 8475, Batch(micro): 8475, Batch (considering grad accum): 1059,  Loss: 5.5923, Time: 3.65s, Token/s: 140.43
Epoch: 0, Step: 8476, Batch(micro): 8476, Batch (considering grad accum): 1059,  Loss: 5.5806, Time: 3.66s, Token/s: 139.74
Epoch: 0, Step: 8477, Batch(micro): 8477, Batch (considering grad accum): 1059,  Loss: 5.3756, Time: 3.54s, Token/s: 144.58
Epoch: 0, Step: 8478, Batch(micro): 8478, Batch (considering grad accum): 1059,  Loss: 5.5390, Time: 3.21s, Token/s: 159.51
Epoch: 0, Step: 8479, Batch(micro): 8479, Batch (considering grad accum): 1059,  Loss: 6.3603, Time: 23.83s, Token/s: 21.49
Epoch: 0, Step: 8480, Batch(micro): 8480, Batch (considering grad accum): 1060,  Loss: 6.1806, Time: 6.66s, Token/s: 76.83
Epoch: 0, Step: 8481, Batch(micro): 8481, Batch (considering grad accum): 1060,  Loss: 5.3169, Time: 3.71s, Token/s: 137.99
Epoch: 0, Step: 8482, Batch(micro): 8482, Batch (considering grad accum): 1060,  Loss: 5.8817, Time: 3.24s, Token/s: 157.97
Epoch: 0, Step: 8483, Batch(micro): 8483, Batch (considering grad accum): 1060,  Loss: 5.7366, Time: 3.24s, Token/s: 157.92
Epoch: 0, Step: 8484, Batch(micro): 8484, Batch (considering grad accum): 1060,  Loss: 6.8250, Time: 3.38s, Token/s: 151.27
Epoch: 0, Step: 8485, Batch(micro): 8485, Batch (considering grad accum): 1060,  Loss: 6.8843, Time: 3.35s, Token/s: 152.89
Epoch: 0, Step: 8486, Batch(micro): 8486, Batch (considering grad accum): 1060,  Loss: 5.3647, Time: 3.52s, Token/s: 145.32
Epoch: 0, Step: 8487, Batch(micro): 8487, Batch (considering grad accum): 1060,  Loss: 5.9877, Time: 23.18s, Token/s: 22.09
Epoch: 0, Step: 8488, Batch(micro): 8488, Batch (considering grad accum): 1061,  Loss: 5.1165, Time: 7.39s, Token/s: 69.33
Epoch: 0, Step: 8489, Batch(micro): 8489, Batch (considering grad accum): 1061,  Loss: 6.7301, Time: 3.94s, Token/s: 129.81
Epoch: 0, Step: 8490, Batch(micro): 8490, Batch (considering grad accum): 1061,  Loss: 6.3232, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 8491, Batch(micro): 8491, Batch (considering grad accum): 1061,  Loss: 6.4616, Time: 3.50s, Token/s: 146.11
Epoch: 0, Step: 8492, Batch(micro): 8492, Batch (considering grad accum): 1061,  Loss: 6.8117, Time: 3.44s, Token/s: 148.65
Epoch: 0, Step: 8493, Batch(micro): 8493, Batch (considering grad accum): 1061,  Loss: 6.6018, Time: 3.49s, Token/s: 146.80
Epoch: 0, Step: 8494, Batch(micro): 8494, Batch (considering grad accum): 1061,  Loss: 6.0319, Time: 3.74s, Token/s: 137.06
Epoch: 0, Step: 8495, Batch(micro): 8495, Batch (considering grad accum): 1061,  Loss: 5.2009, Time: 24.67s, Token/s: 20.75
Epoch: 0, Step: 8496, Batch(micro): 8496, Batch (considering grad accum): 1062,  Loss: 5.9743, Time: 8.30s, Token/s: 61.67
Epoch: 0, Step: 8497, Batch(micro): 8497, Batch (considering grad accum): 1062,  Loss: 4.6974, Time: 3.37s, Token/s: 151.79
Epoch: 0, Step: 8498, Batch(micro): 8498, Batch (considering grad accum): 1062,  Loss: 5.4376, Time: 3.24s, Token/s: 157.88
Epoch: 0, Step: 8499, Batch(micro): 8499, Batch (considering grad accum): 1062,  Loss: 6.4200, Time: 3.27s, Token/s: 156.67
Updating MLP bias
Epoch: 0, Step: 8500, Batch(micro): 8500, Batch (considering grad accum): 1062,  Loss: 7.0599, Time: 3.55s, Token/s: 144.23
Epoch: 0, Step: 8501, Batch(micro): 8501, Batch (considering grad accum): 1062,  Loss: 6.4254, Time: 4.51s, Token/s: 113.47
Epoch: 0, Step: 8502, Batch(micro): 8502, Batch (considering grad accum): 1062,  Loss: 4.9532, Time: 3.46s, Token/s: 147.88
Epoch: 0, Step: 8503, Batch(micro): 8503, Batch (considering grad accum): 1062,  Loss: 5.7618, Time: 25.78s, Token/s: 19.86
Epoch: 0, Step: 8504, Batch(micro): 8504, Batch (considering grad accum): 1063,  Loss: 6.0492, Time: 7.45s, Token/s: 68.73
Epoch: 0, Step: 8505, Batch(micro): 8505, Batch (considering grad accum): 1063,  Loss: 6.6016, Time: 3.70s, Token/s: 138.50
Epoch: 0, Step: 8506, Batch(micro): 8506, Batch (considering grad accum): 1063,  Loss: 6.9939, Time: 3.44s, Token/s: 148.68
Epoch: 0, Step: 8507, Batch(micro): 8507, Batch (considering grad accum): 1063,  Loss: 6.6381, Time: 3.53s, Token/s: 145.24
Epoch: 0, Step: 8508, Batch(micro): 8508, Batch (considering grad accum): 1063,  Loss: 5.8635, Time: 3.52s, Token/s: 145.57
Epoch: 0, Step: 8509, Batch(micro): 8509, Batch (considering grad accum): 1063,  Loss: 5.8374, Time: 3.62s, Token/s: 141.25
Epoch: 0, Step: 8510, Batch(micro): 8510, Batch (considering grad accum): 1063,  Loss: 5.8431, Time: 3.47s, Token/s: 147.68
Epoch: 0, Step: 8511, Batch(micro): 8511, Batch (considering grad accum): 1063,  Loss: 6.0179, Time: 24.14s, Token/s: 21.21
Epoch: 0, Step: 8512, Batch(micro): 8512, Batch (considering grad accum): 1064,  Loss: 6.8044, Time: 5.67s, Token/s: 90.27
Epoch: 0, Step: 8513, Batch(micro): 8513, Batch (considering grad accum): 1064,  Loss: 6.8815, Time: 3.89s, Token/s: 131.72
Epoch: 0, Step: 8514, Batch(micro): 8514, Batch (considering grad accum): 1064,  Loss: 6.5790, Time: 3.21s, Token/s: 159.44
Epoch: 0, Step: 8515, Batch(micro): 8515, Batch (considering grad accum): 1064,  Loss: 6.3957, Time: 3.35s, Token/s: 152.95
Epoch: 0, Step: 8516, Batch(micro): 8516, Batch (considering grad accum): 1064,  Loss: 6.7348, Time: 3.61s, Token/s: 141.91
Epoch: 0, Step: 8517, Batch(micro): 8517, Batch (considering grad accum): 1064,  Loss: 6.3277, Time: 3.69s, Token/s: 138.93
Epoch: 0, Step: 8518, Batch(micro): 8518, Batch (considering grad accum): 1064,  Loss: 5.6280, Time: 3.45s, Token/s: 148.49
Epoch: 0, Step: 8519, Batch(micro): 8519, Batch (considering grad accum): 1064,  Loss: 5.4677, Time: 22.40s, Token/s: 22.86
Epoch: 0, Step: 8520, Batch(micro): 8520, Batch (considering grad accum): 1065,  Loss: 5.5844, Time: 6.87s, Token/s: 74.49
Epoch: 0, Step: 8521, Batch(micro): 8521, Batch (considering grad accum): 1065,  Loss: 5.7098, Time: 3.91s, Token/s: 131.06
Epoch: 0, Step: 8522, Batch(micro): 8522, Batch (considering grad accum): 1065,  Loss: 5.7072, Time: 3.99s, Token/s: 128.22
Epoch: 0, Step: 8523, Batch(micro): 8523, Batch (considering grad accum): 1065,  Loss: 6.1797, Time: 3.60s, Token/s: 142.05
Epoch: 0, Step: 8524, Batch(micro): 8524, Batch (considering grad accum): 1065,  Loss: 6.6915, Time: 3.56s, Token/s: 143.93
Epoch: 0, Step: 8525, Batch(micro): 8525, Batch (considering grad accum): 1065,  Loss: 5.9875, Time: 3.61s, Token/s: 142.02
Epoch: 0, Step: 8526, Batch(micro): 8526, Batch (considering grad accum): 1065,  Loss: 6.0894, Time: 3.63s, Token/s: 140.92
Epoch: 0, Step: 8527, Batch(micro): 8527, Batch (considering grad accum): 1065,  Loss: 5.5695, Time: 23.57s, Token/s: 21.72
Epoch: 0, Step: 8528, Batch(micro): 8528, Batch (considering grad accum): 1066,  Loss: 5.9337, Time: 7.47s, Token/s: 68.58
Epoch: 0, Step: 8529, Batch(micro): 8529, Batch (considering grad accum): 1066,  Loss: 6.7647, Time: 3.79s, Token/s: 135.16
Epoch: 0, Step: 8530, Batch(micro): 8530, Batch (considering grad accum): 1066,  Loss: 5.9419, Time: 3.24s, Token/s: 157.82
Epoch: 0, Step: 8531, Batch(micro): 8531, Batch (considering grad accum): 1066,  Loss: 6.2823, Time: 3.56s, Token/s: 143.91
Epoch: 0, Step: 8532, Batch(micro): 8532, Batch (considering grad accum): 1066,  Loss: 5.9286, Time: 3.50s, Token/s: 146.46
Epoch: 0, Step: 8533, Batch(micro): 8533, Batch (considering grad accum): 1066,  Loss: 5.6229, Time: 3.70s, Token/s: 138.32
Epoch: 0, Step: 8534, Batch(micro): 8534, Batch (considering grad accum): 1066,  Loss: 5.4835, Time: 3.29s, Token/s: 155.62
Epoch: 0, Step: 8535, Batch(micro): 8535, Batch (considering grad accum): 1066,  Loss: 5.6373, Time: 24.59s, Token/s: 20.82
Epoch: 0, Step: 8536, Batch(micro): 8536, Batch (considering grad accum): 1067,  Loss: 5.7302, Time: 9.15s, Token/s: 55.98
Epoch: 0, Step: 8537, Batch(micro): 8537, Batch (considering grad accum): 1067,  Loss: 6.4613, Time: 3.70s, Token/s: 138.40
Epoch: 0, Step: 8538, Batch(micro): 8538, Batch (considering grad accum): 1067,  Loss: 5.5981, Time: 3.22s, Token/s: 159.09
Epoch: 0, Step: 8539, Batch(micro): 8539, Batch (considering grad accum): 1067,  Loss: 5.4184, Time: 3.19s, Token/s: 160.45
Epoch: 0, Step: 8540, Batch(micro): 8540, Batch (considering grad accum): 1067,  Loss: 5.8631, Time: 3.52s, Token/s: 145.58
Epoch: 0, Step: 8541, Batch(micro): 8541, Batch (considering grad accum): 1067,  Loss: 5.9379, Time: 3.36s, Token/s: 152.16
Epoch: 0, Step: 8542, Batch(micro): 8542, Batch (considering grad accum): 1067,  Loss: 5.9206, Time: 3.34s, Token/s: 153.43
Epoch: 0, Step: 8543, Batch(micro): 8543, Batch (considering grad accum): 1067,  Loss: 5.8907, Time: 22.67s, Token/s: 22.58
Epoch: 0, Step: 8544, Batch(micro): 8544, Batch (considering grad accum): 1068,  Loss: 5.7527, Time: 7.03s, Token/s: 72.81
Epoch: 0, Step: 8545, Batch(micro): 8545, Batch (considering grad accum): 1068,  Loss: 5.4267, Time: 4.12s, Token/s: 124.33
Epoch: 0, Step: 8546, Batch(micro): 8546, Batch (considering grad accum): 1068,  Loss: 5.6359, Time: 3.56s, Token/s: 143.73
Epoch: 0, Step: 8547, Batch(micro): 8547, Batch (considering grad accum): 1068,  Loss: 5.6100, Time: 3.68s, Token/s: 139.18
Epoch: 0, Step: 8548, Batch(micro): 8548, Batch (considering grad accum): 1068,  Loss: 6.0101, Time: 3.58s, Token/s: 143.05
Epoch: 0, Step: 8549, Batch(micro): 8549, Batch (considering grad accum): 1068,  Loss: 5.7072, Time: 4.01s, Token/s: 127.69
Epoch: 0, Step: 8550, Batch(micro): 8550, Batch (considering grad accum): 1068,  Loss: 6.3114, Time: 3.45s, Token/s: 148.33
Epoch: 0, Step: 8551, Batch(micro): 8551, Batch (considering grad accum): 1068,  Loss: 5.5233, Time: 22.02s, Token/s: 23.25
Epoch: 0, Step: 8552, Batch(micro): 8552, Batch (considering grad accum): 1069,  Loss: 5.7146, Time: 6.23s, Token/s: 82.23
Epoch: 0, Step: 8553, Batch(micro): 8553, Batch (considering grad accum): 1069,  Loss: 5.9240, Time: 4.11s, Token/s: 124.70
Epoch: 0, Step: 8554, Batch(micro): 8554, Batch (considering grad accum): 1069,  Loss: 6.1994, Time: 3.62s, Token/s: 141.63
Epoch: 0, Step: 8555, Batch(micro): 8555, Batch (considering grad accum): 1069,  Loss: 6.1123, Time: 3.41s, Token/s: 150.03
Epoch: 0, Step: 8556, Batch(micro): 8556, Batch (considering grad accum): 1069,  Loss: 5.8944, Time: 3.51s, Token/s: 145.81
Epoch: 0, Step: 8557, Batch(micro): 8557, Batch (considering grad accum): 1069,  Loss: 5.6150, Time: 3.47s, Token/s: 147.55
Epoch: 0, Step: 8558, Batch(micro): 8558, Batch (considering grad accum): 1069,  Loss: 6.0747, Time: 3.50s, Token/s: 146.22
Epoch: 0, Step: 8559, Batch(micro): 8559, Batch (considering grad accum): 1069,  Loss: 5.5214, Time: 22.55s, Token/s: 22.71
Epoch: 0, Step: 8560, Batch(micro): 8560, Batch (considering grad accum): 1070,  Loss: 5.8064, Time: 7.85s, Token/s: 65.20
Epoch: 0, Step: 8561, Batch(micro): 8561, Batch (considering grad accum): 1070,  Loss: 5.5481, Time: 3.80s, Token/s: 134.73
Epoch: 0, Step: 8562, Batch(micro): 8562, Batch (considering grad accum): 1070,  Loss: 6.0332, Time: 3.59s, Token/s: 142.42
Epoch: 0, Step: 8563, Batch(micro): 8563, Batch (considering grad accum): 1070,  Loss: 6.4856, Time: 3.56s, Token/s: 143.99
Epoch: 0, Step: 8564, Batch(micro): 8564, Batch (considering grad accum): 1070,  Loss: 5.5418, Time: 3.48s, Token/s: 147.25
Epoch: 0, Step: 8565, Batch(micro): 8565, Batch (considering grad accum): 1070,  Loss: 5.7607, Time: 3.47s, Token/s: 147.52
Epoch: 0, Step: 8566, Batch(micro): 8566, Batch (considering grad accum): 1070,  Loss: 6.1916, Time: 3.69s, Token/s: 138.68
Epoch: 0, Step: 8567, Batch(micro): 8567, Batch (considering grad accum): 1070,  Loss: 5.7449, Time: 26.19s, Token/s: 19.55
Epoch: 0, Step: 8568, Batch(micro): 8568, Batch (considering grad accum): 1071,  Loss: 5.5529, Time: 6.93s, Token/s: 73.93
Epoch: 0, Step: 8569, Batch(micro): 8569, Batch (considering grad accum): 1071,  Loss: 5.8769, Time: 4.08s, Token/s: 125.43
Epoch: 0, Step: 8570, Batch(micro): 8570, Batch (considering grad accum): 1071,  Loss: 6.6368, Time: 3.70s, Token/s: 138.36
Epoch: 0, Step: 8571, Batch(micro): 8571, Batch (considering grad accum): 1071,  Loss: 5.9360, Time: 3.52s, Token/s: 145.63
Epoch: 0, Step: 8572, Batch(micro): 8572, Batch (considering grad accum): 1071,  Loss: 6.1399, Time: 3.53s, Token/s: 145.03
Epoch: 0, Step: 8573, Batch(micro): 8573, Batch (considering grad accum): 1071,  Loss: 5.7008, Time: 3.56s, Token/s: 143.72
Epoch: 0, Step: 8574, Batch(micro): 8574, Batch (considering grad accum): 1071,  Loss: 5.4116, Time: 3.68s, Token/s: 139.07
Epoch: 0, Step: 8575, Batch(micro): 8575, Batch (considering grad accum): 1071,  Loss: 6.1777, Time: 23.98s, Token/s: 21.35
Epoch: 0, Step: 8576, Batch(micro): 8576, Batch (considering grad accum): 1072,  Loss: 5.9272, Time: 6.60s, Token/s: 77.54
Epoch: 0, Step: 8577, Batch(micro): 8577, Batch (considering grad accum): 1072,  Loss: 6.1439, Time: 3.77s, Token/s: 135.69
Epoch: 0, Step: 8578, Batch(micro): 8578, Batch (considering grad accum): 1072,  Loss: 5.6013, Time: 3.35s, Token/s: 152.98
Epoch: 0, Step: 8579, Batch(micro): 8579, Batch (considering grad accum): 1072,  Loss: 5.5597, Time: 3.33s, Token/s: 153.89
Epoch: 0, Step: 8580, Batch(micro): 8580, Batch (considering grad accum): 1072,  Loss: 6.2416, Time: 3.53s, Token/s: 144.85
Epoch: 0, Step: 8581, Batch(micro): 8581, Batch (considering grad accum): 1072,  Loss: 6.4531, Time: 3.47s, Token/s: 147.46
Epoch: 0, Step: 8582, Batch(micro): 8582, Batch (considering grad accum): 1072,  Loss: 6.6379, Time: 3.50s, Token/s: 146.12
Epoch: 0, Step: 8583, Batch(micro): 8583, Batch (considering grad accum): 1072,  Loss: 6.8278, Time: 24.28s, Token/s: 21.09
Epoch: 0, Step: 8584, Batch(micro): 8584, Batch (considering grad accum): 1073,  Loss: 6.4836, Time: 6.54s, Token/s: 78.25
Epoch: 0, Step: 8585, Batch(micro): 8585, Batch (considering grad accum): 1073,  Loss: 6.2278, Time: 3.80s, Token/s: 134.66
Epoch: 0, Step: 8586, Batch(micro): 8586, Batch (considering grad accum): 1073,  Loss: 7.4579, Time: 3.66s, Token/s: 139.84
Epoch: 0, Step: 8587, Batch(micro): 8587, Batch (considering grad accum): 1073,  Loss: 6.6861, Time: 3.67s, Token/s: 139.45
Epoch: 0, Step: 8588, Batch(micro): 8588, Batch (considering grad accum): 1073,  Loss: 6.5906, Time: 3.32s, Token/s: 154.14
Epoch: 0, Step: 8589, Batch(micro): 8589, Batch (considering grad accum): 1073,  Loss: 6.3256, Time: 3.48s, Token/s: 146.99
Epoch: 0, Step: 8590, Batch(micro): 8590, Batch (considering grad accum): 1073,  Loss: 6.5302, Time: 3.66s, Token/s: 140.08
Epoch: 0, Step: 8591, Batch(micro): 8591, Batch (considering grad accum): 1073,  Loss: 6.0653, Time: 22.91s, Token/s: 22.35
Epoch: 0, Step: 8592, Batch(micro): 8592, Batch (considering grad accum): 1074,  Loss: 5.7480, Time: 7.52s, Token/s: 68.08
Epoch: 0, Step: 8593, Batch(micro): 8593, Batch (considering grad accum): 1074,  Loss: 5.8489, Time: 3.71s, Token/s: 138.01
Epoch: 0, Step: 8594, Batch(micro): 8594, Batch (considering grad accum): 1074,  Loss: 5.4079, Time: 3.26s, Token/s: 156.96
Epoch: 0, Step: 8595, Batch(micro): 8595, Batch (considering grad accum): 1074,  Loss: 5.4423, Time: 3.18s, Token/s: 161.11
Epoch: 0, Step: 8596, Batch(micro): 8596, Batch (considering grad accum): 1074,  Loss: 5.8656, Time: 3.54s, Token/s: 144.59
Epoch: 0, Step: 8597, Batch(micro): 8597, Batch (considering grad accum): 1074,  Loss: 6.1694, Time: 3.84s, Token/s: 133.19
Epoch: 0, Step: 8598, Batch(micro): 8598, Batch (considering grad accum): 1074,  Loss: 6.2004, Time: 3.39s, Token/s: 150.83
Epoch: 0, Step: 8599, Batch(micro): 8599, Batch (considering grad accum): 1074,  Loss: 5.8351, Time: 23.47s, Token/s: 21.82
Updating MLP bias
Epoch: 0, Step: 8600, Batch(micro): 8600, Batch (considering grad accum): 1075,  Loss: 5.7092, Time: 9.43s, Token/s: 54.32
Epoch: 0, Step: 8601, Batch(micro): 8601, Batch (considering grad accum): 1075,  Loss: 6.1024, Time: 3.52s, Token/s: 145.33
Epoch: 0, Step: 8602, Batch(micro): 8602, Batch (considering grad accum): 1075,  Loss: 5.8188, Time: 3.40s, Token/s: 150.49
Epoch: 0, Step: 8603, Batch(micro): 8603, Batch (considering grad accum): 1075,  Loss: 5.8043, Time: 3.37s, Token/s: 152.15
Epoch: 0, Step: 8604, Batch(micro): 8604, Batch (considering grad accum): 1075,  Loss: 6.4405, Time: 3.39s, Token/s: 150.82
Epoch: 0, Step: 8605, Batch(micro): 8605, Batch (considering grad accum): 1075,  Loss: 6.0550, Time: 3.47s, Token/s: 147.74
Epoch: 0, Step: 8606, Batch(micro): 8606, Batch (considering grad accum): 1075,  Loss: 6.1806, Time: 3.51s, Token/s: 146.08
Epoch: 0, Step: 8607, Batch(micro): 8607, Batch (considering grad accum): 1075,  Loss: 6.1599, Time: 23.85s, Token/s: 21.47
Epoch: 0, Step: 8608, Batch(micro): 8608, Batch (considering grad accum): 1076,  Loss: 6.0127, Time: 6.72s, Token/s: 76.16
Epoch: 0, Step: 8609, Batch(micro): 8609, Batch (considering grad accum): 1076,  Loss: 6.2980, Time: 4.02s, Token/s: 127.24
Epoch: 0, Step: 8610, Batch(micro): 8610, Batch (considering grad accum): 1076,  Loss: 5.6669, Time: 3.59s, Token/s: 142.48
Epoch: 0, Step: 8611, Batch(micro): 8611, Batch (considering grad accum): 1076,  Loss: 6.1751, Time: 3.53s, Token/s: 144.85
Epoch: 0, Step: 8612, Batch(micro): 8612, Batch (considering grad accum): 1076,  Loss: 6.0877, Time: 3.54s, Token/s: 144.48
Epoch: 0, Step: 8613, Batch(micro): 8613, Batch (considering grad accum): 1076,  Loss: 7.4700, Time: 3.50s, Token/s: 146.30
Epoch: 0, Step: 8614, Batch(micro): 8614, Batch (considering grad accum): 1076,  Loss: 6.1025, Time: 3.45s, Token/s: 148.25
Epoch: 0, Step: 8615, Batch(micro): 8615, Batch (considering grad accum): 1076,  Loss: 5.7896, Time: 18.34s, Token/s: 27.92
Epoch: 0, Step: 8616, Batch(micro): 8616, Batch (considering grad accum): 1077,  Loss: 6.1519, Time: 6.74s, Token/s: 75.97
Epoch: 0, Step: 8617, Batch(micro): 8617, Batch (considering grad accum): 1077,  Loss: 5.9661, Time: 3.98s, Token/s: 128.58
Epoch: 0, Step: 8618, Batch(micro): 8618, Batch (considering grad accum): 1077,  Loss: 6.3509, Time: 4.28s, Token/s: 119.63
Epoch: 0, Step: 8619, Batch(micro): 8619, Batch (considering grad accum): 1077,  Loss: 6.0878, Time: 3.89s, Token/s: 131.54
Epoch: 0, Step: 8620, Batch(micro): 8620, Batch (considering grad accum): 1077,  Loss: 5.7937, Time: 4.12s, Token/s: 124.33
Epoch: 0, Step: 8621, Batch(micro): 8621, Batch (considering grad accum): 1077,  Loss: 5.6846, Time: 3.73s, Token/s: 137.09
Epoch: 0, Step: 8622, Batch(micro): 8622, Batch (considering grad accum): 1077,  Loss: 6.4970, Time: 3.36s, Token/s: 152.49
Epoch: 0, Step: 8623, Batch(micro): 8623, Batch (considering grad accum): 1077,  Loss: 5.3598, Time: 18.90s, Token/s: 27.10
Epoch: 0, Step: 8624, Batch(micro): 8624, Batch (considering grad accum): 1078,  Loss: 6.1749, Time: 6.47s, Token/s: 79.14
Epoch: 0, Step: 8625, Batch(micro): 8625, Batch (considering grad accum): 1078,  Loss: 6.1181, Time: 3.75s, Token/s: 136.50
Epoch: 0, Step: 8626, Batch(micro): 8626, Batch (considering grad accum): 1078,  Loss: 6.0163, Time: 3.20s, Token/s: 159.98
Epoch: 0, Step: 8627, Batch(micro): 8627, Batch (considering grad accum): 1078,  Loss: 6.0555, Time: 3.36s, Token/s: 152.24
Epoch: 0, Step: 8628, Batch(micro): 8628, Batch (considering grad accum): 1078,  Loss: 5.6438, Time: 3.42s, Token/s: 149.92
Epoch: 0, Step: 8629, Batch(micro): 8629, Batch (considering grad accum): 1078,  Loss: 5.9494, Time: 3.55s, Token/s: 144.28
Epoch: 0, Step: 8630, Batch(micro): 8630, Batch (considering grad accum): 1078,  Loss: 6.0251, Time: 3.36s, Token/s: 152.45
Epoch: 0, Step: 8631, Batch(micro): 8631, Batch (considering grad accum): 1078,  Loss: 6.0725, Time: 18.85s, Token/s: 27.17
Epoch: 0, Step: 8632, Batch(micro): 8632, Batch (considering grad accum): 1079,  Loss: 5.4260, Time: 5.70s, Token/s: 89.85
Epoch: 0, Step: 8633, Batch(micro): 8633, Batch (considering grad accum): 1079,  Loss: 6.2680, Time: 4.21s, Token/s: 121.57
Epoch: 0, Step: 8634, Batch(micro): 8634, Batch (considering grad accum): 1079,  Loss: 7.4012, Time: 3.55s, Token/s: 144.10
Epoch: 0, Step: 8635, Batch(micro): 8635, Batch (considering grad accum): 1079,  Loss: 6.6212, Time: 3.40s, Token/s: 150.72
Epoch: 0, Step: 8636, Batch(micro): 8636, Batch (considering grad accum): 1079,  Loss: 5.9863, Time: 3.32s, Token/s: 154.38
Epoch: 0, Step: 8637, Batch(micro): 8637, Batch (considering grad accum): 1079,  Loss: 7.4446, Time: 3.46s, Token/s: 148.07
Epoch: 0, Step: 8638, Batch(micro): 8638, Batch (considering grad accum): 1079,  Loss: 7.1691, Time: 3.67s, Token/s: 139.34
Epoch: 0, Step: 8639, Batch(micro): 8639, Batch (considering grad accum): 1079,  Loss: 6.1685, Time: 19.06s, Token/s: 26.86
Epoch: 0, Step: 8640, Batch(micro): 8640, Batch (considering grad accum): 1080,  Loss: 5.6572, Time: 5.82s, Token/s: 87.91
Epoch: 0, Step: 8641, Batch(micro): 8641, Batch (considering grad accum): 1080,  Loss: 5.6130, Time: 3.58s, Token/s: 142.98
Epoch: 0, Step: 8642, Batch(micro): 8642, Batch (considering grad accum): 1080,  Loss: 5.3947, Time: 3.20s, Token/s: 160.10
Epoch: 0, Step: 8643, Batch(micro): 8643, Batch (considering grad accum): 1080,  Loss: 6.1571, Time: 3.26s, Token/s: 157.28
Epoch: 0, Step: 8644, Batch(micro): 8644, Batch (considering grad accum): 1080,  Loss: 6.2882, Time: 3.24s, Token/s: 158.18
Epoch: 0, Step: 8645, Batch(micro): 8645, Batch (considering grad accum): 1080,  Loss: 5.4119, Time: 3.34s, Token/s: 153.31
Epoch: 0, Step: 8646, Batch(micro): 8646, Batch (considering grad accum): 1080,  Loss: 6.0489, Time: 3.46s, Token/s: 148.10
Epoch: 0, Step: 8647, Batch(micro): 8647, Batch (considering grad accum): 1080,  Loss: 6.4177, Time: 18.81s, Token/s: 27.21
Epoch: 0, Step: 8648, Batch(micro): 8648, Batch (considering grad accum): 1081,  Loss: 6.5042, Time: 7.06s, Token/s: 72.54
Epoch: 0, Step: 8649, Batch(micro): 8649, Batch (considering grad accum): 1081,  Loss: 6.2178, Time: 4.02s, Token/s: 127.31
Epoch: 0, Step: 8650, Batch(micro): 8650, Batch (considering grad accum): 1081,  Loss: 6.1748, Time: 3.72s, Token/s: 137.70
Epoch: 0, Step: 8651, Batch(micro): 8651, Batch (considering grad accum): 1081,  Loss: 6.0052, Time: 3.72s, Token/s: 137.52
Epoch: 0, Step: 8652, Batch(micro): 8652, Batch (considering grad accum): 1081,  Loss: 6.2389, Time: 3.58s, Token/s: 142.82
Epoch: 0, Step: 8653, Batch(micro): 8653, Batch (considering grad accum): 1081,  Loss: 5.4486, Time: 3.51s, Token/s: 145.96
Epoch: 0, Step: 8654, Batch(micro): 8654, Batch (considering grad accum): 1081,  Loss: 6.0634, Time: 3.19s, Token/s: 160.41
Epoch: 0, Step: 8655, Batch(micro): 8655, Batch (considering grad accum): 1081,  Loss: 5.5564, Time: 18.16s, Token/s: 28.20
Epoch: 0, Step: 8656, Batch(micro): 8656, Batch (considering grad accum): 1082,  Loss: 5.6978, Time: 6.97s, Token/s: 73.46
Epoch: 0, Step: 8657, Batch(micro): 8657, Batch (considering grad accum): 1082,  Loss: 6.5675, Time: 3.71s, Token/s: 138.01
Epoch: 0, Step: 8658, Batch(micro): 8658, Batch (considering grad accum): 1082,  Loss: 5.9398, Time: 3.34s, Token/s: 153.36
Epoch: 0, Step: 8659, Batch(micro): 8659, Batch (considering grad accum): 1082,  Loss: 6.3225, Time: 3.32s, Token/s: 154.35
Epoch: 0, Step: 8660, Batch(micro): 8660, Batch (considering grad accum): 1082,  Loss: 5.5835, Time: 3.78s, Token/s: 135.28
Epoch: 0, Step: 8661, Batch(micro): 8661, Batch (considering grad accum): 1082,  Loss: 6.3674, Time: 3.51s, Token/s: 145.77
Epoch: 0, Step: 8662, Batch(micro): 8662, Batch (considering grad accum): 1082,  Loss: 6.4083, Time: 3.46s, Token/s: 147.86
Epoch: 0, Step: 8663, Batch(micro): 8663, Batch (considering grad accum): 1082,  Loss: 7.1920, Time: 17.87s, Token/s: 28.65
Epoch: 0, Step: 8664, Batch(micro): 8664, Batch (considering grad accum): 1083,  Loss: 6.3941, Time: 6.35s, Token/s: 80.59
Epoch: 0, Step: 8665, Batch(micro): 8665, Batch (considering grad accum): 1083,  Loss: 5.8785, Time: 3.89s, Token/s: 131.70
Epoch: 0, Step: 8666, Batch(micro): 8666, Batch (considering grad accum): 1083,  Loss: 5.7952, Time: 3.99s, Token/s: 128.38
Epoch: 0, Step: 8667, Batch(micro): 8667, Batch (considering grad accum): 1083,  Loss: 6.6861, Time: 3.63s, Token/s: 140.86
Epoch: 0, Step: 8668, Batch(micro): 8668, Batch (considering grad accum): 1083,  Loss: 6.1115, Time: 3.45s, Token/s: 148.39
Epoch: 0, Step: 8669, Batch(micro): 8669, Batch (considering grad accum): 1083,  Loss: 6.0320, Time: 3.48s, Token/s: 147.07
Epoch: 0, Step: 8670, Batch(micro): 8670, Batch (considering grad accum): 1083,  Loss: 5.7793, Time: 3.51s, Token/s: 145.80
Epoch: 0, Step: 8671, Batch(micro): 8671, Batch (considering grad accum): 1083,  Loss: 6.1334, Time: 18.16s, Token/s: 28.19
Epoch: 0, Step: 8672, Batch(micro): 8672, Batch (considering grad accum): 1084,  Loss: 5.8087, Time: 6.29s, Token/s: 81.46
Epoch: 0, Step: 8673, Batch(micro): 8673, Batch (considering grad accum): 1084,  Loss: 6.7563, Time: 3.81s, Token/s: 134.29
Epoch: 0, Step: 8674, Batch(micro): 8674, Batch (considering grad accum): 1084,  Loss: 5.8726, Time: 3.23s, Token/s: 158.56
Epoch: 0, Step: 8675, Batch(micro): 8675, Batch (considering grad accum): 1084,  Loss: 6.2107, Time: 3.15s, Token/s: 162.41
Epoch: 0, Step: 8676, Batch(micro): 8676, Batch (considering grad accum): 1084,  Loss: 5.6165, Time: 3.23s, Token/s: 158.42
Epoch: 0, Step: 8677, Batch(micro): 8677, Batch (considering grad accum): 1084,  Loss: 6.3018, Time: 3.30s, Token/s: 155.25
Epoch: 0, Step: 8678, Batch(micro): 8678, Batch (considering grad accum): 1084,  Loss: 5.9260, Time: 3.43s, Token/s: 149.25
Epoch: 0, Step: 8679, Batch(micro): 8679, Batch (considering grad accum): 1084,  Loss: 6.2436, Time: 18.99s, Token/s: 26.96
Epoch: 0, Step: 8680, Batch(micro): 8680, Batch (considering grad accum): 1085,  Loss: 5.6642, Time: 6.59s, Token/s: 77.71
Epoch: 0, Step: 8681, Batch(micro): 8681, Batch (considering grad accum): 1085,  Loss: 5.8004, Time: 3.71s, Token/s: 137.90
Epoch: 0, Step: 8682, Batch(micro): 8682, Batch (considering grad accum): 1085,  Loss: 6.1403, Time: 3.16s, Token/s: 161.89
Epoch: 0, Step: 8683, Batch(micro): 8683, Batch (considering grad accum): 1085,  Loss: 5.8873, Time: 3.29s, Token/s: 155.50
Epoch: 0, Step: 8684, Batch(micro): 8684, Batch (considering grad accum): 1085,  Loss: 5.0794, Time: 3.29s, Token/s: 155.46
Epoch: 0, Step: 8685, Batch(micro): 8685, Batch (considering grad accum): 1085,  Loss: 5.3869, Time: 3.88s, Token/s: 132.07
Epoch: 0, Step: 8686, Batch(micro): 8686, Batch (considering grad accum): 1085,  Loss: 6.1821, Time: 3.45s, Token/s: 148.19
Epoch: 0, Step: 8687, Batch(micro): 8687, Batch (considering grad accum): 1085,  Loss: 6.5430, Time: 19.93s, Token/s: 25.70
Epoch: 0, Step: 8688, Batch(micro): 8688, Batch (considering grad accum): 1086,  Loss: 5.8566, Time: 6.89s, Token/s: 74.35
Epoch: 0, Step: 8689, Batch(micro): 8689, Batch (considering grad accum): 1086,  Loss: 6.0450, Time: 3.68s, Token/s: 139.05
Epoch: 0, Step: 8690, Batch(micro): 8690, Batch (considering grad accum): 1086,  Loss: 5.4330, Time: 3.35s, Token/s: 152.96
Epoch: 0, Step: 8691, Batch(micro): 8691, Batch (considering grad accum): 1086,  Loss: 5.7855, Time: 3.34s, Token/s: 153.32
Epoch: 0, Step: 8692, Batch(micro): 8692, Batch (considering grad accum): 1086,  Loss: 5.1156, Time: 3.48s, Token/s: 147.14
Epoch: 0, Step: 8693, Batch(micro): 8693, Batch (considering grad accum): 1086,  Loss: 5.9424, Time: 3.47s, Token/s: 147.74
Epoch: 0, Step: 8694, Batch(micro): 8694, Batch (considering grad accum): 1086,  Loss: 6.0160, Time: 3.58s, Token/s: 142.93
Epoch: 0, Step: 8695, Batch(micro): 8695, Batch (considering grad accum): 1086,  Loss: 6.1926, Time: 18.64s, Token/s: 27.46
Epoch: 0, Step: 8696, Batch(micro): 8696, Batch (considering grad accum): 1087,  Loss: 6.1663, Time: 5.68s, Token/s: 90.07
Epoch: 0, Step: 8697, Batch(micro): 8697, Batch (considering grad accum): 1087,  Loss: 6.5815, Time: 3.59s, Token/s: 142.59
Epoch: 0, Step: 8698, Batch(micro): 8698, Batch (considering grad accum): 1087,  Loss: 5.2601, Time: 3.25s, Token/s: 157.38
Epoch: 0, Step: 8699, Batch(micro): 8699, Batch (considering grad accum): 1087,  Loss: 6.5093, Time: 3.34s, Token/s: 153.23
Updating MLP bias
Epoch: 0, Step: 8700, Batch(micro): 8700, Batch (considering grad accum): 1087,  Loss: 6.6012, Time: 3.52s, Token/s: 145.33
Epoch: 0, Step: 8701, Batch(micro): 8701, Batch (considering grad accum): 1087,  Loss: 5.9670, Time: 3.39s, Token/s: 150.97
Epoch: 0, Step: 8702, Batch(micro): 8702, Batch (considering grad accum): 1087,  Loss: 5.2458, Time: 3.61s, Token/s: 141.99
Epoch: 0, Step: 8703, Batch(micro): 8703, Batch (considering grad accum): 1087,  Loss: 5.7134, Time: 18.04s, Token/s: 28.38
Epoch: 0, Step: 8704, Batch(micro): 8704, Batch (considering grad accum): 1088,  Loss: 6.1040, Time: 6.36s, Token/s: 80.55
Epoch: 0, Step: 8705, Batch(micro): 8705, Batch (considering grad accum): 1088,  Loss: 5.4686, Time: 3.76s, Token/s: 136.05
Epoch: 0, Step: 8706, Batch(micro): 8706, Batch (considering grad accum): 1088,  Loss: 5.8153, Time: 3.17s, Token/s: 161.58
Epoch: 0, Step: 8707, Batch(micro): 8707, Batch (considering grad accum): 1088,  Loss: 5.8748, Time: 3.21s, Token/s: 159.60
Epoch: 0, Step: 8708, Batch(micro): 8708, Batch (considering grad accum): 1088,  Loss: 5.6195, Time: 3.18s, Token/s: 161.22
Epoch: 0, Step: 8709, Batch(micro): 8709, Batch (considering grad accum): 1088,  Loss: 5.1085, Time: 3.30s, Token/s: 155.04
Epoch: 0, Step: 8710, Batch(micro): 8710, Batch (considering grad accum): 1088,  Loss: 5.2451, Time: 3.23s, Token/s: 158.62
Epoch: 0, Step: 8711, Batch(micro): 8711, Batch (considering grad accum): 1088,  Loss: 6.5520, Time: 18.67s, Token/s: 27.43
Epoch: 0, Step: 8712, Batch(micro): 8712, Batch (considering grad accum): 1089,  Loss: 6.6471, Time: 7.40s, Token/s: 69.19
Epoch: 0, Step: 8713, Batch(micro): 8713, Batch (considering grad accum): 1089,  Loss: 6.3098, Time: 4.08s, Token/s: 125.51
Epoch: 0, Step: 8714, Batch(micro): 8714, Batch (considering grad accum): 1089,  Loss: 5.6667, Time: 3.68s, Token/s: 139.22
Epoch: 0, Step: 8715, Batch(micro): 8715, Batch (considering grad accum): 1089,  Loss: 5.7471, Time: 3.56s, Token/s: 143.81
Epoch: 0, Step: 8716, Batch(micro): 8716, Batch (considering grad accum): 1089,  Loss: 5.9114, Time: 3.59s, Token/s: 142.59
Epoch: 0, Step: 8717, Batch(micro): 8717, Batch (considering grad accum): 1089,  Loss: 5.9001, Time: 3.37s, Token/s: 151.92
Epoch: 0, Step: 8718, Batch(micro): 8718, Batch (considering grad accum): 1089,  Loss: 6.1672, Time: 3.69s, Token/s: 138.87
Epoch: 0, Step: 8719, Batch(micro): 8719, Batch (considering grad accum): 1089,  Loss: 6.1542, Time: 25.00s, Token/s: 20.48
Epoch: 0, Step: 8720, Batch(micro): 8720, Batch (considering grad accum): 1090,  Loss: 6.0792, Time: 7.04s, Token/s: 72.71
Epoch: 0, Step: 8721, Batch(micro): 8721, Batch (considering grad accum): 1090,  Loss: 5.9983, Time: 4.03s, Token/s: 127.12
Epoch: 0, Step: 8722, Batch(micro): 8722, Batch (considering grad accum): 1090,  Loss: 5.9631, Time: 4.00s, Token/s: 128.09
Epoch: 0, Step: 8723, Batch(micro): 8723, Batch (considering grad accum): 1090,  Loss: 6.3524, Time: 3.54s, Token/s: 144.75
Epoch: 0, Step: 8724, Batch(micro): 8724, Batch (considering grad accum): 1090,  Loss: 6.3076, Time: 3.58s, Token/s: 143.03
Epoch: 0, Step: 8725, Batch(micro): 8725, Batch (considering grad accum): 1090,  Loss: 5.9949, Time: 3.48s, Token/s: 146.92
Epoch: 0, Step: 8726, Batch(micro): 8726, Batch (considering grad accum): 1090,  Loss: 5.8023, Time: 3.47s, Token/s: 147.49
Epoch: 0, Step: 8727, Batch(micro): 8727, Batch (considering grad accum): 1090,  Loss: 6.6331, Time: 25.31s, Token/s: 20.23
Epoch: 0, Step: 8728, Batch(micro): 8728, Batch (considering grad accum): 1091,  Loss: 5.1041, Time: 7.39s, Token/s: 69.24
Epoch: 0, Step: 8729, Batch(micro): 8729, Batch (considering grad accum): 1091,  Loss: 5.9888, Time: 4.01s, Token/s: 127.52
Epoch: 0, Step: 8730, Batch(micro): 8730, Batch (considering grad accum): 1091,  Loss: 6.2143, Time: 4.47s, Token/s: 114.65
Epoch: 0, Step: 8731, Batch(micro): 8731, Batch (considering grad accum): 1091,  Loss: 6.3232, Time: 3.42s, Token/s: 149.76
Epoch: 0, Step: 8732, Batch(micro): 8732, Batch (considering grad accum): 1091,  Loss: 6.4190, Time: 3.47s, Token/s: 147.53
Epoch: 0, Step: 8733, Batch(micro): 8733, Batch (considering grad accum): 1091,  Loss: 6.1500, Time: 3.31s, Token/s: 154.87
Epoch: 0, Step: 8734, Batch(micro): 8734, Batch (considering grad accum): 1091,  Loss: 6.2307, Time: 3.35s, Token/s: 153.00
Epoch: 0, Step: 8735, Batch(micro): 8735, Batch (considering grad accum): 1091,  Loss: 5.9388, Time: 23.25s, Token/s: 22.03
Epoch: 0, Step: 8736, Batch(micro): 8736, Batch (considering grad accum): 1092,  Loss: 5.8292, Time: 8.12s, Token/s: 63.05
Epoch: 0, Step: 8737, Batch(micro): 8737, Batch (considering grad accum): 1092,  Loss: 5.4369, Time: 3.96s, Token/s: 129.28
Epoch: 0, Step: 8738, Batch(micro): 8738, Batch (considering grad accum): 1092,  Loss: 6.2401, Time: 4.06s, Token/s: 126.17
Epoch: 0, Step: 8739, Batch(micro): 8739, Batch (considering grad accum): 1092,  Loss: 5.7570, Time: 3.43s, Token/s: 149.12
Epoch: 0, Step: 8740, Batch(micro): 8740, Batch (considering grad accum): 1092,  Loss: 5.3655, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 8741, Batch(micro): 8741, Batch (considering grad accum): 1092,  Loss: 5.9173, Time: 3.30s, Token/s: 155.34
Epoch: 0, Step: 8742, Batch(micro): 8742, Batch (considering grad accum): 1092,  Loss: 6.6650, Time: 3.35s, Token/s: 152.97
Epoch: 0, Step: 8743, Batch(micro): 8743, Batch (considering grad accum): 1092,  Loss: 6.0192, Time: 23.00s, Token/s: 22.26
Epoch: 0, Step: 8744, Batch(micro): 8744, Batch (considering grad accum): 1093,  Loss: 5.7824, Time: 6.88s, Token/s: 74.41
Epoch: 0, Step: 8745, Batch(micro): 8745, Batch (considering grad accum): 1093,  Loss: 6.6238, Time: 3.83s, Token/s: 133.62
Epoch: 0, Step: 8746, Batch(micro): 8746, Batch (considering grad accum): 1093,  Loss: 6.9098, Time: 3.61s, Token/s: 141.93
Epoch: 0, Step: 8747, Batch(micro): 8747, Batch (considering grad accum): 1093,  Loss: 6.1673, Time: 3.52s, Token/s: 145.28
Epoch: 0, Step: 8748, Batch(micro): 8748, Batch (considering grad accum): 1093,  Loss: 6.3625, Time: 3.63s, Token/s: 141.23
Epoch: 0, Step: 8749, Batch(micro): 8749, Batch (considering grad accum): 1093,  Loss: 5.6731, Time: 3.55s, Token/s: 144.15
Epoch: 0, Step: 8750, Batch(micro): 8750, Batch (considering grad accum): 1093,  Loss: 5.4526, Time: 4.51s, Token/s: 113.65
Epoch: 0, Step: 8751, Batch(micro): 8751, Batch (considering grad accum): 1093,  Loss: 6.0440, Time: 25.58s, Token/s: 20.01
Epoch: 0, Step: 8752, Batch(micro): 8752, Batch (considering grad accum): 1094,  Loss: 5.5279, Time: 9.49s, Token/s: 53.93
Epoch: 0, Step: 8753, Batch(micro): 8753, Batch (considering grad accum): 1094,  Loss: 7.5326, Time: 3.94s, Token/s: 129.85
Epoch: 0, Step: 8754, Batch(micro): 8754, Batch (considering grad accum): 1094,  Loss: 7.2820, Time: 3.48s, Token/s: 147.03
Epoch: 0, Step: 8755, Batch(micro): 8755, Batch (considering grad accum): 1094,  Loss: 6.0942, Time: 3.39s, Token/s: 150.87
Epoch: 0, Step: 8756, Batch(micro): 8756, Batch (considering grad accum): 1094,  Loss: 5.9318, Time: 3.35s, Token/s: 152.96
Epoch: 0, Step: 8757, Batch(micro): 8757, Batch (considering grad accum): 1094,  Loss: 6.0127, Time: 3.34s, Token/s: 153.21
Epoch: 0, Step: 8758, Batch(micro): 8758, Batch (considering grad accum): 1094,  Loss: 5.6761, Time: 3.73s, Token/s: 137.38
Epoch: 0, Step: 8759, Batch(micro): 8759, Batch (considering grad accum): 1094,  Loss: 5.8169, Time: 22.58s, Token/s: 22.67
Epoch: 0, Step: 8760, Batch(micro): 8760, Batch (considering grad accum): 1095,  Loss: 6.0097, Time: 6.48s, Token/s: 78.98
Epoch: 0, Step: 8761, Batch(micro): 8761, Batch (considering grad accum): 1095,  Loss: 6.4393, Time: 3.82s, Token/s: 133.98
Epoch: 0, Step: 8762, Batch(micro): 8762, Batch (considering grad accum): 1095,  Loss: 5.9522, Time: 3.73s, Token/s: 137.17
Epoch: 0, Step: 8763, Batch(micro): 8763, Batch (considering grad accum): 1095,  Loss: 6.2944, Time: 3.44s, Token/s: 148.81
Epoch: 0, Step: 8764, Batch(micro): 8764, Batch (considering grad accum): 1095,  Loss: 6.3050, Time: 3.32s, Token/s: 154.09
Epoch: 0, Step: 8765, Batch(micro): 8765, Batch (considering grad accum): 1095,  Loss: 5.3407, Time: 3.34s, Token/s: 153.22
Epoch: 0, Step: 8766, Batch(micro): 8766, Batch (considering grad accum): 1095,  Loss: 5.5866, Time: 3.29s, Token/s: 155.81
Epoch: 0, Step: 8767, Batch(micro): 8767, Batch (considering grad accum): 1095,  Loss: 5.3112, Time: 23.72s, Token/s: 21.58
Epoch: 0, Step: 8768, Batch(micro): 8768, Batch (considering grad accum): 1096,  Loss: 6.2504, Time: 6.99s, Token/s: 73.20
Epoch: 0, Step: 8769, Batch(micro): 8769, Batch (considering grad accum): 1096,  Loss: 6.6004, Time: 3.80s, Token/s: 134.59
Epoch: 0, Step: 8770, Batch(micro): 8770, Batch (considering grad accum): 1096,  Loss: 5.5631, Time: 3.49s, Token/s: 146.68
Epoch: 0, Step: 8771, Batch(micro): 8771, Batch (considering grad accum): 1096,  Loss: 5.5463, Time: 3.25s, Token/s: 157.67
Epoch: 0, Step: 8772, Batch(micro): 8772, Batch (considering grad accum): 1096,  Loss: 6.0155, Time: 3.32s, Token/s: 154.42
Epoch: 0, Step: 8773, Batch(micro): 8773, Batch (considering grad accum): 1096,  Loss: 6.0910, Time: 3.40s, Token/s: 150.78
Epoch: 0, Step: 8774, Batch(micro): 8774, Batch (considering grad accum): 1096,  Loss: 6.4706, Time: 3.37s, Token/s: 151.82
Epoch: 0, Step: 8775, Batch(micro): 8775, Batch (considering grad accum): 1096,  Loss: 5.5261, Time: 20.54s, Token/s: 24.93
Epoch: 0, Step: 8776, Batch(micro): 8776, Batch (considering grad accum): 1097,  Loss: 5.9742, Time: 7.19s, Token/s: 71.21
Epoch: 0, Step: 8777, Batch(micro): 8777, Batch (considering grad accum): 1097,  Loss: 5.1172, Time: 3.66s, Token/s: 139.77
Epoch: 0, Step: 8778, Batch(micro): 8778, Batch (considering grad accum): 1097,  Loss: 5.8472, Time: 3.19s, Token/s: 160.61
Epoch: 0, Step: 8779, Batch(micro): 8779, Batch (considering grad accum): 1097,  Loss: 6.2457, Time: 3.19s, Token/s: 160.57
Epoch: 0, Step: 8780, Batch(micro): 8780, Batch (considering grad accum): 1097,  Loss: 6.0676, Time: 3.35s, Token/s: 152.92
Epoch: 0, Step: 8781, Batch(micro): 8781, Batch (considering grad accum): 1097,  Loss: 6.9148, Time: 3.61s, Token/s: 141.78
Epoch: 0, Step: 8782, Batch(micro): 8782, Batch (considering grad accum): 1097,  Loss: 6.6445, Time: 3.32s, Token/s: 154.17
Epoch: 0, Step: 8783, Batch(micro): 8783, Batch (considering grad accum): 1097,  Loss: 6.2298, Time: 22.55s, Token/s: 22.71
Epoch: 0, Step: 8784, Batch(micro): 8784, Batch (considering grad accum): 1098,  Loss: 6.5834, Time: 7.34s, Token/s: 69.73
Epoch: 0, Step: 8785, Batch(micro): 8785, Batch (considering grad accum): 1098,  Loss: 5.9544, Time: 4.33s, Token/s: 118.20
Epoch: 0, Step: 8786, Batch(micro): 8786, Batch (considering grad accum): 1098,  Loss: 6.7753, Time: 3.56s, Token/s: 143.67
Epoch: 0, Step: 8787, Batch(micro): 8787, Batch (considering grad accum): 1098,  Loss: 5.9424, Time: 3.59s, Token/s: 142.44
Epoch: 0, Step: 8788, Batch(micro): 8788, Batch (considering grad accum): 1098,  Loss: 5.2831, Time: 3.62s, Token/s: 141.42
Epoch: 0, Step: 8789, Batch(micro): 8789, Batch (considering grad accum): 1098,  Loss: 4.8467, Time: 3.67s, Token/s: 139.51
Epoch: 0, Step: 8790, Batch(micro): 8790, Batch (considering grad accum): 1098,  Loss: 6.0229, Time: 3.77s, Token/s: 135.70
Epoch: 0, Step: 8791, Batch(micro): 8791, Batch (considering grad accum): 1098,  Loss: 6.3903, Time: 22.51s, Token/s: 22.74
Epoch: 0, Step: 8792, Batch(micro): 8792, Batch (considering grad accum): 1099,  Loss: 5.7989, Time: 6.71s, Token/s: 76.35
Epoch: 0, Step: 8793, Batch(micro): 8793, Batch (considering grad accum): 1099,  Loss: 5.9184, Time: 3.81s, Token/s: 134.49
Epoch: 0, Step: 8794, Batch(micro): 8794, Batch (considering grad accum): 1099,  Loss: 5.5320, Time: 3.36s, Token/s: 152.36
Epoch: 0, Step: 8795, Batch(micro): 8795, Batch (considering grad accum): 1099,  Loss: 6.0831, Time: 3.53s, Token/s: 145.10
Epoch: 0, Step: 8796, Batch(micro): 8796, Batch (considering grad accum): 1099,  Loss: 6.3961, Time: 3.65s, Token/s: 140.31
Epoch: 0, Step: 8797, Batch(micro): 8797, Batch (considering grad accum): 1099,  Loss: 6.1419, Time: 3.43s, Token/s: 149.35
Epoch: 0, Step: 8798, Batch(micro): 8798, Batch (considering grad accum): 1099,  Loss: 6.2975, Time: 3.55s, Token/s: 144.02
Epoch: 0, Step: 8799, Batch(micro): 8799, Batch (considering grad accum): 1099,  Loss: 6.0579, Time: 25.10s, Token/s: 20.40
Updating MLP bias
Epoch: 0, Step: 8800, Batch(micro): 8800, Batch (considering grad accum): 1100,  Loss: 5.8375, Time: 7.83s, Token/s: 65.43
Epoch: 0, Step: 8801, Batch(micro): 8801, Batch (considering grad accum): 1100,  Loss: 6.5359, Time: 4.23s, Token/s: 121.00
Epoch: 0, Step: 8802, Batch(micro): 8802, Batch (considering grad accum): 1100,  Loss: 6.3521, Time: 3.63s, Token/s: 140.96
Epoch: 0, Step: 8803, Batch(micro): 8803, Batch (considering grad accum): 1100,  Loss: 6.3125, Time: 3.69s, Token/s: 138.91
Epoch: 0, Step: 8804, Batch(micro): 8804, Batch (considering grad accum): 1100,  Loss: 6.0131, Time: 3.38s, Token/s: 151.45
Epoch: 0, Step: 8805, Batch(micro): 8805, Batch (considering grad accum): 1100,  Loss: 5.8946, Time: 3.22s, Token/s: 158.92
Epoch: 0, Step: 8806, Batch(micro): 8806, Batch (considering grad accum): 1100,  Loss: 6.7743, Time: 3.27s, Token/s: 156.36
Epoch: 0, Step: 8807, Batch(micro): 8807, Batch (considering grad accum): 1100,  Loss: 6.0120, Time: 24.67s, Token/s: 20.75
Epoch: 0, Step: 8808, Batch(micro): 8808, Batch (considering grad accum): 1101,  Loss: 6.1408, Time: 7.11s, Token/s: 71.98
Epoch: 0, Step: 8809, Batch(micro): 8809, Batch (considering grad accum): 1101,  Loss: 5.5213, Time: 4.05s, Token/s: 126.53
Epoch: 0, Step: 8810, Batch(micro): 8810, Batch (considering grad accum): 1101,  Loss: 5.5747, Time: 3.55s, Token/s: 144.06
Epoch: 0, Step: 8811, Batch(micro): 8811, Batch (considering grad accum): 1101,  Loss: 6.0411, Time: 3.59s, Token/s: 142.57
Epoch: 0, Step: 8812, Batch(micro): 8812, Batch (considering grad accum): 1101,  Loss: 6.4198, Time: 3.29s, Token/s: 155.68
Epoch: 0, Step: 8813, Batch(micro): 8813, Batch (considering grad accum): 1101,  Loss: 5.8540, Time: 3.41s, Token/s: 150.00
Epoch: 0, Step: 8814, Batch(micro): 8814, Batch (considering grad accum): 1101,  Loss: 5.5359, Time: 3.47s, Token/s: 147.51
Epoch: 0, Step: 8815, Batch(micro): 8815, Batch (considering grad accum): 1101,  Loss: 5.6617, Time: 25.06s, Token/s: 20.44
Epoch: 0, Step: 8816, Batch(micro): 8816, Batch (considering grad accum): 1102,  Loss: 5.7119, Time: 7.31s, Token/s: 70.07
Epoch: 0, Step: 8817, Batch(micro): 8817, Batch (considering grad accum): 1102,  Loss: 5.7935, Time: 3.80s, Token/s: 134.66
Epoch: 0, Step: 8818, Batch(micro): 8818, Batch (considering grad accum): 1102,  Loss: 5.7835, Time: 3.46s, Token/s: 148.02
Epoch: 0, Step: 8819, Batch(micro): 8819, Batch (considering grad accum): 1102,  Loss: 5.8170, Time: 3.58s, Token/s: 142.96
Epoch: 0, Step: 8820, Batch(micro): 8820, Batch (considering grad accum): 1102,  Loss: 6.8224, Time: 3.64s, Token/s: 140.68
Epoch: 0, Step: 8821, Batch(micro): 8821, Batch (considering grad accum): 1102,  Loss: 5.9950, Time: 3.72s, Token/s: 137.60
Epoch: 0, Step: 8822, Batch(micro): 8822, Batch (considering grad accum): 1102,  Loss: 6.6054, Time: 3.57s, Token/s: 143.28
Epoch: 0, Step: 8823, Batch(micro): 8823, Batch (considering grad accum): 1102,  Loss: 5.8005, Time: 24.35s, Token/s: 21.03
Epoch: 0, Step: 8824, Batch(micro): 8824, Batch (considering grad accum): 1103,  Loss: 6.3212, Time: 7.92s, Token/s: 64.64
Epoch: 0, Step: 8825, Batch(micro): 8825, Batch (considering grad accum): 1103,  Loss: 5.9976, Time: 3.66s, Token/s: 139.99
Epoch: 0, Step: 8826, Batch(micro): 8826, Batch (considering grad accum): 1103,  Loss: 6.9668, Time: 3.25s, Token/s: 157.32
Epoch: 0, Step: 8827, Batch(micro): 8827, Batch (considering grad accum): 1103,  Loss: 6.1705, Time: 3.28s, Token/s: 155.98
Epoch: 0, Step: 8828, Batch(micro): 8828, Batch (considering grad accum): 1103,  Loss: 5.3982, Time: 3.67s, Token/s: 139.38
Epoch: 0, Step: 8829, Batch(micro): 8829, Batch (considering grad accum): 1103,  Loss: 5.5814, Time: 3.82s, Token/s: 134.08
Epoch: 0, Step: 8830, Batch(micro): 8830, Batch (considering grad accum): 1103,  Loss: 6.7068, Time: 3.56s, Token/s: 143.83
Epoch: 0, Step: 8831, Batch(micro): 8831, Batch (considering grad accum): 1103,  Loss: 5.8179, Time: 23.19s, Token/s: 22.08
Epoch: 0, Step: 8832, Batch(micro): 8832, Batch (considering grad accum): 1104,  Loss: 6.4603, Time: 7.71s, Token/s: 66.44
Epoch: 0, Step: 8833, Batch(micro): 8833, Batch (considering grad accum): 1104,  Loss: 5.9429, Time: 3.95s, Token/s: 129.55
Epoch: 0, Step: 8834, Batch(micro): 8834, Batch (considering grad accum): 1104,  Loss: 6.8319, Time: 3.64s, Token/s: 140.66
Epoch: 0, Step: 8835, Batch(micro): 8835, Batch (considering grad accum): 1104,  Loss: 6.7692, Time: 3.71s, Token/s: 138.06
Epoch: 0, Step: 8836, Batch(micro): 8836, Batch (considering grad accum): 1104,  Loss: 6.4307, Time: 3.76s, Token/s: 136.18
Epoch: 0, Step: 8837, Batch(micro): 8837, Batch (considering grad accum): 1104,  Loss: 6.2490, Time: 3.34s, Token/s: 153.38
Epoch: 0, Step: 8838, Batch(micro): 8838, Batch (considering grad accum): 1104,  Loss: 6.4510, Time: 3.43s, Token/s: 149.09
Epoch: 0, Step: 8839, Batch(micro): 8839, Batch (considering grad accum): 1104,  Loss: 6.8605, Time: 25.43s, Token/s: 20.13
Epoch: 0, Step: 8840, Batch(micro): 8840, Batch (considering grad accum): 1105,  Loss: 6.9007, Time: 8.77s, Token/s: 58.39
Epoch: 0, Step: 8841, Batch(micro): 8841, Batch (considering grad accum): 1105,  Loss: 6.5240, Time: 4.08s, Token/s: 125.37
Epoch: 0, Step: 8842, Batch(micro): 8842, Batch (considering grad accum): 1105,  Loss: 7.3181, Time: 3.57s, Token/s: 143.40
Epoch: 0, Step: 8843, Batch(micro): 8843, Batch (considering grad accum): 1105,  Loss: 5.9337, Time: 3.18s, Token/s: 160.95
Epoch: 0, Step: 8844, Batch(micro): 8844, Batch (considering grad accum): 1105,  Loss: 5.8988, Time: 3.23s, Token/s: 158.70
Epoch: 0, Step: 8845, Batch(micro): 8845, Batch (considering grad accum): 1105,  Loss: 5.7987, Time: 3.38s, Token/s: 151.54
Epoch: 0, Step: 8846, Batch(micro): 8846, Batch (considering grad accum): 1105,  Loss: 5.7634, Time: 3.36s, Token/s: 152.34
Epoch: 0, Step: 8847, Batch(micro): 8847, Batch (considering grad accum): 1105,  Loss: 5.5864, Time: 24.19s, Token/s: 21.16
Epoch: 0, Step: 8848, Batch(micro): 8848, Batch (considering grad accum): 1106,  Loss: 6.2185, Time: 7.31s, Token/s: 70.00
Epoch: 0, Step: 8849, Batch(micro): 8849, Batch (considering grad accum): 1106,  Loss: 6.0645, Time: 3.79s, Token/s: 135.04
Epoch: 0, Step: 8850, Batch(micro): 8850, Batch (considering grad accum): 1106,  Loss: 5.9409, Time: 3.44s, Token/s: 148.82
Epoch: 0, Step: 8851, Batch(micro): 8851, Batch (considering grad accum): 1106,  Loss: 6.1182, Time: 3.39s, Token/s: 150.82
Epoch: 0, Step: 8852, Batch(micro): 8852, Batch (considering grad accum): 1106,  Loss: 5.6053, Time: 3.26s, Token/s: 157.08
Epoch: 0, Step: 8853, Batch(micro): 8853, Batch (considering grad accum): 1106,  Loss: 5.8439, Time: 3.21s, Token/s: 159.50
Epoch: 0, Step: 8854, Batch(micro): 8854, Batch (considering grad accum): 1106,  Loss: 6.5474, Time: 3.24s, Token/s: 158.19
Epoch: 0, Step: 8855, Batch(micro): 8855, Batch (considering grad accum): 1106,  Loss: 5.7486, Time: 25.60s, Token/s: 20.00
Epoch: 0, Step: 8856, Batch(micro): 8856, Batch (considering grad accum): 1107,  Loss: 5.5543, Time: 7.54s, Token/s: 67.87
Epoch: 0, Step: 8857, Batch(micro): 8857, Batch (considering grad accum): 1107,  Loss: 5.5489, Time: 3.69s, Token/s: 138.75
Epoch: 0, Step: 8858, Batch(micro): 8858, Batch (considering grad accum): 1107,  Loss: 6.2616, Time: 3.39s, Token/s: 150.96
Epoch: 0, Step: 8859, Batch(micro): 8859, Batch (considering grad accum): 1107,  Loss: 6.7601, Time: 3.35s, Token/s: 153.01
Epoch: 0, Step: 8860, Batch(micro): 8860, Batch (considering grad accum): 1107,  Loss: 6.1537, Time: 3.46s, Token/s: 148.03
Epoch: 0, Step: 8861, Batch(micro): 8861, Batch (considering grad accum): 1107,  Loss: 6.4748, Time: 3.40s, Token/s: 150.66
Epoch: 0, Step: 8862, Batch(micro): 8862, Batch (considering grad accum): 1107,  Loss: 5.4079, Time: 3.53s, Token/s: 145.22
Epoch: 0, Step: 8863, Batch(micro): 8863, Batch (considering grad accum): 1107,  Loss: 5.9988, Time: 24.07s, Token/s: 21.27
Epoch: 0, Step: 8864, Batch(micro): 8864, Batch (considering grad accum): 1108,  Loss: 6.3434, Time: 6.25s, Token/s: 81.98
Epoch: 0, Step: 8865, Batch(micro): 8865, Batch (considering grad accum): 1108,  Loss: 5.6885, Time: 3.87s, Token/s: 132.16
Epoch: 0, Step: 8866, Batch(micro): 8866, Batch (considering grad accum): 1108,  Loss: 6.0114, Time: 3.88s, Token/s: 131.91
Epoch: 0, Step: 8867, Batch(micro): 8867, Batch (considering grad accum): 1108,  Loss: 6.3162, Time: 3.59s, Token/s: 142.56
Epoch: 0, Step: 8868, Batch(micro): 8868, Batch (considering grad accum): 1108,  Loss: 6.5694, Time: 3.72s, Token/s: 137.46
Epoch: 0, Step: 8869, Batch(micro): 8869, Batch (considering grad accum): 1108,  Loss: 6.4279, Time: 3.70s, Token/s: 138.45
Epoch: 0, Step: 8870, Batch(micro): 8870, Batch (considering grad accum): 1108,  Loss: 6.2393, Time: 3.47s, Token/s: 147.48
Epoch: 0, Step: 8871, Batch(micro): 8871, Batch (considering grad accum): 1108,  Loss: 6.0539, Time: 24.44s, Token/s: 20.95
Epoch: 0, Step: 8872, Batch(micro): 8872, Batch (considering grad accum): 1109,  Loss: 6.0964, Time: 7.69s, Token/s: 66.59
Epoch: 0, Step: 8873, Batch(micro): 8873, Batch (considering grad accum): 1109,  Loss: 6.1228, Time: 4.25s, Token/s: 120.40
Epoch: 0, Step: 8874, Batch(micro): 8874, Batch (considering grad accum): 1109,  Loss: 5.8837, Time: 3.58s, Token/s: 143.05
Epoch: 0, Step: 8875, Batch(micro): 8875, Batch (considering grad accum): 1109,  Loss: 5.9612, Time: 4.01s, Token/s: 127.82
Epoch: 0, Step: 8876, Batch(micro): 8876, Batch (considering grad accum): 1109,  Loss: 6.4353, Time: 3.37s, Token/s: 151.91
Epoch: 0, Step: 8877, Batch(micro): 8877, Batch (considering grad accum): 1109,  Loss: 5.9374, Time: 3.28s, Token/s: 156.24
Epoch: 0, Step: 8878, Batch(micro): 8878, Batch (considering grad accum): 1109,  Loss: 5.5712, Time: 3.24s, Token/s: 158.26
Epoch: 0, Step: 8879, Batch(micro): 8879, Batch (considering grad accum): 1109,  Loss: 5.7067, Time: 23.01s, Token/s: 22.25
Epoch: 0, Step: 8880, Batch(micro): 8880, Batch (considering grad accum): 1110,  Loss: 5.5935, Time: 8.37s, Token/s: 61.19
Epoch: 0, Step: 8881, Batch(micro): 8881, Batch (considering grad accum): 1110,  Loss: 5.1345, Time: 3.87s, Token/s: 132.44
Epoch: 0, Step: 8882, Batch(micro): 8882, Batch (considering grad accum): 1110,  Loss: 5.4515, Time: 3.68s, Token/s: 139.04
Epoch: 0, Step: 8883, Batch(micro): 8883, Batch (considering grad accum): 1110,  Loss: 6.7552, Time: 3.43s, Token/s: 149.20
Epoch: 0, Step: 8884, Batch(micro): 8884, Batch (considering grad accum): 1110,  Loss: 5.9690, Time: 3.72s, Token/s: 137.48
Epoch: 0, Step: 8885, Batch(micro): 8885, Batch (considering grad accum): 1110,  Loss: 5.9803, Time: 3.49s, Token/s: 146.62
Epoch: 0, Step: 8886, Batch(micro): 8886, Batch (considering grad accum): 1110,  Loss: 5.8808, Time: 3.68s, Token/s: 139.17
Epoch: 0, Step: 8887, Batch(micro): 8887, Batch (considering grad accum): 1110,  Loss: 6.2202, Time: 25.48s, Token/s: 20.10
Epoch: 0, Step: 8888, Batch(micro): 8888, Batch (considering grad accum): 1111,  Loss: 5.7543, Time: 6.39s, Token/s: 80.08
Epoch: 0, Step: 8889, Batch(micro): 8889, Batch (considering grad accum): 1111,  Loss: 6.2861, Time: 4.21s, Token/s: 121.58
Epoch: 0, Step: 8890, Batch(micro): 8890, Batch (considering grad accum): 1111,  Loss: 6.6534, Time: 3.56s, Token/s: 143.72
Epoch: 0, Step: 8891, Batch(micro): 8891, Batch (considering grad accum): 1111,  Loss: 5.8243, Time: 3.86s, Token/s: 132.55
Epoch: 0, Step: 8892, Batch(micro): 8892, Batch (considering grad accum): 1111,  Loss: 6.2011, Time: 3.44s, Token/s: 148.69
Epoch: 0, Step: 8893, Batch(micro): 8893, Batch (considering grad accum): 1111,  Loss: 5.9773, Time: 3.45s, Token/s: 148.43
Epoch: 0, Step: 8894, Batch(micro): 8894, Batch (considering grad accum): 1111,  Loss: 6.1321, Time: 3.43s, Token/s: 149.10
Epoch: 0, Step: 8895, Batch(micro): 8895, Batch (considering grad accum): 1111,  Loss: 6.5763, Time: 19.33s, Token/s: 26.49
Epoch: 0, Step: 8896, Batch(micro): 8896, Batch (considering grad accum): 1112,  Loss: 5.6294, Time: 6.67s, Token/s: 76.76
Epoch: 0, Step: 8897, Batch(micro): 8897, Batch (considering grad accum): 1112,  Loss: 5.0334, Time: 3.80s, Token/s: 134.65
Epoch: 0, Step: 8898, Batch(micro): 8898, Batch (considering grad accum): 1112,  Loss: 5.4855, Time: 3.73s, Token/s: 137.30
Epoch: 0, Step: 8899, Batch(micro): 8899, Batch (considering grad accum): 1112,  Loss: 5.7733, Time: 3.59s, Token/s: 142.72
Updating MLP bias
Epoch: 0, Step: 8900, Batch(micro): 8900, Batch (considering grad accum): 1112,  Loss: 5.8654, Time: 3.17s, Token/s: 161.69
Epoch: 0, Step: 8901, Batch(micro): 8901, Batch (considering grad accum): 1112,  Loss: 5.9821, Time: 3.55s, Token/s: 144.06
Epoch: 0, Step: 8902, Batch(micro): 8902, Batch (considering grad accum): 1112,  Loss: 5.9209, Time: 3.47s, Token/s: 147.35
Epoch: 0, Step: 8903, Batch(micro): 8903, Batch (considering grad accum): 1112,  Loss: 5.8127, Time: 17.45s, Token/s: 29.35
Epoch: 0, Step: 8904, Batch(micro): 8904, Batch (considering grad accum): 1113,  Loss: 6.1232, Time: 6.42s, Token/s: 79.76
Epoch: 0, Step: 8905, Batch(micro): 8905, Batch (considering grad accum): 1113,  Loss: 7.2113, Time: 4.20s, Token/s: 121.88
Epoch: 0, Step: 8906, Batch(micro): 8906, Batch (considering grad accum): 1113,  Loss: 6.4692, Time: 3.75s, Token/s: 136.68
Epoch: 0, Step: 8907, Batch(micro): 8907, Batch (considering grad accum): 1113,  Loss: 6.3944, Time: 3.57s, Token/s: 143.48
Epoch: 0, Step: 8908, Batch(micro): 8908, Batch (considering grad accum): 1113,  Loss: 5.9275, Time: 3.47s, Token/s: 147.39
Epoch: 0, Step: 8909, Batch(micro): 8909, Batch (considering grad accum): 1113,  Loss: 6.6970, Time: 3.54s, Token/s: 144.74
Epoch: 0, Step: 8910, Batch(micro): 8910, Batch (considering grad accum): 1113,  Loss: 5.6748, Time: 3.72s, Token/s: 137.60
Epoch: 0, Step: 8911, Batch(micro): 8911, Batch (considering grad accum): 1113,  Loss: 5.4354, Time: 19.03s, Token/s: 26.90
Epoch: 0, Step: 8912, Batch(micro): 8912, Batch (considering grad accum): 1114,  Loss: 6.2083, Time: 6.32s, Token/s: 80.96
Epoch: 0, Step: 8913, Batch(micro): 8913, Batch (considering grad accum): 1114,  Loss: 6.3454, Time: 3.64s, Token/s: 140.48
Epoch: 0, Step: 8914, Batch(micro): 8914, Batch (considering grad accum): 1114,  Loss: 5.9776, Time: 3.34s, Token/s: 153.21
Epoch: 0, Step: 8915, Batch(micro): 8915, Batch (considering grad accum): 1114,  Loss: 6.2572, Time: 3.36s, Token/s: 152.16
Epoch: 0, Step: 8916, Batch(micro): 8916, Batch (considering grad accum): 1114,  Loss: 5.6243, Time: 3.67s, Token/s: 139.66
Epoch: 0, Step: 8917, Batch(micro): 8917, Batch (considering grad accum): 1114,  Loss: 5.3537, Time: 3.71s, Token/s: 137.99
Epoch: 0, Step: 8918, Batch(micro): 8918, Batch (considering grad accum): 1114,  Loss: 5.4518, Time: 3.75s, Token/s: 136.65
Epoch: 0, Step: 8919, Batch(micro): 8919, Batch (considering grad accum): 1114,  Loss: 5.8329, Time: 18.94s, Token/s: 27.03
Epoch: 0, Step: 8920, Batch(micro): 8920, Batch (considering grad accum): 1115,  Loss: 6.1784, Time: 6.51s, Token/s: 78.70
Epoch: 0, Step: 8921, Batch(micro): 8921, Batch (considering grad accum): 1115,  Loss: 6.1263, Time: 3.86s, Token/s: 132.70
Epoch: 0, Step: 8922, Batch(micro): 8922, Batch (considering grad accum): 1115,  Loss: 6.0421, Time: 3.32s, Token/s: 154.12
Epoch: 0, Step: 8923, Batch(micro): 8923, Batch (considering grad accum): 1115,  Loss: 6.4209, Time: 3.57s, Token/s: 143.34
Epoch: 0, Step: 8924, Batch(micro): 8924, Batch (considering grad accum): 1115,  Loss: 6.3094, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 8925, Batch(micro): 8925, Batch (considering grad accum): 1115,  Loss: 6.0463, Time: 3.39s, Token/s: 150.84
Epoch: 0, Step: 8926, Batch(micro): 8926, Batch (considering grad accum): 1115,  Loss: 5.9511, Time: 3.54s, Token/s: 144.72
Epoch: 0, Step: 8927, Batch(micro): 8927, Batch (considering grad accum): 1115,  Loss: 6.7890, Time: 18.76s, Token/s: 27.29
Epoch: 0, Step: 8928, Batch(micro): 8928, Batch (considering grad accum): 1116,  Loss: 6.0237, Time: 6.69s, Token/s: 76.52
Epoch: 0, Step: 8929, Batch(micro): 8929, Batch (considering grad accum): 1116,  Loss: 6.8676, Time: 3.68s, Token/s: 139.02
Epoch: 0, Step: 8930, Batch(micro): 8930, Batch (considering grad accum): 1116,  Loss: 5.9464, Time: 3.32s, Token/s: 154.15
Epoch: 0, Step: 8931, Batch(micro): 8931, Batch (considering grad accum): 1116,  Loss: 5.9731, Time: 3.16s, Token/s: 162.07
Epoch: 0, Step: 8932, Batch(micro): 8932, Batch (considering grad accum): 1116,  Loss: 5.5976, Time: 3.45s, Token/s: 148.34
Epoch: 0, Step: 8933, Batch(micro): 8933, Batch (considering grad accum): 1116,  Loss: 5.3823, Time: 3.64s, Token/s: 140.83
Epoch: 0, Step: 8934, Batch(micro): 8934, Batch (considering grad accum): 1116,  Loss: 5.5848, Time: 3.77s, Token/s: 135.87
Epoch: 0, Step: 8935, Batch(micro): 8935, Batch (considering grad accum): 1116,  Loss: 7.3846, Time: 19.50s, Token/s: 26.26
Epoch: 0, Step: 8936, Batch(micro): 8936, Batch (considering grad accum): 1117,  Loss: 6.2935, Time: 7.00s, Token/s: 73.10
Epoch: 0, Step: 8937, Batch(micro): 8937, Batch (considering grad accum): 1117,  Loss: 5.3868, Time: 3.87s, Token/s: 132.43
Epoch: 0, Step: 8938, Batch(micro): 8938, Batch (considering grad accum): 1117,  Loss: 6.0544, Time: 3.49s, Token/s: 146.82
Epoch: 0, Step: 8939, Batch(micro): 8939, Batch (considering grad accum): 1117,  Loss: 6.0270, Time: 3.44s, Token/s: 148.88
Epoch: 0, Step: 8940, Batch(micro): 8940, Batch (considering grad accum): 1117,  Loss: 5.9562, Time: 3.60s, Token/s: 142.41
Epoch: 0, Step: 8941, Batch(micro): 8941, Batch (considering grad accum): 1117,  Loss: 6.0476, Time: 3.64s, Token/s: 140.64
Epoch: 0, Step: 8942, Batch(micro): 8942, Batch (considering grad accum): 1117,  Loss: 6.2438, Time: 3.43s, Token/s: 149.36
Epoch: 0, Step: 8943, Batch(micro): 8943, Batch (considering grad accum): 1117,  Loss: 6.0735, Time: 20.18s, Token/s: 25.37
Epoch: 0, Step: 8944, Batch(micro): 8944, Batch (considering grad accum): 1118,  Loss: 6.0492, Time: 7.27s, Token/s: 70.38
Epoch: 0, Step: 8945, Batch(micro): 8945, Batch (considering grad accum): 1118,  Loss: 6.6175, Time: 3.96s, Token/s: 129.27
Epoch: 0, Step: 8946, Batch(micro): 8946, Batch (considering grad accum): 1118,  Loss: 6.3758, Time: 3.68s, Token/s: 139.12
Epoch: 0, Step: 8947, Batch(micro): 8947, Batch (considering grad accum): 1118,  Loss: 5.9847, Time: 3.86s, Token/s: 132.69
Epoch: 0, Step: 8948, Batch(micro): 8948, Batch (considering grad accum): 1118,  Loss: 6.3386, Time: 3.53s, Token/s: 144.97
Epoch: 0, Step: 8949, Batch(micro): 8949, Batch (considering grad accum): 1118,  Loss: 6.1910, Time: 3.23s, Token/s: 158.48
Epoch: 0, Step: 8950, Batch(micro): 8950, Batch (considering grad accum): 1118,  Loss: 6.4420, Time: 3.27s, Token/s: 156.79
Epoch: 0, Step: 8951, Batch(micro): 8951, Batch (considering grad accum): 1118,  Loss: 5.5169, Time: 18.98s, Token/s: 26.98
Epoch: 0, Step: 8952, Batch(micro): 8952, Batch (considering grad accum): 1119,  Loss: 6.0188, Time: 6.49s, Token/s: 78.84
Epoch: 0, Step: 8953, Batch(micro): 8953, Batch (considering grad accum): 1119,  Loss: 6.1071, Time: 3.74s, Token/s: 136.76
Epoch: 0, Step: 8954, Batch(micro): 8954, Batch (considering grad accum): 1119,  Loss: 5.6529, Time: 3.25s, Token/s: 157.33
Epoch: 0, Step: 8955, Batch(micro): 8955, Batch (considering grad accum): 1119,  Loss: 6.1062, Time: 3.37s, Token/s: 151.96
Epoch: 0, Step: 8956, Batch(micro): 8956, Batch (considering grad accum): 1119,  Loss: 6.1948, Time: 3.38s, Token/s: 151.41
Epoch: 0, Step: 8957, Batch(micro): 8957, Batch (considering grad accum): 1119,  Loss: 5.9903, Time: 3.81s, Token/s: 134.39
Epoch: 0, Step: 8958, Batch(micro): 8958, Batch (considering grad accum): 1119,  Loss: 6.0391, Time: 3.53s, Token/s: 144.93
Epoch: 0, Step: 8959, Batch(micro): 8959, Batch (considering grad accum): 1119,  Loss: 6.3890, Time: 19.44s, Token/s: 26.34
Epoch: 0, Step: 8960, Batch(micro): 8960, Batch (considering grad accum): 1120,  Loss: 6.5567, Time: 8.17s, Token/s: 62.63
Epoch: 0, Step: 8961, Batch(micro): 8961, Batch (considering grad accum): 1120,  Loss: 7.4550, Time: 3.44s, Token/s: 148.71
Epoch: 0, Step: 8962, Batch(micro): 8962, Batch (considering grad accum): 1120,  Loss: 6.0357, Time: 3.28s, Token/s: 156.31
Epoch: 0, Step: 8963, Batch(micro): 8963, Batch (considering grad accum): 1120,  Loss: 6.0805, Time: 3.28s, Token/s: 156.32
Epoch: 0, Step: 8964, Batch(micro): 8964, Batch (considering grad accum): 1120,  Loss: 5.5518, Time: 3.46s, Token/s: 147.78
Epoch: 0, Step: 8965, Batch(micro): 8965, Batch (considering grad accum): 1120,  Loss: 6.1167, Time: 3.47s, Token/s: 147.50
Epoch: 0, Step: 8966, Batch(micro): 8966, Batch (considering grad accum): 1120,  Loss: 5.7013, Time: 3.55s, Token/s: 144.09
Epoch: 0, Step: 8967, Batch(micro): 8967, Batch (considering grad accum): 1120,  Loss: 6.4592, Time: 18.94s, Token/s: 27.03
Epoch: 0, Step: 8968, Batch(micro): 8968, Batch (considering grad accum): 1121,  Loss: 6.1411, Time: 6.55s, Token/s: 78.13
Epoch: 0, Step: 8969, Batch(micro): 8969, Batch (considering grad accum): 1121,  Loss: 6.0237, Time: 3.68s, Token/s: 138.96
Epoch: 0, Step: 8970, Batch(micro): 8970, Batch (considering grad accum): 1121,  Loss: 7.6133, Time: 3.83s, Token/s: 133.76
Epoch: 0, Step: 8971, Batch(micro): 8971, Batch (considering grad accum): 1121,  Loss: 7.2527, Time: 3.51s, Token/s: 146.01
Epoch: 0, Step: 8972, Batch(micro): 8972, Batch (considering grad accum): 1121,  Loss: 6.7289, Time: 3.28s, Token/s: 156.24
Epoch: 0, Step: 8973, Batch(micro): 8973, Batch (considering grad accum): 1121,  Loss: 6.2344, Time: 3.27s, Token/s: 156.59
Epoch: 0, Step: 8974, Batch(micro): 8974, Batch (considering grad accum): 1121,  Loss: 7.1486, Time: 3.29s, Token/s: 155.41
Epoch: 0, Step: 8975, Batch(micro): 8975, Batch (considering grad accum): 1121,  Loss: 6.0167, Time: 24.53s, Token/s: 20.87
Epoch: 0, Step: 8976, Batch(micro): 8976, Batch (considering grad accum): 1122,  Loss: 5.6715, Time: 7.36s, Token/s: 69.58
Epoch: 0, Step: 8977, Batch(micro): 8977, Batch (considering grad accum): 1122,  Loss: 5.9424, Time: 3.79s, Token/s: 135.05
Epoch: 0, Step: 8978, Batch(micro): 8978, Batch (considering grad accum): 1122,  Loss: 5.5644, Time: 3.22s, Token/s: 159.05
Epoch: 0, Step: 8979, Batch(micro): 8979, Batch (considering grad accum): 1122,  Loss: 5.7262, Time: 3.25s, Token/s: 157.36
Epoch: 0, Step: 8980, Batch(micro): 8980, Batch (considering grad accum): 1122,  Loss: 5.8872, Time: 3.41s, Token/s: 150.04
Epoch: 0, Step: 8981, Batch(micro): 8981, Batch (considering grad accum): 1122,  Loss: 5.3338, Time: 3.48s, Token/s: 147.14
Epoch: 0, Step: 8982, Batch(micro): 8982, Batch (considering grad accum): 1122,  Loss: 6.2563, Time: 3.67s, Token/s: 139.45
Epoch: 0, Step: 8983, Batch(micro): 8983, Batch (considering grad accum): 1122,  Loss: 6.4065, Time: 24.38s, Token/s: 21.00
Epoch: 0, Step: 8984, Batch(micro): 8984, Batch (considering grad accum): 1123,  Loss: 7.2855, Time: 8.19s, Token/s: 62.54
Epoch: 0, Step: 8985, Batch(micro): 8985, Batch (considering grad accum): 1123,  Loss: 5.5492, Time: 3.85s, Token/s: 133.03
Epoch: 0, Step: 8986, Batch(micro): 8986, Batch (considering grad accum): 1123,  Loss: 6.1186, Time: 3.54s, Token/s: 144.77
Epoch: 0, Step: 8987, Batch(micro): 8987, Batch (considering grad accum): 1123,  Loss: 5.9299, Time: 3.54s, Token/s: 144.66
Epoch: 0, Step: 8988, Batch(micro): 8988, Batch (considering grad accum): 1123,  Loss: 5.6572, Time: 3.55s, Token/s: 144.13
Epoch: 0, Step: 8989, Batch(micro): 8989, Batch (considering grad accum): 1123,  Loss: 5.1211, Time: 3.58s, Token/s: 142.95
Epoch: 0, Step: 8990, Batch(micro): 8990, Batch (considering grad accum): 1123,  Loss: 5.9483, Time: 3.29s, Token/s: 155.72
Epoch: 0, Step: 8991, Batch(micro): 8991, Batch (considering grad accum): 1123,  Loss: 5.8548, Time: 22.57s, Token/s: 22.69
Epoch: 0, Step: 8992, Batch(micro): 8992, Batch (considering grad accum): 1124,  Loss: 5.6765, Time: 7.27s, Token/s: 70.44
Epoch: 0, Step: 8993, Batch(micro): 8993, Batch (considering grad accum): 1124,  Loss: 6.0040, Time: 4.17s, Token/s: 122.81
Epoch: 0, Step: 8994, Batch(micro): 8994, Batch (considering grad accum): 1124,  Loss: 6.1869, Time: 3.85s, Token/s: 133.15
Epoch: 0, Step: 8995, Batch(micro): 8995, Batch (considering grad accum): 1124,  Loss: 6.9318, Time: 3.53s, Token/s: 145.05
Epoch: 0, Step: 8996, Batch(micro): 8996, Batch (considering grad accum): 1124,  Loss: 5.6322, Time: 3.58s, Token/s: 142.96
Epoch: 0, Step: 8997, Batch(micro): 8997, Batch (considering grad accum): 1124,  Loss: 5.6729, Time: 3.22s, Token/s: 158.92
Epoch: 0, Step: 8998, Batch(micro): 8998, Batch (considering grad accum): 1124,  Loss: 5.3903, Time: 3.31s, Token/s: 154.71
Epoch: 0, Step: 8999, Batch(micro): 8999, Batch (considering grad accum): 1124,  Loss: 6.3068, Time: 24.94s, Token/s: 20.53
Updating MLP bias
Epoch: 0, Step: 9000, Batch(micro): 9000, Batch (considering grad accum): 1125,  Loss: 5.9198, Time: 8.27s, Token/s: 61.88
Saved checkpoint at step 9000
What is Gravity? Or what you can also contribute to read a few common sense of the world. Chapter 3: The 2: A big day, I didn
Epoch: 0, Step: 9001, Batch(micro): 9001, Batch (considering grad accum): 1125,  Loss: 5.4960, Time: 15.90s, Token/s: 32.19
Epoch: 0, Step: 9002, Batch(micro): 9002, Batch (considering grad accum): 1125,  Loss: 5.8063, Time: 3.76s, Token/s: 136.07
Epoch: 0, Step: 9003, Batch(micro): 9003, Batch (considering grad accum): 1125,  Loss: 6.1580, Time: 3.59s, Token/s: 142.68
Epoch: 0, Step: 9004, Batch(micro): 9004, Batch (considering grad accum): 1125,  Loss: 6.2208, Time: 3.69s, Token/s: 138.61
Epoch: 0, Step: 9005, Batch(micro): 9005, Batch (considering grad accum): 1125,  Loss: 5.5626, Time: 3.35s, Token/s: 152.90
Epoch: 0, Step: 9006, Batch(micro): 9006, Batch (considering grad accum): 1125,  Loss: 5.8936, Time: 3.48s, Token/s: 147.02
Epoch: 0, Step: 9007, Batch(micro): 9007, Batch (considering grad accum): 1125,  Loss: 5.4308, Time: 25.34s, Token/s: 20.21
Epoch: 0, Step: 9008, Batch(micro): 9008, Batch (considering grad accum): 1126,  Loss: 5.8687, Time: 8.18s, Token/s: 62.57
Epoch: 0, Step: 9009, Batch(micro): 9009, Batch (considering grad accum): 1126,  Loss: 5.6612, Time: 3.88s, Token/s: 131.96
Epoch: 0, Step: 9010, Batch(micro): 9010, Batch (considering grad accum): 1126,  Loss: 6.1151, Time: 3.80s, Token/s: 134.90
Epoch: 0, Step: 9011, Batch(micro): 9011, Batch (considering grad accum): 1126,  Loss: 6.7992, Time: 3.90s, Token/s: 131.41
Epoch: 0, Step: 9012, Batch(micro): 9012, Batch (considering grad accum): 1126,  Loss: 6.3156, Time: 3.80s, Token/s: 134.58
Epoch: 0, Step: 9013, Batch(micro): 9013, Batch (considering grad accum): 1126,  Loss: 6.2929, Time: 3.46s, Token/s: 147.87
Epoch: 0, Step: 9014, Batch(micro): 9014, Batch (considering grad accum): 1126,  Loss: 5.9087, Time: 3.25s, Token/s: 157.40
Epoch: 0, Step: 9015, Batch(micro): 9015, Batch (considering grad accum): 1126,  Loss: 6.2963, Time: 23.40s, Token/s: 21.88
Epoch: 0, Step: 9016, Batch(micro): 9016, Batch (considering grad accum): 1127,  Loss: 6.2404, Time: 6.69s, Token/s: 76.51
Epoch: 0, Step: 9017, Batch(micro): 9017, Batch (considering grad accum): 1127,  Loss: 5.9495, Time: 3.89s, Token/s: 131.74
Epoch: 0, Step: 9018, Batch(micro): 9018, Batch (considering grad accum): 1127,  Loss: 6.4683, Time: 3.77s, Token/s: 135.81
Epoch: 0, Step: 9019, Batch(micro): 9019, Batch (considering grad accum): 1127,  Loss: 5.7847, Time: 3.78s, Token/s: 135.55
Epoch: 0, Step: 9020, Batch(micro): 9020, Batch (considering grad accum): 1127,  Loss: 6.1750, Time: 3.72s, Token/s: 137.79
Epoch: 0, Step: 9021, Batch(micro): 9021, Batch (considering grad accum): 1127,  Loss: 5.5429, Time: 3.38s, Token/s: 151.58
Epoch: 0, Step: 9022, Batch(micro): 9022, Batch (considering grad accum): 1127,  Loss: 5.7338, Time: 3.18s, Token/s: 161.19
Epoch: 0, Step: 9023, Batch(micro): 9023, Batch (considering grad accum): 1127,  Loss: 5.5155, Time: 21.40s, Token/s: 23.93
Epoch: 0, Step: 9024, Batch(micro): 9024, Batch (considering grad accum): 1128,  Loss: 6.4891, Time: 7.74s, Token/s: 66.13
Epoch: 0, Step: 9025, Batch(micro): 9025, Batch (considering grad accum): 1128,  Loss: 6.1789, Time: 4.03s, Token/s: 127.02
Epoch: 0, Step: 9026, Batch(micro): 9026, Batch (considering grad accum): 1128,  Loss: 5.6639, Time: 4.02s, Token/s: 127.35
Epoch: 0, Step: 9027, Batch(micro): 9027, Batch (considering grad accum): 1128,  Loss: 5.8660, Time: 3.73s, Token/s: 137.37
Epoch: 0, Step: 9028, Batch(micro): 9028, Batch (considering grad accum): 1128,  Loss: 5.8540, Time: 3.40s, Token/s: 150.68
Epoch: 0, Step: 9029, Batch(micro): 9029, Batch (considering grad accum): 1128,  Loss: 5.6315, Time: 3.42s, Token/s: 149.91
Epoch: 0, Step: 9030, Batch(micro): 9030, Batch (considering grad accum): 1128,  Loss: 6.2392, Time: 3.29s, Token/s: 155.80
Epoch: 0, Step: 9031, Batch(micro): 9031, Batch (considering grad accum): 1128,  Loss: 6.3471, Time: 20.80s, Token/s: 24.61
Epoch: 0, Step: 9032, Batch(micro): 9032, Batch (considering grad accum): 1129,  Loss: 6.2863, Time: 9.53s, Token/s: 53.75
Epoch: 0, Step: 9033, Batch(micro): 9033, Batch (considering grad accum): 1129,  Loss: 5.9573, Time: 3.32s, Token/s: 154.24
Epoch: 0, Step: 9034, Batch(micro): 9034, Batch (considering grad accum): 1129,  Loss: 6.5681, Time: 3.20s, Token/s: 160.03
Epoch: 0, Step: 9035, Batch(micro): 9035, Batch (considering grad accum): 1129,  Loss: 5.4455, Time: 3.18s, Token/s: 160.89
Epoch: 0, Step: 9036, Batch(micro): 9036, Batch (considering grad accum): 1129,  Loss: 5.5308, Time: 3.85s, Token/s: 132.83
Epoch: 0, Step: 9037, Batch(micro): 9037, Batch (considering grad accum): 1129,  Loss: 6.2678, Time: 3.49s, Token/s: 146.85
Epoch: 0, Step: 9038, Batch(micro): 9038, Batch (considering grad accum): 1129,  Loss: 6.3819, Time: 3.61s, Token/s: 141.89
Epoch: 0, Step: 9039, Batch(micro): 9039, Batch (considering grad accum): 1129,  Loss: 5.2996, Time: 22.89s, Token/s: 22.37
Epoch: 0, Step: 9040, Batch(micro): 9040, Batch (considering grad accum): 1130,  Loss: 5.6152, Time: 6.94s, Token/s: 73.80
Epoch: 0, Step: 9041, Batch(micro): 9041, Batch (considering grad accum): 1130,  Loss: 5.4530, Time: 3.68s, Token/s: 139.18
Epoch: 0, Step: 9042, Batch(micro): 9042, Batch (considering grad accum): 1130,  Loss: 6.0849, Time: 3.26s, Token/s: 156.96
Epoch: 0, Step: 9043, Batch(micro): 9043, Batch (considering grad accum): 1130,  Loss: 5.8853, Time: 3.28s, Token/s: 156.16
Epoch: 0, Step: 9044, Batch(micro): 9044, Batch (considering grad accum): 1130,  Loss: 6.4956, Time: 3.22s, Token/s: 159.18
Epoch: 0, Step: 9045, Batch(micro): 9045, Batch (considering grad accum): 1130,  Loss: 5.9940, Time: 3.32s, Token/s: 154.21
Epoch: 0, Step: 9046, Batch(micro): 9046, Batch (considering grad accum): 1130,  Loss: 6.2041, Time: 3.80s, Token/s: 134.83
Epoch: 0, Step: 9047, Batch(micro): 9047, Batch (considering grad accum): 1130,  Loss: 6.8031, Time: 20.81s, Token/s: 24.61
Epoch: 0, Step: 9048, Batch(micro): 9048, Batch (considering grad accum): 1131,  Loss: 6.1625, Time: 7.41s, Token/s: 69.14
Epoch: 0, Step: 9049, Batch(micro): 9049, Batch (considering grad accum): 1131,  Loss: 6.2907, Time: 3.96s, Token/s: 129.15
Epoch: 0, Step: 9050, Batch(micro): 9050, Batch (considering grad accum): 1131,  Loss: 6.0684, Time: 3.61s, Token/s: 141.87
Epoch: 0, Step: 9051, Batch(micro): 9051, Batch (considering grad accum): 1131,  Loss: 5.6118, Time: 4.04s, Token/s: 126.67
Epoch: 0, Step: 9052, Batch(micro): 9052, Batch (considering grad accum): 1131,  Loss: 5.2552, Time: 3.53s, Token/s: 145.02
Epoch: 0, Step: 9053, Batch(micro): 9053, Batch (considering grad accum): 1131,  Loss: 5.4479, Time: 3.60s, Token/s: 142.36
Epoch: 0, Step: 9054, Batch(micro): 9054, Batch (considering grad accum): 1131,  Loss: 5.1538, Time: 3.31s, Token/s: 154.61
Epoch: 0, Step: 9055, Batch(micro): 9055, Batch (considering grad accum): 1131,  Loss: 5.5590, Time: 25.17s, Token/s: 20.34
Epoch: 0, Step: 9056, Batch(micro): 9056, Batch (considering grad accum): 1132,  Loss: 5.8007, Time: 7.44s, Token/s: 68.81
Epoch: 0, Step: 9057, Batch(micro): 9057, Batch (considering grad accum): 1132,  Loss: 6.1726, Time: 4.13s, Token/s: 123.86
Epoch: 0, Step: 9058, Batch(micro): 9058, Batch (considering grad accum): 1132,  Loss: 5.4529, Time: 3.47s, Token/s: 147.39
Epoch: 0, Step: 9059, Batch(micro): 9059, Batch (considering grad accum): 1132,  Loss: 8.8400, Time: 3.49s, Token/s: 146.85
Epoch: 0, Step: 9060, Batch(micro): 9060, Batch (considering grad accum): 1132,  Loss: 5.4771, Time: 3.41s, Token/s: 149.93
Epoch: 0, Step: 9061, Batch(micro): 9061, Batch (considering grad accum): 1132,  Loss: 5.9171, Time: 3.39s, Token/s: 151.13
Epoch: 0, Step: 9062, Batch(micro): 9062, Batch (considering grad accum): 1132,  Loss: 5.9926, Time: 3.21s, Token/s: 159.46
Epoch: 0, Step: 9063, Batch(micro): 9063, Batch (considering grad accum): 1132,  Loss: 5.5382, Time: 25.45s, Token/s: 20.12
Epoch: 0, Step: 9064, Batch(micro): 9064, Batch (considering grad accum): 1133,  Loss: 5.4927, Time: 7.82s, Token/s: 65.49
Epoch: 0, Step: 9065, Batch(micro): 9065, Batch (considering grad accum): 1133,  Loss: 5.3226, Time: 4.05s, Token/s: 126.28
Epoch: 0, Step: 9066, Batch(micro): 9066, Batch (considering grad accum): 1133,  Loss: 6.2677, Time: 3.35s, Token/s: 152.85
Epoch: 0, Step: 9067, Batch(micro): 9067, Batch (considering grad accum): 1133,  Loss: 7.0858, Time: 3.47s, Token/s: 147.44
Epoch: 0, Step: 9068, Batch(micro): 9068, Batch (considering grad accum): 1133,  Loss: 6.4604, Time: 3.44s, Token/s: 148.71
Epoch: 0, Step: 9069, Batch(micro): 9069, Batch (considering grad accum): 1133,  Loss: 5.8343, Time: 3.47s, Token/s: 147.42
Epoch: 0, Step: 9070, Batch(micro): 9070, Batch (considering grad accum): 1133,  Loss: 6.0066, Time: 3.41s, Token/s: 150.19
Epoch: 0, Step: 9071, Batch(micro): 9071, Batch (considering grad accum): 1133,  Loss: 6.2662, Time: 25.37s, Token/s: 20.18
Epoch: 0, Step: 9072, Batch(micro): 9072, Batch (considering grad accum): 1134,  Loss: 6.2590, Time: 7.29s, Token/s: 70.26
Epoch: 0, Step: 9073, Batch(micro): 9073, Batch (considering grad accum): 1134,  Loss: 5.6215, Time: 4.06s, Token/s: 126.23
Epoch: 0, Step: 9074, Batch(micro): 9074, Batch (considering grad accum): 1134,  Loss: 6.0477, Time: 3.72s, Token/s: 137.48
Epoch: 0, Step: 9075, Batch(micro): 9075, Batch (considering grad accum): 1134,  Loss: 5.9133, Time: 3.62s, Token/s: 141.39
Epoch: 0, Step: 9076, Batch(micro): 9076, Batch (considering grad accum): 1134,  Loss: 6.3335, Time: 3.30s, Token/s: 155.03
Epoch: 0, Step: 9077, Batch(micro): 9077, Batch (considering grad accum): 1134,  Loss: 5.9226, Time: 3.46s, Token/s: 148.06
Epoch: 0, Step: 9078, Batch(micro): 9078, Batch (considering grad accum): 1134,  Loss: 5.9626, Time: 3.59s, Token/s: 142.72
Epoch: 0, Step: 9079, Batch(micro): 9079, Batch (considering grad accum): 1134,  Loss: 6.0532, Time: 24.32s, Token/s: 21.06
Epoch: 0, Step: 9080, Batch(micro): 9080, Batch (considering grad accum): 1135,  Loss: 6.4957, Time: 6.72s, Token/s: 76.22
Epoch: 0, Step: 9081, Batch(micro): 9081, Batch (considering grad accum): 1135,  Loss: 6.0747, Time: 3.59s, Token/s: 142.60
Epoch: 0, Step: 9082, Batch(micro): 9082, Batch (considering grad accum): 1135,  Loss: 6.9736, Time: 3.21s, Token/s: 159.72
Epoch: 0, Step: 9083, Batch(micro): 9083, Batch (considering grad accum): 1135,  Loss: 5.4222, Time: 3.20s, Token/s: 159.80
Epoch: 0, Step: 9084, Batch(micro): 9084, Batch (considering grad accum): 1135,  Loss: 5.5012, Time: 3.33s, Token/s: 153.95
Epoch: 0, Step: 9085, Batch(micro): 9085, Batch (considering grad accum): 1135,  Loss: 5.3773, Time: 3.49s, Token/s: 146.90
Epoch: 0, Step: 9086, Batch(micro): 9086, Batch (considering grad accum): 1135,  Loss: 5.5784, Time: 3.45s, Token/s: 148.51
Epoch: 0, Step: 9087, Batch(micro): 9087, Batch (considering grad accum): 1135,  Loss: 6.3916, Time: 23.58s, Token/s: 21.71
Epoch: 0, Step: 9088, Batch(micro): 9088, Batch (considering grad accum): 1136,  Loss: 6.0244, Time: 8.74s, Token/s: 58.61
Epoch: 0, Step: 9089, Batch(micro): 9089, Batch (considering grad accum): 1136,  Loss: 6.0831, Time: 3.29s, Token/s: 155.64
Epoch: 0, Step: 9090, Batch(micro): 9090, Batch (considering grad accum): 1136,  Loss: 5.8449, Time: 3.21s, Token/s: 159.25
Epoch: 0, Step: 9091, Batch(micro): 9091, Batch (considering grad accum): 1136,  Loss: 5.8819, Time: 3.14s, Token/s: 163.23
Epoch: 0, Step: 9092, Batch(micro): 9092, Batch (considering grad accum): 1136,  Loss: 5.2000, Time: 3.41s, Token/s: 150.18
Epoch: 0, Step: 9093, Batch(micro): 9093, Batch (considering grad accum): 1136,  Loss: 6.0679, Time: 3.42s, Token/s: 149.57
Epoch: 0, Step: 9094, Batch(micro): 9094, Batch (considering grad accum): 1136,  Loss: 6.1013, Time: 3.49s, Token/s: 146.67
Epoch: 0, Step: 9095, Batch(micro): 9095, Batch (considering grad accum): 1136,  Loss: 6.3401, Time: 22.52s, Token/s: 22.73
Epoch: 0, Step: 9096, Batch(micro): 9096, Batch (considering grad accum): 1137,  Loss: 5.4007, Time: 7.55s, Token/s: 67.84
Epoch: 0, Step: 9097, Batch(micro): 9097, Batch (considering grad accum): 1137,  Loss: 5.7460, Time: 4.40s, Token/s: 116.29
Epoch: 0, Step: 9098, Batch(micro): 9098, Batch (considering grad accum): 1137,  Loss: 6.3213, Time: 3.54s, Token/s: 144.55
Epoch: 0, Step: 9099, Batch(micro): 9099, Batch (considering grad accum): 1137,  Loss: 6.4329, Time: 3.50s, Token/s: 146.36
Updating MLP bias
Epoch: 0, Step: 9100, Batch(micro): 9100, Batch (considering grad accum): 1137,  Loss: 5.6915, Time: 3.62s, Token/s: 141.62
Epoch: 0, Step: 9101, Batch(micro): 9101, Batch (considering grad accum): 1137,  Loss: 6.4649, Time: 3.26s, Token/s: 157.05
Epoch: 0, Step: 9102, Batch(micro): 9102, Batch (considering grad accum): 1137,  Loss: 6.1397, Time: 4.12s, Token/s: 124.32
Epoch: 0, Step: 9103, Batch(micro): 9103, Batch (considering grad accum): 1137,  Loss: 6.5844, Time: 24.53s, Token/s: 20.87
Epoch: 0, Step: 9104, Batch(micro): 9104, Batch (considering grad accum): 1138,  Loss: 6.7163, Time: 8.31s, Token/s: 61.58
Epoch: 0, Step: 9105, Batch(micro): 9105, Batch (considering grad accum): 1138,  Loss: 6.3027, Time: 4.00s, Token/s: 127.86
Epoch: 0, Step: 9106, Batch(micro): 9106, Batch (considering grad accum): 1138,  Loss: 6.1622, Time: 3.62s, Token/s: 141.48
Epoch: 0, Step: 9107, Batch(micro): 9107, Batch (considering grad accum): 1138,  Loss: 6.1054, Time: 3.63s, Token/s: 140.99
Epoch: 0, Step: 9108, Batch(micro): 9108, Batch (considering grad accum): 1138,  Loss: 6.1855, Time: 3.58s, Token/s: 142.97
Epoch: 0, Step: 9109, Batch(micro): 9109, Batch (considering grad accum): 1138,  Loss: 6.7233, Time: 3.57s, Token/s: 143.60
Epoch: 0, Step: 9110, Batch(micro): 9110, Batch (considering grad accum): 1138,  Loss: 6.4946, Time: 3.63s, Token/s: 140.87
Epoch: 0, Step: 9111, Batch(micro): 9111, Batch (considering grad accum): 1138,  Loss: 5.5009, Time: 24.96s, Token/s: 20.51
Epoch: 0, Step: 9112, Batch(micro): 9112, Batch (considering grad accum): 1139,  Loss: 5.4702, Time: 7.56s, Token/s: 67.70
Epoch: 0, Step: 9113, Batch(micro): 9113, Batch (considering grad accum): 1139,  Loss: 5.7092, Time: 3.84s, Token/s: 133.24
Epoch: 0, Step: 9114, Batch(micro): 9114, Batch (considering grad accum): 1139,  Loss: 4.9435, Time: 3.39s, Token/s: 150.83
Epoch: 0, Step: 9115, Batch(micro): 9115, Batch (considering grad accum): 1139,  Loss: 5.8442, Time: 3.39s, Token/s: 150.88
Epoch: 0, Step: 9116, Batch(micro): 9116, Batch (considering grad accum): 1139,  Loss: 5.6121, Time: 3.39s, Token/s: 151.15
Epoch: 0, Step: 9117, Batch(micro): 9117, Batch (considering grad accum): 1139,  Loss: 6.0662, Time: 3.35s, Token/s: 152.79
Epoch: 0, Step: 9118, Batch(micro): 9118, Batch (considering grad accum): 1139,  Loss: 5.6194, Time: 3.30s, Token/s: 155.13
Epoch: 0, Step: 9119, Batch(micro): 9119, Batch (considering grad accum): 1139,  Loss: 6.9314, Time: 23.81s, Token/s: 21.50
Epoch: 0, Step: 9120, Batch(micro): 9120, Batch (considering grad accum): 1140,  Loss: 6.1804, Time: 6.91s, Token/s: 74.06
Epoch: 0, Step: 9121, Batch(micro): 9121, Batch (considering grad accum): 1140,  Loss: 7.1484, Time: 3.97s, Token/s: 129.12
Epoch: 0, Step: 9122, Batch(micro): 9122, Batch (considering grad accum): 1140,  Loss: 6.6000, Time: 3.69s, Token/s: 138.86
Epoch: 0, Step: 9123, Batch(micro): 9123, Batch (considering grad accum): 1140,  Loss: 6.1764, Time: 3.51s, Token/s: 145.72
Epoch: 0, Step: 9124, Batch(micro): 9124, Batch (considering grad accum): 1140,  Loss: 5.8119, Time: 3.27s, Token/s: 156.80
Epoch: 0, Step: 9125, Batch(micro): 9125, Batch (considering grad accum): 1140,  Loss: 6.1062, Time: 3.25s, Token/s: 157.53
Epoch: 0, Step: 9126, Batch(micro): 9126, Batch (considering grad accum): 1140,  Loss: 6.0564, Time: 3.22s, Token/s: 159.24
Epoch: 0, Step: 9127, Batch(micro): 9127, Batch (considering grad accum): 1140,  Loss: 5.7080, Time: 22.01s, Token/s: 23.26
Epoch: 0, Step: 9128, Batch(micro): 9128, Batch (considering grad accum): 1141,  Loss: 6.2610, Time: 6.43s, Token/s: 79.57
Epoch: 0, Step: 9129, Batch(micro): 9129, Batch (considering grad accum): 1141,  Loss: 5.7400, Time: 3.49s, Token/s: 146.73
Epoch: 0, Step: 9130, Batch(micro): 9130, Batch (considering grad accum): 1141,  Loss: 6.4490, Time: 3.44s, Token/s: 148.63
Epoch: 0, Step: 9131, Batch(micro): 9131, Batch (considering grad accum): 1141,  Loss: 5.8088, Time: 3.46s, Token/s: 148.11
Epoch: 0, Step: 9132, Batch(micro): 9132, Batch (considering grad accum): 1141,  Loss: 6.1840, Time: 3.49s, Token/s: 146.54
Epoch: 0, Step: 9133, Batch(micro): 9133, Batch (considering grad accum): 1141,  Loss: 6.3030, Time: 3.64s, Token/s: 140.82
Epoch: 0, Step: 9134, Batch(micro): 9134, Batch (considering grad accum): 1141,  Loss: 6.5090, Time: 3.45s, Token/s: 148.40
Epoch: 0, Step: 9135, Batch(micro): 9135, Batch (considering grad accum): 1141,  Loss: 5.7364, Time: 18.93s, Token/s: 27.05
Epoch: 0, Step: 9136, Batch(micro): 9136, Batch (considering grad accum): 1142,  Loss: 6.2557, Time: 6.82s, Token/s: 75.04
Epoch: 0, Step: 9137, Batch(micro): 9137, Batch (considering grad accum): 1142,  Loss: 7.0579, Time: 3.80s, Token/s: 134.61
Epoch: 0, Step: 9138, Batch(micro): 9138, Batch (considering grad accum): 1142,  Loss: 6.4087, Time: 3.41s, Token/s: 149.95
Epoch: 0, Step: 9139, Batch(micro): 9139, Batch (considering grad accum): 1142,  Loss: 6.2348, Time: 3.57s, Token/s: 143.51
Epoch: 0, Step: 9140, Batch(micro): 9140, Batch (considering grad accum): 1142,  Loss: 7.2891, Time: 3.30s, Token/s: 155.19
Epoch: 0, Step: 9141, Batch(micro): 9141, Batch (considering grad accum): 1142,  Loss: 8.0218, Time: 3.33s, Token/s: 153.71
Epoch: 0, Step: 9142, Batch(micro): 9142, Batch (considering grad accum): 1142,  Loss: 6.7050, Time: 3.32s, Token/s: 153.99
Epoch: 0, Step: 9143, Batch(micro): 9143, Batch (considering grad accum): 1142,  Loss: 5.6462, Time: 18.29s, Token/s: 28.00
Epoch: 0, Step: 9144, Batch(micro): 9144, Batch (considering grad accum): 1143,  Loss: 6.7728, Time: 7.50s, Token/s: 68.29
Epoch: 0, Step: 9145, Batch(micro): 9145, Batch (considering grad accum): 1143,  Loss: 5.9418, Time: 4.07s, Token/s: 125.66
Epoch: 0, Step: 9146, Batch(micro): 9146, Batch (considering grad accum): 1143,  Loss: 5.7479, Time: 3.53s, Token/s: 145.21
Epoch: 0, Step: 9147, Batch(micro): 9147, Batch (considering grad accum): 1143,  Loss: 5.7613, Time: 3.52s, Token/s: 145.65
Epoch: 0, Step: 9148, Batch(micro): 9148, Batch (considering grad accum): 1143,  Loss: 5.5427, Time: 3.32s, Token/s: 154.26
Epoch: 0, Step: 9149, Batch(micro): 9149, Batch (considering grad accum): 1143,  Loss: 5.9497, Time: 3.35s, Token/s: 152.65
Epoch: 0, Step: 9150, Batch(micro): 9150, Batch (considering grad accum): 1143,  Loss: 5.9286, Time: 3.52s, Token/s: 145.29
Epoch: 0, Step: 9151, Batch(micro): 9151, Batch (considering grad accum): 1143,  Loss: 5.7827, Time: 19.53s, Token/s: 26.21
Epoch: 0, Step: 9152, Batch(micro): 9152, Batch (considering grad accum): 1144,  Loss: 5.7851, Time: 6.54s, Token/s: 78.25
Epoch: 0, Step: 9153, Batch(micro): 9153, Batch (considering grad accum): 1144,  Loss: 5.6741, Time: 4.13s, Token/s: 123.89
Epoch: 0, Step: 9154, Batch(micro): 9154, Batch (considering grad accum): 1144,  Loss: 5.9905, Time: 3.64s, Token/s: 140.65
Epoch: 0, Step: 9155, Batch(micro): 9155, Batch (considering grad accum): 1144,  Loss: 6.5522, Time: 3.54s, Token/s: 144.49
Epoch: 0, Step: 9156, Batch(micro): 9156, Batch (considering grad accum): 1144,  Loss: 6.2052, Time: 3.73s, Token/s: 137.21
Epoch: 0, Step: 9157, Batch(micro): 9157, Batch (considering grad accum): 1144,  Loss: 5.3150, Time: 3.56s, Token/s: 143.90
Epoch: 0, Step: 9158, Batch(micro): 9158, Batch (considering grad accum): 1144,  Loss: 5.8147, Time: 3.17s, Token/s: 161.40
Epoch: 0, Step: 9159, Batch(micro): 9159, Batch (considering grad accum): 1144,  Loss: 6.6293, Time: 18.98s, Token/s: 26.98
Epoch: 0, Step: 9160, Batch(micro): 9160, Batch (considering grad accum): 1145,  Loss: 6.3981, Time: 7.39s, Token/s: 69.28
Epoch: 0, Step: 9161, Batch(micro): 9161, Batch (considering grad accum): 1145,  Loss: 5.0982, Time: 4.11s, Token/s: 124.69
Epoch: 0, Step: 9162, Batch(micro): 9162, Batch (considering grad accum): 1145,  Loss: 6.8489, Time: 3.52s, Token/s: 145.49
Epoch: 0, Step: 9163, Batch(micro): 9163, Batch (considering grad accum): 1145,  Loss: 5.9114, Time: 3.69s, Token/s: 138.84
Epoch: 0, Step: 9164, Batch(micro): 9164, Batch (considering grad accum): 1145,  Loss: 5.3672, Time: 3.52s, Token/s: 145.53
Epoch: 0, Step: 9165, Batch(micro): 9165, Batch (considering grad accum): 1145,  Loss: 5.8538, Time: 3.46s, Token/s: 147.79
Epoch: 0, Step: 9166, Batch(micro): 9166, Batch (considering grad accum): 1145,  Loss: 6.8586, Time: 3.47s, Token/s: 147.36
Epoch: 0, Step: 9167, Batch(micro): 9167, Batch (considering grad accum): 1145,  Loss: 6.1961, Time: 18.76s, Token/s: 27.29
Epoch: 0, Step: 9168, Batch(micro): 9168, Batch (considering grad accum): 1146,  Loss: 6.1492, Time: 7.04s, Token/s: 72.68
Epoch: 0, Step: 9169, Batch(micro): 9169, Batch (considering grad accum): 1146,  Loss: 6.1413, Time: 3.88s, Token/s: 132.04
Epoch: 0, Step: 9170, Batch(micro): 9170, Batch (considering grad accum): 1146,  Loss: 6.1701, Time: 3.69s, Token/s: 138.58
Epoch: 0, Step: 9171, Batch(micro): 9171, Batch (considering grad accum): 1146,  Loss: 5.7267, Time: 3.83s, Token/s: 133.56
Epoch: 0, Step: 9172, Batch(micro): 9172, Batch (considering grad accum): 1146,  Loss: 5.9803, Time: 3.56s, Token/s: 143.72
Epoch: 0, Step: 9173, Batch(micro): 9173, Batch (considering grad accum): 1146,  Loss: 6.4106, Time: 3.27s, Token/s: 156.43
Epoch: 0, Step: 9174, Batch(micro): 9174, Batch (considering grad accum): 1146,  Loss: 5.7951, Time: 3.24s, Token/s: 157.80
Epoch: 0, Step: 9175, Batch(micro): 9175, Batch (considering grad accum): 1146,  Loss: 5.7846, Time: 18.17s, Token/s: 28.17
Epoch: 0, Step: 9176, Batch(micro): 9176, Batch (considering grad accum): 1147,  Loss: 5.5802, Time: 6.13s, Token/s: 83.53
Epoch: 0, Step: 9177, Batch(micro): 9177, Batch (considering grad accum): 1147,  Loss: 5.6703, Time: 3.43s, Token/s: 149.18
Epoch: 0, Step: 9178, Batch(micro): 9178, Batch (considering grad accum): 1147,  Loss: 6.1886, Time: 3.51s, Token/s: 145.87
Epoch: 0, Step: 9179, Batch(micro): 9179, Batch (considering grad accum): 1147,  Loss: 6.8771, Time: 3.49s, Token/s: 146.57
Epoch: 0, Step: 9180, Batch(micro): 9180, Batch (considering grad accum): 1147,  Loss: 5.8861, Time: 3.74s, Token/s: 136.81
Epoch: 0, Step: 9181, Batch(micro): 9181, Batch (considering grad accum): 1147,  Loss: 5.9136, Time: 3.51s, Token/s: 145.70
Epoch: 0, Step: 9182, Batch(micro): 9182, Batch (considering grad accum): 1147,  Loss: 5.9724, Time: 3.26s, Token/s: 157.05
Epoch: 0, Step: 9183, Batch(micro): 9183, Batch (considering grad accum): 1147,  Loss: 6.6379, Time: 18.37s, Token/s: 27.87
Epoch: 0, Step: 9184, Batch(micro): 9184, Batch (considering grad accum): 1148,  Loss: 6.7698, Time: 5.22s, Token/s: 98.12
Epoch: 0, Step: 9185, Batch(micro): 9185, Batch (considering grad accum): 1148,  Loss: 5.7942, Time: 3.97s, Token/s: 129.03
Epoch: 0, Step: 9186, Batch(micro): 9186, Batch (considering grad accum): 1148,  Loss: 6.4199, Time: 3.47s, Token/s: 147.50
Epoch: 0, Step: 9187, Batch(micro): 9187, Batch (considering grad accum): 1148,  Loss: 6.8559, Time: 3.42s, Token/s: 149.79
Epoch: 0, Step: 9188, Batch(micro): 9188, Batch (considering grad accum): 1148,  Loss: 6.3526, Time: 3.41s, Token/s: 150.05
Epoch: 0, Step: 9189, Batch(micro): 9189, Batch (considering grad accum): 1148,  Loss: 5.9288, Time: 3.37s, Token/s: 151.90
Epoch: 0, Step: 9190, Batch(micro): 9190, Batch (considering grad accum): 1148,  Loss: 5.2864, Time: 3.35s, Token/s: 152.86
Epoch: 0, Step: 9191, Batch(micro): 9191, Batch (considering grad accum): 1148,  Loss: 5.9101, Time: 20.24s, Token/s: 25.29
Epoch: 0, Step: 9192, Batch(micro): 9192, Batch (considering grad accum): 1149,  Loss: 4.7359, Time: 6.99s, Token/s: 73.24
Epoch: 0, Step: 9193, Batch(micro): 9193, Batch (considering grad accum): 1149,  Loss: 6.2869, Time: 4.35s, Token/s: 117.70
Epoch: 0, Step: 9194, Batch(micro): 9194, Batch (considering grad accum): 1149,  Loss: 6.1464, Time: 3.38s, Token/s: 151.45
Epoch: 0, Step: 9195, Batch(micro): 9195, Batch (considering grad accum): 1149,  Loss: 5.4804, Time: 3.35s, Token/s: 153.00
Epoch: 0, Step: 9196, Batch(micro): 9196, Batch (considering grad accum): 1149,  Loss: 5.5148, Time: 3.54s, Token/s: 144.81
Epoch: 0, Step: 9197, Batch(micro): 9197, Batch (considering grad accum): 1149,  Loss: 5.7132, Time: 3.39s, Token/s: 151.24
Epoch: 0, Step: 9198, Batch(micro): 9198, Batch (considering grad accum): 1149,  Loss: 5.5226, Time: 3.49s, Token/s: 146.80
Epoch: 0, Step: 9199, Batch(micro): 9199, Batch (considering grad accum): 1149,  Loss: 6.2987, Time: 21.07s, Token/s: 24.30
Updating MLP bias
Epoch: 0, Step: 9200, Batch(micro): 9200, Batch (considering grad accum): 1150,  Loss: 6.4314, Time: 9.34s, Token/s: 54.80
Epoch: 0, Step: 9201, Batch(micro): 9201, Batch (considering grad accum): 1150,  Loss: 5.6079, Time: 3.53s, Token/s: 145.13
Epoch: 0, Step: 9202, Batch(micro): 9202, Batch (considering grad accum): 1150,  Loss: 5.9585, Time: 3.49s, Token/s: 146.81
Epoch: 0, Step: 9203, Batch(micro): 9203, Batch (considering grad accum): 1150,  Loss: 5.6779, Time: 3.62s, Token/s: 141.59
Epoch: 0, Step: 9204, Batch(micro): 9204, Batch (considering grad accum): 1150,  Loss: 5.8035, Time: 3.43s, Token/s: 149.14
Epoch: 0, Step: 9205, Batch(micro): 9205, Batch (considering grad accum): 1150,  Loss: 6.2470, Time: 3.40s, Token/s: 150.68
Epoch: 0, Step: 9206, Batch(micro): 9206, Batch (considering grad accum): 1150,  Loss: 6.2219, Time: 3.46s, Token/s: 148.11
Epoch: 0, Step: 9207, Batch(micro): 9207, Batch (considering grad accum): 1150,  Loss: 7.2321, Time: 24.62s, Token/s: 20.80
Epoch: 0, Step: 9208, Batch(micro): 9208, Batch (considering grad accum): 1151,  Loss: 5.6634, Time: 6.67s, Token/s: 76.71
Epoch: 0, Step: 9209, Batch(micro): 9209, Batch (considering grad accum): 1151,  Loss: 5.9294, Time: 3.90s, Token/s: 131.21
Epoch: 0, Step: 9210, Batch(micro): 9210, Batch (considering grad accum): 1151,  Loss: 5.9152, Time: 3.27s, Token/s: 156.45
Epoch: 0, Step: 9211, Batch(micro): 9211, Batch (considering grad accum): 1151,  Loss: 5.7239, Time: 3.41s, Token/s: 150.32
Epoch: 0, Step: 9212, Batch(micro): 9212, Batch (considering grad accum): 1151,  Loss: 5.7526, Time: 3.44s, Token/s: 148.62
Epoch: 0, Step: 9213, Batch(micro): 9213, Batch (considering grad accum): 1151,  Loss: 5.7381, Time: 3.29s, Token/s: 155.41
Epoch: 0, Step: 9214, Batch(micro): 9214, Batch (considering grad accum): 1151,  Loss: 5.7989, Time: 3.45s, Token/s: 148.25
Epoch: 0, Step: 9215, Batch(micro): 9215, Batch (considering grad accum): 1151,  Loss: 5.7926, Time: 23.38s, Token/s: 21.90
Epoch: 0, Step: 9216, Batch(micro): 9216, Batch (considering grad accum): 1152,  Loss: 6.1394, Time: 8.89s, Token/s: 57.59
Epoch: 0, Step: 9217, Batch(micro): 9217, Batch (considering grad accum): 1152,  Loss: 5.6932, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 9218, Batch(micro): 9218, Batch (considering grad accum): 1152,  Loss: 6.2205, Time: 3.42s, Token/s: 149.82
Epoch: 0, Step: 9219, Batch(micro): 9219, Batch (considering grad accum): 1152,  Loss: 6.5826, Time: 3.42s, Token/s: 149.55
Epoch: 0, Step: 9220, Batch(micro): 9220, Batch (considering grad accum): 1152,  Loss: 5.8896, Time: 3.27s, Token/s: 156.45
Epoch: 0, Step: 9221, Batch(micro): 9221, Batch (considering grad accum): 1152,  Loss: 5.9029, Time: 3.37s, Token/s: 151.78
Epoch: 0, Step: 9222, Batch(micro): 9222, Batch (considering grad accum): 1152,  Loss: 5.7257, Time: 3.36s, Token/s: 152.39
Epoch: 0, Step: 9223, Batch(micro): 9223, Batch (considering grad accum): 1152,  Loss: 5.7947, Time: 21.40s, Token/s: 23.92
Epoch: 0, Step: 9224, Batch(micro): 9224, Batch (considering grad accum): 1153,  Loss: 5.9371, Time: 6.70s, Token/s: 76.47
Epoch: 0, Step: 9225, Batch(micro): 9225, Batch (considering grad accum): 1153,  Loss: 5.5875, Time: 3.91s, Token/s: 131.07
Epoch: 0, Step: 9226, Batch(micro): 9226, Batch (considering grad accum): 1153,  Loss: 5.4936, Time: 3.71s, Token/s: 137.99
Epoch: 0, Step: 9227, Batch(micro): 9227, Batch (considering grad accum): 1153,  Loss: 6.0482, Time: 3.62s, Token/s: 141.30
Epoch: 0, Step: 9228, Batch(micro): 9228, Batch (considering grad accum): 1153,  Loss: 6.4246, Time: 3.67s, Token/s: 139.38
Epoch: 0, Step: 9229, Batch(micro): 9229, Batch (considering grad accum): 1153,  Loss: 6.8589, Time: 3.44s, Token/s: 149.00
Epoch: 0, Step: 9230, Batch(micro): 9230, Batch (considering grad accum): 1153,  Loss: 6.1949, Time: 3.50s, Token/s: 146.29
Epoch: 0, Step: 9231, Batch(micro): 9231, Batch (considering grad accum): 1153,  Loss: 5.5684, Time: 21.19s, Token/s: 24.16
Epoch: 0, Step: 9232, Batch(micro): 9232, Batch (considering grad accum): 1154,  Loss: 5.9800, Time: 7.53s, Token/s: 68.00
Epoch: 0, Step: 9233, Batch(micro): 9233, Batch (considering grad accum): 1154,  Loss: 5.7692, Time: 3.78s, Token/s: 135.29
Epoch: 0, Step: 9234, Batch(micro): 9234, Batch (considering grad accum): 1154,  Loss: 6.2890, Time: 3.37s, Token/s: 151.77
Epoch: 0, Step: 9235, Batch(micro): 9235, Batch (considering grad accum): 1154,  Loss: 5.9204, Time: 3.52s, Token/s: 145.62
Epoch: 0, Step: 9236, Batch(micro): 9236, Batch (considering grad accum): 1154,  Loss: 5.6856, Time: 4.47s, Token/s: 114.47
Epoch: 0, Step: 9237, Batch(micro): 9237, Batch (considering grad accum): 1154,  Loss: 5.8404, Time: 3.50s, Token/s: 146.41
Epoch: 0, Step: 9238, Batch(micro): 9238, Batch (considering grad accum): 1154,  Loss: 6.1567, Time: 3.62s, Token/s: 141.30
Epoch: 0, Step: 9239, Batch(micro): 9239, Batch (considering grad accum): 1154,  Loss: 5.8683, Time: 25.49s, Token/s: 20.08
Epoch: 0, Step: 9240, Batch(micro): 9240, Batch (considering grad accum): 1155,  Loss: 5.9493, Time: 7.08s, Token/s: 72.28
Epoch: 0, Step: 9241, Batch(micro): 9241, Batch (considering grad accum): 1155,  Loss: 4.9342, Time: 3.91s, Token/s: 131.00
Epoch: 0, Step: 9242, Batch(micro): 9242, Batch (considering grad accum): 1155,  Loss: 5.8501, Time: 3.45s, Token/s: 148.30
Epoch: 0, Step: 9243, Batch(micro): 9243, Batch (considering grad accum): 1155,  Loss: 5.6103, Time: 3.30s, Token/s: 155.09
Epoch: 0, Step: 9244, Batch(micro): 9244, Batch (considering grad accum): 1155,  Loss: 5.2079, Time: 3.25s, Token/s: 157.44
Epoch: 0, Step: 9245, Batch(micro): 9245, Batch (considering grad accum): 1155,  Loss: 5.8244, Time: 3.39s, Token/s: 150.85
Epoch: 0, Step: 9246, Batch(micro): 9246, Batch (considering grad accum): 1155,  Loss: 6.7819, Time: 3.61s, Token/s: 141.80
Epoch: 0, Step: 9247, Batch(micro): 9247, Batch (considering grad accum): 1155,  Loss: 6.1080, Time: 27.09s, Token/s: 18.90
Epoch: 0, Step: 9248, Batch(micro): 9248, Batch (considering grad accum): 1156,  Loss: 6.0833, Time: 7.34s, Token/s: 69.78
Epoch: 0, Step: 9249, Batch(micro): 9249, Batch (considering grad accum): 1156,  Loss: 5.8254, Time: 4.06s, Token/s: 126.19
Epoch: 0, Step: 9250, Batch(micro): 9250, Batch (considering grad accum): 1156,  Loss: 5.4710, Time: 3.55s, Token/s: 144.18
Epoch: 0, Step: 9251, Batch(micro): 9251, Batch (considering grad accum): 1156,  Loss: 5.6922, Time: 3.33s, Token/s: 153.53
Epoch: 0, Step: 9252, Batch(micro): 9252, Batch (considering grad accum): 1156,  Loss: 5.9402, Time: 3.54s, Token/s: 144.63
Epoch: 0, Step: 9253, Batch(micro): 9253, Batch (considering grad accum): 1156,  Loss: 5.9987, Time: 3.61s, Token/s: 141.91
Epoch: 0, Step: 9254, Batch(micro): 9254, Batch (considering grad accum): 1156,  Loss: 5.9459, Time: 3.38s, Token/s: 151.70
Epoch: 0, Step: 9255, Batch(micro): 9255, Batch (considering grad accum): 1156,  Loss: 6.1728, Time: 25.03s, Token/s: 20.46
Epoch: 0, Step: 9256, Batch(micro): 9256, Batch (considering grad accum): 1157,  Loss: 5.9947, Time: 6.33s, Token/s: 80.82
Epoch: 0, Step: 9257, Batch(micro): 9257, Batch (considering grad accum): 1157,  Loss: 6.0370, Time: 3.91s, Token/s: 130.81
Epoch: 0, Step: 9258, Batch(micro): 9258, Batch (considering grad accum): 1157,  Loss: 6.1012, Time: 3.98s, Token/s: 128.78
Epoch: 0, Step: 9259, Batch(micro): 9259, Batch (considering grad accum): 1157,  Loss: 6.5195, Time: 3.24s, Token/s: 157.86
Epoch: 0, Step: 9260, Batch(micro): 9260, Batch (considering grad accum): 1157,  Loss: 5.5846, Time: 3.28s, Token/s: 156.08
Epoch: 0, Step: 9261, Batch(micro): 9261, Batch (considering grad accum): 1157,  Loss: 5.3492, Time: 3.38s, Token/s: 151.56
Epoch: 0, Step: 9262, Batch(micro): 9262, Batch (considering grad accum): 1157,  Loss: 5.5933, Time: 3.53s, Token/s: 145.04
Epoch: 0, Step: 9263, Batch(micro): 9263, Batch (considering grad accum): 1157,  Loss: 5.9407, Time: 22.72s, Token/s: 22.53
Epoch: 0, Step: 9264, Batch(micro): 9264, Batch (considering grad accum): 1158,  Loss: 5.3477, Time: 7.53s, Token/s: 68.01
Epoch: 0, Step: 9265, Batch(micro): 9265, Batch (considering grad accum): 1158,  Loss: 5.9004, Time: 3.75s, Token/s: 136.36
Epoch: 0, Step: 9266, Batch(micro): 9266, Batch (considering grad accum): 1158,  Loss: 5.6862, Time: 3.62s, Token/s: 141.39
Epoch: 0, Step: 9267, Batch(micro): 9267, Batch (considering grad accum): 1158,  Loss: 5.8554, Time: 3.65s, Token/s: 140.34
Epoch: 0, Step: 9268, Batch(micro): 9268, Batch (considering grad accum): 1158,  Loss: 5.7047, Time: 3.51s, Token/s: 146.05
Epoch: 0, Step: 9269, Batch(micro): 9269, Batch (considering grad accum): 1158,  Loss: 5.6238, Time: 3.49s, Token/s: 146.65
Epoch: 0, Step: 9270, Batch(micro): 9270, Batch (considering grad accum): 1158,  Loss: 6.2527, Time: 3.35s, Token/s: 152.92
Epoch: 0, Step: 9271, Batch(micro): 9271, Batch (considering grad accum): 1158,  Loss: 5.9373, Time: 23.52s, Token/s: 21.77
Epoch: 0, Step: 9272, Batch(micro): 9272, Batch (considering grad accum): 1159,  Loss: 5.3521, Time: 6.85s, Token/s: 74.78
Epoch: 0, Step: 9273, Batch(micro): 9273, Batch (considering grad accum): 1159,  Loss: 5.0678, Time: 3.74s, Token/s: 136.82
Epoch: 0, Step: 9274, Batch(micro): 9274, Batch (considering grad accum): 1159,  Loss: 5.6625, Time: 3.29s, Token/s: 155.64
Epoch: 0, Step: 9275, Batch(micro): 9275, Batch (considering grad accum): 1159,  Loss: 5.7849, Time: 3.23s, Token/s: 158.60
Epoch: 0, Step: 9276, Batch(micro): 9276, Batch (considering grad accum): 1159,  Loss: 5.8025, Time: 3.38s, Token/s: 151.54
Epoch: 0, Step: 9277, Batch(micro): 9277, Batch (considering grad accum): 1159,  Loss: 5.8822, Time: 3.62s, Token/s: 141.31
Epoch: 0, Step: 9278, Batch(micro): 9278, Batch (considering grad accum): 1159,  Loss: 6.1244, Time: 3.75s, Token/s: 136.71
Epoch: 0, Step: 9279, Batch(micro): 9279, Batch (considering grad accum): 1159,  Loss: 5.9295, Time: 25.76s, Token/s: 19.87
Epoch: 0, Step: 9280, Batch(micro): 9280, Batch (considering grad accum): 1160,  Loss: 5.7228, Time: 8.08s, Token/s: 63.36
Epoch: 0, Step: 9281, Batch(micro): 9281, Batch (considering grad accum): 1160,  Loss: 6.3576, Time: 4.31s, Token/s: 118.79
Epoch: 0, Step: 9282, Batch(micro): 9282, Batch (considering grad accum): 1160,  Loss: 5.6592, Time: 3.81s, Token/s: 134.46
Epoch: 0, Step: 9283, Batch(micro): 9283, Batch (considering grad accum): 1160,  Loss: 6.1071, Time: 3.85s, Token/s: 133.03
Epoch: 0, Step: 9284, Batch(micro): 9284, Batch (considering grad accum): 1160,  Loss: 6.0999, Time: 3.32s, Token/s: 154.33
Epoch: 0, Step: 9285, Batch(micro): 9285, Batch (considering grad accum): 1160,  Loss: 5.6392, Time: 3.23s, Token/s: 158.61
Epoch: 0, Step: 9286, Batch(micro): 9286, Batch (considering grad accum): 1160,  Loss: 5.7803, Time: 3.37s, Token/s: 151.73
Epoch: 0, Step: 9287, Batch(micro): 9287, Batch (considering grad accum): 1160,  Loss: 6.4490, Time: 25.86s, Token/s: 19.80
Epoch: 0, Step: 9288, Batch(micro): 9288, Batch (considering grad accum): 1161,  Loss: 6.6485, Time: 6.96s, Token/s: 73.60
Epoch: 0, Step: 9289, Batch(micro): 9289, Batch (considering grad accum): 1161,  Loss: 5.7060, Time: 4.12s, Token/s: 124.12
Epoch: 0, Step: 9290, Batch(micro): 9290, Batch (considering grad accum): 1161,  Loss: 5.9012, Time: 3.78s, Token/s: 135.48
Epoch: 0, Step: 9291, Batch(micro): 9291, Batch (considering grad accum): 1161,  Loss: 6.3269, Time: 3.39s, Token/s: 151.22
Epoch: 0, Step: 9292, Batch(micro): 9292, Batch (considering grad accum): 1161,  Loss: 5.6414, Time: 3.35s, Token/s: 153.02
Epoch: 0, Step: 9293, Batch(micro): 9293, Batch (considering grad accum): 1161,  Loss: 5.4640, Time: 3.45s, Token/s: 148.22
Epoch: 0, Step: 9294, Batch(micro): 9294, Batch (considering grad accum): 1161,  Loss: 6.3011, Time: 3.37s, Token/s: 151.88
Epoch: 0, Step: 9295, Batch(micro): 9295, Batch (considering grad accum): 1161,  Loss: 6.9142, Time: 23.36s, Token/s: 21.92
Epoch: 0, Step: 9296, Batch(micro): 9296, Batch (considering grad accum): 1162,  Loss: 6.3493, Time: 7.04s, Token/s: 72.75
Epoch: 0, Step: 9297, Batch(micro): 9297, Batch (considering grad accum): 1162,  Loss: 5.6570, Time: 3.73s, Token/s: 137.08
Epoch: 0, Step: 9298, Batch(micro): 9298, Batch (considering grad accum): 1162,  Loss: 6.1374, Time: 3.44s, Token/s: 148.67
Epoch: 0, Step: 9299, Batch(micro): 9299, Batch (considering grad accum): 1162,  Loss: 6.0550, Time: 3.40s, Token/s: 150.63
Updating MLP bias
Epoch: 0, Step: 9300, Batch(micro): 9300, Batch (considering grad accum): 1162,  Loss: 5.3448, Time: 3.28s, Token/s: 155.95
Epoch: 0, Step: 9301, Batch(micro): 9301, Batch (considering grad accum): 1162,  Loss: 5.1172, Time: 3.20s, Token/s: 160.08
Epoch: 0, Step: 9302, Batch(micro): 9302, Batch (considering grad accum): 1162,  Loss: 6.6353, Time: 3.57s, Token/s: 143.35
Epoch: 0, Step: 9303, Batch(micro): 9303, Batch (considering grad accum): 1162,  Loss: 6.3490, Time: 24.66s, Token/s: 20.76
Epoch: 0, Step: 9304, Batch(micro): 9304, Batch (considering grad accum): 1163,  Loss: 5.7604, Time: 7.01s, Token/s: 73.06
Epoch: 0, Step: 9305, Batch(micro): 9305, Batch (considering grad accum): 1163,  Loss: 5.8545, Time: 3.99s, Token/s: 128.44
Epoch: 0, Step: 9306, Batch(micro): 9306, Batch (considering grad accum): 1163,  Loss: 6.2892, Time: 3.75s, Token/s: 136.35
Epoch: 0, Step: 9307, Batch(micro): 9307, Batch (considering grad accum): 1163,  Loss: 6.3243, Time: 3.61s, Token/s: 141.79
Epoch: 0, Step: 9308, Batch(micro): 9308, Batch (considering grad accum): 1163,  Loss: 5.3934, Time: 3.24s, Token/s: 157.82
Epoch: 0, Step: 9309, Batch(micro): 9309, Batch (considering grad accum): 1163,  Loss: 5.2917, Time: 3.29s, Token/s: 155.68
Epoch: 0, Step: 9310, Batch(micro): 9310, Batch (considering grad accum): 1163,  Loss: 5.6270, Time: 3.25s, Token/s: 157.50
Epoch: 0, Step: 9311, Batch(micro): 9311, Batch (considering grad accum): 1163,  Loss: 5.5292, Time: 23.25s, Token/s: 22.02
Epoch: 0, Step: 9312, Batch(micro): 9312, Batch (considering grad accum): 1164,  Loss: 6.2454, Time: 5.84s, Token/s: 87.72
Epoch: 0, Step: 9313, Batch(micro): 9313, Batch (considering grad accum): 1164,  Loss: 6.5753, Time: 4.11s, Token/s: 124.64
Epoch: 0, Step: 9314, Batch(micro): 9314, Batch (considering grad accum): 1164,  Loss: 5.9373, Time: 3.79s, Token/s: 135.03
Epoch: 0, Step: 9315, Batch(micro): 9315, Batch (considering grad accum): 1164,  Loss: 6.0179, Time: 3.45s, Token/s: 148.43
Epoch: 0, Step: 9316, Batch(micro): 9316, Batch (considering grad accum): 1164,  Loss: 5.9833, Time: 3.39s, Token/s: 150.96
Epoch: 0, Step: 9317, Batch(micro): 9317, Batch (considering grad accum): 1164,  Loss: 5.6961, Time: 3.50s, Token/s: 146.22
Epoch: 0, Step: 9318, Batch(micro): 9318, Batch (considering grad accum): 1164,  Loss: 5.7349, Time: 3.37s, Token/s: 151.93
Epoch: 0, Step: 9319, Batch(micro): 9319, Batch (considering grad accum): 1164,  Loss: 5.4940, Time: 21.17s, Token/s: 24.19
Epoch: 0, Step: 9320, Batch(micro): 9320, Batch (considering grad accum): 1165,  Loss: 5.4943, Time: 7.25s, Token/s: 70.60
Epoch: 0, Step: 9321, Batch(micro): 9321, Batch (considering grad accum): 1165,  Loss: 5.8584, Time: 3.83s, Token/s: 133.81
Epoch: 0, Step: 9322, Batch(micro): 9322, Batch (considering grad accum): 1165,  Loss: 6.1549, Time: 3.90s, Token/s: 131.15
Epoch: 0, Step: 9323, Batch(micro): 9323, Batch (considering grad accum): 1165,  Loss: 6.3933, Time: 3.49s, Token/s: 146.61
Epoch: 0, Step: 9324, Batch(micro): 9324, Batch (considering grad accum): 1165,  Loss: 5.9809, Time: 3.46s, Token/s: 147.96
Epoch: 0, Step: 9325, Batch(micro): 9325, Batch (considering grad accum): 1165,  Loss: 6.1825, Time: 3.20s, Token/s: 159.99
Epoch: 0, Step: 9326, Batch(micro): 9326, Batch (considering grad accum): 1165,  Loss: 5.9125, Time: 3.50s, Token/s: 146.27
Epoch: 0, Step: 9327, Batch(micro): 9327, Batch (considering grad accum): 1165,  Loss: 5.4736, Time: 25.76s, Token/s: 19.87
Epoch: 0, Step: 9328, Batch(micro): 9328, Batch (considering grad accum): 1166,  Loss: 6.1332, Time: 9.04s, Token/s: 56.64
Epoch: 0, Step: 9329, Batch(micro): 9329, Batch (considering grad accum): 1166,  Loss: 5.7293, Time: 3.51s, Token/s: 145.68
Epoch: 0, Step: 9330, Batch(micro): 9330, Batch (considering grad accum): 1166,  Loss: 5.6227, Time: 3.24s, Token/s: 158.18
Epoch: 0, Step: 9331, Batch(micro): 9331, Batch (considering grad accum): 1166,  Loss: 5.2345, Time: 3.21s, Token/s: 159.65
Epoch: 0, Step: 9332, Batch(micro): 9332, Batch (considering grad accum): 1166,  Loss: 5.2826, Time: 3.55s, Token/s: 144.29
Epoch: 0, Step: 9333, Batch(micro): 9333, Batch (considering grad accum): 1166,  Loss: 5.4276, Time: 3.51s, Token/s: 146.02
Epoch: 0, Step: 9334, Batch(micro): 9334, Batch (considering grad accum): 1166,  Loss: 6.1880, Time: 3.23s, Token/s: 158.71
Epoch: 0, Step: 9335, Batch(micro): 9335, Batch (considering grad accum): 1166,  Loss: 5.8375, Time: 23.65s, Token/s: 21.65
Epoch: 0, Step: 9336, Batch(micro): 9336, Batch (considering grad accum): 1167,  Loss: 6.1167, Time: 6.45s, Token/s: 79.39
Epoch: 0, Step: 9337, Batch(micro): 9337, Batch (considering grad accum): 1167,  Loss: 6.2602, Time: 3.94s, Token/s: 129.92
Epoch: 0, Step: 9338, Batch(micro): 9338, Batch (considering grad accum): 1167,  Loss: 5.3481, Time: 3.90s, Token/s: 131.35
Epoch: 0, Step: 9339, Batch(micro): 9339, Batch (considering grad accum): 1167,  Loss: 5.2327, Time: 3.25s, Token/s: 157.34
Epoch: 0, Step: 9340, Batch(micro): 9340, Batch (considering grad accum): 1167,  Loss: 5.7872, Time: 3.44s, Token/s: 148.97
Epoch: 0, Step: 9341, Batch(micro): 9341, Batch (considering grad accum): 1167,  Loss: 6.0789, Time: 3.37s, Token/s: 152.00
Epoch: 0, Step: 9342, Batch(micro): 9342, Batch (considering grad accum): 1167,  Loss: 5.9726, Time: 3.78s, Token/s: 135.38
Epoch: 0, Step: 9343, Batch(micro): 9343, Batch (considering grad accum): 1167,  Loss: 5.4178, Time: 23.91s, Token/s: 21.41
Epoch: 0, Step: 9344, Batch(micro): 9344, Batch (considering grad accum): 1168,  Loss: 6.0980, Time: 7.16s, Token/s: 71.50
Epoch: 0, Step: 9345, Batch(micro): 9345, Batch (considering grad accum): 1168,  Loss: 4.8906, Time: 3.74s, Token/s: 136.98
Epoch: 0, Step: 9346, Batch(micro): 9346, Batch (considering grad accum): 1168,  Loss: 4.9554, Time: 3.55s, Token/s: 144.30
Epoch: 0, Step: 9347, Batch(micro): 9347, Batch (considering grad accum): 1168,  Loss: 5.7009, Time: 3.52s, Token/s: 145.47
Epoch: 0, Step: 9348, Batch(micro): 9348, Batch (considering grad accum): 1168,  Loss: 5.5991, Time: 3.32s, Token/s: 154.00
Epoch: 0, Step: 9349, Batch(micro): 9349, Batch (considering grad accum): 1168,  Loss: 5.3533, Time: 3.31s, Token/s: 154.48
Epoch: 0, Step: 9350, Batch(micro): 9350, Batch (considering grad accum): 1168,  Loss: 5.7577, Time: 3.58s, Token/s: 143.02
Epoch: 0, Step: 9351, Batch(micro): 9351, Batch (considering grad accum): 1168,  Loss: 5.6905, Time: 18.28s, Token/s: 28.01
Epoch: 0, Step: 9352, Batch(micro): 9352, Batch (considering grad accum): 1169,  Loss: 6.0343, Time: 5.98s, Token/s: 85.63
Epoch: 0, Step: 9353, Batch(micro): 9353, Batch (considering grad accum): 1169,  Loss: 5.4988, Time: 4.46s, Token/s: 114.88
Epoch: 0, Step: 9354, Batch(micro): 9354, Batch (considering grad accum): 1169,  Loss: 6.0281, Time: 3.51s, Token/s: 145.67
Epoch: 0, Step: 9355, Batch(micro): 9355, Batch (considering grad accum): 1169,  Loss: 5.9533, Time: 3.35s, Token/s: 152.89
Epoch: 0, Step: 9356, Batch(micro): 9356, Batch (considering grad accum): 1169,  Loss: 5.9394, Time: 3.52s, Token/s: 145.32
Epoch: 0, Step: 9357, Batch(micro): 9357, Batch (considering grad accum): 1169,  Loss: 5.3607, Time: 3.40s, Token/s: 150.39
Epoch: 0, Step: 9358, Batch(micro): 9358, Batch (considering grad accum): 1169,  Loss: 5.5893, Time: 3.70s, Token/s: 138.32
Epoch: 0, Step: 9359, Batch(micro): 9359, Batch (considering grad accum): 1169,  Loss: 7.1509, Time: 19.32s, Token/s: 26.50
Epoch: 0, Step: 9360, Batch(micro): 9360, Batch (considering grad accum): 1170,  Loss: 5.8588, Time: 7.11s, Token/s: 72.04
Epoch: 0, Step: 9361, Batch(micro): 9361, Batch (considering grad accum): 1170,  Loss: 5.9326, Time: 3.83s, Token/s: 133.76
Epoch: 0, Step: 9362, Batch(micro): 9362, Batch (considering grad accum): 1170,  Loss: 5.8190, Time: 3.18s, Token/s: 161.11
Epoch: 0, Step: 9363, Batch(micro): 9363, Batch (considering grad accum): 1170,  Loss: 5.9601, Time: 3.41s, Token/s: 150.13
Epoch: 0, Step: 9364, Batch(micro): 9364, Batch (considering grad accum): 1170,  Loss: 6.2888, Time: 3.21s, Token/s: 159.45
Epoch: 0, Step: 9365, Batch(micro): 9365, Batch (considering grad accum): 1170,  Loss: 5.7374, Time: 3.17s, Token/s: 161.33
Epoch: 0, Step: 9366, Batch(micro): 9366, Batch (considering grad accum): 1170,  Loss: 6.3439, Time: 3.13s, Token/s: 163.79
Epoch: 0, Step: 9367, Batch(micro): 9367, Batch (considering grad accum): 1170,  Loss: 5.8864, Time: 17.93s, Token/s: 28.56
Epoch: 0, Step: 9368, Batch(micro): 9368, Batch (considering grad accum): 1171,  Loss: 5.4454, Time: 6.76s, Token/s: 75.75
Epoch: 0, Step: 9369, Batch(micro): 9369, Batch (considering grad accum): 1171,  Loss: 5.2348, Time: 3.80s, Token/s: 134.82
Epoch: 0, Step: 9370, Batch(micro): 9370, Batch (considering grad accum): 1171,  Loss: 5.5868, Time: 3.56s, Token/s: 143.84
Epoch: 0, Step: 9371, Batch(micro): 9371, Batch (considering grad accum): 1171,  Loss: 5.5387, Time: 3.27s, Token/s: 156.61
Epoch: 0, Step: 9372, Batch(micro): 9372, Batch (considering grad accum): 1171,  Loss: 5.8494, Time: 3.23s, Token/s: 158.60
Epoch: 0, Step: 9373, Batch(micro): 9373, Batch (considering grad accum): 1171,  Loss: 5.5859, Time: 2.94s, Token/s: 174.25
Epoch: 0, Step: 9374, Batch(micro): 9374, Batch (considering grad accum): 1171,  Loss: 6.4406, Time: 3.06s, Token/s: 167.57
Epoch: 0, Step: 9375, Batch(micro): 9375, Batch (considering grad accum): 1171,  Loss: 6.4276, Time: 18.63s, Token/s: 27.49
Epoch: 0, Step: 9376, Batch(micro): 9376, Batch (considering grad accum): 1172,  Loss: 5.9932, Time: 6.94s, Token/s: 73.75
Epoch: 0, Step: 9377, Batch(micro): 9377, Batch (considering grad accum): 1172,  Loss: 6.3933, Time: 3.72s, Token/s: 137.77
Epoch: 0, Step: 9378, Batch(micro): 9378, Batch (considering grad accum): 1172,  Loss: 5.3381, Time: 3.30s, Token/s: 155.23
Epoch: 0, Step: 9379, Batch(micro): 9379, Batch (considering grad accum): 1172,  Loss: 6.0978, Time: 3.26s, Token/s: 157.16
Epoch: 0, Step: 9380, Batch(micro): 9380, Batch (considering grad accum): 1172,  Loss: 5.9248, Time: 2.88s, Token/s: 177.83
Epoch: 0, Step: 9381, Batch(micro): 9381, Batch (considering grad accum): 1172,  Loss: 6.2220, Time: 3.71s, Token/s: 138.13
Epoch: 0, Step: 9382, Batch(micro): 9382, Batch (considering grad accum): 1172,  Loss: 5.0297, Time: 2.73s, Token/s: 187.43
Epoch: 0, Step: 9383, Batch(micro): 9383, Batch (considering grad accum): 1172,  Loss: 5.7426, Time: 18.56s, Token/s: 27.59
Epoch: 0, Step: 9384, Batch(micro): 9384, Batch (considering grad accum): 1173,  Loss: 5.6217, Time: 7.06s, Token/s: 72.49
Epoch: 0, Step: 9385, Batch(micro): 9385, Batch (considering grad accum): 1173,  Loss: 6.5222, Time: 4.10s, Token/s: 124.98
Epoch: 0, Step: 9386, Batch(micro): 9386, Batch (considering grad accum): 1173,  Loss: 5.4926, Time: 3.72s, Token/s: 137.67
Epoch: 0, Step: 9387, Batch(micro): 9387, Batch (considering grad accum): 1173,  Loss: 5.9552, Time: 3.59s, Token/s: 142.72
Epoch: 0, Step: 9388, Batch(micro): 9388, Batch (considering grad accum): 1173,  Loss: 5.6095, Time: 3.50s, Token/s: 146.13
Epoch: 0, Step: 9389, Batch(micro): 9389, Batch (considering grad accum): 1173,  Loss: 5.5788, Time: 3.43s, Token/s: 149.46
Epoch: 0, Step: 9390, Batch(micro): 9390, Batch (considering grad accum): 1173,  Loss: 5.6593, Time: 3.53s, Token/s: 144.90
Epoch: 0, Step: 9391, Batch(micro): 9391, Batch (considering grad accum): 1173,  Loss: 6.0338, Time: 18.97s, Token/s: 26.99
Epoch: 0, Step: 9392, Batch(micro): 9392, Batch (considering grad accum): 1174,  Loss: 5.5519, Time: 6.92s, Token/s: 74.01
Epoch: 0, Step: 9393, Batch(micro): 9393, Batch (considering grad accum): 1174,  Loss: 6.1997, Time: 3.80s, Token/s: 134.58
Epoch: 0, Step: 9394, Batch(micro): 9394, Batch (considering grad accum): 1174,  Loss: 5.2480, Time: 3.46s, Token/s: 148.08
Epoch: 0, Step: 9395, Batch(micro): 9395, Batch (considering grad accum): 1174,  Loss: 5.6035, Time: 3.36s, Token/s: 152.22
Epoch: 0, Step: 9396, Batch(micro): 9396, Batch (considering grad accum): 1174,  Loss: 5.4982, Time: 3.40s, Token/s: 150.61
Epoch: 0, Step: 9397, Batch(micro): 9397, Batch (considering grad accum): 1174,  Loss: 5.8412, Time: 3.57s, Token/s: 143.47
Epoch: 0, Step: 9398, Batch(micro): 9398, Batch (considering grad accum): 1174,  Loss: 5.8837, Time: 3.55s, Token/s: 144.25
Epoch: 0, Step: 9399, Batch(micro): 9399, Batch (considering grad accum): 1174,  Loss: 5.8066, Time: 18.86s, Token/s: 27.14
Updating MLP bias
Epoch: 0, Step: 9400, Batch(micro): 9400, Batch (considering grad accum): 1175,  Loss: 5.1767, Time: 7.10s, Token/s: 72.08
Epoch: 0, Step: 9401, Batch(micro): 9401, Batch (considering grad accum): 1175,  Loss: 5.5244, Time: 3.62s, Token/s: 141.59
Epoch: 0, Step: 9402, Batch(micro): 9402, Batch (considering grad accum): 1175,  Loss: 6.0290, Time: 3.60s, Token/s: 142.38
Epoch: 0, Step: 9403, Batch(micro): 9403, Batch (considering grad accum): 1175,  Loss: 5.5480, Time: 3.38s, Token/s: 151.46
Epoch: 0, Step: 9404, Batch(micro): 9404, Batch (considering grad accum): 1175,  Loss: 6.7479, Time: 3.63s, Token/s: 140.99
Epoch: 0, Step: 9405, Batch(micro): 9405, Batch (considering grad accum): 1175,  Loss: 6.6806, Time: 3.40s, Token/s: 150.44
Epoch: 0, Step: 9406, Batch(micro): 9406, Batch (considering grad accum): 1175,  Loss: 6.6883, Time: 3.46s, Token/s: 148.06
Epoch: 0, Step: 9407, Batch(micro): 9407, Batch (considering grad accum): 1175,  Loss: 6.2570, Time: 18.29s, Token/s: 27.99
Epoch: 0, Step: 9408, Batch(micro): 9408, Batch (considering grad accum): 1176,  Loss: 6.2020, Time: 5.49s, Token/s: 93.28
Epoch: 0, Step: 9409, Batch(micro): 9409, Batch (considering grad accum): 1176,  Loss: 6.0050, Time: 3.60s, Token/s: 142.07
Epoch: 0, Step: 9410, Batch(micro): 9410, Batch (considering grad accum): 1176,  Loss: 5.9158, Time: 3.67s, Token/s: 139.55
Epoch: 0, Step: 9411, Batch(micro): 9411, Batch (considering grad accum): 1176,  Loss: 6.4130, Time: 3.54s, Token/s: 144.75
Epoch: 0, Step: 9412, Batch(micro): 9412, Batch (considering grad accum): 1176,  Loss: 5.8856, Time: 3.49s, Token/s: 146.75
Epoch: 0, Step: 9413, Batch(micro): 9413, Batch (considering grad accum): 1176,  Loss: 6.2441, Time: 3.59s, Token/s: 142.74
Epoch: 0, Step: 9414, Batch(micro): 9414, Batch (considering grad accum): 1176,  Loss: 5.8678, Time: 3.41s, Token/s: 150.09
Epoch: 0, Step: 9415, Batch(micro): 9415, Batch (considering grad accum): 1176,  Loss: 6.2184, Time: 19.03s, Token/s: 26.91
Epoch: 0, Step: 9416, Batch(micro): 9416, Batch (considering grad accum): 1177,  Loss: 5.9130, Time: 6.69s, Token/s: 76.54
Epoch: 0, Step: 9417, Batch(micro): 9417, Batch (considering grad accum): 1177,  Loss: 6.2101, Time: 4.44s, Token/s: 115.20
Epoch: 0, Step: 9418, Batch(micro): 9418, Batch (considering grad accum): 1177,  Loss: 5.4696, Time: 3.63s, Token/s: 141.10
Epoch: 0, Step: 9419, Batch(micro): 9419, Batch (considering grad accum): 1177,  Loss: 5.3874, Time: 3.50s, Token/s: 146.20
Epoch: 0, Step: 9420, Batch(micro): 9420, Batch (considering grad accum): 1177,  Loss: 5.5807, Time: 3.81s, Token/s: 134.55
Epoch: 0, Step: 9421, Batch(micro): 9421, Batch (considering grad accum): 1177,  Loss: 5.9376, Time: 3.52s, Token/s: 145.49
Epoch: 0, Step: 9422, Batch(micro): 9422, Batch (considering grad accum): 1177,  Loss: 5.7516, Time: 3.49s, Token/s: 146.55
Epoch: 0, Step: 9423, Batch(micro): 9423, Batch (considering grad accum): 1177,  Loss: 6.8034, Time: 18.53s, Token/s: 27.63
Epoch: 0, Step: 9424, Batch(micro): 9424, Batch (considering grad accum): 1178,  Loss: 5.5527, Time: 6.51s, Token/s: 78.70
Epoch: 0, Step: 9425, Batch(micro): 9425, Batch (considering grad accum): 1178,  Loss: 5.9136, Time: 4.14s, Token/s: 123.60
Epoch: 0, Step: 9426, Batch(micro): 9426, Batch (considering grad accum): 1178,  Loss: 5.6841, Time: 3.51s, Token/s: 145.96
Epoch: 0, Step: 9427, Batch(micro): 9427, Batch (considering grad accum): 1178,  Loss: 5.4996, Time: 3.50s, Token/s: 146.14
Epoch: 0, Step: 9428, Batch(micro): 9428, Batch (considering grad accum): 1178,  Loss: 6.3545, Time: 3.60s, Token/s: 142.10
Epoch: 0, Step: 9429, Batch(micro): 9429, Batch (considering grad accum): 1178,  Loss: 6.5872, Time: 3.38s, Token/s: 151.46
Epoch: 0, Step: 9430, Batch(micro): 9430, Batch (considering grad accum): 1178,  Loss: 6.3549, Time: 3.23s, Token/s: 158.58
Epoch: 0, Step: 9431, Batch(micro): 9431, Batch (considering grad accum): 1178,  Loss: 6.0292, Time: 19.67s, Token/s: 26.03
Epoch: 0, Step: 9432, Batch(micro): 9432, Batch (considering grad accum): 1179,  Loss: 6.0950, Time: 6.77s, Token/s: 75.60
Epoch: 0, Step: 9433, Batch(micro): 9433, Batch (considering grad accum): 1179,  Loss: 5.3875, Time: 3.91s, Token/s: 130.82
Epoch: 0, Step: 9434, Batch(micro): 9434, Batch (considering grad accum): 1179,  Loss: 5.4733, Time: 3.52s, Token/s: 145.57
Epoch: 0, Step: 9435, Batch(micro): 9435, Batch (considering grad accum): 1179,  Loss: 5.2509, Time: 3.73s, Token/s: 137.45
Epoch: 0, Step: 9436, Batch(micro): 9436, Batch (considering grad accum): 1179,  Loss: 5.1000, Time: 3.37s, Token/s: 152.15
Epoch: 0, Step: 9437, Batch(micro): 9437, Batch (considering grad accum): 1179,  Loss: 5.8775, Time: 3.44s, Token/s: 148.64
Epoch: 0, Step: 9438, Batch(micro): 9438, Batch (considering grad accum): 1179,  Loss: 5.6421, Time: 3.19s, Token/s: 160.56
Epoch: 0, Step: 9439, Batch(micro): 9439, Batch (considering grad accum): 1179,  Loss: 6.1369, Time: 20.08s, Token/s: 25.49
Epoch: 0, Step: 9440, Batch(micro): 9440, Batch (considering grad accum): 1180,  Loss: 5.5470, Time: 7.10s, Token/s: 72.06
Epoch: 0, Step: 9441, Batch(micro): 9441, Batch (considering grad accum): 1180,  Loss: 5.2834, Time: 3.71s, Token/s: 137.90
Epoch: 0, Step: 9442, Batch(micro): 9442, Batch (considering grad accum): 1180,  Loss: 6.0187, Time: 3.31s, Token/s: 154.75
Epoch: 0, Step: 9443, Batch(micro): 9443, Batch (considering grad accum): 1180,  Loss: 5.6702, Time: 3.26s, Token/s: 157.10
Epoch: 0, Step: 9444, Batch(micro): 9444, Batch (considering grad accum): 1180,  Loss: 6.0010, Time: 3.22s, Token/s: 159.10
Epoch: 0, Step: 9445, Batch(micro): 9445, Batch (considering grad accum): 1180,  Loss: 5.4692, Time: 3.57s, Token/s: 143.55
Epoch: 0, Step: 9446, Batch(micro): 9446, Batch (considering grad accum): 1180,  Loss: 5.6988, Time: 3.54s, Token/s: 144.57
Epoch: 0, Step: 9447, Batch(micro): 9447, Batch (considering grad accum): 1180,  Loss: 6.8057, Time: 17.80s, Token/s: 28.77
Epoch: 0, Step: 9448, Batch(micro): 9448, Batch (considering grad accum): 1181,  Loss: 5.4884, Time: 6.62s, Token/s: 77.34
Epoch: 0, Step: 9449, Batch(micro): 9449, Batch (considering grad accum): 1181,  Loss: 5.9376, Time: 3.80s, Token/s: 134.86
Epoch: 0, Step: 9450, Batch(micro): 9450, Batch (considering grad accum): 1181,  Loss: 5.8077, Time: 3.65s, Token/s: 140.30
Epoch: 0, Step: 9451, Batch(micro): 9451, Batch (considering grad accum): 1181,  Loss: 5.5305, Time: 3.83s, Token/s: 133.68
Epoch: 0, Step: 9452, Batch(micro): 9452, Batch (considering grad accum): 1181,  Loss: 5.7048, Time: 3.46s, Token/s: 148.03
Epoch: 0, Step: 9453, Batch(micro): 9453, Batch (considering grad accum): 1181,  Loss: 6.3031, Time: 3.46s, Token/s: 148.16
Epoch: 0, Step: 9454, Batch(micro): 9454, Batch (considering grad accum): 1181,  Loss: 6.5398, Time: 3.46s, Token/s: 148.15
Epoch: 0, Step: 9455, Batch(micro): 9455, Batch (considering grad accum): 1181,  Loss: 6.2449, Time: 20.73s, Token/s: 24.70
Epoch: 0, Step: 9456, Batch(micro): 9456, Batch (considering grad accum): 1182,  Loss: 6.0290, Time: 7.20s, Token/s: 71.13
Epoch: 0, Step: 9457, Batch(micro): 9457, Batch (considering grad accum): 1182,  Loss: 6.1819, Time: 3.76s, Token/s: 136.27
Epoch: 0, Step: 9458, Batch(micro): 9458, Batch (considering grad accum): 1182,  Loss: 7.4798, Time: 3.70s, Token/s: 138.45
Epoch: 0, Step: 9459, Batch(micro): 9459, Batch (considering grad accum): 1182,  Loss: 5.6769, Time: 3.53s, Token/s: 144.99
Epoch: 0, Step: 9460, Batch(micro): 9460, Batch (considering grad accum): 1182,  Loss: 5.1587, Time: 3.39s, Token/s: 151.12
Epoch: 0, Step: 9461, Batch(micro): 9461, Batch (considering grad accum): 1182,  Loss: 5.1624, Time: 3.46s, Token/s: 147.78
Epoch: 0, Step: 9462, Batch(micro): 9462, Batch (considering grad accum): 1182,  Loss: 5.9793, Time: 3.21s, Token/s: 159.70
Epoch: 0, Step: 9463, Batch(micro): 9463, Batch (considering grad accum): 1182,  Loss: 5.7917, Time: 24.03s, Token/s: 21.30
Epoch: 0, Step: 9464, Batch(micro): 9464, Batch (considering grad accum): 1183,  Loss: 6.3234, Time: 7.58s, Token/s: 67.58
Epoch: 0, Step: 9465, Batch(micro): 9465, Batch (considering grad accum): 1183,  Loss: 6.5291, Time: 3.93s, Token/s: 130.29
Epoch: 0, Step: 9466, Batch(micro): 9466, Batch (considering grad accum): 1183,  Loss: 5.8634, Time: 3.38s, Token/s: 151.57
Epoch: 0, Step: 9467, Batch(micro): 9467, Batch (considering grad accum): 1183,  Loss: 6.1369, Time: 3.34s, Token/s: 153.09
Epoch: 0, Step: 9468, Batch(micro): 9468, Batch (considering grad accum): 1183,  Loss: 5.7607, Time: 3.99s, Token/s: 128.16
Epoch: 0, Step: 9469, Batch(micro): 9469, Batch (considering grad accum): 1183,  Loss: 5.5777, Time: 3.29s, Token/s: 155.85
Epoch: 0, Step: 9470, Batch(micro): 9470, Batch (considering grad accum): 1183,  Loss: 6.0896, Time: 3.34s, Token/s: 153.47
Epoch: 0, Step: 9471, Batch(micro): 9471, Batch (considering grad accum): 1183,  Loss: 5.7181, Time: 24.41s, Token/s: 20.98
Epoch: 0, Step: 9472, Batch(micro): 9472, Batch (considering grad accum): 1184,  Loss: 6.5786, Time: 6.98s, Token/s: 73.36
Epoch: 0, Step: 9473, Batch(micro): 9473, Batch (considering grad accum): 1184,  Loss: 6.1679, Time: 3.84s, Token/s: 133.26
Epoch: 0, Step: 9474, Batch(micro): 9474, Batch (considering grad accum): 1184,  Loss: 6.0581, Time: 3.30s, Token/s: 155.31
Epoch: 0, Step: 9475, Batch(micro): 9475, Batch (considering grad accum): 1184,  Loss: 6.1825, Time: 3.24s, Token/s: 157.97
Epoch: 0, Step: 9476, Batch(micro): 9476, Batch (considering grad accum): 1184,  Loss: 6.0310, Time: 3.23s, Token/s: 158.53
Epoch: 0, Step: 9477, Batch(micro): 9477, Batch (considering grad accum): 1184,  Loss: 7.2502, Time: 3.26s, Token/s: 156.94
Epoch: 0, Step: 9478, Batch(micro): 9478, Batch (considering grad accum): 1184,  Loss: 5.9168, Time: 3.62s, Token/s: 141.43
Epoch: 0, Step: 9479, Batch(micro): 9479, Batch (considering grad accum): 1184,  Loss: 5.6235, Time: 26.86s, Token/s: 19.06
Epoch: 0, Step: 9480, Batch(micro): 9480, Batch (considering grad accum): 1185,  Loss: 5.1640, Time: 9.11s, Token/s: 56.22
Epoch: 0, Step: 9481, Batch(micro): 9481, Batch (considering grad accum): 1185,  Loss: 5.5595, Time: 3.84s, Token/s: 133.46
Epoch: 0, Step: 9482, Batch(micro): 9482, Batch (considering grad accum): 1185,  Loss: 5.2822, Time: 3.50s, Token/s: 146.26
Epoch: 0, Step: 9483, Batch(micro): 9483, Batch (considering grad accum): 1185,  Loss: 6.3956, Time: 3.43s, Token/s: 149.34
Epoch: 0, Step: 9484, Batch(micro): 9484, Batch (considering grad accum): 1185,  Loss: 6.1201, Time: 3.41s, Token/s: 150.35
Epoch: 0, Step: 9485, Batch(micro): 9485, Batch (considering grad accum): 1185,  Loss: 6.3783, Time: 3.74s, Token/s: 137.06
Epoch: 0, Step: 9486, Batch(micro): 9486, Batch (considering grad accum): 1185,  Loss: 5.7309, Time: 3.66s, Token/s: 139.93
Epoch: 0, Step: 9487, Batch(micro): 9487, Batch (considering grad accum): 1185,  Loss: 5.0957, Time: 23.54s, Token/s: 21.75
Epoch: 0, Step: 9488, Batch(micro): 9488, Batch (considering grad accum): 1186,  Loss: 6.8049, Time: 6.53s, Token/s: 78.43
Epoch: 0, Step: 9489, Batch(micro): 9489, Batch (considering grad accum): 1186,  Loss: 6.8107, Time: 3.88s, Token/s: 132.04
Epoch: 0, Step: 9490, Batch(micro): 9490, Batch (considering grad accum): 1186,  Loss: 6.1760, Time: 3.83s, Token/s: 133.73
Epoch: 0, Step: 9491, Batch(micro): 9491, Batch (considering grad accum): 1186,  Loss: 6.4330, Time: 3.63s, Token/s: 141.17
Epoch: 0, Step: 9492, Batch(micro): 9492, Batch (considering grad accum): 1186,  Loss: 6.2453, Time: 3.25s, Token/s: 157.52
Epoch: 0, Step: 9493, Batch(micro): 9493, Batch (considering grad accum): 1186,  Loss: 5.7961, Time: 3.24s, Token/s: 158.22
Epoch: 0, Step: 9494, Batch(micro): 9494, Batch (considering grad accum): 1186,  Loss: 5.6009, Time: 3.23s, Token/s: 158.48
Epoch: 0, Step: 9495, Batch(micro): 9495, Batch (considering grad accum): 1186,  Loss: 5.9923, Time: 22.44s, Token/s: 22.82
Epoch: 0, Step: 9496, Batch(micro): 9496, Batch (considering grad accum): 1187,  Loss: 6.1221, Time: 6.49s, Token/s: 78.93
Epoch: 0, Step: 9497, Batch(micro): 9497, Batch (considering grad accum): 1187,  Loss: 5.6843, Time: 3.82s, Token/s: 134.18
Epoch: 0, Step: 9498, Batch(micro): 9498, Batch (considering grad accum): 1187,  Loss: 5.7979, Time: 3.47s, Token/s: 147.34
Epoch: 0, Step: 9499, Batch(micro): 9499, Batch (considering grad accum): 1187,  Loss: 6.4010, Time: 3.50s, Token/s: 146.31
Updating MLP bias
Epoch: 0, Step: 9500, Batch(micro): 9500, Batch (considering grad accum): 1187,  Loss: 6.2369, Time: 3.64s, Token/s: 140.57
Epoch: 0, Step: 9501, Batch(micro): 9501, Batch (considering grad accum): 1187,  Loss: 5.9394, Time: 3.94s, Token/s: 129.93
Epoch: 0, Step: 9502, Batch(micro): 9502, Batch (considering grad accum): 1187,  Loss: 6.1529, Time: 3.53s, Token/s: 145.06
Epoch: 0, Step: 9503, Batch(micro): 9503, Batch (considering grad accum): 1187,  Loss: 5.8760, Time: 25.57s, Token/s: 20.02
Epoch: 0, Step: 9504, Batch(micro): 9504, Batch (considering grad accum): 1188,  Loss: 5.6276, Time: 8.91s, Token/s: 57.46
Epoch: 0, Step: 9505, Batch(micro): 9505, Batch (considering grad accum): 1188,  Loss: 5.9628, Time: 3.43s, Token/s: 149.12
Epoch: 0, Step: 9506, Batch(micro): 9506, Batch (considering grad accum): 1188,  Loss: 6.2612, Time: 3.26s, Token/s: 156.89
Epoch: 0, Step: 9507, Batch(micro): 9507, Batch (considering grad accum): 1188,  Loss: 6.6756, Time: 3.17s, Token/s: 161.53
Epoch: 0, Step: 9508, Batch(micro): 9508, Batch (considering grad accum): 1188,  Loss: 6.7977, Time: 3.37s, Token/s: 151.75
Epoch: 0, Step: 9509, Batch(micro): 9509, Batch (considering grad accum): 1188,  Loss: 6.2413, Time: 3.49s, Token/s: 146.58
Epoch: 0, Step: 9510, Batch(micro): 9510, Batch (considering grad accum): 1188,  Loss: 6.3281, Time: 3.54s, Token/s: 144.61
Epoch: 0, Step: 9511, Batch(micro): 9511, Batch (considering grad accum): 1188,  Loss: 5.5589, Time: 25.20s, Token/s: 20.32
Epoch: 0, Step: 9512, Batch(micro): 9512, Batch (considering grad accum): 1189,  Loss: 5.3154, Time: 7.65s, Token/s: 66.95
Epoch: 0, Step: 9513, Batch(micro): 9513, Batch (considering grad accum): 1189,  Loss: 6.0200, Time: 4.31s, Token/s: 118.77
Epoch: 0, Step: 9514, Batch(micro): 9514, Batch (considering grad accum): 1189,  Loss: 6.0710, Time: 3.58s, Token/s: 143.15
Epoch: 0, Step: 9515, Batch(micro): 9515, Batch (considering grad accum): 1189,  Loss: 5.6590, Time: 3.39s, Token/s: 151.12
Epoch: 0, Step: 9516, Batch(micro): 9516, Batch (considering grad accum): 1189,  Loss: 5.5036, Time: 3.31s, Token/s: 154.61
Epoch: 0, Step: 9517, Batch(micro): 9517, Batch (considering grad accum): 1189,  Loss: 5.9563, Time: 3.41s, Token/s: 150.02
Epoch: 0, Step: 9518, Batch(micro): 9518, Batch (considering grad accum): 1189,  Loss: 5.6530, Time: 3.54s, Token/s: 144.79
Epoch: 0, Step: 9519, Batch(micro): 9519, Batch (considering grad accum): 1189,  Loss: 5.8668, Time: 23.98s, Token/s: 21.35
Epoch: 0, Step: 9520, Batch(micro): 9520, Batch (considering grad accum): 1190,  Loss: 5.9401, Time: 7.77s, Token/s: 65.86
Epoch: 0, Step: 9521, Batch(micro): 9521, Batch (considering grad accum): 1190,  Loss: 5.7711, Time: 4.14s, Token/s: 123.63
Epoch: 0, Step: 9522, Batch(micro): 9522, Batch (considering grad accum): 1190,  Loss: 6.1555, Time: 3.74s, Token/s: 137.08
Epoch: 0, Step: 9523, Batch(micro): 9523, Batch (considering grad accum): 1190,  Loss: 5.7747, Time: 3.68s, Token/s: 139.15
Epoch: 0, Step: 9524, Batch(micro): 9524, Batch (considering grad accum): 1190,  Loss: 5.5494, Time: 3.65s, Token/s: 140.40
Epoch: 0, Step: 9525, Batch(micro): 9525, Batch (considering grad accum): 1190,  Loss: 5.4490, Time: 3.35s, Token/s: 152.93
Epoch: 0, Step: 9526, Batch(micro): 9526, Batch (considering grad accum): 1190,  Loss: 5.6721, Time: 3.50s, Token/s: 146.37
Epoch: 0, Step: 9527, Batch(micro): 9527, Batch (considering grad accum): 1190,  Loss: 6.3562, Time: 25.87s, Token/s: 19.79
Epoch: 0, Step: 9528, Batch(micro): 9528, Batch (considering grad accum): 1191,  Loss: 5.5382, Time: 7.56s, Token/s: 67.69
Epoch: 0, Step: 9529, Batch(micro): 9529, Batch (considering grad accum): 1191,  Loss: 5.4437, Time: 3.86s, Token/s: 132.67
Epoch: 0, Step: 9530, Batch(micro): 9530, Batch (considering grad accum): 1191,  Loss: 5.9058, Time: 3.48s, Token/s: 147.32
Epoch: 0, Step: 9531, Batch(micro): 9531, Batch (considering grad accum): 1191,  Loss: 6.1053, Time: 3.67s, Token/s: 139.53
Epoch: 0, Step: 9532, Batch(micro): 9532, Batch (considering grad accum): 1191,  Loss: 6.0996, Time: 3.97s, Token/s: 129.09
Epoch: 0, Step: 9533, Batch(micro): 9533, Batch (considering grad accum): 1191,  Loss: 5.6499, Time: 3.51s, Token/s: 146.01
Epoch: 0, Step: 9534, Batch(micro): 9534, Batch (considering grad accum): 1191,  Loss: 5.8632, Time: 3.41s, Token/s: 149.97
Epoch: 0, Step: 9535, Batch(micro): 9535, Batch (considering grad accum): 1191,  Loss: 5.3674, Time: 23.54s, Token/s: 21.75
Epoch: 0, Step: 9536, Batch(micro): 9536, Batch (considering grad accum): 1192,  Loss: 5.8908, Time: 6.93s, Token/s: 73.86
Epoch: 0, Step: 9537, Batch(micro): 9537, Batch (considering grad accum): 1192,  Loss: 6.0984, Time: 3.95s, Token/s: 129.62
Epoch: 0, Step: 9538, Batch(micro): 9538, Batch (considering grad accum): 1192,  Loss: 5.9599, Time: 3.84s, Token/s: 133.38
Epoch: 0, Step: 9539, Batch(micro): 9539, Batch (considering grad accum): 1192,  Loss: 6.6419, Time: 3.63s, Token/s: 140.85
Epoch: 0, Step: 9540, Batch(micro): 9540, Batch (considering grad accum): 1192,  Loss: 6.1375, Time: 3.55s, Token/s: 144.42
Epoch: 0, Step: 9541, Batch(micro): 9541, Batch (considering grad accum): 1192,  Loss: 5.8847, Time: 3.33s, Token/s: 153.84
Epoch: 0, Step: 9542, Batch(micro): 9542, Batch (considering grad accum): 1192,  Loss: 6.3450, Time: 3.42s, Token/s: 149.57
Epoch: 0, Step: 9543, Batch(micro): 9543, Batch (considering grad accum): 1192,  Loss: 6.0793, Time: 22.95s, Token/s: 22.31
Epoch: 0, Step: 9544, Batch(micro): 9544, Batch (considering grad accum): 1193,  Loss: 6.6939, Time: 7.67s, Token/s: 66.76
Epoch: 0, Step: 9545, Batch(micro): 9545, Batch (considering grad accum): 1193,  Loss: 5.6659, Time: 3.83s, Token/s: 133.61
Epoch: 0, Step: 9546, Batch(micro): 9546, Batch (considering grad accum): 1193,  Loss: 6.9232, Time: 3.61s, Token/s: 141.77
Epoch: 0, Step: 9547, Batch(micro): 9547, Batch (considering grad accum): 1193,  Loss: 6.4093, Time: 3.37s, Token/s: 151.79
Epoch: 0, Step: 9548, Batch(micro): 9548, Batch (considering grad accum): 1193,  Loss: 6.1209, Time: 3.41s, Token/s: 150.29
Epoch: 0, Step: 9549, Batch(micro): 9549, Batch (considering grad accum): 1193,  Loss: 5.5424, Time: 3.44s, Token/s: 148.86
Epoch: 0, Step: 9550, Batch(micro): 9550, Batch (considering grad accum): 1193,  Loss: 6.0262, Time: 3.26s, Token/s: 157.13
Epoch: 0, Step: 9551, Batch(micro): 9551, Batch (considering grad accum): 1193,  Loss: 5.7942, Time: 24.26s, Token/s: 21.11
Epoch: 0, Step: 9552, Batch(micro): 9552, Batch (considering grad accum): 1194,  Loss: 5.7185, Time: 9.71s, Token/s: 52.73
Epoch: 0, Step: 9553, Batch(micro): 9553, Batch (considering grad accum): 1194,  Loss: 5.8480, Time: 3.94s, Token/s: 130.07
Epoch: 0, Step: 9554, Batch(micro): 9554, Batch (considering grad accum): 1194,  Loss: 6.0992, Time: 3.54s, Token/s: 144.46
Epoch: 0, Step: 9555, Batch(micro): 9555, Batch (considering grad accum): 1194,  Loss: 5.6843, Time: 3.70s, Token/s: 138.40
Epoch: 0, Step: 9556, Batch(micro): 9556, Batch (considering grad accum): 1194,  Loss: 5.8507, Time: 3.53s, Token/s: 144.85
Epoch: 0, Step: 9557, Batch(micro): 9557, Batch (considering grad accum): 1194,  Loss: 6.3596, Time: 3.63s, Token/s: 141.15
Epoch: 0, Step: 9558, Batch(micro): 9558, Batch (considering grad accum): 1194,  Loss: 6.2518, Time: 3.51s, Token/s: 146.03
Epoch: 0, Step: 9559, Batch(micro): 9559, Batch (considering grad accum): 1194,  Loss: 6.5670, Time: 24.76s, Token/s: 20.68
Epoch: 0, Step: 9560, Batch(micro): 9560, Batch (considering grad accum): 1195,  Loss: 6.7026, Time: 7.80s, Token/s: 65.61
Epoch: 0, Step: 9561, Batch(micro): 9561, Batch (considering grad accum): 1195,  Loss: 6.3926, Time: 3.96s, Token/s: 129.19
Epoch: 0, Step: 9562, Batch(micro): 9562, Batch (considering grad accum): 1195,  Loss: 6.6953, Time: 3.56s, Token/s: 143.92
Epoch: 0, Step: 9563, Batch(micro): 9563, Batch (considering grad accum): 1195,  Loss: 6.0543, Time: 3.47s, Token/s: 147.76
Epoch: 0, Step: 9564, Batch(micro): 9564, Batch (considering grad accum): 1195,  Loss: 5.7229, Time: 3.43s, Token/s: 149.05
Epoch: 0, Step: 9565, Batch(micro): 9565, Batch (considering grad accum): 1195,  Loss: 5.6995, Time: 3.52s, Token/s: 145.30
Epoch: 0, Step: 9566, Batch(micro): 9566, Batch (considering grad accum): 1195,  Loss: 6.0686, Time: 3.81s, Token/s: 134.22
Epoch: 0, Step: 9567, Batch(micro): 9567, Batch (considering grad accum): 1195,  Loss: 6.6885, Time: 22.85s, Token/s: 22.40
Epoch: 0, Step: 9568, Batch(micro): 9568, Batch (considering grad accum): 1196,  Loss: 5.7339, Time: 6.85s, Token/s: 74.78
Epoch: 0, Step: 9569, Batch(micro): 9569, Batch (considering grad accum): 1196,  Loss: 5.9028, Time: 3.72s, Token/s: 137.64
Epoch: 0, Step: 9570, Batch(micro): 9570, Batch (considering grad accum): 1196,  Loss: 6.3483, Time: 3.37s, Token/s: 152.14
Epoch: 0, Step: 9571, Batch(micro): 9571, Batch (considering grad accum): 1196,  Loss: 6.4836, Time: 4.04s, Token/s: 126.68
Epoch: 0, Step: 9572, Batch(micro): 9572, Batch (considering grad accum): 1196,  Loss: 5.6612, Time: 3.81s, Token/s: 134.55
Epoch: 0, Step: 9573, Batch(micro): 9573, Batch (considering grad accum): 1196,  Loss: 5.6171, Time: 3.82s, Token/s: 134.20
Epoch: 0, Step: 9574, Batch(micro): 9574, Batch (considering grad accum): 1196,  Loss: 5.5472, Time: 3.47s, Token/s: 147.66
Epoch: 0, Step: 9575, Batch(micro): 9575, Batch (considering grad accum): 1196,  Loss: 6.1074, Time: 23.63s, Token/s: 21.66
Epoch: 0, Step: 9576, Batch(micro): 9576, Batch (considering grad accum): 1197,  Loss: 6.7841, Time: 6.36s, Token/s: 80.50
Epoch: 0, Step: 9577, Batch(micro): 9577, Batch (considering grad accum): 1197,  Loss: 5.9487, Time: 3.74s, Token/s: 136.81
Epoch: 0, Step: 9578, Batch(micro): 9578, Batch (considering grad accum): 1197,  Loss: 5.3305, Time: 3.33s, Token/s: 153.98
Epoch: 0, Step: 9579, Batch(micro): 9579, Batch (considering grad accum): 1197,  Loss: 5.3587, Time: 3.37s, Token/s: 151.90
Epoch: 0, Step: 9580, Batch(micro): 9580, Batch (considering grad accum): 1197,  Loss: 6.0371, Time: 3.61s, Token/s: 141.79
Epoch: 0, Step: 9581, Batch(micro): 9581, Batch (considering grad accum): 1197,  Loss: 6.1912, Time: 3.65s, Token/s: 140.32
Epoch: 0, Step: 9582, Batch(micro): 9582, Batch (considering grad accum): 1197,  Loss: 5.9674, Time: 3.52s, Token/s: 145.48
Epoch: 0, Step: 9583, Batch(micro): 9583, Batch (considering grad accum): 1197,  Loss: 5.8346, Time: 23.00s, Token/s: 22.27
Epoch: 0, Step: 9584, Batch(micro): 9584, Batch (considering grad accum): 1198,  Loss: 5.8575, Time: 6.71s, Token/s: 76.25
Epoch: 0, Step: 9585, Batch(micro): 9585, Batch (considering grad accum): 1198,  Loss: 6.1376, Time: 3.66s, Token/s: 139.79
Epoch: 0, Step: 9586, Batch(micro): 9586, Batch (considering grad accum): 1198,  Loss: 5.6695, Time: 3.50s, Token/s: 146.14
Epoch: 0, Step: 9587, Batch(micro): 9587, Batch (considering grad accum): 1198,  Loss: 5.2717, Time: 3.68s, Token/s: 139.18
Epoch: 0, Step: 9588, Batch(micro): 9588, Batch (considering grad accum): 1198,  Loss: 4.8310, Time: 3.59s, Token/s: 142.73
Epoch: 0, Step: 9589, Batch(micro): 9589, Batch (considering grad accum): 1198,  Loss: 4.9001, Time: 3.34s, Token/s: 153.32
Epoch: 0, Step: 9590, Batch(micro): 9590, Batch (considering grad accum): 1198,  Loss: 6.0154, Time: 3.85s, Token/s: 133.02
Epoch: 0, Step: 9591, Batch(micro): 9591, Batch (considering grad accum): 1198,  Loss: 6.0309, Time: 22.65s, Token/s: 22.61
Epoch: 0, Step: 9592, Batch(micro): 9592, Batch (considering grad accum): 1199,  Loss: 6.5568, Time: 6.77s, Token/s: 75.68
Epoch: 0, Step: 9593, Batch(micro): 9593, Batch (considering grad accum): 1199,  Loss: 6.4098, Time: 3.87s, Token/s: 132.30
Epoch: 0, Step: 9594, Batch(micro): 9594, Batch (considering grad accum): 1199,  Loss: 5.6907, Time: 3.39s, Token/s: 150.88
Epoch: 0, Step: 9595, Batch(micro): 9595, Batch (considering grad accum): 1199,  Loss: 5.1840, Time: 3.64s, Token/s: 140.57
Epoch: 0, Step: 9596, Batch(micro): 9596, Batch (considering grad accum): 1199,  Loss: 6.1838, Time: 3.62s, Token/s: 141.35
Epoch: 0, Step: 9597, Batch(micro): 9597, Batch (considering grad accum): 1199,  Loss: 6.5236, Time: 3.60s, Token/s: 142.05
Epoch: 0, Step: 9598, Batch(micro): 9598, Batch (considering grad accum): 1199,  Loss: 5.8430, Time: 3.81s, Token/s: 134.39
Epoch: 0, Step: 9599, Batch(micro): 9599, Batch (considering grad accum): 1199,  Loss: 5.7154, Time: 24.00s, Token/s: 21.34
Updating MLP bias
Epoch: 0, Step: 9600, Batch(micro): 9600, Batch (considering grad accum): 1200,  Loss: 5.6867, Time: 7.87s, Token/s: 65.05
Epoch: 0, Step: 9601, Batch(micro): 9601, Batch (considering grad accum): 1200,  Loss: 5.9468, Time: 3.82s, Token/s: 133.94
Epoch: 0, Step: 9602, Batch(micro): 9602, Batch (considering grad accum): 1200,  Loss: 6.8534, Time: 3.31s, Token/s: 154.60
Epoch: 0, Step: 9603, Batch(micro): 9603, Batch (considering grad accum): 1200,  Loss: 6.1791, Time: 3.35s, Token/s: 152.87
Epoch: 0, Step: 9604, Batch(micro): 9604, Batch (considering grad accum): 1200,  Loss: 5.5849, Time: 3.85s, Token/s: 133.01
Epoch: 0, Step: 9605, Batch(micro): 9605, Batch (considering grad accum): 1200,  Loss: 5.3574, Time: 3.54s, Token/s: 144.54
Epoch: 0, Step: 9606, Batch(micro): 9606, Batch (considering grad accum): 1200,  Loss: 5.8053, Time: 3.53s, Token/s: 144.85
Epoch: 0, Step: 9607, Batch(micro): 9607, Batch (considering grad accum): 1200,  Loss: 5.7383, Time: 26.00s, Token/s: 19.70
Epoch: 0, Step: 9608, Batch(micro): 9608, Batch (considering grad accum): 1201,  Loss: 5.8446, Time: 6.57s, Token/s: 77.94
Epoch: 0, Step: 9609, Batch(micro): 9609, Batch (considering grad accum): 1201,  Loss: 5.9328, Time: 4.06s, Token/s: 126.21
Epoch: 0, Step: 9610, Batch(micro): 9610, Batch (considering grad accum): 1201,  Loss: 6.3021, Time: 3.71s, Token/s: 137.91
Epoch: 0, Step: 9611, Batch(micro): 9611, Batch (considering grad accum): 1201,  Loss: 5.9582, Time: 3.92s, Token/s: 130.58
Epoch: 0, Step: 9612, Batch(micro): 9612, Batch (considering grad accum): 1201,  Loss: 5.4425, Time: 3.60s, Token/s: 142.36
Epoch: 0, Step: 9613, Batch(micro): 9613, Batch (considering grad accum): 1201,  Loss: 5.9746, Time: 3.47s, Token/s: 147.40
Epoch: 0, Step: 9614, Batch(micro): 9614, Batch (considering grad accum): 1201,  Loss: 6.6473, Time: 3.40s, Token/s: 150.75
Epoch: 0, Step: 9615, Batch(micro): 9615, Batch (considering grad accum): 1201,  Loss: 5.8304, Time: 26.28s, Token/s: 19.48
Epoch: 0, Step: 9616, Batch(micro): 9616, Batch (considering grad accum): 1202,  Loss: 5.2496, Time: 8.85s, Token/s: 57.84
Epoch: 0, Step: 9617, Batch(micro): 9617, Batch (considering grad accum): 1202,  Loss: 5.8090, Time: 3.81s, Token/s: 134.48
Epoch: 0, Step: 9618, Batch(micro): 9618, Batch (considering grad accum): 1202,  Loss: 5.8734, Time: 3.44s, Token/s: 148.79
Epoch: 0, Step: 9619, Batch(micro): 9619, Batch (considering grad accum): 1202,  Loss: 6.0814, Time: 3.37s, Token/s: 151.83
Epoch: 0, Step: 9620, Batch(micro): 9620, Batch (considering grad accum): 1202,  Loss: 6.3364, Time: 3.43s, Token/s: 149.34
Epoch: 0, Step: 9621, Batch(micro): 9621, Batch (considering grad accum): 1202,  Loss: 5.9469, Time: 3.29s, Token/s: 155.69
Epoch: 0, Step: 9622, Batch(micro): 9622, Batch (considering grad accum): 1202,  Loss: 6.0973, Time: 3.48s, Token/s: 147.08
Epoch: 0, Step: 9623, Batch(micro): 9623, Batch (considering grad accum): 1202,  Loss: 6.1318, Time: 25.79s, Token/s: 19.85
Epoch: 0, Step: 9624, Batch(micro): 9624, Batch (considering grad accum): 1203,  Loss: 6.0961, Time: 6.85s, Token/s: 74.71
Epoch: 0, Step: 9625, Batch(micro): 9625, Batch (considering grad accum): 1203,  Loss: 6.7171, Time: 3.82s, Token/s: 133.94
Epoch: 0, Step: 9626, Batch(micro): 9626, Batch (considering grad accum): 1203,  Loss: 6.1835, Time: 3.29s, Token/s: 155.41
Epoch: 0, Step: 9627, Batch(micro): 9627, Batch (considering grad accum): 1203,  Loss: 5.1951, Time: 3.31s, Token/s: 154.49
Epoch: 0, Step: 9628, Batch(micro): 9628, Batch (considering grad accum): 1203,  Loss: 5.8538, Time: 3.21s, Token/s: 159.50
Epoch: 0, Step: 9629, Batch(micro): 9629, Batch (considering grad accum): 1203,  Loss: 6.7666, Time: 3.31s, Token/s: 154.64
Epoch: 0, Step: 9630, Batch(micro): 9630, Batch (considering grad accum): 1203,  Loss: 6.3444, Time: 3.24s, Token/s: 158.26
Epoch: 0, Step: 9631, Batch(micro): 9631, Batch (considering grad accum): 1203,  Loss: 6.3721, Time: 23.78s, Token/s: 21.53
Epoch: 0, Step: 9632, Batch(micro): 9632, Batch (considering grad accum): 1204,  Loss: 6.4989, Time: 7.03s, Token/s: 72.83
Epoch: 0, Step: 9633, Batch(micro): 9633, Batch (considering grad accum): 1204,  Loss: 6.7074, Time: 3.92s, Token/s: 130.71
Epoch: 0, Step: 9634, Batch(micro): 9634, Batch (considering grad accum): 1204,  Loss: 5.9000, Time: 3.48s, Token/s: 146.95
Epoch: 0, Step: 9635, Batch(micro): 9635, Batch (considering grad accum): 1204,  Loss: 6.1482, Time: 3.54s, Token/s: 144.68
Epoch: 0, Step: 9636, Batch(micro): 9636, Batch (considering grad accum): 1204,  Loss: 5.6985, Time: 3.42s, Token/s: 149.69
Epoch: 0, Step: 9637, Batch(micro): 9637, Batch (considering grad accum): 1204,  Loss: 5.8788, Time: 3.48s, Token/s: 146.93
Epoch: 0, Step: 9638, Batch(micro): 9638, Batch (considering grad accum): 1204,  Loss: 5.1819, Time: 3.63s, Token/s: 141.20
Epoch: 0, Step: 9639, Batch(micro): 9639, Batch (considering grad accum): 1204,  Loss: 5.7580, Time: 24.33s, Token/s: 21.04
Epoch: 0, Step: 9640, Batch(micro): 9640, Batch (considering grad accum): 1205,  Loss: 6.0814, Time: 6.93s, Token/s: 73.86
Epoch: 0, Step: 9641, Batch(micro): 9641, Batch (considering grad accum): 1205,  Loss: 6.3678, Time: 3.88s, Token/s: 131.84
Epoch: 0, Step: 9642, Batch(micro): 9642, Batch (considering grad accum): 1205,  Loss: 6.0429, Time: 3.75s, Token/s: 136.40
Epoch: 0, Step: 9643, Batch(micro): 9643, Batch (considering grad accum): 1205,  Loss: 5.7141, Time: 3.72s, Token/s: 137.70
Epoch: 0, Step: 9644, Batch(micro): 9644, Batch (considering grad accum): 1205,  Loss: 6.9483, Time: 3.58s, Token/s: 143.12
Epoch: 0, Step: 9645, Batch(micro): 9645, Batch (considering grad accum): 1205,  Loss: 5.7501, Time: 3.48s, Token/s: 147.21
Epoch: 0, Step: 9646, Batch(micro): 9646, Batch (considering grad accum): 1205,  Loss: 5.8804, Time: 3.49s, Token/s: 146.56
Epoch: 0, Step: 9647, Batch(micro): 9647, Batch (considering grad accum): 1205,  Loss: 5.4463, Time: 24.28s, Token/s: 21.08
Epoch: 0, Step: 9648, Batch(micro): 9648, Batch (considering grad accum): 1206,  Loss: 5.5211, Time: 7.80s, Token/s: 65.62
Epoch: 0, Step: 9649, Batch(micro): 9649, Batch (considering grad accum): 1206,  Loss: 5.6869, Time: 3.97s, Token/s: 128.86
Epoch: 0, Step: 9650, Batch(micro): 9650, Batch (considering grad accum): 1206,  Loss: 5.7824, Time: 3.77s, Token/s: 135.83
Epoch: 0, Step: 9651, Batch(micro): 9651, Batch (considering grad accum): 1206,  Loss: 5.8853, Time: 3.65s, Token/s: 140.15
Epoch: 0, Step: 9652, Batch(micro): 9652, Batch (considering grad accum): 1206,  Loss: 6.5739, Time: 3.66s, Token/s: 140.07
Epoch: 0, Step: 9653, Batch(micro): 9653, Batch (considering grad accum): 1206,  Loss: 5.5979, Time: 3.49s, Token/s: 146.50
Epoch: 0, Step: 9654, Batch(micro): 9654, Batch (considering grad accum): 1206,  Loss: 6.0419, Time: 3.23s, Token/s: 158.33
Epoch: 0, Step: 9655, Batch(micro): 9655, Batch (considering grad accum): 1206,  Loss: 6.1983, Time: 18.39s, Token/s: 27.84
Epoch: 0, Step: 9656, Batch(micro): 9656, Batch (considering grad accum): 1207,  Loss: 6.0866, Time: 8.24s, Token/s: 62.16
Epoch: 0, Step: 9657, Batch(micro): 9657, Batch (considering grad accum): 1207,  Loss: 6.0877, Time: 3.86s, Token/s: 132.73
Epoch: 0, Step: 9658, Batch(micro): 9658, Batch (considering grad accum): 1207,  Loss: 5.0915, Time: 3.62s, Token/s: 141.58
Epoch: 0, Step: 9659, Batch(micro): 9659, Batch (considering grad accum): 1207,  Loss: 5.4606, Time: 3.62s, Token/s: 141.58
Epoch: 0, Step: 9660, Batch(micro): 9660, Batch (considering grad accum): 1207,  Loss: 5.4274, Time: 3.71s, Token/s: 137.87
Epoch: 0, Step: 9661, Batch(micro): 9661, Batch (considering grad accum): 1207,  Loss: 6.2631, Time: 3.54s, Token/s: 144.83
Epoch: 0, Step: 9662, Batch(micro): 9662, Batch (considering grad accum): 1207,  Loss: 6.3787, Time: 3.49s, Token/s: 146.82
Epoch: 0, Step: 9663, Batch(micro): 9663, Batch (considering grad accum): 1207,  Loss: 6.7133, Time: 18.82s, Token/s: 27.21
Epoch: 0, Step: 9664, Batch(micro): 9664, Batch (considering grad accum): 1208,  Loss: 6.0119, Time: 6.57s, Token/s: 77.93
Epoch: 0, Step: 9665, Batch(micro): 9665, Batch (considering grad accum): 1208,  Loss: 5.7333, Time: 3.85s, Token/s: 133.02
Epoch: 0, Step: 9666, Batch(micro): 9666, Batch (considering grad accum): 1208,  Loss: 5.7500, Time: 3.60s, Token/s: 142.07
Epoch: 0, Step: 9667, Batch(micro): 9667, Batch (considering grad accum): 1208,  Loss: 6.1182, Time: 3.77s, Token/s: 135.65
Epoch: 0, Step: 9668, Batch(micro): 9668, Batch (considering grad accum): 1208,  Loss: 5.8514, Time: 4.16s, Token/s: 123.06
Epoch: 0, Step: 9669, Batch(micro): 9669, Batch (considering grad accum): 1208,  Loss: 5.8956, Time: 3.36s, Token/s: 152.24
Epoch: 0, Step: 9670, Batch(micro): 9670, Batch (considering grad accum): 1208,  Loss: 6.4549, Time: 3.25s, Token/s: 157.50
Epoch: 0, Step: 9671, Batch(micro): 9671, Batch (considering grad accum): 1208,  Loss: 5.9571, Time: 18.28s, Token/s: 28.01
Epoch: 0, Step: 9672, Batch(micro): 9672, Batch (considering grad accum): 1209,  Loss: 6.6412, Time: 6.72s, Token/s: 76.15
Epoch: 0, Step: 9673, Batch(micro): 9673, Batch (considering grad accum): 1209,  Loss: 5.7647, Time: 3.75s, Token/s: 136.48
Epoch: 0, Step: 9674, Batch(micro): 9674, Batch (considering grad accum): 1209,  Loss: 5.8873, Time: 3.22s, Token/s: 158.93
Epoch: 0, Step: 9675, Batch(micro): 9675, Batch (considering grad accum): 1209,  Loss: 5.5642, Time: 3.30s, Token/s: 155.07
Epoch: 0, Step: 9676, Batch(micro): 9676, Batch (considering grad accum): 1209,  Loss: 5.6880, Time: 3.21s, Token/s: 159.40
Epoch: 0, Step: 9677, Batch(micro): 9677, Batch (considering grad accum): 1209,  Loss: 5.6913, Time: 3.19s, Token/s: 160.60
Epoch: 0, Step: 9678, Batch(micro): 9678, Batch (considering grad accum): 1209,  Loss: 6.0454, Time: 3.29s, Token/s: 155.83
Epoch: 0, Step: 9679, Batch(micro): 9679, Batch (considering grad accum): 1209,  Loss: 6.8830, Time: 20.76s, Token/s: 24.67
Epoch: 0, Step: 9680, Batch(micro): 9680, Batch (considering grad accum): 1210,  Loss: 5.4902, Time: 6.47s, Token/s: 79.08
Epoch: 0, Step: 9681, Batch(micro): 9681, Batch (considering grad accum): 1210,  Loss: 5.4433, Time: 3.94s, Token/s: 129.90
Epoch: 0, Step: 9682, Batch(micro): 9682, Batch (considering grad accum): 1210,  Loss: 6.3232, Time: 3.71s, Token/s: 138.00
Epoch: 0, Step: 9683, Batch(micro): 9683, Batch (considering grad accum): 1210,  Loss: 7.2604, Time: 3.66s, Token/s: 139.76
Epoch: 0, Step: 9684, Batch(micro): 9684, Batch (considering grad accum): 1210,  Loss: 7.4667, Time: 3.35s, Token/s: 152.63
Epoch: 0, Step: 9685, Batch(micro): 9685, Batch (considering grad accum): 1210,  Loss: 6.3946, Time: 3.74s, Token/s: 137.00
Epoch: 0, Step: 9686, Batch(micro): 9686, Batch (considering grad accum): 1210,  Loss: 5.8854, Time: 3.47s, Token/s: 147.53
Epoch: 0, Step: 9687, Batch(micro): 9687, Batch (considering grad accum): 1210,  Loss: 5.9272, Time: 20.12s, Token/s: 25.45
Epoch: 0, Step: 9688, Batch(micro): 9688, Batch (considering grad accum): 1211,  Loss: 5.7471, Time: 7.54s, Token/s: 67.92
Epoch: 0, Step: 9689, Batch(micro): 9689, Batch (considering grad accum): 1211,  Loss: 6.1623, Time: 3.96s, Token/s: 129.36
Epoch: 0, Step: 9690, Batch(micro): 9690, Batch (considering grad accum): 1211,  Loss: 5.8402, Time: 3.82s, Token/s: 134.12
Epoch: 0, Step: 9691, Batch(micro): 9691, Batch (considering grad accum): 1211,  Loss: 5.8666, Time: 3.60s, Token/s: 142.17
Epoch: 0, Step: 9692, Batch(micro): 9692, Batch (considering grad accum): 1211,  Loss: 6.4888, Time: 3.58s, Token/s: 143.18
Epoch: 0, Step: 9693, Batch(micro): 9693, Batch (considering grad accum): 1211,  Loss: 6.0718, Time: 3.25s, Token/s: 157.36
Epoch: 0, Step: 9694, Batch(micro): 9694, Batch (considering grad accum): 1211,  Loss: 5.8080, Time: 3.20s, Token/s: 160.06
Epoch: 0, Step: 9695, Batch(micro): 9695, Batch (considering grad accum): 1211,  Loss: 6.3994, Time: 17.89s, Token/s: 28.63
Epoch: 0, Step: 9696, Batch(micro): 9696, Batch (considering grad accum): 1212,  Loss: 6.0319, Time: 6.76s, Token/s: 75.76
Epoch: 0, Step: 9697, Batch(micro): 9697, Batch (considering grad accum): 1212,  Loss: 5.8236, Time: 4.08s, Token/s: 125.45
Epoch: 0, Step: 9698, Batch(micro): 9698, Batch (considering grad accum): 1212,  Loss: 5.7472, Time: 3.72s, Token/s: 137.78
Epoch: 0, Step: 9699, Batch(micro): 9699, Batch (considering grad accum): 1212,  Loss: 6.2893, Time: 3.44s, Token/s: 149.03
Updating MLP bias
Epoch: 0, Step: 9700, Batch(micro): 9700, Batch (considering grad accum): 1212,  Loss: 5.1608, Time: 3.82s, Token/s: 134.03
Epoch: 0, Step: 9701, Batch(micro): 9701, Batch (considering grad accum): 1212,  Loss: 6.5277, Time: 3.44s, Token/s: 148.90
Epoch: 0, Step: 9702, Batch(micro): 9702, Batch (considering grad accum): 1212,  Loss: 5.5160, Time: 4.05s, Token/s: 126.53
Epoch: 0, Step: 9703, Batch(micro): 9703, Batch (considering grad accum): 1212,  Loss: 5.2710, Time: 17.72s, Token/s: 28.89
Epoch: 0, Step: 9704, Batch(micro): 9704, Batch (considering grad accum): 1213,  Loss: 6.0677, Time: 6.26s, Token/s: 81.75
Epoch: 0, Step: 9705, Batch(micro): 9705, Batch (considering grad accum): 1213,  Loss: 5.9689, Time: 3.79s, Token/s: 134.94
Epoch: 0, Step: 9706, Batch(micro): 9706, Batch (considering grad accum): 1213,  Loss: 5.7579, Time: 3.53s, Token/s: 145.22
Epoch: 0, Step: 9707, Batch(micro): 9707, Batch (considering grad accum): 1213,  Loss: 5.4430, Time: 3.70s, Token/s: 138.45
Epoch: 0, Step: 9708, Batch(micro): 9708, Batch (considering grad accum): 1213,  Loss: 5.8287, Time: 3.42s, Token/s: 149.73
Epoch: 0, Step: 9709, Batch(micro): 9709, Batch (considering grad accum): 1213,  Loss: 6.3120, Time: 3.72s, Token/s: 137.79
Epoch: 0, Step: 9710, Batch(micro): 9710, Batch (considering grad accum): 1213,  Loss: 5.6748, Time: 3.56s, Token/s: 143.98
Epoch: 0, Step: 9711, Batch(micro): 9711, Batch (considering grad accum): 1213,  Loss: 5.3571, Time: 24.87s, Token/s: 20.59
Epoch: 0, Step: 9712, Batch(micro): 9712, Batch (considering grad accum): 1214,  Loss: 5.6742, Time: 7.34s, Token/s: 69.79
Epoch: 0, Step: 9713, Batch(micro): 9713, Batch (considering grad accum): 1214,  Loss: 6.0194, Time: 4.02s, Token/s: 127.50
Epoch: 0, Step: 9714, Batch(micro): 9714, Batch (considering grad accum): 1214,  Loss: 5.7877, Time: 3.90s, Token/s: 131.26
Epoch: 0, Step: 9715, Batch(micro): 9715, Batch (considering grad accum): 1214,  Loss: 6.4102, Time: 4.13s, Token/s: 123.95
Epoch: 0, Step: 9716, Batch(micro): 9716, Batch (considering grad accum): 1214,  Loss: 5.7860, Time: 3.44s, Token/s: 148.83
Epoch: 0, Step: 9717, Batch(micro): 9717, Batch (considering grad accum): 1214,  Loss: 5.9612, Time: 3.46s, Token/s: 148.15
Epoch: 0, Step: 9718, Batch(micro): 9718, Batch (considering grad accum): 1214,  Loss: 5.8147, Time: 3.57s, Token/s: 143.42
Epoch: 0, Step: 9719, Batch(micro): 9719, Batch (considering grad accum): 1214,  Loss: 5.6586, Time: 23.85s, Token/s: 21.47
Epoch: 0, Step: 9720, Batch(micro): 9720, Batch (considering grad accum): 1215,  Loss: 6.6657, Time: 8.18s, Token/s: 62.56
Epoch: 0, Step: 9721, Batch(micro): 9721, Batch (considering grad accum): 1215,  Loss: 6.1167, Time: 3.37s, Token/s: 152.15
Epoch: 0, Step: 9722, Batch(micro): 9722, Batch (considering grad accum): 1215,  Loss: 5.9707, Time: 3.44s, Token/s: 148.73
Epoch: 0, Step: 9723, Batch(micro): 9723, Batch (considering grad accum): 1215,  Loss: 6.3768, Time: 3.29s, Token/s: 155.52
Epoch: 0, Step: 9724, Batch(micro): 9724, Batch (considering grad accum): 1215,  Loss: 5.6734, Time: 4.82s, Token/s: 106.28
Epoch: 0, Step: 9725, Batch(micro): 9725, Batch (considering grad accum): 1215,  Loss: 5.8875, Time: 3.71s, Token/s: 138.02
Epoch: 0, Step: 9726, Batch(micro): 9726, Batch (considering grad accum): 1215,  Loss: 7.0109, Time: 3.48s, Token/s: 147.28
Epoch: 0, Step: 9727, Batch(micro): 9727, Batch (considering grad accum): 1215,  Loss: 7.0043, Time: 23.10s, Token/s: 22.16
Epoch: 0, Step: 9728, Batch(micro): 9728, Batch (considering grad accum): 1216,  Loss: 6.0420, Time: 7.00s, Token/s: 73.10
Epoch: 0, Step: 9729, Batch(micro): 9729, Batch (considering grad accum): 1216,  Loss: 5.8132, Time: 4.21s, Token/s: 121.75
Epoch: 0, Step: 9730, Batch(micro): 9730, Batch (considering grad accum): 1216,  Loss: 5.6962, Time: 3.63s, Token/s: 141.18
Epoch: 0, Step: 9731, Batch(micro): 9731, Batch (considering grad accum): 1216,  Loss: 5.8658, Time: 3.52s, Token/s: 145.42
Epoch: 0, Step: 9732, Batch(micro): 9732, Batch (considering grad accum): 1216,  Loss: 5.7192, Time: 3.72s, Token/s: 137.74
Epoch: 0, Step: 9733, Batch(micro): 9733, Batch (considering grad accum): 1216,  Loss: 6.0146, Time: 3.66s, Token/s: 140.06
Epoch: 0, Step: 9734, Batch(micro): 9734, Batch (considering grad accum): 1216,  Loss: 6.6995, Time: 3.66s, Token/s: 139.81
Epoch: 0, Step: 9735, Batch(micro): 9735, Batch (considering grad accum): 1216,  Loss: 5.8270, Time: 25.70s, Token/s: 19.93
Epoch: 0, Step: 9736, Batch(micro): 9736, Batch (considering grad accum): 1217,  Loss: 5.1525, Time: 9.47s, Token/s: 54.05
Epoch: 0, Step: 9737, Batch(micro): 9737, Batch (considering grad accum): 1217,  Loss: 5.2866, Time: 3.73s, Token/s: 137.23
Epoch: 0, Step: 9738, Batch(micro): 9738, Batch (considering grad accum): 1217,  Loss: 6.3497, Time: 3.21s, Token/s: 159.51
Epoch: 0, Step: 9739, Batch(micro): 9739, Batch (considering grad accum): 1217,  Loss: 6.6689, Time: 3.20s, Token/s: 160.17
Epoch: 0, Step: 9740, Batch(micro): 9740, Batch (considering grad accum): 1217,  Loss: 5.7668, Time: 3.23s, Token/s: 158.74
Epoch: 0, Step: 9741, Batch(micro): 9741, Batch (considering grad accum): 1217,  Loss: 6.8042, Time: 3.24s, Token/s: 157.87
Epoch: 0, Step: 9742, Batch(micro): 9742, Batch (considering grad accum): 1217,  Loss: 6.2548, Time: 3.53s, Token/s: 144.94
Epoch: 0, Step: 9743, Batch(micro): 9743, Batch (considering grad accum): 1217,  Loss: 6.1902, Time: 24.05s, Token/s: 21.29
Epoch: 0, Step: 9744, Batch(micro): 9744, Batch (considering grad accum): 1218,  Loss: 5.3901, Time: 6.86s, Token/s: 74.63
Epoch: 0, Step: 9745, Batch(micro): 9745, Batch (considering grad accum): 1218,  Loss: 6.1022, Time: 3.80s, Token/s: 134.66
Epoch: 0, Step: 9746, Batch(micro): 9746, Batch (considering grad accum): 1218,  Loss: 6.3209, Time: 3.27s, Token/s: 156.74
Epoch: 0, Step: 9747, Batch(micro): 9747, Batch (considering grad accum): 1218,  Loss: 5.8281, Time: 3.24s, Token/s: 158.10
Epoch: 0, Step: 9748, Batch(micro): 9748, Batch (considering grad accum): 1218,  Loss: 6.2598, Time: 3.25s, Token/s: 157.61
Epoch: 0, Step: 9749, Batch(micro): 9749, Batch (considering grad accum): 1218,  Loss: 6.2810, Time: 3.29s, Token/s: 155.63
Epoch: 0, Step: 9750, Batch(micro): 9750, Batch (considering grad accum): 1218,  Loss: 6.3808, Time: 3.22s, Token/s: 158.92
Epoch: 0, Step: 9751, Batch(micro): 9751, Batch (considering grad accum): 1218,  Loss: 5.8551, Time: 24.19s, Token/s: 21.16
Epoch: 0, Step: 9752, Batch(micro): 9752, Batch (considering grad accum): 1219,  Loss: 5.6810, Time: 6.35s, Token/s: 80.61
Epoch: 0, Step: 9753, Batch(micro): 9753, Batch (considering grad accum): 1219,  Loss: 6.1313, Time: 4.06s, Token/s: 125.96
Epoch: 0, Step: 9754, Batch(micro): 9754, Batch (considering grad accum): 1219,  Loss: 5.5028, Time: 3.74s, Token/s: 136.97
Epoch: 0, Step: 9755, Batch(micro): 9755, Batch (considering grad accum): 1219,  Loss: 5.9999, Time: 3.66s, Token/s: 139.98
Epoch: 0, Step: 9756, Batch(micro): 9756, Batch (considering grad accum): 1219,  Loss: 6.2831, Time: 3.47s, Token/s: 147.76
Epoch: 0, Step: 9757, Batch(micro): 9757, Batch (considering grad accum): 1219,  Loss: 6.1230, Time: 3.68s, Token/s: 139.29
Epoch: 0, Step: 9758, Batch(micro): 9758, Batch (considering grad accum): 1219,  Loss: 5.9508, Time: 3.67s, Token/s: 139.32
Epoch: 0, Step: 9759, Batch(micro): 9759, Batch (considering grad accum): 1219,  Loss: 5.9610, Time: 24.57s, Token/s: 20.84
Epoch: 0, Step: 9760, Batch(micro): 9760, Batch (considering grad accum): 1220,  Loss: 6.4467, Time: 6.59s, Token/s: 77.72
Epoch: 0, Step: 9761, Batch(micro): 9761, Batch (considering grad accum): 1220,  Loss: 5.1254, Time: 3.78s, Token/s: 135.47
Epoch: 0, Step: 9762, Batch(micro): 9762, Batch (considering grad accum): 1220,  Loss: 6.0297, Time: 3.54s, Token/s: 144.69
Epoch: 0, Step: 9763, Batch(micro): 9763, Batch (considering grad accum): 1220,  Loss: 6.2592, Time: 3.80s, Token/s: 134.87
Epoch: 0, Step: 9764, Batch(micro): 9764, Batch (considering grad accum): 1220,  Loss: 6.5449, Time: 3.61s, Token/s: 141.77
Epoch: 0, Step: 9765, Batch(micro): 9765, Batch (considering grad accum): 1220,  Loss: 5.7446, Time: 3.74s, Token/s: 136.81
Epoch: 0, Step: 9766, Batch(micro): 9766, Batch (considering grad accum): 1220,  Loss: 5.6019, Time: 3.53s, Token/s: 145.08
Epoch: 0, Step: 9767, Batch(micro): 9767, Batch (considering grad accum): 1220,  Loss: 5.5632, Time: 23.96s, Token/s: 21.37
Epoch: 0, Step: 9768, Batch(micro): 9768, Batch (considering grad accum): 1221,  Loss: 6.0025, Time: 7.59s, Token/s: 67.50
Epoch: 0, Step: 9769, Batch(micro): 9769, Batch (considering grad accum): 1221,  Loss: 5.5583, Time: 3.43s, Token/s: 149.21
Epoch: 0, Step: 9770, Batch(micro): 9770, Batch (considering grad accum): 1221,  Loss: 5.6624, Time: 3.48s, Token/s: 146.99
Epoch: 0, Step: 9771, Batch(micro): 9771, Batch (considering grad accum): 1221,  Loss: 6.3391, Time: 3.49s, Token/s: 146.81
Epoch: 0, Step: 9772, Batch(micro): 9772, Batch (considering grad accum): 1221,  Loss: 5.6234, Time: 3.69s, Token/s: 138.79
Epoch: 0, Step: 9773, Batch(micro): 9773, Batch (considering grad accum): 1221,  Loss: 5.7032, Time: 3.71s, Token/s: 138.08
Epoch: 0, Step: 9774, Batch(micro): 9774, Batch (considering grad accum): 1221,  Loss: 5.6635, Time: 3.65s, Token/s: 140.44
Epoch: 0, Step: 9775, Batch(micro): 9775, Batch (considering grad accum): 1221,  Loss: 4.7657, Time: 27.12s, Token/s: 18.88
Epoch: 0, Step: 9776, Batch(micro): 9776, Batch (considering grad accum): 1222,  Loss: 5.3539, Time: 9.96s, Token/s: 51.43
Epoch: 0, Step: 9777, Batch(micro): 9777, Batch (considering grad accum): 1222,  Loss: 5.6691, Time: 4.32s, Token/s: 118.51
Epoch: 0, Step: 9778, Batch(micro): 9778, Batch (considering grad accum): 1222,  Loss: 5.7601, Time: 4.29s, Token/s: 119.29
Epoch: 0, Step: 9779, Batch(micro): 9779, Batch (considering grad accum): 1222,  Loss: 5.3136, Time: 3.72s, Token/s: 137.78
Epoch: 0, Step: 9780, Batch(micro): 9780, Batch (considering grad accum): 1222,  Loss: 5.3162, Time: 3.83s, Token/s: 133.52
Epoch: 0, Step: 9781, Batch(micro): 9781, Batch (considering grad accum): 1222,  Loss: 6.5178, Time: 3.61s, Token/s: 141.86
Epoch: 0, Step: 9782, Batch(micro): 9782, Batch (considering grad accum): 1222,  Loss: 6.5100, Time: 3.70s, Token/s: 138.40
Epoch: 0, Step: 9783, Batch(micro): 9783, Batch (considering grad accum): 1222,  Loss: 5.9819, Time: 29.27s, Token/s: 17.49
Epoch: 0, Step: 9784, Batch(micro): 9784, Batch (considering grad accum): 1223,  Loss: 5.4231, Time: 8.59s, Token/s: 59.63
Epoch: 0, Step: 9785, Batch(micro): 9785, Batch (considering grad accum): 1223,  Loss: 6.2699, Time: 4.05s, Token/s: 126.39
Epoch: 0, Step: 9786, Batch(micro): 9786, Batch (considering grad accum): 1223,  Loss: 5.8266, Time: 3.68s, Token/s: 138.99
Epoch: 0, Step: 9787, Batch(micro): 9787, Batch (considering grad accum): 1223,  Loss: 6.3176, Time: 3.59s, Token/s: 142.44
Epoch: 0, Step: 9788, Batch(micro): 9788, Batch (considering grad accum): 1223,  Loss: 5.3961, Time: 3.68s, Token/s: 139.02
Epoch: 0, Step: 9789, Batch(micro): 9789, Batch (considering grad accum): 1223,  Loss: 6.7509, Time: 3.51s, Token/s: 145.85
Epoch: 0, Step: 9790, Batch(micro): 9790, Batch (considering grad accum): 1223,  Loss: 7.9042, Time: 3.76s, Token/s: 136.28
Epoch: 0, Step: 9791, Batch(micro): 9791, Batch (considering grad accum): 1223,  Loss: 7.0738, Time: 26.64s, Token/s: 19.22
Epoch: 0, Step: 9792, Batch(micro): 9792, Batch (considering grad accum): 1224,  Loss: 5.8462, Time: 7.74s, Token/s: 66.14
Epoch: 0, Step: 9793, Batch(micro): 9793, Batch (considering grad accum): 1224,  Loss: 6.2232, Time: 3.97s, Token/s: 128.92
Epoch: 0, Step: 9794, Batch(micro): 9794, Batch (considering grad accum): 1224,  Loss: 6.3003, Time: 3.64s, Token/s: 140.61
Epoch: 0, Step: 9795, Batch(micro): 9795, Batch (considering grad accum): 1224,  Loss: 5.6728, Time: 3.47s, Token/s: 147.39
Epoch: 0, Step: 9796, Batch(micro): 9796, Batch (considering grad accum): 1224,  Loss: 6.2716, Time: 3.33s, Token/s: 153.85
Epoch: 0, Step: 9797, Batch(micro): 9797, Batch (considering grad accum): 1224,  Loss: 5.8423, Time: 3.85s, Token/s: 132.84
Epoch: 0, Step: 9798, Batch(micro): 9798, Batch (considering grad accum): 1224,  Loss: 5.6729, Time: 3.38s, Token/s: 151.26
Epoch: 0, Step: 9799, Batch(micro): 9799, Batch (considering grad accum): 1224,  Loss: 5.8877, Time: 24.49s, Token/s: 20.90
Updating MLP bias
Epoch: 0, Step: 9800, Batch(micro): 9800, Batch (considering grad accum): 1225,  Loss: 5.9830, Time: 6.84s, Token/s: 74.84
Epoch: 0, Step: 9801, Batch(micro): 9801, Batch (considering grad accum): 1225,  Loss: 5.4753, Time: 3.78s, Token/s: 135.36
Epoch: 0, Step: 9802, Batch(micro): 9802, Batch (considering grad accum): 1225,  Loss: 5.6939, Time: 3.77s, Token/s: 135.92
Epoch: 0, Step: 9803, Batch(micro): 9803, Batch (considering grad accum): 1225,  Loss: 6.5210, Time: 3.75s, Token/s: 136.54
Epoch: 0, Step: 9804, Batch(micro): 9804, Batch (considering grad accum): 1225,  Loss: 6.3629, Time: 3.47s, Token/s: 147.41
Epoch: 0, Step: 9805, Batch(micro): 9805, Batch (considering grad accum): 1225,  Loss: 5.7010, Time: 3.13s, Token/s: 163.77
Epoch: 0, Step: 9806, Batch(micro): 9806, Batch (considering grad accum): 1225,  Loss: 5.7522, Time: 3.18s, Token/s: 160.86
Epoch: 0, Step: 9807, Batch(micro): 9807, Batch (considering grad accum): 1225,  Loss: 5.9895, Time: 22.80s, Token/s: 22.45
Epoch: 0, Step: 9808, Batch(micro): 9808, Batch (considering grad accum): 1226,  Loss: 5.5997, Time: 6.79s, Token/s: 75.39
Epoch: 0, Step: 9809, Batch(micro): 9809, Batch (considering grad accum): 1226,  Loss: 5.5921, Time: 3.76s, Token/s: 136.14
Epoch: 0, Step: 9810, Batch(micro): 9810, Batch (considering grad accum): 1226,  Loss: 5.2121, Time: 3.53s, Token/s: 145.22
Epoch: 0, Step: 9811, Batch(micro): 9811, Batch (considering grad accum): 1226,  Loss: 5.2245, Time: 3.81s, Token/s: 134.44
Epoch: 0, Step: 9812, Batch(micro): 9812, Batch (considering grad accum): 1226,  Loss: 6.1410, Time: 3.79s, Token/s: 135.27
Epoch: 0, Step: 9813, Batch(micro): 9813, Batch (considering grad accum): 1226,  Loss: 5.9676, Time: 3.49s, Token/s: 146.60
Epoch: 0, Step: 9814, Batch(micro): 9814, Batch (considering grad accum): 1226,  Loss: 6.3737, Time: 3.44s, Token/s: 148.78
Epoch: 0, Step: 9815, Batch(micro): 9815, Batch (considering grad accum): 1226,  Loss: 5.6880, Time: 23.84s, Token/s: 21.48
Epoch: 0, Step: 9816, Batch(micro): 9816, Batch (considering grad accum): 1227,  Loss: 5.2832, Time: 9.42s, Token/s: 54.37
Epoch: 0, Step: 9817, Batch(micro): 9817, Batch (considering grad accum): 1227,  Loss: 5.6972, Time: 4.39s, Token/s: 116.66
Epoch: 0, Step: 9818, Batch(micro): 9818, Batch (considering grad accum): 1227,  Loss: 7.0286, Time: 3.46s, Token/s: 148.12
Epoch: 0, Step: 9819, Batch(micro): 9819, Batch (considering grad accum): 1227,  Loss: 6.1762, Time: 3.61s, Token/s: 141.95
Epoch: 0, Step: 9820, Batch(micro): 9820, Batch (considering grad accum): 1227,  Loss: 5.9620, Time: 3.49s, Token/s: 146.51
Epoch: 0, Step: 9821, Batch(micro): 9821, Batch (considering grad accum): 1227,  Loss: 5.3525, Time: 3.44s, Token/s: 148.76
Epoch: 0, Step: 9822, Batch(micro): 9822, Batch (considering grad accum): 1227,  Loss: 5.5359, Time: 3.56s, Token/s: 143.88
Epoch: 0, Step: 9823, Batch(micro): 9823, Batch (considering grad accum): 1227,  Loss: 4.6174, Time: 25.77s, Token/s: 19.87
Epoch: 0, Step: 9824, Batch(micro): 9824, Batch (considering grad accum): 1228,  Loss: 5.0213, Time: 9.73s, Token/s: 52.60
Epoch: 0, Step: 9825, Batch(micro): 9825, Batch (considering grad accum): 1228,  Loss: 5.9613, Time: 4.03s, Token/s: 127.13
Epoch: 0, Step: 9826, Batch(micro): 9826, Batch (considering grad accum): 1228,  Loss: 5.4672, Time: 3.35s, Token/s: 152.77
Epoch: 0, Step: 9827, Batch(micro): 9827, Batch (considering grad accum): 1228,  Loss: 5.7760, Time: 3.51s, Token/s: 145.91
Epoch: 0, Step: 9828, Batch(micro): 9828, Batch (considering grad accum): 1228,  Loss: 6.2238, Time: 3.26s, Token/s: 156.88
Epoch: 0, Step: 9829, Batch(micro): 9829, Batch (considering grad accum): 1228,  Loss: 6.0439, Time: 3.34s, Token/s: 153.18
Epoch: 0, Step: 9830, Batch(micro): 9830, Batch (considering grad accum): 1228,  Loss: 6.1920, Time: 3.48s, Token/s: 147.01
Epoch: 0, Step: 9831, Batch(micro): 9831, Batch (considering grad accum): 1228,  Loss: 5.3076, Time: 23.99s, Token/s: 21.34
Epoch: 0, Step: 9832, Batch(micro): 9832, Batch (considering grad accum): 1229,  Loss: 5.6035, Time: 7.38s, Token/s: 69.34
Epoch: 0, Step: 9833, Batch(micro): 9833, Batch (considering grad accum): 1229,  Loss: 5.9494, Time: 3.89s, Token/s: 131.55
Epoch: 0, Step: 9834, Batch(micro): 9834, Batch (considering grad accum): 1229,  Loss: 6.2145, Time: 3.60s, Token/s: 142.13
Epoch: 0, Step: 9835, Batch(micro): 9835, Batch (considering grad accum): 1229,  Loss: 5.8952, Time: 3.58s, Token/s: 143.10
Epoch: 0, Step: 9836, Batch(micro): 9836, Batch (considering grad accum): 1229,  Loss: 6.2306, Time: 3.33s, Token/s: 153.74
Epoch: 0, Step: 9837, Batch(micro): 9837, Batch (considering grad accum): 1229,  Loss: 6.5167, Time: 3.42s, Token/s: 149.78
Epoch: 0, Step: 9838, Batch(micro): 9838, Batch (considering grad accum): 1229,  Loss: 5.6534, Time: 3.34s, Token/s: 153.11
Epoch: 0, Step: 9839, Batch(micro): 9839, Batch (considering grad accum): 1229,  Loss: 5.9139, Time: 23.16s, Token/s: 22.10
Epoch: 0, Step: 9840, Batch(micro): 9840, Batch (considering grad accum): 1230,  Loss: 5.6622, Time: 6.62s, Token/s: 77.30
Epoch: 0, Step: 9841, Batch(micro): 9841, Batch (considering grad accum): 1230,  Loss: 6.1921, Time: 3.96s, Token/s: 129.29
Epoch: 0, Step: 9842, Batch(micro): 9842, Batch (considering grad accum): 1230,  Loss: 5.3958, Time: 3.54s, Token/s: 144.78
Epoch: 0, Step: 9843, Batch(micro): 9843, Batch (considering grad accum): 1230,  Loss: 5.7689, Time: 3.18s, Token/s: 160.89
Epoch: 0, Step: 9844, Batch(micro): 9844, Batch (considering grad accum): 1230,  Loss: 7.0175, Time: 3.32s, Token/s: 154.40
Epoch: 0, Step: 9845, Batch(micro): 9845, Batch (considering grad accum): 1230,  Loss: 5.5806, Time: 3.28s, Token/s: 156.24
Epoch: 0, Step: 9846, Batch(micro): 9846, Batch (considering grad accum): 1230,  Loss: 5.9816, Time: 3.31s, Token/s: 154.57
Epoch: 0, Step: 9847, Batch(micro): 9847, Batch (considering grad accum): 1230,  Loss: 5.6752, Time: 23.28s, Token/s: 21.99
Epoch: 0, Step: 9848, Batch(micro): 9848, Batch (considering grad accum): 1231,  Loss: 5.9570, Time: 7.30s, Token/s: 70.12
Epoch: 0, Step: 9849, Batch(micro): 9849, Batch (considering grad accum): 1231,  Loss: 5.7282, Time: 3.83s, Token/s: 133.62
Epoch: 0, Step: 9850, Batch(micro): 9850, Batch (considering grad accum): 1231,  Loss: 5.4607, Time: 3.15s, Token/s: 162.56
Epoch: 0, Step: 9851, Batch(micro): 9851, Batch (considering grad accum): 1231,  Loss: 6.5791, Time: 3.50s, Token/s: 146.42
Epoch: 0, Step: 9852, Batch(micro): 9852, Batch (considering grad accum): 1231,  Loss: 5.8533, Time: 3.15s, Token/s: 162.60
Epoch: 0, Step: 9853, Batch(micro): 9853, Batch (considering grad accum): 1231,  Loss: 5.8533, Time: 3.16s, Token/s: 162.21
Epoch: 0, Step: 9854, Batch(micro): 9854, Batch (considering grad accum): 1231,  Loss: 5.9526, Time: 3.16s, Token/s: 161.95
Epoch: 0, Step: 9855, Batch(micro): 9855, Batch (considering grad accum): 1231,  Loss: 5.8854, Time: 26.46s, Token/s: 19.35
Epoch: 0, Step: 9856, Batch(micro): 9856, Batch (considering grad accum): 1232,  Loss: 5.6580, Time: 7.73s, Token/s: 66.22
Epoch: 0, Step: 9857, Batch(micro): 9857, Batch (considering grad accum): 1232,  Loss: 5.8719, Time: 3.95s, Token/s: 129.48
Epoch: 0, Step: 9858, Batch(micro): 9858, Batch (considering grad accum): 1232,  Loss: 5.5341, Time: 3.87s, Token/s: 132.41
Epoch: 0, Step: 9859, Batch(micro): 9859, Batch (considering grad accum): 1232,  Loss: 6.1546, Time: 3.79s, Token/s: 135.20
Epoch: 0, Step: 9860, Batch(micro): 9860, Batch (considering grad accum): 1232,  Loss: 6.0699, Time: 3.68s, Token/s: 139.19
Epoch: 0, Step: 9861, Batch(micro): 9861, Batch (considering grad accum): 1232,  Loss: 5.9032, Time: 3.58s, Token/s: 143.13
Epoch: 0, Step: 9862, Batch(micro): 9862, Batch (considering grad accum): 1232,  Loss: 6.1610, Time: 3.47s, Token/s: 147.48
Epoch: 0, Step: 9863, Batch(micro): 9863, Batch (considering grad accum): 1232,  Loss: 6.1804, Time: 25.19s, Token/s: 20.33
Epoch: 0, Step: 9864, Batch(micro): 9864, Batch (considering grad accum): 1233,  Loss: 6.0958, Time: 7.15s, Token/s: 71.63
Epoch: 0, Step: 9865, Batch(micro): 9865, Batch (considering grad accum): 1233,  Loss: 5.7859, Time: 3.97s, Token/s: 129.11
Epoch: 0, Step: 9866, Batch(micro): 9866, Batch (considering grad accum): 1233,  Loss: 6.7307, Time: 3.51s, Token/s: 145.83
Epoch: 0, Step: 9867, Batch(micro): 9867, Batch (considering grad accum): 1233,  Loss: 5.5477, Time: 3.32s, Token/s: 154.05
Epoch: 0, Step: 9868, Batch(micro): 9868, Batch (considering grad accum): 1233,  Loss: 6.0180, Time: 3.69s, Token/s: 138.83
Epoch: 0, Step: 9869, Batch(micro): 9869, Batch (considering grad accum): 1233,  Loss: 6.0059, Time: 3.41s, Token/s: 150.15
Epoch: 0, Step: 9870, Batch(micro): 9870, Batch (considering grad accum): 1233,  Loss: 6.5645, Time: 3.73s, Token/s: 137.31
Epoch: 0, Step: 9871, Batch(micro): 9871, Batch (considering grad accum): 1233,  Loss: 5.5678, Time: 24.64s, Token/s: 20.78
Epoch: 0, Step: 9872, Batch(micro): 9872, Batch (considering grad accum): 1234,  Loss: 5.4462, Time: 7.35s, Token/s: 69.70
Epoch: 0, Step: 9873, Batch(micro): 9873, Batch (considering grad accum): 1234,  Loss: 6.5996, Time: 3.89s, Token/s: 131.54
Epoch: 0, Step: 9874, Batch(micro): 9874, Batch (considering grad accum): 1234,  Loss: 5.6247, Time: 4.00s, Token/s: 128.09
Epoch: 0, Step: 9875, Batch(micro): 9875, Batch (considering grad accum): 1234,  Loss: 5.8318, Time: 3.57s, Token/s: 143.41
Epoch: 0, Step: 9876, Batch(micro): 9876, Batch (considering grad accum): 1234,  Loss: 6.8253, Time: 3.38s, Token/s: 151.61
Epoch: 0, Step: 9877, Batch(micro): 9877, Batch (considering grad accum): 1234,  Loss: 6.1299, Time: 3.41s, Token/s: 150.19
Epoch: 0, Step: 9878, Batch(micro): 9878, Batch (considering grad accum): 1234,  Loss: 6.0433, Time: 3.76s, Token/s: 136.10
Epoch: 0, Step: 9879, Batch(micro): 9879, Batch (considering grad accum): 1234,  Loss: 5.7186, Time: 23.14s, Token/s: 22.12
Epoch: 0, Step: 9880, Batch(micro): 9880, Batch (considering grad accum): 1235,  Loss: 5.7092, Time: 7.46s, Token/s: 68.63
Epoch: 0, Step: 9881, Batch(micro): 9881, Batch (considering grad accum): 1235,  Loss: 6.0346, Time: 3.73s, Token/s: 137.32
Epoch: 0, Step: 9882, Batch(micro): 9882, Batch (considering grad accum): 1235,  Loss: 5.9193, Time: 3.43s, Token/s: 149.21
Epoch: 0, Step: 9883, Batch(micro): 9883, Batch (considering grad accum): 1235,  Loss: 5.9082, Time: 3.68s, Token/s: 139.31
Epoch: 0, Step: 9884, Batch(micro): 9884, Batch (considering grad accum): 1235,  Loss: 6.5157, Time: 3.59s, Token/s: 142.53
Epoch: 0, Step: 9885, Batch(micro): 9885, Batch (considering grad accum): 1235,  Loss: 6.6922, Time: 3.37s, Token/s: 152.06
Epoch: 0, Step: 9886, Batch(micro): 9886, Batch (considering grad accum): 1235,  Loss: 6.0920, Time: 3.51s, Token/s: 145.69
Epoch: 0, Step: 9887, Batch(micro): 9887, Batch (considering grad accum): 1235,  Loss: 6.1575, Time: 21.34s, Token/s: 23.99
Epoch: 0, Step: 9888, Batch(micro): 9888, Batch (considering grad accum): 1236,  Loss: 6.2470, Time: 6.85s, Token/s: 74.72
Epoch: 0, Step: 9889, Batch(micro): 9889, Batch (considering grad accum): 1236,  Loss: 6.2020, Time: 3.89s, Token/s: 131.57
Epoch: 0, Step: 9890, Batch(micro): 9890, Batch (considering grad accum): 1236,  Loss: 6.0180, Time: 3.37s, Token/s: 152.01
Epoch: 0, Step: 9891, Batch(micro): 9891, Batch (considering grad accum): 1236,  Loss: 5.8458, Time: 3.71s, Token/s: 137.87
Epoch: 0, Step: 9892, Batch(micro): 9892, Batch (considering grad accum): 1236,  Loss: 6.4366, Time: 3.72s, Token/s: 137.63
Epoch: 0, Step: 9893, Batch(micro): 9893, Batch (considering grad accum): 1236,  Loss: 5.9149, Time: 3.68s, Token/s: 139.04
Epoch: 0, Step: 9894, Batch(micro): 9894, Batch (considering grad accum): 1236,  Loss: 5.7805, Time: 3.55s, Token/s: 144.25
Epoch: 0, Step: 9895, Batch(micro): 9895, Batch (considering grad accum): 1236,  Loss: 6.9650, Time: 20.44s, Token/s: 25.04
Epoch: 0, Step: 9896, Batch(micro): 9896, Batch (considering grad accum): 1237,  Loss: 6.3771, Time: 5.94s, Token/s: 86.18
Epoch: 0, Step: 9897, Batch(micro): 9897, Batch (considering grad accum): 1237,  Loss: 5.8857, Time: 3.57s, Token/s: 143.62
Epoch: 0, Step: 9898, Batch(micro): 9898, Batch (considering grad accum): 1237,  Loss: 5.7643, Time: 3.53s, Token/s: 144.89
Epoch: 0, Step: 9899, Batch(micro): 9899, Batch (considering grad accum): 1237,  Loss: 6.1484, Time: 3.45s, Token/s: 148.33
Updating MLP bias
Epoch: 0, Step: 9900, Batch(micro): 9900, Batch (considering grad accum): 1237,  Loss: 6.2764, Time: 3.03s, Token/s: 169.13
Epoch: 0, Step: 9901, Batch(micro): 9901, Batch (considering grad accum): 1237,  Loss: 5.6427, Time: 2.97s, Token/s: 172.23
Epoch: 0, Step: 9902, Batch(micro): 9902, Batch (considering grad accum): 1237,  Loss: 5.6080, Time: 3.09s, Token/s: 165.71
Epoch: 0, Step: 9903, Batch(micro): 9903, Batch (considering grad accum): 1237,  Loss: 5.6252, Time: 20.21s, Token/s: 25.33
Epoch: 0, Step: 9904, Batch(micro): 9904, Batch (considering grad accum): 1238,  Loss: 5.4741, Time: 6.77s, Token/s: 75.67
Epoch: 0, Step: 9905, Batch(micro): 9905, Batch (considering grad accum): 1238,  Loss: 5.5452, Time: 3.68s, Token/s: 139.14
Epoch: 0, Step: 9906, Batch(micro): 9906, Batch (considering grad accum): 1238,  Loss: 5.6948, Time: 3.38s, Token/s: 151.38
Epoch: 0, Step: 9907, Batch(micro): 9907, Batch (considering grad accum): 1238,  Loss: 5.7814, Time: 2.98s, Token/s: 171.58
Epoch: 0, Step: 9908, Batch(micro): 9908, Batch (considering grad accum): 1238,  Loss: 6.4773, Time: 2.95s, Token/s: 173.54
Epoch: 0, Step: 9909, Batch(micro): 9909, Batch (considering grad accum): 1238,  Loss: 5.3373, Time: 3.02s, Token/s: 169.41
Epoch: 0, Step: 9910, Batch(micro): 9910, Batch (considering grad accum): 1238,  Loss: 5.6459, Time: 3.10s, Token/s: 165.23
Epoch: 0, Step: 9911, Batch(micro): 9911, Batch (considering grad accum): 1238,  Loss: 5.1791, Time: 18.60s, Token/s: 27.53
Epoch: 0, Step: 9912, Batch(micro): 9912, Batch (considering grad accum): 1239,  Loss: 5.5627, Time: 7.08s, Token/s: 72.33
Epoch: 0, Step: 9913, Batch(micro): 9913, Batch (considering grad accum): 1239,  Loss: 6.5196, Time: 3.48s, Token/s: 147.19
Epoch: 0, Step: 9914, Batch(micro): 9914, Batch (considering grad accum): 1239,  Loss: 5.8215, Time: 3.21s, Token/s: 159.66
Epoch: 0, Step: 9915, Batch(micro): 9915, Batch (considering grad accum): 1239,  Loss: 5.0613, Time: 3.36s, Token/s: 152.29
Epoch: 0, Step: 9916, Batch(micro): 9916, Batch (considering grad accum): 1239,  Loss: 5.8540, Time: 3.06s, Token/s: 167.18
Epoch: 0, Step: 9917, Batch(micro): 9917, Batch (considering grad accum): 1239,  Loss: 6.3519, Time: 3.25s, Token/s: 157.68
Epoch: 0, Step: 9918, Batch(micro): 9918, Batch (considering grad accum): 1239,  Loss: 6.2854, Time: 3.48s, Token/s: 146.98
Epoch: 0, Step: 9919, Batch(micro): 9919, Batch (considering grad accum): 1239,  Loss: 5.8529, Time: 18.36s, Token/s: 27.89
Epoch: 0, Step: 9920, Batch(micro): 9920, Batch (considering grad accum): 1240,  Loss: 5.8290, Time: 6.66s, Token/s: 76.85
Epoch: 0, Step: 9921, Batch(micro): 9921, Batch (considering grad accum): 1240,  Loss: 6.1303, Time: 3.77s, Token/s: 135.76
Epoch: 0, Step: 9922, Batch(micro): 9922, Batch (considering grad accum): 1240,  Loss: 6.1547, Time: 3.30s, Token/s: 155.28
Epoch: 0, Step: 9923, Batch(micro): 9923, Batch (considering grad accum): 1240,  Loss: 5.9157, Time: 3.42s, Token/s: 149.92
Epoch: 0, Step: 9924, Batch(micro): 9924, Batch (considering grad accum): 1240,  Loss: 6.7983, Time: 3.18s, Token/s: 161.11
Epoch: 0, Step: 9925, Batch(micro): 9925, Batch (considering grad accum): 1240,  Loss: 5.9790, Time: 3.30s, Token/s: 155.14
Epoch: 0, Step: 9926, Batch(micro): 9926, Batch (considering grad accum): 1240,  Loss: 6.5026, Time: 3.69s, Token/s: 138.91
Epoch: 0, Step: 9927, Batch(micro): 9927, Batch (considering grad accum): 1240,  Loss: 5.4918, Time: 18.97s, Token/s: 26.99
Epoch: 0, Step: 9928, Batch(micro): 9928, Batch (considering grad accum): 1241,  Loss: 5.8675, Time: 6.10s, Token/s: 83.94
Epoch: 0, Step: 9929, Batch(micro): 9929, Batch (considering grad accum): 1241,  Loss: 6.8177, Time: 3.45s, Token/s: 148.21
Epoch: 0, Step: 9930, Batch(micro): 9930, Batch (considering grad accum): 1241,  Loss: 5.6418, Time: 3.14s, Token/s: 163.11
Epoch: 0, Step: 9931, Batch(micro): 9931, Batch (considering grad accum): 1241,  Loss: 5.5629, Time: 3.23s, Token/s: 158.32
Epoch: 0, Step: 9932, Batch(micro): 9932, Batch (considering grad accum): 1241,  Loss: 5.8966, Time: 3.33s, Token/s: 153.81
Epoch: 0, Step: 9933, Batch(micro): 9933, Batch (considering grad accum): 1241,  Loss: 5.4585, Time: 3.22s, Token/s: 158.77
Epoch: 0, Step: 9934, Batch(micro): 9934, Batch (considering grad accum): 1241,  Loss: 6.1322, Time: 3.38s, Token/s: 151.54
Epoch: 0, Step: 9935, Batch(micro): 9935, Batch (considering grad accum): 1241,  Loss: 5.6854, Time: 21.87s, Token/s: 23.41
Epoch: 0, Step: 9936, Batch(micro): 9936, Batch (considering grad accum): 1242,  Loss: 6.0553, Time: 7.72s, Token/s: 66.33
Epoch: 0, Step: 9937, Batch(micro): 9937, Batch (considering grad accum): 1242,  Loss: 5.8257, Time: 3.75s, Token/s: 136.39
Epoch: 0, Step: 9938, Batch(micro): 9938, Batch (considering grad accum): 1242,  Loss: 5.7810, Time: 3.33s, Token/s: 153.82
Epoch: 0, Step: 9939, Batch(micro): 9939, Batch (considering grad accum): 1242,  Loss: 6.6044, Time: 3.35s, Token/s: 152.98
Epoch: 0, Step: 9940, Batch(micro): 9940, Batch (considering grad accum): 1242,  Loss: 6.5151, Time: 3.15s, Token/s: 162.73
Epoch: 0, Step: 9941, Batch(micro): 9941, Batch (considering grad accum): 1242,  Loss: 7.2899, Time: 2.94s, Token/s: 174.22
Epoch: 0, Step: 9942, Batch(micro): 9942, Batch (considering grad accum): 1242,  Loss: 7.6783, Time: 2.95s, Token/s: 173.65
Epoch: 0, Step: 9943, Batch(micro): 9943, Batch (considering grad accum): 1242,  Loss: 6.7775, Time: 17.84s, Token/s: 28.71
Epoch: 0, Step: 9944, Batch(micro): 9944, Batch (considering grad accum): 1243,  Loss: 6.3340, Time: 5.74s, Token/s: 89.15
Epoch: 0, Step: 9945, Batch(micro): 9945, Batch (considering grad accum): 1243,  Loss: 5.8207, Time: 3.63s, Token/s: 141.24
Epoch: 0, Step: 9946, Batch(micro): 9946, Batch (considering grad accum): 1243,  Loss: 6.5483, Time: 3.41s, Token/s: 149.95
Epoch: 0, Step: 9947, Batch(micro): 9947, Batch (considering grad accum): 1243,  Loss: 5.5035, Time: 3.45s, Token/s: 148.57
Epoch: 0, Step: 9948, Batch(micro): 9948, Batch (considering grad accum): 1243,  Loss: 5.8221, Time: 3.20s, Token/s: 160.19
Epoch: 0, Step: 9949, Batch(micro): 9949, Batch (considering grad accum): 1243,  Loss: 5.5689, Time: 3.41s, Token/s: 150.09
Epoch: 0, Step: 9950, Batch(micro): 9950, Batch (considering grad accum): 1243,  Loss: 5.7894, Time: 3.22s, Token/s: 159.08
Epoch: 0, Step: 9951, Batch(micro): 9951, Batch (considering grad accum): 1243,  Loss: 5.8203, Time: 22.22s, Token/s: 23.05
Epoch: 0, Step: 9952, Batch(micro): 9952, Batch (considering grad accum): 1244,  Loss: 5.9486, Time: 10.02s, Token/s: 51.08
Epoch: 0, Step: 9953, Batch(micro): 9953, Batch (considering grad accum): 1244,  Loss: 6.2918, Time: 3.22s, Token/s: 158.91
Epoch: 0, Step: 9954, Batch(micro): 9954, Batch (considering grad accum): 1244,  Loss: 5.6856, Time: 2.99s, Token/s: 171.33
Epoch: 0, Step: 9955, Batch(micro): 9955, Batch (considering grad accum): 1244,  Loss: 5.9642, Time: 2.88s, Token/s: 177.69
Epoch: 0, Step: 9956, Batch(micro): 9956, Batch (considering grad accum): 1244,  Loss: 5.8138, Time: 3.21s, Token/s: 159.56
Epoch: 0, Step: 9957, Batch(micro): 9957, Batch (considering grad accum): 1244,  Loss: 6.1212, Time: 3.22s, Token/s: 158.95
Epoch: 0, Step: 9958, Batch(micro): 9958, Batch (considering grad accum): 1244,  Loss: 5.8068, Time: 3.12s, Token/s: 164.31
Epoch: 0, Step: 9959, Batch(micro): 9959, Batch (considering grad accum): 1244,  Loss: 5.5556, Time: 26.86s, Token/s: 19.06
Epoch: 0, Step: 9960, Batch(micro): 9960, Batch (considering grad accum): 1245,  Loss: 5.8424, Time: 10.26s, Token/s: 49.91
Epoch: 0, Step: 9961, Batch(micro): 9961, Batch (considering grad accum): 1245,  Loss: 6.0490, Time: 3.69s, Token/s: 138.64
Epoch: 0, Step: 9962, Batch(micro): 9962, Batch (considering grad accum): 1245,  Loss: 5.3921, Time: 3.43s, Token/s: 149.34
Epoch: 0, Step: 9963, Batch(micro): 9963, Batch (considering grad accum): 1245,  Loss: 5.8835, Time: 3.52s, Token/s: 145.31
Epoch: 0, Step: 9964, Batch(micro): 9964, Batch (considering grad accum): 1245,  Loss: 6.5004, Time: 3.48s, Token/s: 146.93
Epoch: 0, Step: 9965, Batch(micro): 9965, Batch (considering grad accum): 1245,  Loss: 6.0512, Time: 3.34s, Token/s: 153.23
Epoch: 0, Step: 9966, Batch(micro): 9966, Batch (considering grad accum): 1245,  Loss: 5.8078, Time: 3.28s, Token/s: 156.06
Epoch: 0, Step: 9967, Batch(micro): 9967, Batch (considering grad accum): 1245,  Loss: 6.2937, Time: 24.46s, Token/s: 20.93
Epoch: 0, Step: 9968, Batch(micro): 9968, Batch (considering grad accum): 1246,  Loss: 5.5693, Time: 6.18s, Token/s: 82.81
Epoch: 0, Step: 9969, Batch(micro): 9969, Batch (considering grad accum): 1246,  Loss: 6.2541, Time: 3.50s, Token/s: 146.45
Epoch: 0, Step: 9970, Batch(micro): 9970, Batch (considering grad accum): 1246,  Loss: 5.9942, Time: 3.33s, Token/s: 153.63
Epoch: 0, Step: 9971, Batch(micro): 9971, Batch (considering grad accum): 1246,  Loss: 5.9760, Time: 4.79s, Token/s: 106.81
Epoch: 0, Step: 9972, Batch(micro): 9972, Batch (considering grad accum): 1246,  Loss: 5.6793, Time: 3.14s, Token/s: 163.14
Epoch: 0, Step: 9973, Batch(micro): 9973, Batch (considering grad accum): 1246,  Loss: 5.5543, Time: 3.11s, Token/s: 164.79
Epoch: 0, Step: 9974, Batch(micro): 9974, Batch (considering grad accum): 1246,  Loss: 5.1675, Time: 2.89s, Token/s: 177.40
Epoch: 0, Step: 9975, Batch(micro): 9975, Batch (considering grad accum): 1246,  Loss: 6.3464, Time: 26.67s, Token/s: 19.20
Epoch: 0, Step: 9976, Batch(micro): 9976, Batch (considering grad accum): 1247,  Loss: 5.0121, Time: 8.22s, Token/s: 62.32
Epoch: 0, Step: 9977, Batch(micro): 9977, Batch (considering grad accum): 1247,  Loss: 5.6031, Time: 3.81s, Token/s: 134.52
Epoch: 0, Step: 9978, Batch(micro): 9978, Batch (considering grad accum): 1247,  Loss: 5.2036, Time: 3.30s, Token/s: 155.30
Epoch: 0, Step: 9979, Batch(micro): 9979, Batch (considering grad accum): 1247,  Loss: 5.8488, Time: 3.41s, Token/s: 150.29
Epoch: 0, Step: 9980, Batch(micro): 9980, Batch (considering grad accum): 1247,  Loss: 5.5426, Time: 3.35s, Token/s: 152.67
Epoch: 0, Step: 9981, Batch(micro): 9981, Batch (considering grad accum): 1247,  Loss: 5.9472, Time: 2.96s, Token/s: 172.72
Epoch: 0, Step: 9982, Batch(micro): 9982, Batch (considering grad accum): 1247,  Loss: 6.1986, Time: 2.96s, Token/s: 172.88
Epoch: 0, Step: 9983, Batch(micro): 9983, Batch (considering grad accum): 1247,  Loss: 6.9993, Time: 24.32s, Token/s: 21.05
Epoch: 0, Step: 9984, Batch(micro): 9984, Batch (considering grad accum): 1248,  Loss: 6.0005, Time: 9.14s, Token/s: 56.00
Epoch: 0, Step: 9985, Batch(micro): 9985, Batch (considering grad accum): 1248,  Loss: 5.8038, Time: 2.97s, Token/s: 172.52
Epoch: 0, Step: 9986, Batch(micro): 9986, Batch (considering grad accum): 1248,  Loss: 6.0800, Time: 2.95s, Token/s: 173.36
Epoch: 0, Step: 9987, Batch(micro): 9987, Batch (considering grad accum): 1248,  Loss: 5.5349, Time: 2.96s, Token/s: 172.72
Epoch: 0, Step: 9988, Batch(micro): 9988, Batch (considering grad accum): 1248,  Loss: 5.5305, Time: 3.20s, Token/s: 160.09
Epoch: 0, Step: 9989, Batch(micro): 9989, Batch (considering grad accum): 1248,  Loss: 6.0319, Time: 3.17s, Token/s: 161.46
Epoch: 0, Step: 9990, Batch(micro): 9990, Batch (considering grad accum): 1248,  Loss: 5.6256, Time: 3.20s, Token/s: 159.86
Epoch: 0, Step: 9991, Batch(micro): 9991, Batch (considering grad accum): 1248,  Loss: 6.0101, Time: 23.11s, Token/s: 22.16
Epoch: 0, Step: 9992, Batch(micro): 9992, Batch (considering grad accum): 1249,  Loss: 6.0728, Time: 8.10s, Token/s: 63.19
Epoch: 0, Step: 9993, Batch(micro): 9993, Batch (considering grad accum): 1249,  Loss: 6.0325, Time: 4.13s, Token/s: 124.00
Epoch: 0, Step: 9994, Batch(micro): 9994, Batch (considering grad accum): 1249,  Loss: 6.5742, Time: 3.99s, Token/s: 128.38
Epoch: 0, Step: 9995, Batch(micro): 9995, Batch (considering grad accum): 1249,  Loss: 6.3211, Time: 3.12s, Token/s: 163.85
Epoch: 0, Step: 9996, Batch(micro): 9996, Batch (considering grad accum): 1249,  Loss: 5.6372, Time: 3.30s, Token/s: 155.23
Epoch: 0, Step: 9997, Batch(micro): 9997, Batch (considering grad accum): 1249,  Loss: 5.7401, Time: 3.38s, Token/s: 151.69
Epoch: 0, Step: 9998, Batch(micro): 9998, Batch (considering grad accum): 1249,  Loss: 5.5218, Time: 3.51s, Token/s: 146.05
Epoch: 0, Step: 9999, Batch(micro): 9999, Batch (considering grad accum): 1249,  Loss: 5.7883, Time: 22.42s, Token/s: 22.83
Updating MLP bias
Epoch: 0, Step: 10000, Batch(micro): 10000, Batch (considering grad accum): 1250,  Loss: 6.9143, Time: 28.94s, Token/s: 17.69
Saved checkpoint at step 10000
What is Gravity? Or how they could be better understand what makes each other cultures and explore key concepts and how the game to the way, I think about the following this
Saved final checkpoint
What is Gravity? This is a time, or other animals. She spoke up, which allows us that your own unique style. This chapter, we call these ideas without
Saved the trained model
Training complete!